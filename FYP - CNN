{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FYP - CNN","provenance":[{"file_id":"1Y2dLhz2azFZnr_EuRXDSLC3Xxcb7wALQ","timestamp":1581426194048}],"collapsed_sections":[],"authorship_tag":"ABX9TyMtBVuhaAe2HbRtHz3azE5E"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"zWvybEnjh0sf","colab_type":"code","outputId":"15cb7d7d-8529-457e-da67-65c84c50f128","executionInfo":{"status":"ok","timestamp":1581599586300,"user_tz":-480,"elapsed":24403,"user":{"displayName":"Ng KY","photoUrl":"","userId":"03424929306239991969"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q8IbBDQwoGu8","colab_type":"code","colab":{}},"source":["pip install json-lines"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8ekuuz7nMNA","colab_type":"code","colab":{}},"source":["pip install sentence-transformers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H1H_Y6_-dhNr","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mnxfWXxjL4LR","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Config\n","\"\"\"\n","Configuration file for the project.\n","\"\"\"\n","\n","\"\"\"\n","Base directory.\n","\"\"\"\n","PWD = '/content/drive/My Drive/KY, FYP/Code/'\n","\n","\"\"\"\n","File directories.\n","\"\"\"\n","# Directory for the word embeddings\n","GLOVE_DIR = PWD + '/glove.6B'\n","\n","# Directory for storing citation function data\n","DATA_DIR = PWD + '/data/data'\n","\n","\"\"\"\n","Data files: the citation and provenance dataset.\n","MTL refers to the aligned dataset.\n","\"\"\"\n","DATA_FILES = {\n","    'func': {\n","        'golden_train': 'processed/golden_train.func.json',\n","        'golden_test': 'processed/golden_test.func.json',\n","    },\n","    'scicite': {\n","        'train': 'scicite/train.jsonl',\n","        'test': 'scicite/test.jsonl',\n","        'dev': 'scicite/dev.jsonl'\n","    },\n","    'acl-arc': {\n","        'train': 'acl-arc/train.jsonl',\n","        'test': 'acl-arc/test.jsonl',\n","        'dev': 'acl-arc/dev.jsonl'\n","    },\n","    'prov': {\n","        'golden_train': 'processed/golden_train.prov.json',\n","        'golden_test': 'processed/golden_test.prov.json',\n","    },\n","    'mtl': {\n","        'golden_train': 'processed/golden_train.mtl.json',\n","        'golden_test': 'processed/golden_test.mtl.json'\n","    }\n","}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yy6scFMoNxK","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Data\n","\"\"\"\n","Common data operations.\n","\"\"\"\n","import json\n","import os\n","import json_lines\n","\n","import numpy as np\n","\n","def read_json_data(filename):\n","    \"\"\"\n","    Read the given JSON file.\n","    \"\"\"\n","    path = os.path.join(DATA_DIR, filename)\n","    with open(path, 'rb') as fp:\n","        content = json.load(fp)\n","        return content\n","\n","def read_jsonl_data(filename):\n","    \"\"\"\n","    Read the given JSONL file.\n","    \"\"\"\n","    path = os.path.join(DATA_DIR, filename)\n","    content = []\n","    print (type(content))\n","    with open(path, 'rb') as fp:\n","        for item in json_lines.reader(fp):\n","            content.append(item)\n","        return content\n","\n","\"\"\"\n","Custom cross validation.\n","\"\"\"\n","\n","\n","def compress_y(ys):\n","    \"\"\"\n","    For each y in ys, if y is of the form [0 0 ... 1 ... 0], compress it to a\n","    single integer.\n","    \"\"\"\n","    if len(ys) < 1:\n","        return ys\n","\n","    if isinstance(ys[0], np.ndarray):\n","        # A hack >.<\n","        return map(lambda x: x.tolist().index(1), ys)\n","    else:\n","        return ys\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9uQhU4hEdvmi","colab_type":"code","colab":{}},"source":["# from google.colab import files\n","# files.download('SpacePrediction_title_SBERT.h5') "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SRrQ-rX73tI","colab_type":"code","outputId":"92d26671-df04-4117-895e-75d6c720a874","executionInfo":{"status":"ok","timestamp":1581321742641,"user_tz":-480,"elapsed":1504,"user":{"displayName":"Ng KY","photoUrl":"","userId":"03424929306239991969"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["os.listdir()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.config', 'drive', 'model-acl.h5', 'model-scicite.h5', 'sample_data']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"dFZ9MQpfn9Jp","colab_type":"code","cellView":"both","colab":{}},"source":["#@title Main.py -> TRAIN TEST SPLIT\n","\n","# Integrated Cosine Sim into the model\n","# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n","\n","# import lib.logger, os, sys, random, math\n","import numpy as np\n","import os\n","\n","from functools import reduce\n","\n","# import config.config as config\n","# import data.data as data\n","# import data.data_func as data_func\n","import sklearn.metrics as metrics\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.model_selection import KFold, train_test_split\n","import pandas as pd\n","\n","# from tensorflow.python import debug as tf_debug\n","\n","from sklearn.utils import class_weight\n","\n","# import keras.backend as K\n","from tensorflow.keras import utils\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n","    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n","from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.losses import cosine_proximity, categorical_crossentropy\n","\n","from sentence_transformers import SentenceTransformer\n","\n","# import matplotlib.pyplot as plt\n","# from sklearn.decomposition import PCA\n","\n","import random\n","\"\"\"\n","Set random seed and fix bug on Dropout usage.\n","\"\"\"\n","import tensorflow as tf\n","\n","import tensorflow_hub as hub\n","\n","def embed_sentence(sentence):\n","  with tf.Session() as session:\n","    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","    message_embeddings = session.run(embed(sentence))\n","    return message_embeddings\n","\n","def ilen(iterable):\n","    return reduce(lambda sum, element: sum + 1, iterable, 0)\n","\n","\n","def build_knn(model, output_size):\n","    # Flatten feature vector\n","    flat_dim_size = np.prod(model.output_shape[1:])\n","    x = Reshape(target_shape=(flat_dim_size,),\n","                name='features_flat')(model.output)\n","\n","    # Dot product between feature vector and reference vectors\n","    x = Dense(units=output_size,\n","              activation='linear',\n","              use_bias=False)(x)\n","\n","    classifier = Model(inputs=[model.input], outputs=x)\n","    return classifier\n","\n","def normalize_encodings(encodings):\n","    ref_norms = np.linalg.norm(encodings, axis=0)\n","    return encodings / ref_norms\n","\n","seed = 1020\n","np.random.seed(seed)\n","# tf.python.control_flow_ops = tf\n","tf.compat.v1.set_random_seed(seed)\n","\n","MAX_NB_WORDS = 20000\n","MAX_SEQUENCE_LENGTH = 50\n","EMBEDDING_DIM = 100\n","\n","\"\"\"\n","Data reading and saving from disk (so that data processing is done only once).\n","\"\"\"\n","directory = DATA_DIR\n","funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n","               'Extends': 5}\n","\n","\n","# Function dataset start\n","datafiles = DATA_FILES['acl-arc']\n","test = read_jsonl_data(datafiles['test'])\n","train = read_jsonl_data((datafiles['train']))\n","\n","dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n","dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n","\n","# dataset_func = list(filter(lambda d: d['intent'] == 'CompareOrContrast', test + train))\n","# dataset_func2 = list(filter(lambda d: d['intent'] == 'Background', test + train))\n","# dataset_func3 = list(filter(lambda d: d['intent'] == 'Motivation', test + train))\n","#\n","# dataset_func = dataset_func + dataset_func2 + dataset_func3\n","\n","\n","\n","# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n","# print(\"loaded Hub Module\")\n","\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","# Function dataset end\n","#############################################################################3\n","\n","texts = list(map(lambda d: d['text'], dataset_func))\n","\n","ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n","\n","\n","print('Found %s texts.' % len(texts))\n","\n","tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(texts)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","kf = KFold(n_splits=5)\n","\n","y_pred_func_all = []\n","y_test_func_all = []\n","y_pred_prov_all = []\n","y_test_prov_all = []\n","y_pred_only_func_all = []\n","y_test_only_func_all = []\n","y_pred_only_prov_all = []\n","y_test_only_prov_all = []\n","y_pred_func = []\n","y_test_func = []\n","\n","embeddings_index = {}\n","f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","# for word, i in word_index.items():\n","#     embedding_vector = embeddings_index.get(word)\n","#     if embedding_vector is not None:\n","#         # words not found in embedding index will be all-zeros.\n","#         embedding_matrix[i] = embedding_vector\n","\n","# -------------------------\n","texts = map(lambda d: d['text'], dataset_func)\n","sequences = tokenizer.texts_to_sequences(texts)\n","xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","ys = np.asarray(ys)\n","\n","batch_num = 0\n","average_list = {}\n","\n","x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=42)\n","\n","x_train = np.array(x_train)\n","x_test = np.array(x_test)\n","y_train = np.array(y_train)\n","y_test = np.array(y_test)\n","\n","y_train_unique, indices = np.unique(y_train, return_index=True)\n","# Proportional Reduction\n","# ------------------------------------\n","\n","# new_x_train = []\n","# new_y_train = []\n","# arr = {}\n","# for index in range(len(funcs_index)):\n","#     print(index)\n","#     print(\"-----------------------\")\n","#     arr[index] = []\n","#     for i, value in enumerate(y_train):\n","#         if (value == index):\n","#             arr[index].append(i)\n","#     print(len(arr[index]))\n","#     sample_length = len(arr[index]) / 20  # 5% of data\n","#     sample_length = int(sample_length)\n","#     for j in range(sample_length):\n","#         new_x_train.append(x_train[arr[index][j]])\n","#         new_y_train.append(y_train[arr[index][j]])\n","#\n","# new_x_train = np.asarray(new_x_train)\n","# new_y_train = np.asarray(new_y_train)\n","# x_train = new_x_train\n","# y_train = new_y_train\n","\n","# ------------------------------------------\n","\n","# Few Shot\n","# ------------------------------------------\n","\n","# x_train_unique = [x_train[i] for i in indices]\n","# x_train_unique = np.asarray(x_train_unique)\n","# print(x_train_unique)\n","# y_train_unique = [y_train[i] for i in indices]\n","# y_train_unique = np.asarray(y_train_unique)\n","# x_train = x_train_unique\n","# y_train = y_train_unique\n","\n","# ------------------------------------------\n","\n","y_test = utils.to_categorical(np.asarray(y_test))\n","y_train = utils.to_categorical(np.asarray(y_train))\n","\n","print (x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n","\n","\n","NB_FILTER = 128\n","BATCH_SIZE = 32\n","count = 0\n","EPOCH = 15 # 20\n","indices = []\n","indices_type = []\n","\n","# ---------- Only citation function ----------\n","\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","    # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n","\n","embedding_layer = Embedding(len(word_index) + 1,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            name=\"embedding_layerA\")\n","\n","sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n","                        name=\"sequence_input\")\n","embedded_sequences = embedding_layer(sequence_input)\n","x = Convolution1D(filters=NB_FILTER,\n","                kernel_size=5,\n","                padding='valid',\n","                activation='relu',\n","                name='convolution_layer')(embedded_sequences)\n","\n","x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n","x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n","# x = Dropout(0.3)(x)\n","preds = Dense(len(funcs_index), activation='softmax')(x)\n","# preds = Dense(768, name=\"output_layer\")(x)\n","# output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n","\n","model = Model(sequence_input, preds)\n","\n","model.compile(loss=categorical_crossentropy,\n","            # optimizer='adam',\n","            optimizer='rmsprop',\n","            metrics=['acc'])\n","\n","print(model.summary())\n","\n","# import datetime\n","# from keras.callbacks import TensorBoard\n","\n","# log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","# tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","\n","model.load_weights('model-scicite.h5', by_name=True)\n","\n","# model.fit(x_train, y_train,\n","#         nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n","\n","# model.save_weights('model-acl.h5')\n","\n","# new_model = build_knn(model, encoded_classes.shape[1])\n","# print(new_model.summary())\n","# encoded_classes_norm = normalize_encodings(encoded_classes)\n","# temp_weights = new_model.get_weights()\n","# temp_weights[-1] = encoded_classes_norm\n","# new_model.set_weights(temp_weights)\n","\n","y_pred_probs = model.predict(x_test)\n","\n","y_pred_func = []\n","\n","y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n","\n","# y_test_list = []\n","# sim = {}\n","\n","# for i, sample in enumerate(y_pred_probs):\n","#     for j in range(len(funcs_index)):\n","#         # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n","#         if np.array_equal(y_test[i], words[j]):\n","#             y_test_list.append(j)\n","#     # greatest_sim = max(sim, key=sim.get)\n","\n","#     # y_pred_func.append(greatest_sim)\n","\n","# y_test = y_test_list\n","\n","\n","# y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n","\n","# new_y_pred = [1] * len(y_pred_func)\n","    # Generate classificat\n","# y_pred_func = new_y_pred\n","y_test = compress_y(y_test)\n","\n","        #print('y_pred_func_A')\n","        #print(y_pred_func)\n","\n","y_pred_only_func_all += y_pred_func\n","y_test_only_func_all += y_test\n","\n","        # ---------- End of citation function ----------\n","\n","print('Plain_Func')\n","# print(average_list)\n","print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n","print(\"Finish\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NvNMIDUcmvBi","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":944},"outputId":"52b0b4cd-bb26-4d58-94d1-3ac72addb948","executionInfo":{"status":"ok","timestamp":1581602885849,"user_tz":-480,"elapsed":26678,"user":{"displayName":"Ng KY","photoUrl":"","userId":"03424929306239991969"}}},"source":["#@title Transferred model with Training Acl anthology (4 classes)\n","\n","# New file authored 28 Jan 2018\n","# For citation provenance\n","# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n","\n","\"\"\"\n","This file tests the model that is trained (on acl-arc dataset, 6 classes) that outputs a 512 dimensional vector based on USE\n","Using the 4 Citation taxonomy dataset, (golden_test) as the target test set\n","\"\"\"\n","\n","\"\"\"\n","Perform the experiments on bootstrapped data and actual annotated data.\n","\"\"\"\n","# import lib.logger, os, sys, random, math\n","import numpy as np\n","import os\n","\n","from functools import reduce\n","\n","# import config.config as config\n","# import data.data as data\n","# import data.data_func as data_func\n","import sklearn.metrics as metrics\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.model_selection import KFold, train_test_split\n","import pandas as pd\n","\n","# from tensorflow.python import debug as tf_debug\n","\n","from sklearn.utils import class_weight\n","\n","# import keras.backend as K\n","from tensorflow.keras import utils\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n","    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n","from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.losses import cosine_proximity, categorical_crossentropy\n","\n","from sentence_transformers import SentenceTransformer\n","\n","# import matplotlib.pyplot as plt\n","# from sklearn.decomposition import PCA\n","\n","import random\n","\"\"\"\n","Set random seed and fix bug on Dropout usage.\n","\"\"\"\n","import tensorflow as tf\n","\n","\n","# import tensorflow_hub as hub\n","\n","# def embed_sentence(sentence):\n","#   with tf.Session() as session:\n","#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","#     message_embeddings = session.run(embed(sentence))\n","#     return message_embeddings\n","\n","def ilen(iterable):\n","    return reduce(lambda sum, element: sum + 1, iterable, 0)\n","\n","def build_knn(model, output_size):\n","  # Flatten feature vector\n","  flat_dim_size = np.prod(model.output_shape[1:])\n","  x = Reshape(target_shape=(flat_dim_size,),\n","              name='features_flat')(model.output)\n","\n","  # Dot product between feature vector and reference vectors\n","  x = Dense(units=output_size,\n","            activation='linear',\n","            use_bias=False)(x)\n","\n","  classifier = Model(inputs=[model.input], outputs=x)\n","  return classifier\n","\n","def normalize_encodings(encodings):\n","    ref_norms = np.linalg.norm(encodings, axis=0)\n","    return encodings / ref_norms\n","\n","\n","\n","seed = 1020\n","np.random.seed(seed)\n","# tf.python.control_flow_ops = tf\n","tf.compat.v1.set_random_seed(seed)\n","\n","MAX_NB_WORDS = 20000\n","MAX_SEQUENCE_LENGTH = 50\n","# GLOVE_DIR = GLOVE_DIR\n","EMBEDDING_DIM = 100\n","\n","\"\"\"\n","Data reading and saving from disk (so that data processing is done only once).\n","\"\"\"\n","directory = DATA_DIR\n","funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n","\n","\n","# Function dataset start\n","datafiles = DATA_FILES['func']\n","test = read_json_data(datafiles['golden_test'])\n","# train = data.read_jsonl_data((datafiles['train']))\n","train = read_json_data(datafiles['golden_train'])\n","\n","dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n","dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n","\n","dataset = dataset_train + dataset_test\n","# dataset_func = dataset_func + dataset_func2 + dataset_func3\n","\n","# random.shuffle(dataset_func)\n","\n","\n","# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n","# print(\"loaded Hub Module\")\n","\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","# Function dataset end\n","#############################################################################3\n","\n","texts_train = list(map(lambda d: d['context'][0], dataset_train))\n","\n","texts_test = list(map(lambda d: d['context'][0], dataset_test))\n","\n","texts = texts_train + texts_test\n","\n","y_train = list(map(lambda d: funcs_index[d['label']], dataset_train))\n","y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n","\n","ys = y_train + y_test\n","\n","x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=42)\n","\n","y_train_unique, indices = np.unique(y_train, return_index=True)\n","\n","print('Found %s texts.' % len(texts))\n","\n","tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(texts)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","y_pred_func_all = []\n","y_test_func_all = []\n","y_pred_only_func_all = []\n","y_test_only_func_all = []\n","y_pred_func = []\n","y_test_func = []\n","\n","embeddings_index = {}\n","f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","# for word, i in word_index.items():\n","#     embedding_vector = embeddings_index.get(word)\n","#     if embedding_vector is not None:\n","#         # words not found in embedding index will be all-zeros.\n","#         embedding_matrix[i] = embedding_vector\n","\n","# -------------------------\n","texts = map(lambda d: d['context'][0], dataset)\n","sequences = tokenizer.texts_to_sequences(texts)\n","xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","ys = np.asarray(ys)\n","xs = np.asarray(xs)\n","\n","print(xs.shape, ys.shape)\n","\n","sequences = tokenizer.texts_to_sequences(x_train)\n","x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","y_train = np.asarray(y_train)\n","x_train = np.asarray(x_train)\n","\n","\n","# Proportional Reduction\n","# ------------------------------------\n","\n","# new_x_train = []\n","# new_y_train = []\n","# arr = {}\n","# for index in range(len(funcs_index)):\n","#     print(index)\n","#     print(\"-----------------------\")\n","#     arr[index] = []\n","#     for i, value in enumerate(y_train):\n","#         if (value == index):\n","#             arr[index].append(i)\n","#     print(len(arr[index]))\n","#     sample_length = len(arr[index]) / 20  # 5% of data\n","#     sample_length = int(sample_length)\n","#     for j in range(sample_length):\n","#         new_x_train.append(x_train[arr[index][j]])\n","#         new_y_train.append(y_train[arr[index][j]])\n","#\n","# new_x_train = np.asarray(new_x_train)\n","# new_y_train = np.asarray(new_y_train)\n","# x_train = new_x_train\n","# y_train = new_y_train\n","\n","# ------------------------------------------\n","\n","# One Shot\n","# ------------------------------------------\n","\n","# x_train_unique = [x_train[i] for i in indices]\n","# x_train_unique = np.asarray(x_train_unique)\n","# y_train_unique = [y_train[i] for i in indices]\n","# y_train_unique = np.asarray(y_train_unique)\n","# x_train = x_train_unique\n","# y_train = y_train_unique\n","\n","# ------------------------------------------\n","\n","sequences = tokenizer.texts_to_sequences(x_test)\n","x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","y_test = np.asarray(y_test)\n","x_test = np.asarray(x_test)\n","\n","y_test = utils.to_categorical(np.asarray(y_test))\n","y_train = utils.to_categorical(np.asarray(y_train))\n","\n","batch_num = 0\n","average_list = {}\n","\n","\n","NB_FILTER = 128\n","BATCH_SIZE = 32\n","count = 0\n","EPOCH = 15 # 20\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","    # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n","\n","embedding_layer = Embedding(len(word_index) + 1,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            name=\"embedding_layerC\")\n","\n","sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n","                        name=\"sequence_input\")\n","embedded_sequences = embedding_layer(sequence_input)\n","x = Convolution1D(filters=NB_FILTER,\n","                kernel_size=5,\n","                padding='valid',\n","                activation='relu',\n","                name='convolution_layer')(embedded_sequences)\n","\n","x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n","x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n","# x = Dropout(0.3)(x)\n","preds = Dense(len(funcs_index), activation='softmax')(x)\n","# preds = Dense(768, name=\"output_layer\")(x)\n","# output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n","\n","model = Model(sequence_input, preds)\n","\n","model.compile(loss=categorical_crossentropy,\n","            # optimizer='adam',\n","            optimizer='rmsprop',\n","            metrics=['acc'])\n","\n","model.load_weights('model-scicite.h5', by_name=True)\n","\n","model.fit(x_train, y_train,\n","        nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2)\n","\n","y_pred_probs = model.predict(x_test)\n","# y_test = ys\n","\n","# total_diff = 0\n","# sample_count = 0\n","# for i, sample in enumerate(y_pred_probs):\n","#     total_diff += 1-cosine_similarity(y_pred_probs[i],ys[i])\n","#     sample_count += 1\n","# average = total_diff/sample_count\n","# print(\"The average cosine difference between the predictions and test are :\")\n","# print(average)\n","# average_list[batch_num] = average\n","# batch_num += 1\n","y_pred_func = []\n","\n","y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n","\n","\n","y_test = compress_y(y_test)\n","\n","y_pred_only_func_all += y_pred_func\n","y_test_only_func_all += y_test\n","\n","    # ---------- End of citation function ----------\n","\n","print('Plain_Func')\n","# print(average_list)\n","print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n","# print(\"Finish\")"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Found 1432 texts.\n","Found 4194 unique tokens.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n","  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"],"name":"stderr"},{"output_type":"stream","text":["Found 400000 word vectors.\n","(1432, 50) (1432,)\n","Train on 916 samples, validate on 229 samples\n","Epoch 1/15\n","916/916 [==============================] - 1s 1ms/sample - loss: 0.9407 - acc: 0.6638 - val_loss: 0.7300 - val_acc: 0.7555\n","Epoch 2/15\n","916/916 [==============================] - 1s 722us/sample - loss: 0.7063 - acc: 0.7194 - val_loss: 0.6920 - val_acc: 0.7555\n","Epoch 3/15\n","916/916 [==============================] - 1s 712us/sample - loss: 0.5283 - acc: 0.7871 - val_loss: 0.6945 - val_acc: 0.7467\n","Epoch 4/15\n","916/916 [==============================] - 1s 719us/sample - loss: 0.3896 - acc: 0.8559 - val_loss: 0.7405 - val_acc: 0.7511\n","Epoch 5/15\n","916/916 [==============================] - 1s 726us/sample - loss: 0.3068 - acc: 0.9017 - val_loss: 0.8047 - val_acc: 0.7293\n","Epoch 6/15\n","916/916 [==============================] - 1s 719us/sample - loss: 0.2601 - acc: 0.9116 - val_loss: 0.8255 - val_acc: 0.7511\n","Epoch 7/15\n","916/916 [==============================] - 1s 708us/sample - loss: 0.2290 - acc: 0.9225 - val_loss: 0.8023 - val_acc: 0.7162\n","Epoch 8/15\n","916/916 [==============================] - 1s 714us/sample - loss: 0.2176 - acc: 0.9312 - val_loss: 1.1404 - val_acc: 0.7467\n","Epoch 9/15\n","916/916 [==============================] - 1s 706us/sample - loss: 0.2144 - acc: 0.9279 - val_loss: 0.8741 - val_acc: 0.6594\n","Epoch 10/15\n","916/916 [==============================] - 1s 708us/sample - loss: 0.2059 - acc: 0.9225 - val_loss: 0.9506 - val_acc: 0.6288\n","Epoch 11/15\n","916/916 [==============================] - 1s 705us/sample - loss: 0.1847 - acc: 0.9367 - val_loss: 0.9777 - val_acc: 0.6376\n","Epoch 12/15\n","916/916 [==============================] - 1s 713us/sample - loss: 0.1784 - acc: 0.9378 - val_loss: 0.9888 - val_acc: 0.5983\n","Epoch 13/15\n","916/916 [==============================] - 1s 739us/sample - loss: 0.1719 - acc: 0.9334 - val_loss: 0.9123 - val_acc: 0.7380\n","Epoch 14/15\n","916/916 [==============================] - 1s 703us/sample - loss: 0.1620 - acc: 0.9334 - val_loss: 0.9860 - val_acc: 0.6419\n","Epoch 15/15\n","916/916 [==============================] - 1s 718us/sample - loss: 0.1584 - acc: 0.9345 - val_loss: 1.0603 - val_acc: 0.7467\n","Plain_Func\n","              precision    recall  f1-score   support\n","\n","           0     0.0000    0.0000    0.0000         5\n","           1     0.7500    0.3333    0.4615        18\n","           2     0.4444    0.1600    0.2353        50\n","           3     0.7739    0.9439    0.8505       214\n","\n","    accuracy                         0.7526       287\n","   macro avg     0.4921    0.3593    0.3868       287\n","weighted avg     0.7016    0.7526    0.7041       287\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zCKKTxPT-zWL","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Transferred model with Training SciCite (3 Classes)\n","\n","\"\"\"\n","This file tests the model that is trained (on acl-arc dataset, 6 classes) that outputs a 512 dimensional vector based on USE\n","Using the 4 Citation taxonomy dataset, (golden_test) as the target test set\n","\"\"\"\n","\n","\"\"\"\n","Perform the experiments on bootstrapped data and actual annotated data.\n","\"\"\"\n","# import lib.logger, os, sys, random, math\n","import numpy as np\n","import os\n","\n","from functools import reduce\n","\n","# import config.config as config\n","# import data.data as data\n","# import data.data_func as data_func\n","import sklearn.metrics as metrics\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.model_selection import KFold, train_test_split\n","import pandas as pd\n","\n","# from tensorflow.python import debug as tf_debug\n","\n","from sklearn.utils import class_weight\n","\n","# import keras.backend as K\n","from tensorflow.keras import utils\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n","    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n","from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.losses import cosine_proximity, categorical_crossentropy\n","\n","from sentence_transformers import SentenceTransformer\n","\n","import random\n","\"\"\"\n","Set random seed and fix bug on Dropout usage.\n","\"\"\"\n","import tensorflow as tf\n","\n","\n","# import tensorflow_hub as hub\n","\n","# def embed_sentence(sentence):\n","#   with tf.Session() as session:\n","#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","#     message_embeddings = session.run(embed(sentence))\n","#     return message_embeddings\n","\n","def ilen(iterable):\n","    return reduce(lambda sum, element: sum + 1, iterable, 0)\n","\n","def build_knn(model, output_size):\n","  # Flatten feature vector\n","  flat_dim_size = np.prod(model.output_shape[1:])\n","  x = Reshape(target_shape=(flat_dim_size,),\n","              name='features_flat')(model.output)\n","\n","  # Dot product between feature vector and reference vectors\n","  x = Dense(units=output_size,\n","            activation='linear',\n","            use_bias=False)(x)\n","\n","  classifier = Model(inputs=[model.input], outputs=x)\n","  return classifier\n","\n","def normalize_encodings(encodings):\n","    ref_norms = np.linalg.norm(encodings, axis=0)\n","    return encodings / ref_norms\n","\n","\n","\n","seed = 1020\n","np.random.seed(seed)\n","# tf.python.control_flow_ops = tf\n","tf.compat.v1.set_random_seed(seed)\n","\n","MAX_NB_WORDS = 20000\n","MAX_SEQUENCE_LENGTH = 50\n","# GLOVE_DIR = GLOVE_DIR\n","EMBEDDING_DIM = 100\n","\n","\"\"\"\n","Data reading and saving from disk (so that data processing is done only once).\n","\"\"\"\n","directory = DATA_DIR\n","funcs_index = {'background': 0, 'method': 1, 'result': 2}\n","\n","\n","# Function dataset start\n","datafiles = DATA_FILES['scicite']\n","test = read_jsonl_data(datafiles['test'])\n","train = read_jsonl_data(datafiles['train'])\n","\n","dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n","dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n","\n","dataset = dataset_train + dataset_test\n","\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","\n","# Function dataset end\n","#############################################################################3\n","\n","texts_train = list(map(lambda d: d['string'], dataset_train))\n","\n","texts_test = list(map(lambda d: d['string'], dataset_test))\n","\n","texts = texts_train + texts_test\n","\n","y_train = list(map(lambda d: funcs_index[d['label']], dataset_train))\n","y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n","\n","ys = y_train + y_test\n","\n","x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=42)\n","\n","y_train_unique, indices = np.unique(y_train, return_index=True)\n","\n","print('Found %s texts.' % len(texts))\n","\n","tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(texts)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","y_pred_func_all = []\n","y_test_func_all = []\n","y_pred_only_func_all = []\n","y_test_only_func_all = []\n","y_pred_func = []\n","y_test_func = []\n","\n","embeddings_index = {}\n","f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","sequences = tokenizer.texts_to_sequences(x_train)\n","x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","y_train = np.asarray(y_train)\n","x_train = np.asarray(x_train)\n","\n","\n","# Proportional Reduction\n","# ------------------------------------\n","\n","# new_x_train = []\n","# new_y_train = []\n","# arr = {}\n","# for index in range(len(funcs_index)):\n","#     print(index)\n","#     print(\"-----------------------\")\n","#     arr[index] = []\n","#     for i, value in enumerate(y_train):\n","#         if (value == index):\n","#             arr[index].append(i)\n","#     print(len(arr[index]))\n","#     sample_length = len(arr[index]) / 20  # 5% of data\n","#     sample_length = int(sample_length)\n","#     for j in range(sample_length):\n","#         new_x_train.append(x_train[arr[index][j]])\n","#         new_y_train.append(y_train[arr[index][j]])\n","\n","\n","# for i,element in enumerate(new_x_train):\n","#   new_y_train[i] = words[new_y_train[i]]\n","\n","# new_x_train = np.asarray(new_x_train)\n","# new_y_train = np.asarray(new_y_train)\n","# x_train = new_x_train\n","# y_train = new_y_train\n","\n","# ------------------------------------------\n","\n","# One Shot\n","# ------------------------------------------\n","\n","# x_train_unique = [x_train[i] for i in indices]\n","# x_train_unique = np.asarray(x_train_unique)\n","# y_train_unique = [y_train[i] for i in indices]\n","# y_train_unique = np.asarray(y_train_unique)\n","# x_train = x_train_unique\n","# y_train = y_train_unique\n","\n","# ------------------------------------------\n","\n","\n","\n","sequences = tokenizer.texts_to_sequences(x_test)\n","x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","y_test = np.asarray(y_test)\n","x_test = np.asarray(x_test)\n","\n","y_test = utils.to_categorical(np.asarray(y_test))\n","y_train = utils.to_categorical(np.asarray(y_train))\n","\n","batch_num = 0\n","average_list = {}\n","\n","NB_FILTER = 128\n","BATCH_SIZE = 32\n","count = 0\n","EPOCH = 15 # 20\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","    # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n","\n","embedding_layer = Embedding(len(word_index) + 1,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            name=\"embedding_layerB\")\n","\n","sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n","                        name=\"sequence_input\")\n","embedded_sequences = embedding_layer(sequence_input)\n","x = Convolution1D(filters=NB_FILTER,\n","                kernel_size=5,\n","                padding='valid',\n","                activation='relu',\n","                name='convolution_layer')(embedded_sequences)\n","\n","x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n","x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n","# x = Dropout(0.3)(x)\n","preds = Dense(len(funcs_index), activation='softmax')(x)\n","\n","model = Model(sequence_input, preds)\n","\n","model.compile(loss=categorical_crossentropy,\n","            # optimizer='adam',\n","            optimizer='rmsprop',\n","            metrics=['acc'])\n","\n","# model.load_weights('model-acl.h5', by_name=True)\n","\n","model.fit(x_train, y_train,\n","        nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2)\n","\n","model.save_weights('model-scicite.h5')\n","\n","y_pred_probs = model.predict(x_test)\n","# y_test = ys\n","\n","# total_diff = 0\n","# sample_count = 0\n","# for i, sample in enumerate(y_pred_probs):\n","#     total_diff += 1-cosine_similarity(y_pred_probs[i],ys[i])\n","#     sample_count += 1\n","# average = total_diff/sample_count\n","# print(\"The average cosine difference between the predictions and test are :\")\n","# print(average)\n","# average_list[batch_num] = average\n","# batch_num += 1\n","y_pred_func = []\n","\n","y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n","\n","y_test = compress_y(y_test)\n","\n","y_pred_only_func_all += y_pred_func\n","y_test_only_func_all += y_test\n","\n","    # ---------- End of citation function ----------\n","\n","print('Plain_Func')\n","# print(average_list)\n","print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n","# print(\"Finish\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XcSoS-izTpjV","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Combination (Aclarc + Scicite)\n","\n","# Integrated Cosine Sim into the model\n","# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n","\n","# import lib.logger, os, sys, random, math\n","import numpy as np\n","import os\n","\n","from functools import reduce\n","\n","# import config.config as config\n","# import data.data as data\n","# import data.data_func as data_func\n","import sklearn.metrics as metrics\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.model_selection import KFold, train_test_split\n","from sklearn.utils.random import sample_without_replacement\n","import pandas as pd\n","\n","# from tensorflow.python import debug as tf_debug\n","\n","from sklearn.utils import class_weight\n","\n","# import keras.backend as K\n","from tensorflow.keras import utils\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n","    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n","from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.losses import cosine_proximity\n","\n","from sentence_transformers import SentenceTransformer\n","\n","# import matplotlib.pyplot as plt\n","# from sklearn.decomposition import PCA\n","\n","import random\n","\"\"\"\n","Set random seed and fix bug on Dropout usage.\n","\"\"\"\n","import tensorflow as tf\n","\n","import tensorflow_hub as hub\n","\n","def embed_sentence(sentence):\n","  with tf.Session() as session:\n","    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","    message_embeddings = session.run(embed(sentence))\n","    return message_embeddings\n","\n","def ilen(iterable):\n","    return reduce(lambda sum, element: sum + 1, iterable, 0)\n","\n","\n","def build_knn(model, output_size):\n","    # Flatten feature vector\n","    flat_dim_size = np.prod(model.output_shape[1:])\n","    x = Reshape(target_shape=(flat_dim_size,),\n","                name='features_flat')(model.output)\n","\n","    # Dot product between feature vector and reference vectors\n","    x = Dense(units=output_size,\n","              activation='linear',\n","              use_bias=False)(x)\n","\n","    classifier = Model(inputs=[model.input], outputs=x)\n","    return classifier\n","\n","def normalize_encodings(encodings):\n","    ref_norms = np.linalg.norm(encodings, axis=0)\n","    return encodings / ref_norms\n","\n","seed = 1020\n","np.random.seed(seed)\n","# tf.python.control_flow_ops = tf\n","tf.compat.v1.set_random_seed(seed)\n","random.seed(seed)\n","\n","MAX_NB_WORDS = 20000\n","MAX_SEQUENCE_LENGTH = 50\n","EMBEDDING_DIM = 100\n","\n","\"\"\"\n","Data reading and saving from disk (so that data processing is done only once).\n","\"\"\"\n","directory = DATA_DIR\n","funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n","               'Extends': 5, 'background': 6, 'method': 7, 'result': 8}\n","\n","\n","# Function dataset start\n","datafiles = DATA_FILES['acl-arc']\n","test = read_jsonl_data(datafiles['test'])\n","train = read_jsonl_data((datafiles['train']))\n","\n","s_datafiles = DATA_FILES['scicite']\n","s_test = read_jsonl_data(s_datafiles['test'])\n","s_train = read_jsonl_data(s_datafiles['train'])\n","\n","s_dataset_train = list(filter(lambda x: x['label'] != 'Error',s_train))\n","s_dataset_test = list(filter(lambda x: x['label'] != 'Error', s_test))\n","\n","texts_train = list(map(lambda d: d['string'], s_train))\n","\n","texts_test = list(map(lambda d: d['string'], s_test))\n","\n","s_texts = texts_train + texts_test\n","\n","y_train = list(map(lambda d: funcs_index[d['label']], s_dataset_train))\n","y_test = list(map(lambda d: funcs_index[d['label']], s_dataset_test))\n","\n","s_ys = y_train + y_test\n","\n","temp_list0 = []\n","temp_list1 = []\n","r_indices = sample_without_replacement(len(s_test + s_train),len(test + train),random_state=42)\n","for index in r_indices:\n","  temp_list0.append(s_texts[index])\n","  temp_list1.append(s_ys[index])\n","s_texts = temp_list0\n","s_ys = temp_list1\n","\n","dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n","dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n","\n","random.shuffle(dataset_func)\n","\n","\n","# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n","# print(\"loaded Hub Module\")\n","\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","  \n","s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n","\n","words = {}\n","\n","# ## Raw Description\n","# words[0] = [\"P provides relevant information for this domain.\"]\n","# words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n","# words[2] = [\"Uses data, methods, etc., from P\"]\n","# words[3] = [\"Extends P’s data, methods, etc. \"]\n","# words[4] = [\"Expresses similarity/differences to P.\"]\n","# words[5] = [\"P is a potential avenue for future work.\"]\n","\n","# Summarized\n","words[0] = [\"provides relevant information\"]\n","words[1] = [\"Illustrates need\"]\n","words[2] = [\"Uses\"]\n","words[3] = [\"Extends\"]\n","words[4] = [\"similarity differences\"]\n","words[5] = [\"Potential Future\"]\n","## Mod Description ##\n","words[6] = [\"states background\"]\n","words[7] = [\"making use method approach\"]\n","words[8] = [\"Comparison results findings\"]\n","\n","\n","# Class Title\n","# words[0] = [\"Background\"]\n","# words[1] = [\"Motivation\"]\n","# words[2] = [\"Uses\"]\n","# words[3] = [\"Extension\"]\n","# words[4] = [\"Compare Or Contrast\"]\n","# words[5] = [\"Future\"]\n","\n","\n","for i, word in enumerate(words):\n","    # words[i] = embed_sentence(words[i])\n","    words[i] = np.array(s_transformer.encode(words[i]))\n","\n","# Function dataset end\n","#############################################################################3\n","\n","texts = list(map(lambda d: d['text'], dataset_func))\n","texts = texts + s_texts\n","\n","ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n","ys = ys + s_ys\n","\n","for i,element in enumerate(ys):\n","    ys[i] = words[ys[i]]\n","\n","print('Found %s texts.' % len(texts))\n","\n","tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(texts)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","kf = KFold(n_splits=5)\n","\n","y_pred_func_all = []\n","y_test_func_all = []\n","y_pred_prov_all = []\n","y_test_prov_all = []\n","y_pred_only_func_all = []\n","y_test_only_func_all = []\n","y_pred_only_prov_all = []\n","y_test_only_prov_all = []\n","y_pred_func = []\n","y_test_func = []\n","\n","embeddings_index = {}\n","f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","# for word, i in word_index.items():\n","#     embedding_vector = embeddings_index.get(word)\n","#     if embedding_vector is not None:\n","#         # words not found in embedding index will be all-zeros.\n","#         embedding_matrix[i] = embedding_vector\n","\n","# -------------------------\n","# texts = map(lambda d: d['text'], dataset_func)\n","sequences = tokenizer.texts_to_sequences(texts)\n","xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","ys = np.asarray(ys)\n","\n","batch_num = 0\n","average_list = {}\n","encoded_classes = words[0]\n","for i in range(len(words)-1):\n","    encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n","encoded_classes = encoded_classes.transpose()\n","\n","x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=42)\n","\n","x_train = np.array(x_train)\n","x_test = np.array(x_test)\n","y_train = np.array(y_train)\n","y_test = np.array(y_test)\n","\n","NB_FILTER = 128\n","BATCH_SIZE = 32\n","count = 0\n","EPOCH = 15 # 20\n","indices = []\n","indices_type = []\n","\n","# ---------- Only citation function ----------\n","\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","    # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n","\n","embedding_layer = Embedding(len(word_index) + 1,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            name=\"embedding_layerA\")\n","\n","sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n","                        name=\"sequence_input\")\n","embedded_sequences = embedding_layer(sequence_input)\n","x = Convolution1D(filters=NB_FILTER,\n","                kernel_size=5,\n","                padding='valid',\n","                activation='relu',\n","                name='convolution_layer')(embedded_sequences)\n","\n","x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n","x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n","# x = Dropout(0.3)(x)\n","# preds = Dense(len(funcs_index), activation='softmax')(x)\n","preds = Dense(768, name=\"output_layer\")(x)\n","output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n","\n","model = Model(sequence_input, output_reshape)\n","\n","model.compile(loss=cosine_proximity,\n","            # optimizer='adam',\n","            optimizer='rmsprop',\n","            metrics=['acc'])\n","\n","print(model.summary())\n","\n","# import datetime\n","# from keras.callbacks import TensorBoard\n","\n","# log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","# tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","\n","# model.load_weights('model-scicite.h5', by_name=True)\n","\n","model.fit(x_train, y_train,\n","        nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n","\n","model.save_weights('model-aclarc_scicite.h5')\n","\n","new_model = build_knn(model, encoded_classes.shape[1])\n","print(new_model.summary())\n","encoded_classes_norm = normalize_encodings(encoded_classes)\n","temp_weights = new_model.get_weights()\n","temp_weights[-1] = encoded_classes_norm\n","new_model.set_weights(temp_weights)\n","\n","y_pred_probs = new_model.predict(x_test)\n","\n","y_pred_func = []\n","\n","y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n","\n","y_test_list = []\n","sim = {}\n","\n","for i, sample in enumerate(y_pred_probs):\n","    for j in range(len(funcs_index)):\n","        # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n","        if np.array_equal(y_test[i], words[j]):\n","            y_test_list.append(j)\n","    # greatest_sim = max(sim, key=sim.get)\n","\n","    # y_pred_func.append(greatest_sim)\n","\n","y_test = y_test_list\n","\n","\n","# y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n","\n","# new_y_pred = [1] * len(y_pred_func)\n","    # Generate classificat\n","# y_pred_func = new_y_pred\n","# y_test = data.compress_y(y_test)\n","\n","        #print('y_pred_func_A')\n","        #print(y_pred_func)\n","\n","y_pred_only_func_all += y_pred_func\n","y_test_only_func_all += y_test\n","\n","        # ---------- End of citation function ----------\n","\n","print('Plain_Func')\n","# print(average_list)\n","print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n","print(\"Finish\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ll8Od_QDJr9c","colab_type":"code","outputId":"da435dc3-2a84-4814-bee4-32672dde2dcb","executionInfo":{"status":"ok","timestamp":1581339429446,"user_tz":-480,"elapsed":583,"user":{"displayName":"Ng KY","photoUrl":"","userId":"03424929306239991969"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(len(s_texts))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1827\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RNrBUXX53bcH","colab_type":"text"},"source":["left to right\n","\n","import categorical_crossentropy\n","remove random.seed?\n","\n","remove random.shuffule(dataset_func)\n","\n","remove words part\n","\n","remove encoded classes\n","\n","add in the utils.tocategorical thingy\n","porportional reduction\n","....\n","utils.tocategorical\n","\n","Name the layers (for cross weights)\n","\n","remove the 768 layer onwards\n","uncomment the len(func_index) layer\n","\n","compile loss = categorical_crossentropy\n","\n","y_pred_probs = model.predict(x_test)\n","\n","y_test = compress_y(y_test)\n","\n","done"]},{"cell_type":"code","metadata":{"id":"ehHN4PhKYIAR","colab_type":"code","outputId":"a5d27d73-ec43-46ae-eb45-cb465fbb6474","executionInfo":{"status":"ok","timestamp":1581602400540,"user_tz":-480,"elapsed":629,"user":{"displayName":"Ng KY","photoUrl":"","userId":"03424929306239991969"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import os\n","os.listdir()"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.config', 'model-acl.h5', 'model-scicite.h5', 'drive', 'sample_data']"]},"metadata":{"tags":[]},"execution_count":19}]}]}