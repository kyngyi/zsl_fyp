[{"label": "CoCo", "current": "The methodology we employed is similar to that endorsed by (Biber, 1995).", "context": ["2 Methodology", "The methodology we employed is similar to that endorsed by (Biber, 1995).", "It is summarised as follows:"], "vector_1": {"endors": 1, "methodolog": 2, "employ": 1, "summaris": 1, "follow": 1, "similar": 1}, "marker": "(Biber, 1995)", "article": "P96-1026", "vector_2": [1, 0.06033143448990161, 1, 2, 2, 0]}, {"label": "Neut", "current": "As suggested by the evaluators (Belz et al., 2009), this was due in large part to our reliance on the list of REs being in a particular order, which had changed for the NEG task.", "context": ["Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task.", "As suggested by the evaluators (Belz et al., 2009), this was due in large part to our reliance on the list of REs being in a particular order, which had changed for the NEG task.", "2 Method"], "vector_1": {"reasonablywel": 1, "evalu": 1, "predict": 1, "system": 2, "relianc": 1, "msr": 1, "compar": 1, "perform": 2, "neg": 2, "due": 1, "accuraci": 1, "re": 1, "score": 1, "compet": 1, "larg": 1, "method": 1, "string": 1, "especi": 1, "part": 1, "although": 1, "particular": 1, "regtyp": 1, "task": 3, "list": 1, "suggest": 1, "chang": 1, "disappointinglylow": 1, "order": 1}, "marker": "(Belz et al., 2009)", "article": "W10-4231", "vector_2": [1, 0.21082055906221822, 1, 2, 0, 0]}, {"label": "Neut", "current": "This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.", "context": ["While often ignored, the grammar constant |G |typically dominates the runtime in practice.", "This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.", "Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006)."], "vector_1": {"constant": 1, "often": 1, "process": 1, "converg": 1, "number": 1, "high": 1, "somewher": 1, "second": 1, "ghz": 1, "year": 1, "comput": 2, "nontermin": 1, "clock": 1, "doubl": 1, "system": 1, "accuraci": 1, "symbol": 1, "era": 1, "core": 1, "everi": 1, "around": 1, "sentenc": 1, "thousand": 1, "million": 1, "word": 1, "domin": 1, "averag": 1, "grammar": 2, "practic": 1, "g": 1, "contextfre": 1, "frequenc": 1, "rule": 1, "n": 1, "meanwhil": 1, "ignor": 1, "runtim": 1, "enter": 1, "typic": 1, "manycor": 1}, "marker": "Charniak, 2000", "article": "W11-2921", "vector_2": [11, 0.048299693709259406, 4, 1, 1, 0]}, {"label": "Neut", "current": "In the post-editing step, a prediction engine helps to decrease the amount of human interaction (Och et al., 2003).", "context": ["Then, one possible procedure for skilled human translators is to provide the oral translation of a given source text and then to post-edit the recognized text.", "In the post-editing step, a prediction engine helps to decrease the amount of human interaction (Och et al., 2003).", "In a CAT system with integrated speech, two sources of information are available to recognize the speech input: the target language speech and the given source language text."], "vector_1": {"help": 1, "predict": 1, "text": 3, "procedur": 1, "one": 1, "decreas": 1, "human": 2, "skill": 1, "postedit": 2, "languag": 2, "given": 2, "engin": 1, "interact": 1, "system": 1, "avail": 1, "speech": 3, "input": 1, "sourc": 3, "recogn": 2, "step": 1, "translat": 2, "target": 1, "possibl": 1, "provid": 1, "cat": 1, "inform": 1, "amount": 1, "integr": 1, "oral": 1, "two": 1}, "marker": "(Och et al., 2003)", "article": "P06-2061", "vector_2": [3, 0.05725563909774436, 1, 4, 14, 1]}, {"label": "Neut", "current": "Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available).", "context": ["a parser to a new domain with less annotation effort.", "Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available).", "The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents."], "vector_1": {"em": 1, "correct": 1, "domain": 1, "constitu": 1, "less": 1, "parser": 1, "high": 1, "tag": 1, "effort": 1, "proport": 1, "find": 1, "compat": 1, "nois": 1, "semisupervis": 1, "suggest": 1, "support": 1, "avail": 1, "alway": 1, "confidencebas": 1, "merialdo": 1, "new": 1, "po": 1, "introduc": 1, "suitabl": 1, "deriv": 2, "especi": 1, "elworthi": 1, "train": 2, "data": 1, "outweigh": 1, "success": 1, "level": 1, "indomain": 1, "annot": 1, "method": 2, "benefit": 1, "bracket": 1, "incorpor": 1, "contain": 1}, "marker": "(1994)", "article": "W07-2203", "vector_2": [13, 0.9437033925007877, 2, 4, 0, 0]}, {"label": "Pos", "current": "By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%.", "context": ["More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.", "By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%.", "2 Ranking and Reranking"], "vector_1": {"bleu": 1, "featur": 1, "rank": 1, "dataset": 1, "best": 1, "baselin": 1, "och": 1, "differ": 1, "system": 1, "score": 1, "test": 1, "wellformed": 1, "function": 1, "syntact": 1, "rerank": 2, "gener": 1, "use": 1, "improv": 2, "list": 1, "mt": 2, "output": 1, "order": 1}, "marker": "(2003)", "article": "N04-1023", "vector_2": [1, 0.23125892202631274, 2, 6, 5, 1]}, {"label": "Neut", "current": "A well-known study evaluated usage of Twitter in the aftermath of the 2010 earthquake in Japan (Sakaki et al., 2010).", "context": ["More work has been done in the latter category: analysis of social phenomena in a non-English context.", "A well-known study evaluated usage of Twitter in the aftermath of the 2010 earthquake in Japan (Sakaki et al., 2010).", "Another Japanese-oriented study evaluated the impact of television on tweeted content (Akioka et al., 2010)."], "vector_1": {"evalu": 2, "twitter": 1, "phenomena": 1, "nonenglish": 1, "done": 1, "japan": 1, "impact": 1, "content": 1, "televis": 1, "anoth": 1, "latter": 1, "categori": 1, "wellknown": 1, "analysi": 1, "usag": 1, "japaneseori": 1, "aftermath": 1, "work": 1, "earthquak": 1, "context": 1, "social": 1, "studi": 2, "tweet": 1}, "marker": "(Sakaki et al., 2010)", "article": "D13-1114", "vector_2": [3, 0.16237106387611247, 2, 2, 0, 0]}, {"label": "Neut", "current": "The model parameters A are estimated such that the model assigns maximum log-likelihood to the training data subject to a Gaussian prior centered at 0, A  N(0, diag(u2i )), that ensures smoothing (Chen and Rosenfeld, 2000):", "context": ["3.1.2 Parameter Estimation", "The model parameters A are estimated such that the model assigns maximum log-likelihood to the training data subject to a Gaussian prior centered at 0, A  N(0, diag(u2i )), that ensures smoothing (Chen and Rosenfeld, 2000):", "L(A) = E p(x, y)log pA(y|x) (1) x,y n P(T|W) = i=1 E Z(x, A) = y i 2' + const(A)i"], "vector_1": {"payx": 1, "gaussian": 1, "ptw": 1, "xy": 1, "zx": 1, "ensur": 1, "loglikelihood": 1, "subject": 1, "la": 1, "constai": 1, "px": 1, "estim": 2, "paramet": 2, "ylog": 1, "train": 1, "data": 1, "e": 2, "center": 1, "i": 1, "smooth": 1, "maximum": 1, "n": 2, "prior": 1, "diagui": 1, "model": 2, "assign": 1}, "marker": "(Chen and Rosenfeld, 2000)", "article": "W04-3237", "vector_2": [4, 0.4870454847424598, 1, 5, 0, 0]}, {"label": "Pos", "current": "As our third corpus, we use the Alex portion of the Providence corpus (Demuth et al., 2006; Borschinger et al., 2012).", "context": ["ing models of Bayesian word segmentation (Brent, 1999; Goldwater, 2007; Goldwater et al., 2009; Johnson and Goldwater, 2009), comprising in total 9790 utterances.", "As our third corpus, we use the Alex portion of the Providence corpus (Demuth et al., 2006; Borschinger et al., 2012).", "A major benefit of the Providence corpus is that the video-recordings from which the transcripts were produced are available through CHILDES alongside the transcripts."], "vector_1": {"corpu": 3, "major": 1, "alongsid": 1, "utter": 1, "total": 1, "videorecord": 1, "use": 1, "ing": 1, "segment": 1, "benefit": 1, "avail": 1, "child": 1, "bayesian": 1, "alex": 1, "transcript": 2, "word": 1, "third": 1, "provid": 2, "portion": 1, "compris": 1, "model": 1, "produc": 1}, "marker": "(Demuth et al., 2006", "article": "Q14-1008", "vector_2": [8, 0.5116248418388866, 6, 1, 0, 0]}, {"label": "Neut", "current": "The notion of various schemata interacting is supported by Kellogg (1996), who proposes that resources from the central executive of Baddeleys model of the working-memory, e.g., Baddeley (1974), are needed to perform both lower-level writing processes such as spelling, grammar and motor movements and higher-level writing processes such as planning and revising. (qtd.", "context": ["To clarify this idea, though, subsequent investigations will investigate pauses at the boundaries of MWEs.", "The notion of various schemata interacting is supported by Kellogg (1996), who proposes that resources from the central executive of Baddeleys model of the working-memory, e.g., Baddeley (1974), are needed to perform both lower-level writing processes such as spelling, grammar and motor movements and higher-level writing processes such as planning and revising. (qtd.", "in Johansson, 2010)."], "vector_1": {"paus": 1, "execut": 1, "process": 2, "eg": 1, "idea": 1, "baddeley": 2, "motor": 1, "need": 1, "plan": 1, "revis": 1, "higherlevel": 1, "perform": 1, "boundari": 1, "support": 1, "write": 2, "mwe": 1, "subsequ": 1, "movement": 1, "clarifi": 1, "investig": 2, "resourc": 1, "variou": 1, "though": 1, "spell": 1, "grammar": 1, "kellogg": 1, "lowerlevel": 1, "workingmemori": 1, "central": 1, "qtd": 1, "notion": 1, "schemata": 1, "model": 1, "interact": 1, "propos": 1}, "marker": "(1974)", "article": "W15-0914", "vector_2": [41, 0.8410434073772162, 3, 1, 0, 0]}, {"label": "Pos", "current": "KNN algorithms are also commonly used in Graph-Based Semi-Supervised Learning approaches (Das and Petrov, 2011; Altun et al., 2006; Subramanya et al., 2010), with the knearest-neighbour sets determining the edges that structure the graph.", "context": ["Thus, these clusters engender a form of distributional similarity comparable to that used in our KNN algorithm.", "KNN algorithms are also commonly used in Graph-Based Semi-Supervised Learning approaches (Das and Petrov, 2011; Altun et al., 2006; Subramanya et al., 2010), with the knearest-neighbour sets determining the edges that structure the graph.", "POS tags are then propagated through the graph from labelled to unlabelled data."], "vector_1": {"propag": 1, "set": 1, "edg": 1, "cluster": 1, "tag": 1, "use": 2, "compar": 1, "also": 1, "graph": 2, "label": 1, "knearestneighbour": 1, "approach": 1, "engend": 1, "unlabel": 1, "graphbas": 1, "distribut": 1, "form": 1, "data": 1, "knn": 2, "algorithm": 2, "thu": 1, "semisupervis": 1, "structur": 1, "po": 1, "determin": 1, "learn": 1, "similar": 1, "commonli": 1}, "marker": "(Das and Petrov, 2011", "article": "W15-2610", "vector_2": [4, 0.9507793525378916, 3, 1, 4, 0]}, {"label": "Neut", "current": "This led Dahlmann and Adolphs (2007) to study pausing within spoken MWEs.", "context": ["In studies of speech, Erman (2007) notes that a pause can be caused by the cognitive demands of lexical retrieval, and Pawley (1985) notes that pauses are much less acceptable within a lexicalized phrase than within a free expression.", "This led Dahlmann and Adolphs (2007) to study pausing within spoken MWEs.", "A central finding of Dahlmann  Adolphs is that MWEs are often surrounded by pauses, and that pausality is unique within and around MWEs."], "vector_1": {"paus": 4, "erman": 1, "less": 1, "spoken": 1, "within": 4, "pausal": 1, "accept": 1, "pawley": 1, "phrase": 1, "cognit": 1, "find": 1, "often": 1, "dahlmann": 2, "note": 2, "much": 1, "speech": 1, "mwe": 3, "led": 1, "around": 1, "express": 1, "adolph": 2, "free": 1, "lexic": 2, "demand": 1, "central": 1, "retriev": 1, "uniqu": 1, "surround": 1, "caus": 1, "studi": 2}, "marker": "(2007)", "article": "W15-0914", "vector_2": [8, 0.24336661911554922, 3, 6, 0, 0]}, {"label": "Pos", "current": "Following Christiansen et al (1998) and Doyle and Levy (2013), we use the Korman corpus (Korman, 1984) as one of our corpora.", "context": ["4.1 Corpora and corpus creation", "Following Christiansen et al (1998) and Doyle and Levy (2013), we use the Korman corpus (Korman, 1984) as one of our corpora.", "It comprises childdirected speech for very young infants, aged between 6 and 16 weeks and, like all other corpora used in this paper, is available through the CHILDES database (MacWhinney, 2000)."], "vector_1": {"corpu": 2, "infant": 1, "creation": 1, "al": 1, "one": 1, "paper": 1, "follow": 1, "et": 1, "avail": 1, "use": 2, "databas": 1, "young": 1, "1": 1, "speech": 1, "doyl": 1, "week": 1, "korman": 1, "levi": 1, "child": 1, "like": 1, "christiansen": 1, "age": 1, "corpora": 3, "childdirect": 1, "compris": 1}, "marker": "(1998)", "article": "Q14-1008", "vector_2": [16, 0.4482549557148882, 4, 4, 1, 0]}, {"label": "Neut", "current": "(For standard, in-depth, introductions to this fascinating field, see, e.g., Nida [1949], Jensen [1990], Spencer and Zwicky [1998], or Haspelmath [2002].)", "context": ["If a language has suffixing and/or prefixing-sometimes called concatenative morphology-it obviously follows that text words in that language can be segmented into a sequence of morphological elements: a stem and a number of suffixes after the stem and/or prefixes before the stem.1 Morphology is one of the oldest linguistic subdisciplines, and this brief presentation by necessity omits many intricacies and greatly simplifies a vast scholarship.", "(For standard, in-depth, introductions to this fascinating field, see, e.g., Nida [1949], Jensen [1990], Spencer and Zwicky [1998], or Haspelmath [2002].)", "In language technology applications, a morphological component forms a bridge between texts and structured information about the vocabulary of a language."], "vector_1": {"suffix": 2, "applic": 1, "text": 2, "eg": 1, "vast": 1, "number": 1, "one": 1, "prefix": 1, "subdisciplin": 1, "follow": 1, "standard": 1, "oldest": 1, "languag": 4, "andor": 2, "compon": 1, "morphologyit": 1, "brief": 1, "morpholog": 3, "field": 1, "fascin": 1, "haspelmath": 1, "call": 1, "prefixingsometim": 1, "indepth": 1, "simplifi": 1, "nida": 1, "zwicki": 1, "jensen": 1, "form": 1, "intricaci": 1, "sequenc": 1, "stem": 3, "vocabulari": 1, "necess": 1, "segment": 1, "present": 1, "introduct": 1, "concaten": 1, "word": 1, "greatli": 1, "omit": 1, "technolog": 1, "see": 1, "structur": 1, "element": 1, "inform": 1, "bridg": 1, "obvious": 1, "mani": 1, "linguist": 1, "spencer": 1, "scholarship": 1}, "marker": "[1949]", "article": "J11-2002", "vector_2": [62, 0.04249377840610359, 4, 1, 44, 1]}, {"label": "Pos", "current": "We initiallv experimented with tools such as GIZA++, but found Phonetisaurus produced better alignments compared to other tools as it uses manv-to-manv alignments developed specificallv for grapheme to phoneme svstems (Jiampojamarn et al., 2007).", "context": ["Romanizing the input verv marginallv improves the character alignment process, since both input and output are then alphabetic scripts (as compared to svllabic to alphabetic alignment).", "We initiallv experimented with tools such as GIZA++, but found Phonetisaurus produced better alignments compared to other tools as it uses manv-to-manv alignments developed specificallv for grapheme to phoneme svstems (Jiampojamarn et al., 2007).", "A sample alignment sequence from Phonetisaurus is given below: melillem I mellil'lem  m}m e}e 1}lel i}i l}l1' l}l e}e m}m gadvemtlvan I gaddientlean  g}g a}a d}dld v}i e}e m}n t}t l}l v}e a}a n}n bharatasarkva I bharotasarkea  bh}blh a}a r}r a}o t}t a}a s}s a}a r}r k}k v}e a}a bhiveli I bhieli  bh}blh ilv}i e}e l}l i}i Where } denotes individual character alignment, I between characters indicates grapheme chunks, and d}dld implies that the source grapheme ed is mapped to the target graphemic chunk edd."], "vector_1": {"gaddientlean": 1, "aa": 6, "impli": 1, "chunk": 2, "process": 1, "graphem": 4, "bharatasarkva": 1, "bhieli": 1, "ao": 1, "rr": 2, "ii": 2, "sampl": 1, "phonem": 1, "denot": 1, "svllabic": 1, "giza": 1, "improv": 1, "ee": 4, "specificallv": 1, "use": 1, "manvtomanv": 1, "edd": 1, "compar": 2, "script": 1, "alphabet": 2, "ll": 1, "initiallv": 1, "gadvemtlvan": 1, "bhive": 1, "better": 1, "marginallv": 1, "ed": 1, "lel": 1, "indic": 1, "sequenc": 1, "input": 2, "experi": 1, "individu": 1, "map": 1, "sourc": 1, "svstem": 1, "verv": 1, "vi": 1, "phonetisauru": 2, "tool": 2, "mellillem": 1, "given": 1, "nn": 1, "kk": 1, "output": 1, "develop": 1, "sinc": 1, "target": 1, "gg": 1, "melillem": 1, "mm": 2, "align": 6, "mn": 1, "charact": 3, "bhblh": 2, "ss": 1, "roman": 1, "tt": 2, "ilvi": 1, "found": 1, "ddld": 2, "produc": 1, "bharotasarkea": 1}, "marker": "(Jiampojamarn et al., 2007)", "article": "W14-5502", "vector_2": [7, 0.7137423818906659, 1, 1, 0, 0]}, {"label": "Neut", "current": "There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).", "context": ["3.6 Large Margin Classifiers", "There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).", "The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization."], "vector_1": {"quit": 1, "svm": 2, "linear": 2, "superior": 1, "perform": 1, "boost": 1, "winnow": 1, "maxim": 1, "classifi": 3, "separ": 1, "larg": 2, "perceptron": 1, "sampl": 1, "abil": 1, "margin": 3}, "marker": "(Vapnik, 1998)", "article": "N04-1023", "vector_2": [6, 0.5118253019020795, 4, 1, 0, 0]}, {"label": "Neut", "current": "2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005).", "context": ["We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel.", "2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005).", "The T2 statistics for a pair of words e; and fj is computed as"], "vector_1": {"comput": 1, "although": 1, "vogel": 1, "loglikelihood": 1, "use": 1, "ratio": 1, "weight": 1, "score": 2, "statist": 1, "competit": 1, "zhang": 1, "train": 1, "link": 3, "005": 1, "mani": 1, "pair": 1, "e": 1, "fj": 1, "modifi": 1, "choic": 1, "word": 1, "algorithm": 1, "possibl": 1, "discrimin": 1, "t": 2, "x": 1}, "marker": "(Dunning, 1993)", "article": "P08-1113", "vector_2": [15, 0.49680801740432967, 3, 1, 0, 0]}, {"label": "Neut", "current": "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", "context": ["To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", "Plan of the paper."], "vector_1": {"detect": 2, "word": 1, "closest": 1, "unknown": 3, "howev": 1, "knowledg": 1, "next": 1, "known": 1, "step": 1, "paper": 1, "exist": 1, "plan": 1, "determin": 1, "complementari": 1, "date": 1, "sens": 3, "problem": 2, "approach": 2, "logic": 1, "view": 1}, "marker": "(Widdows, 2003", "article": "N06-1017", "vector_2": [3, 0.15268411114108443, 3, 2, 0, 0]}, {"label": "Neut", "current": "The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages.", "context": ["These rules were learned using a word-aligned parallel corpus.", "The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages.", "Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005)."], "vector_1": {"corpu": 1, "bleu": 1, "al": 1, "rate": 1, "minimum": 1, "treetagg": 1, "et": 1, "languag": 1, "use": 3, "perform": 1, "system": 1, "decod": 1, "po": 1, "reorder": 1, "wordalign": 1, "optim": 1, "gener": 1, "error": 1, "train": 1, "translat": 1, "parallel": 1, "rule": 1, "venugop": 1, "tag": 1, "sttk": 1, "learn": 1, "model": 1, "toward": 1, "propos": 1}, "marker": "(Schmid, 1994)", "article": "W10-1719", "vector_2": [16, 0.1341596557703423, 3, 1, 0, 0]}, {"label": "Pos", "current": "We also added stress information to the BrentBernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent, 1999), following the procedure just outlined.", "context": ["As function words account for roughly 50% of the tokens but only roughly 5% of the types in our corpora, this means that the type and token distribution of stress patterns differs dramatically in all our corpora, as can be seen from Table 2.", "We also added stress information to the BrentBernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent, 1999), following the procedure just outlined.", "This corpus is a de-facto standard for evaluat"], "vector_1": {"corpu": 2, "outlin": 1, "ad": 1, "evaluat": 1, "procedur": 1, "defacto": 1, "seen": 1, "tabl": 1, "follow": 1, "differ": 1, "pattern": 1, "also": 1, "type": 2, "function": 1, "distribut": 1, "stress": 2, "standard": 1, "account": 1, "word": 1, "corpora": 2, "brentbernsteinratn": 1, "dramat": 1, "inform": 1, "token": 2, "roughli": 2, "mean": 1}, "marker": "(Bernstein-Ratner, 1987", "article": "Q14-1008", "vector_2": [27, 0.4849219738506959, 2, 1, 0, 0]}, {"label": "Neut", "current": "We used the first 1000 sentences from Section 22 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993) as our benchmark set.", "context": ["The grammar has 852,591 binary rules and 114,419 unary rules.", "We used the first 1000 sentences from Section 22 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993) as our benchmark set.", "We verified for each sentence that our parallel implementation obtains exactly the same parse tree and score as the sequential implementation."], "vector_1": {"unari": 1, "set": 1, "wall": 1, "treebank": 1, "obtain": 1, "street": 1, "use": 1, "section": 1, "binari": 1, "score": 1, "verifi": 1, "sentenc": 2, "journal": 1, "benchmark": 1, "pars": 1, "penn": 1, "exactli": 1, "parallel": 1, "grammar": 1, "wsj": 1, "tree": 1, "rule": 2, "portion": 1, "implement": 2, "sequenti": 1, "first": 1}, "marker": "(Marcus et al., 1993)", "article": "W11-2921", "vector_2": [18, 0.7318254404565565, 1, 1, 0, 0]}, {"label": "Neut", "current": "To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j, all of which employ a standard context vector model of the individual words/tokens in the multiword expression arguments between the reference and machine translations, as descibed by Lo et al (2012) and Tumuluru et al (2012).", "context": ["3 Comparison of multiword expression association approaches", "To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j, all of which employ a standard context vector model of the individual words/tokens in the multiword expression arguments between the reference and machine translations, as descibed by Lo et al (2012) and Tumuluru et al (2012).", "3.1 Bag of words (geometric mean)"], "vector_1": {"machin": 1, "comput": 1, "al": 2, "individu": 1, "prefer": 1, "argument": 1, "four": 1, "sipr": 1, "et": 2, "1": 1, "bag": 1, "lo": 1, "access": 1, "multiword": 3, "sij": 1, "approach": 2, "refer": 1, "altern": 2, "express": 3, "descib": 1, "lexic": 2, "standard": 1, "wordstoken": 1, "translat": 1, "consid": 1, "associ": 2, "assess": 1, "comparison": 1, "word": 1, "constraint": 1, "employ": 1, "defin": 1, "vector": 1, "geometr": 1, "context": 1, "model": 1, "mean": 1, "similar": 1, "tumuluru": 1}, "marker": "(2012)", "article": "W14-4719", "vector_2": [2, 0.3797189859492975, 2, 3, 23, 0]}, {"label": "Neut", "current": "Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.", "context": ["Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.", "These methods reconstruct language family trees based on linguistic features that exist within words including lexical, phonological, and morphological features."], "vector_1": {"corpu": 1, "featur": 2, "phonolog": 1, "appli": 1, "within": 1, "cluster": 1, "linguist": 1, "exist": 1, "famili": 2, "languag": 2, "use": 1, "techniqu": 1, "morpholog": 1, "other": 1, "includ": 1, "method": 1, "lexic": 1, "base": 1, "reconstruct": 2, "task": 1, "word": 1, "tree": 2, "statist": 1, "purpos": 1}, "marker": "Rama and Singh, 2009)", "article": "P13-1112", "vector_2": [4, 0.9316055819659561, 7, 1, 0, 0]}, {"label": "Neut", "current": "For our adaptation of the Wikipedia Link-based Measure (WLM) approach to spreading activation, we define the WLM Agglomerative Approach (henceforth called AA-wlm2) as 2AA-wlm is our adaptation of WLM (Witten and Milne, 2008) for SA, not to be confused with their method, which we simply call WLM.", "context": ["(4) ||Ai |Aj||", "For our adaptation of the Wikipedia Link-based Measure (WLM) approach to spreading activation, we define the WLM Agglomerative Approach (henceforth called AA-wlm2) as 2AA-wlm is our adaptation of WLM (Witten and Milne, 2008) for SA, not to be confused with their method, which we simply call WLM.", "simAA,cos(Ai, Aj) A Ai  Aj"], "vector_1": {"simaacosai": 1, "measur": 1, "aawlm": 2, "simpli": 1, "confus": 1, "defin": 1, "ai": 2, "activ": 1, "aj": 3, "wikipedia": 1, "method": 1, "henceforth": 1, "linkbas": 1, "spread": 1, "call": 2, "adapt": 2, "agglom": 1, "approach": 2, "sa": 1, "wlm": 4}, "marker": "(Witten and Milne, 2008)", "article": "W10-3506", "vector_2": [2, 0.5121039314173496, 1, 7, 1, 0]}, {"label": "Neut", "current": "Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).", "context": ["The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection.", "Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).", "However, we may not have a big background collection that matches the test domain for a reliable IDF estimate."], "vector_1": {"domain": 1, "identifi": 1, "eg": 1, "idea": 1, "extract": 1, "occur": 1, "shown": 1, "appear": 1, "much": 1, "test": 1, "document": 2, "match": 1, "may": 1, "big": 1, "journal": 1, "effect": 1, "entir": 1, "background": 1, "word": 1, "keyword": 1, "scientif": 1, "howev": 1, "work": 1, "tfidf": 1, "collect": 2, "estim": 1, "idf": 1, "reliabl": 1, "frequent": 2}, "marker": "Hulth, 2003", "article": "N09-1070", "vector_2": [6, 0.12530221462879984, 3, 1, 0, 0]}, {"label": "Neut", "current": "In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al (2007).", "context": ["Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system (Marino et al., 2006).", "In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al (2007).", "We present a summary of the TALP-UPC Ngrambased SMT system used for this shared task."], "vector_1": {"campaign": 1, "ngrambas": 3, "share": 1, "al": 1, "past": 1, "year": 1, "et": 1, "shown": 1, "group": 1, "evalu": 1, "compar": 1, "prove": 1, "smt": 3, "system": 3, "approach": 1, "statist": 1, "machin": 1, "koehn": 1, "previou": 1, "use": 1, "translat": 1, "develop": 1, "present": 1, "task": 1, "callisonburch": 1, "phrasebas": 1, "talpupc": 2, "stateoftheart": 1, "monz": 1, "summari": 1}, "marker": "(2006)", "article": "W08-0315", "vector_2": [2, 0.09016393442622951, 3, 1, 1, 0]}, {"label": "Neut", "current": "Inui et al (1997) describe the probability model utilized in the system where a transition is represented by the probability of moving from one stack state, i1, (an instance of the graph structured stack) to another, i.", "context": ["shift/reduce or reduce/reduce) actions.", "Inui et al (1997) describe the probability model utilized in the system where a transition is represented by the probability of moving from one stack state, i1, (an instance of the graph structured stack) to another, i.", "They estimate this probability using the stack-top state si1, next input symbol li and next action ai."], "vector_1": {"ai": 1, "transit": 1, "move": 1, "al": 1, "one": 1, "et": 1, "li": 1, "probabl": 3, "use": 1, "shiftreduc": 1, "describ": 1, "graph": 1, "system": 1, "next": 2, "state": 2, "inui": 1, "input": 1, "reducereduc": 1, "symbol": 1, "util": 1, "repres": 1, "stack": 2, "i": 1, "stacktop": 1, "structur": 1, "si": 1, "instanc": 1, "estim": 1, "anoth": 1, "action": 2, "model": 1}, "marker": "(1997)", "article": "W07-2203", "vector_2": [10, 0.28824003080908867, 1, 3, 0, 0]}, {"label": "Pos", "current": "An interesting difference with respect to previous descriptions is the use of the true (or direct) imperative to express an action in the procedure genre, as results from (Paris and Scott, 1994) seem to indicate that the infinitive-form of the imperative is preferred in French.", "context": ["Our analysis goes beyond previous work by identifying within the discourse context the means for exercising explicit control over a text generator.", "An interesting difference with respect to previous descriptions is the use of the true (or direct) imperative to express an action in the procedure genre, as results from (Paris and Scott, 1994) seem to indicate that the infinitive-form of the imperative is preferred in French.", "These results, however, were obtained from a corpus of instructions mostly for domestic appliances as opposed to software manuals."], "vector_1": {"control": 1, "corpu": 1, "identifi": 1, "softwar": 1, "applianc": 1, "text": 1, "within": 1, "prefer": 1, "direct": 1, "procedur": 1, "instruct": 1, "result": 2, "oppos": 1, "respect": 1, "seem": 1, "beyond": 1, "infinitiveform": 1, "differ": 1, "goe": 1, "descript": 1, "exercis": 1, "domest": 1, "analysi": 1, "discours": 1, "express": 1, "interest": 1, "gener": 1, "previou": 2, "use": 1, "french": 1, "obtain": 1, "indic": 1, "mostli": 1, "true": 1, "manual": 1, "howev": 1, "work": 1, "explicit": 1, "imper": 2, "genr": 1, "context": 1, "action": 1, "mean": 1}, "marker": "(Paris and Scott, 1994)", "article": "P96-1026", "vector_2": [2, 0.8983687208700155, 1, 3, 5, 0]}, {"label": "Neut", "current": "The work of Sleeman (1982) on inferring defective algebra rules (mal-rules) is based on the observation that the a priori listing of misconceptions is a difficult, if not impossible, task.", "context": ["The major problem with these systems is due to their inability to reason about the misconception itself, they are completely at a loss when faced with a misconception absent from their a priori listing.", "The work of Sleeman (1982) on inferring defective algebra rules (mal-rules) is based on the observation that the a priori listing of misconceptions is a difficult, if not impossible, task.", "Sleeman proposes on-line inference of mal-rules based on the answer the student has given to a particular problem."], "vector_1": {"major": 1, "imposs": 1, "defect": 1, "system": 1, "onlin": 1, "given": 1, "algebra": 1, "due": 1, "priori": 2, "misconcept": 3, "answer": 1, "infer": 2, "complet": 1, "inabl": 1, "loss": 1, "malrul": 2, "reason": 1, "base": 2, "student": 1, "particular": 1, "absent": 1, "task": 1, "sleeman": 2, "work": 1, "list": 2, "rule": 1, "face": 1, "problem": 2, "difficult": 1, "observ": 1, "propos": 1}, "marker": "(1982)", "article": "J88-3005", "vector_2": [6, 0.15673126519784836, 1, 1, 0, 0]}, {"label": "Neut", "current": "In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).", "context": ["Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.", "In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).", "However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation."], "vector_1": {"corpu": 1, "featur": 1, "process": 1, "decreas": 1, "loglinear": 1, "cw": 2, "sever": 1, "leverag": 1, "accur": 1, "develop": 1, "semisupervis": 1, "rather": 1, "data": 2, "smt": 1, "figur": 1, "translat": 1, "previous": 1, "method": 1, "unlabel": 1, "machin": 1, "perplex": 1, "cascad": 1, "standard": 1, "train": 1, "multilevel": 1, "vise": 1, "segment": 2, "word": 1, "howev": 1, "manual": 1, "focu": 1, "structur": 1, "caus": 1, "achiev": 1, "statist": 1, "bilingu": 2, "model": 2, "align": 1, "fact": 1, "propos": 1}, "marker": "(Xi et al., 2012)", "article": "D15-1142", "vector_2": [3, 0.1979655444052471, 5, 3, 0, 0]}, {"label": "Neut", "current": "Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.", "context": ["Since the first release of WordNet, researchers have tried to use it to simulate similarity.", "Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.", "Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011)."], "vector_1": {"synonymi": 1, "joubarn": 1, "releas": 1, "wordnet": 1, "lesk": 1, "use": 1, "describ": 1, "anoth": 1, "inkpen": 1, "except": 1, "research": 1, "languag": 1, "approach": 1, "across": 1, "differ": 1, "hyponymi": 1, "base": 1, "pair": 1, "sinc": 1, "tri": 1, "measur": 2, "algorithm": 1, "simul": 1, "vector": 2, "similar": 2, "first": 1}, "marker": "(Patwardhan and Pedersen, 2006)", "article": "W14-0118", "vector_2": [8, 0.16076516076516076, 4, 2, 9, 0]}, {"label": "Neut", "current": "Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective.", "context": ["Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima.", "Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective.", "Since then, there has been a large body of work addressing the flaws of the EM-based approach."], "vector_1": {"em": 2, "encount": 1, "via": 1, "obstacl": 1, "inappropri": 1, "ineffect": 1, "rewrit": 1, "two": 1, "stuck": 1, "bracket": 1, "larg": 1, "local": 1, "earli": 1, "flaw": 1, "get": 1, "induc": 1, "object": 1, "bodi": 1, "embas": 1, "address": 1, "approach": 1, "likelihood": 1, "sinc": 1, "addit": 1, "tendenc": 1, "grammar": 2, "constraint": 1, "seriou": 1, "unsupervis": 1, "work": 2, "rule": 1, "without": 1, "allow": 1, "learn": 1, "optima": 1}, "marker": "(Carroll and Charniak, 1992)", "article": "P08-1100", "vector_2": [16, 0.03462149643303918, 2, 2, 0, 0]}, {"label": "Pos", "current": "While our manually grouped subcorpora approximate those used by Chen et al (2013), exact agreement was impossible to obtain, illustrating that it is not trivial to manually generate optimal subcorpus labels.", "context": ["Table 3 lists the corpus statistics of the training data, split by manual subcorpus labels as used for the subcorpus VSM variant (see Section 3.2).", "While our manually grouped subcorpora approximate those used by Chen et al (2013), exact agreement was impossible to obtain, illustrating that it is not trivial to manually generate optimal subcorpus labels.", "We tokenize all Arabic data using MADA (Habash and Rambow, 2005), ATB scheme."], "vector_1": {"corpu": 1, "illustr": 1, "list": 1, "approxim": 1, "al": 1, "see": 1, "tabl": 1, "chen": 1, "et": 1, "trivial": 1, "vsm": 1, "use": 3, "group": 1, "exact": 1, "section": 1, "label": 2, "subcorpora": 1, "2": 1, "split": 1, "scheme": 1, "optim": 1, "gener": 1, "variant": 1, "agreement": 1, "obtain": 1, "arab": 1, "train": 1, "data": 2, "atb": 1, "mada": 1, "manual": 3, "token": 1, "statist": 1, "subcorpu": 3, "imposs": 1}, "marker": "(2013)", "article": "W15-2518", "vector_2": [2, 0.6161702971606122, 2, 6, 2, 0]}, {"label": "Neut", "current": "Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "context": ["In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.", "Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose."], "vector_1": {"corpu": 1, "domain": 1, "task": 1, "comput": 1, "techniqu": 1, "appli": 1, "use": 2, "tree": 2, "histor": 1, "research": 1, "other": 1, "cluster": 1, "corpusbas": 1, "statist": 1, "famili": 2, "reconstruct": 2, "linguist": 1, "method": 1, "languag": 2, "purpos": 1}, "marker": "Gray and Atkinson, 2003", "article": "P13-1112", "vector_2": [10, 0.9278944947094004, 7, 2, 0, 0]}, {"label": "Neut", "current": "The system developed by (Decadt et al., 2004) uses two cascaded memory-based classifiers, combined with the use of a genetic algorithm for joint parameter optimization and feature selection.", "context": ["In recent SENSEVAL-3 evaluations, the most successful approaches for all words word sense disambiguation relied on information drawn from annotated corpora.", "The system developed by (Decadt et al., 2004) uses two cascaded memory-based classifiers, combined with the use of a genetic algorithm for joint parameter optimization and feature selection.", "A separate \"word expert\" is learned for each ambiguous word, using a concatenated corpus of English sense"], "vector_1": {"corpu": 1, "featur": 1, "evalu": 1, "two": 1, "select": 1, "paramet": 1, "use": 3, "develop": 1, "expert": 1, "system": 1, "classifi": 1, "reli": 1, "disambigu": 1, "drawn": 1, "approach": 1, "sensev": 1, "optim": 1, "learn": 1, "genet": 1, "joint": 1, "sens": 2, "recent": 1, "concaten": 1, "word": 4, "algorithm": 1, "success": 1, "english": 1, "corpora": 1, "memorybas": 1, "annot": 1, "ambigu": 1, "separ": 1, "inform": 1, "combin": 1, "cascad": 1}, "marker": "(Decadt et al., 2004)", "article": "P05-3014", "vector_2": [1, 0.20631720430107528, 1, 3, 2, 0]}, {"label": "Neut", "current": "ponym/hypernym relations but also all 26 available semantic relations found in WordNet in addition to relations extracted from each of the eXtended WordNet (Harabagiu et al., 1999) synset's logical form.", "context": ["Figure 1: Example of the semantic network around the word car.", "ponym/hypernym relations but also all 26 available semantic relations found in WordNet in addition to relations extracted from each of the eXtended WordNet (Harabagiu et al., 1999) synset's logical form.", "To implement our idea, we created a weighted and directed semantic network based on the relations of WordNet and eXtended WordNet."], "vector_1": {"semant": 3, "creat": 1, "idea": 1, "extract": 1, "wordnet": 4, "form": 1, "ponymhypernym": 1, "avail": 1, "also": 1, "figur": 1, "synset": 1, "around": 1, "extend": 2, "relat": 4, "weight": 1, "direct": 1, "base": 1, "addit": 1, "word": 1, "car": 1, "network": 2, "exampl": 1, "logic": 1, "found": 1, "implement": 1}, "marker": "(Harabagiu et al., 1999)", "article": "S13-2019", "vector_2": [14, 0.19911958029307122, 1, 1, 0, 0]}, {"label": "Pos", "current": "There has been substantial work on spelling correction (see the excellent review by Kukich [1992]).", "context": ["Spelling correction is an important application for error-tolerant recognition.", "There has been substantial work on spelling correction (see the excellent review by Kukich [1992]).", "All methods essentially enumerate plausible candidates that resemble the incorrect word, and use additional heuristics to rank the results.'"], "vector_1": {"excel": 1, "rank": 1, "see": 1, "result": 1, "substanti": 1, "heurist": 1, "incorrect": 1, "use": 1, "resembl": 1, "review": 1, "enumer": 1, "recognit": 1, "import": 1, "method": 1, "applic": 1, "spell": 2, "candid": 1, "word": 1, "plausibl": 1, "addit": 1, "essenti": 1, "work": 1, "correct": 2, "errortoler": 1, "kukich": 1}, "marker": "[1992]", "article": "J96-1003", "vector_2": [4, 0.6369203608176326, 1, 1, 1, 1]}, {"label": "Neut", "current": "As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).", "context": ["In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.", "As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).", "Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation."], "vector_1": {"nlp": 1, "domain": 1, "corpora": 1, "sacaleanu": 1, "directli": 1, "text": 1, "continu": 1, "one": 1, "explor": 1, "pass": 1, "et": 1, "gildea": 1, "kilgarriff": 1, "system": 1, "cavaglia": 1, "roland": 1, "sekin": 1, "quantifi": 1, "type": 1, "relat": 2, "al": 1, "mention": 1, "address": 1, "line": 1, "buitelaar": 1, "present": 1, "work": 1, "item": 1, "disambigu": 1, "sens": 1, "similar": 1, "first": 1}, "marker": "(2001)", "article": "J03-3001", "vector_2": [2, 0.7800167075918285, 6, 2, 0, 0]}, {"label": "Neut", "current": "For example, the evidence relation of [Mann and Thompson, 1987] may hold between two text spans even if the speaker did not intend such a relation, all that is required is that the hearer's belief in the nucleus is increased though the understanding of the satellite.", "context": ["For non-illocutionary acts, the intention of the speaker is not relevant - these actions can be produced as side-effects of the speaker's intention, so that a determination of the intention is not necessary to determining whether the act was performed.", "For example, the evidence relation of [Mann and Thompson, 1987] may hold between two text spans even if the speaker did not intend such a relation, all that is required is that the hearer's belief in the nucleus is increased though the understanding of the satellite.", "For an illocutionary act, on the other hand, the recognition of communicative intention is crucial to understanding."], "vector_1": {"text": 1, "hearer": 1, "nonillocutionari": 1, "sideeffect": 1, "span": 1, "commun": 1, "perform": 1, "crucial": 1, "two": 1, "recognit": 1, "satellit": 1, "speaker": 3, "even": 1, "intend": 1, "belief": 1, "evid": 1, "though": 1, "may": 1, "relat": 2, "hand": 1, "intent": 4, "illocutionari": 1, "increas": 1, "hold": 1, "relev": 1, "requir": 1, "nucleu": 1, "whether": 1, "exampl": 1, "necessari": 1, "determin": 2, "act": 3, "action": 1, "understand": 2, "produc": 1}, "marker": "Mann and Thompson, 1987]", "article": "W93-0235", "vector_2": [6, 0.4754498598426621, 1, 1, 0, 0]}, {"label": "Neut", "current": "1 Across the Atlantic, a resurgence in empiricism was led by the success of the noisy-channel model in speech recognition (see Church and Mercer [1993] for references).", "context": ["Arguments raged, and it was not clear whether corpus work was an acceptable", "1 Across the Atlantic, a resurgence in empiricism was led by the success of the noisy-channel model in speech recognition (see Church and Mercer [1993] for references).", "334"], "vector_1": {"corpu": 1, "led": 1, "success": 1, "rage": 1, "whether": 1, "clear": 1, "work": 1, "argument": 1, "accept": 1, "recognit": 1, "atlant": 1, "empiric": 1, "see": 1, "church": 1, "resurg": 1, "speech": 1, "model": 1, "refer": 1, "mercer": 1, "across": 1, "noisychannel": 1}, "marker": "[1993]", "article": "J03-3001", "vector_2": [10, 0.152596005366681, 1, 1, 4, 1]}, {"label": "Neut", "current": "The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English.", "context": ["POS information for the source and the target languages was considered for both translation tasks that we have participated.", "The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English.", "The number of classes for English is 44, while Spanish is considered as a more inflectional language, and the tag set contains 376 different tags."], "vector_1": {"softwar": 1, "set": 1, "number": 1, "tag": 2, "spanish": 2, "languag": 2, "differ": 1, "perform": 1, "particip": 1, "avail": 1, "freel": 1, "inflect": 1, "sourc": 1, "tool": 1, "contain": 1, "postag": 1, "translat": 1, "consid": 2, "class": 1, "tnt": 1, "task": 1, "target": 1, "po": 1, "inform": 1, "english": 2}, "marker": "(Carreras et al., 2004)", "article": "W08-0315", "vector_2": [4, 0.7208838203848895, 2, 1, 0, 0]}, {"label": "Neut", "current": "Particularly, Thompson and Mulac (1991) consider this epistemic phrase has achieved a hedging state through a process of grammaticalization.", "context": ["Early discussion about interpreting epistemic phrases as hedges originated in the analysis I think.", "Particularly, Thompson and Mulac (1991) consider this epistemic phrase has achieved a hedging state through a process of grammaticalization.", "Their view is that I think is roughly similar to maybe when used to express the degree of speaker commitment, thus comprising a grammatical sub-category of adverbs."], "vector_1": {"origin": 1, "epistem": 2, "process": 1, "particularli": 1, "subcategori": 1, "phrase": 2, "use": 1, "adverb": 1, "degre": 1, "state": 1, "speaker": 1, "thompson": 1, "hedg": 2, "earli": 1, "express": 1, "mayb": 1, "mulac": 1, "analysi": 1, "consid": 1, "discuss": 1, "interpret": 1, "grammat": 1, "thu": 1, "roughli": 1, "achiev": 1, "compris": 1, "grammatic": 1, "commit": 1, "similar": 1, "think": 2, "view": 1}, "marker": "(1991)", "article": "W15-0302", "vector_2": [24, 0.4846324242004707, 1, 2, 0, 0]}, {"label": "Neut", "current": "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", "context": ["To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", "Plan of the paper."], "vector_1": {"detect": 2, "word": 1, "closest": 1, "unknown": 3, "howev": 1, "knowledg": 1, "next": 1, "known": 1, "step": 1, "paper": 1, "exist": 1, "plan": 1, "determin": 1, "complementari": 1, "date": 1, "sens": 3, "problem": 2, "approach": 2, "logic": 1, "view": 1}, "marker": "Burchardt et al., 2005)", "article": "N06-1017", "vector_2": [1, 0.15268411114108443, 3, 3, 0, 1]}, {"label": "Neut", "current": "Johnson, 2005), to address the problem.", "context": ["BioNLP 2007: Biological, translational, and clinical language processing, pages 209-216, Prague, June 2007. c2007 Association for Computational Linguistics", "Johnson, 2005), to address the problem.", "Reranking enables us to incorporate truly global features to the model of named entity tagging, and we aim to realize the state-of-the-art performance without depending on rule-based post-processes."], "vector_1": {"featur": 1, "process": 1, "global": 1, "biolog": 1, "tag": 1, "languag": 1, "entiti": 1, "depend": 1, "perform": 1, "postprocess": 1, "truli": 1, "rerank": 1, "bionlp": 1, "june": 1, "realiz": 1, "translat": 1, "address": 1, "problem": 1, "name": 1, "c": 1, "enabl": 1, "pragu": 1, "rulebas": 1, "clinic": 1, "us": 1, "aim": 1, "without": 1, "stateoftheart": 1, "incorpor": 1, "model": 1, "page": 1}, "marker": "Johnson, 2005)", "article": "W07-1033", "vector_2": [2, 0.12349635960747071, 1, 1, 0, 0]}, {"label": "Neut", "current": "Spejewski (1994) developed a tree-based model of the temporal structure of a sequence of sentences.", "context": ["In this respect, they differ from TDMs, which do not commit to specific rhetorical relations.", "Spejewski (1994) developed a tree-based model of the temporal structure of a sequence of sentences.", "Her approach is based on relations of temporal coordination and subordination, and is thus a major motivation for our own approach."], "vector_1": {"major": 1, "rhetor": 1, "motiv": 1, "respect": 1, "differ": 1, "develop": 1, "sequenc": 1, "approach": 2, "spejewski": 1, "tdm": 1, "sentenc": 1, "relat": 2, "coordin": 1, "base": 1, "commit": 1, "specif": 1, "thu": 1, "structur": 1, "tempor": 2, "treebas": 1, "model": 1, "subordin": 1}, "marker": "(1994)", "article": "W04-0208", "vector_2": [10, 0.7021703001270897, 1, 1, 0, 0]}, {"label": "Neut", "current": "In contrast, other approaches have used a pipelined approach, with separate models to first do tokenization and then part-of-speech tagging (Diab et al., 2007; Diab, 2009).", "context": ["1SAMA-v3.1 is an updated version of BAMA, with many significant differences in analysis.", "In contrast, other approaches have used a pipelined approach, with separate models to first do tokenization and then part-of-speech tagging (Diab et al., 2007; Diab, 2009).", "While these approaches have somewhat lower performance than the joint approach, they have the advantage that they do not rely on the presence of a full-blown morphological analyzer, which may not always be available or appropriate as the data shifts to different genres or Arabic dialects."], "vector_1": {"partofspeech": 1, "advantag": 1, "dialect": 1, "bama": 1, "signific": 1, "tag": 1, "avail": 1, "differ": 2, "fullblown": 1, "perform": 1, "morpholog": 1, "alway": 1, "reli": 1, "version": 1, "analyz": 1, "approach": 4, "contrast": 1, "updat": 1, "analysi": 1, "samav3": 1, "pipelin": 1, "may": 1, "presenc": 1, "use": 1, "arab": 1, "joint": 1, "somewhat": 1, "data": 1, "lower": 1, "appropri": 1, "shift": 1, "genr": 1, "separ": 1, "token": 1, "mani": 1, "model": 1, "first": 1}, "marker": "(Diab et al., 2007", "article": "P10-2063", "vector_2": [3, 0.0850498338870432, 2, 4, 4, 0]}, {"label": "Neut", "current": "[Traum and Hinkelmau, 1992] presents a multistratal theory of Conversation Acts,", "context": ["The cmly difference between speech acts and rhetorical relations is that the latter are explicitly concerned with the linkage of separate segments of language.", "[Traum and Hinkelmau, 1992] presents a multistratal theory of Conversation Acts,", "This material is based upon work supported in part by the NSF under research grant no."], "vector_1": {"rhetor": 1, "nsf": 1, "linkag": 1, "languag": 1, "concern": 1, "differ": 1, "explicitli": 1, "grant": 1, "latter": 1, "research": 1, "speech": 1, "relat": 1, "upon": 1, "cmli": 1, "base": 1, "theori": 1, "segment": 1, "multistrat": 1, "present": 1, "convers": 1, "materi": 1, "work": 1, "separ": 1, "part": 1, "act": 2, "support": 1}, "marker": "Traum and Hinkelmau, 1992]", "article": "W93-0235", "vector_2": [1, 0.2162944208337101, 1, 1, 1, 0]}, {"label": "Neut", "current": "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "context": ["Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).", "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training."], "vector_1": {"corpu": 1, "satisfactori": 1, "result": 2, "year": 1, "largescal": 1, "intermedi": 1, "research": 1, "build": 1, "method": 1, "machin": 1, "train": 1, "link": 1, "translat": 1, "associ": 1, "requir": 1, "recent": 1, "measur": 1, "word": 1, "align": 2, "employ": 1, "achiev": 1, "statist": 2, "bilingu": 1, "mani": 1, "model": 1, "propos": 1, "order": 1, "first": 1}, "marker": "Och and Ney, 2003", "article": "P05-1058", "vector_2": [2, 0.039255958147645806, 7, 2, 0, 0]}, {"label": "Neut", "current": "Difficulty prediction of C-tests has up to now only been performed on the paragraph level (KleinBraley, 1984; Traxel and Dresemann, 2010).", "context": ["Having introduced all four dimensions of C-test difficulty, we now report on the results of the actual difficulty prediction.", "Difficulty prediction of C-tests has up to now only been performed on the paragraph level (KleinBraley, 1984; Traxel and Dresemann, 2010).", "In this article, we go beyond paragraphs and predict the difficulty of gaps."], "vector_1": {"dimens": 1, "difficulti": 4, "actual": 1, "level": 1, "predict": 3, "articl": 1, "four": 1, "perform": 1, "gap": 1, "paragraph": 2, "introduc": 1, "go": 1, "report": 1, "beyond": 1, "ctest": 2, "result": 1}, "marker": "(KleinBraley, 1984", "article": "Q14-1040", "vector_2": [30, 0.6746049410193634, 2, 4, 7, 0]}, {"label": "Neut", "current": "Because hand-labeling individual words and word boundaries is very difficult (Jiao et al., 2006), producing segmented Chinese texts is very time-consuming and expensive.", "context": ["However, the reliability of CWS that can be achieved using machine learning techniques relies heavily on the availability of a large amount of high-quality, manually segmented data.", "Because hand-labeling individual words and word boundaries is very difficult (Jiao et al., 2006), producing segmented Chinese texts is very time-consuming and expensive.", "Although a number of manually segmented datasets have been constructed by various organizations, it is not feasible to combine them into a single complete dataset because of their incompatibility due to the use of various segmenting standards."], "vector_1": {"due": 1, "text": 1, "individu": 1, "number": 1, "timeconsum": 1, "dataset": 2, "heavili": 1, "incompat": 1, "cw": 1, "use": 2, "techniqu": 1, "data": 1, "boundari": 1, "achiev": 1, "construct": 1, "avail": 1, "reli": 1, "larg": 1, "singl": 1, "difficult": 1, "machin": 1, "variou": 2, "chines": 1, "standard": 1, "feasibl": 1, "although": 1, "segment": 4, "highqual": 1, "word": 2, "organ": 1, "howev": 1, "manual": 2, "produc": 1, "amount": 1, "combin": 1, "expens": 1, "learn": 1, "handlabel": 1, "complet": 1, "reliabl": 1}, "marker": "(Jiao et al., 2006)", "article": "D15-1142", "vector_2": [9, 0.06569168065212179, 1, 2, 0, 0]}, {"label": "Pos", "current": "In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010).", "context": ["Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks.", "In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010).", "It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models."], "vector_1": {"essay": 1, "predict": 1, "proven": 1, "critic": 1, "multidocu": 1, "summar": 1, "entiti": 1, "use": 2, "appear": 1, "perform": 1, "readabl": 1, "compon": 1, "also": 1, "stori": 1, "score": 1, "varieti": 1, "applic": 1, "sentenc": 2, "gener": 1, "initi": 1, "grid": 1, "sinc": 1, "addit": 1, "wide": 1, "task": 1, "remain": 1, "combin": 1, "stateoftheart": 1, "independentlytrain": 1, "model": 2, "propos": 1, "typic": 1, "order": 2, "first": 1}, "marker": "(McIntyre and Lapata, 2010)", "article": "P11-2022", "vector_2": [1, 0.15667131566713158, 8, 1, 11, 0]}, {"label": "Neut", "current": "of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs.", "context": ["3Relative to the current word, whose tag is assigned a probability value by the MEMM.", "of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs.", "The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000)."], "vector_1": {"often": 1, "fair": 1, "gaussian": 1, "tag": 1, "explor": 1, "baselin": 1, "whose": 1, "use": 3, "prior": 1, "overlook": 1, "label": 1, "current": 1, "favor": 1, "adapt": 1, "rel": 1, "paramet": 1, "probabl": 1, "build": 1, "map": 1, "memm": 2, "sequenc": 1, "hmm": 2, "train": 1, "likelihood": 1, "model": 3, "valu": 1, "case": 1, "word": 1, "work": 1, "smooth": 2, "maximum": 1, "maxent": 1, "discrimin": 1, "present": 1, "inadequ": 1, "problem": 1, "assign": 1}, "marker": "(Collins, 2002)", "article": "W04-3237", "vector_2": [2, 0.3400062004517472, 2, 1, 0, 0]}, {"label": "Neut", "current": "The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.", "context": ["A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).", "The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.", "Precision is 41.6%, and recall is 23.3%, which we will show is worse than a simple baseline of assuming every syllable is a word."], "vector_1": {"corpu": 1, "infant": 1, "evalu": 1, "show": 1, "transit": 2, "wors": 1, "number": 1, "procedur": 1, "daysold": 1, "result": 1, "onlin": 1, "artifici": 1, "assum": 1, "surprisingli": 1, "languag": 1, "probabl": 2, "even": 1, "syllabifi": 1, "use": 4, "support": 1, "perfectli": 1, "learner": 1, "young": 1, "tp": 2, "research": 1, "identif": 1, "reli": 1, "experiment": 2, "unit": 1, "basic": 1, "input": 1, "precis": 1, "baselin": 1, "local": 1, "poor": 1, "assumpt": 1, "recal": 1, "everi": 1, "syllab": 1, "base": 1, "segment": 2, "studi": 1, "word": 2, "16": 1, "simpl": 1, "work": 1, "psychologicallymotiv": 1, "syllabl": 3, "inform": 1, "alreadi": 1, "learn": 2, "model": 1, "psycholog": 1}, "marker": "(Bijeljac-Babic et al., 1993)", "article": "W10-2912", "vector_2": [17, 0.17841787185423494, 5, 1, 0, 0]}, {"label": "Pos", "current": "To limit the permutations, we used an approach as in (Kanthak et al., 2005).", "context": ["a sequence of nodes with one incoming arc and one outgoing arc, the words of source language text are placed consecutively in the arcs of the acceptor, 2. an acceptor containing possible permutations.", "To limit the permutations, we used an approach as in (Kanthak et al., 2005).", "Each of these two acceptors results in different constraints for the generation of the hypotheses."], "vector_1": {"text": 1, "one": 2, "permut": 2, "result": 1, "incom": 1, "languag": 1, "use": 1, "two": 1, "outgo": 1, "consecut": 1, "place": 1, "approach": 1, "node": 1, "sourc": 1, "sequenc": 1, "gener": 1, "differ": 1, "acceptor": 3, "word": 1, "possibl": 1, "hypothes": 1, "constraint": 1, "arc": 3, "limit": 1, "contain": 1}, "marker": "(Kanthak et al., 2005)", "article": "P06-2061", "vector_2": [1, 0.8316917293233083, 1, 2, 16, 1]}, {"label": "Neut", "current": "To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and Ramat, 2006).", "context": ["2 Approach", "To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and Ramat, 2006).", "If the reconstructed tree is sufficiently similar to the original Indo-European family tree, it will support the hypothesis."], "vector_1": {"origin": 1, "whose": 1, "famili": 2, "hypothesi": 2, "text": 1, "tongu": 1, "mother": 1, "tree": 3, "indoeuropean": 2, "one": 1, "written": 1, "suffici": 1, "speaker": 1, "examin": 1, "english": 2, "reconstruct": 2, "similar": 1, "approach": 1, "support": 1, "languag": 2, "nonn": 1}, "marker": "Ramat and Ramat, 2006)", "article": "P13-1112", "vector_2": [7, 0.14506977457445178, 2, 1, 0, 0]}, {"label": "Neut", "current": "A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible.", "context": ["Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.", "A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible.", "3 MEMM for Sequence Labeling"], "vector_1": {"domain": 1, "featur": 1, "weight": 1, "tag": 1, "techniqu": 1, "perform": 1, "label": 1, "memm": 1, "adapt": 3, "way": 1, "taken": 1, "new": 1, "approach": 2, "final": 1, "contrast": 1, "deriv": 1, "extend": 1, "unclear": 1, "sequenc": 2, "probabilist": 2, "although": 1, "formal": 1, "sound": 1, "remark": 1, "scenario": 1, "possibl": 1, "rulebas": 2, "maxent": 1, "easili": 1, "crf": 1, "allow": 1, "model": 4, "principl": 1}, "marker": "(Ratnaparkhi, 1996)", "article": "W04-3237", "vector_2": [8, 0.3690597457814784, 2, 3, 0, 0]}, {"label": "Neut", "current": "Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).", "context": ["It should be noted that there are a also number of orthogonal approaches for accelerating natural language parsers.", "Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).", "These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest."], "vector_1": {"acceler": 1, "often": 1, "orthogon": 1, "natur": 1, "approxim": 1, "parser": 1, "number": 1, "languag": 1, "use": 1, "space": 1, "note": 1, "also": 1, "reli": 1, "interest": 2, "appli": 1, "approach": 2, "final": 1, "pars": 1, "constrain": 1, "search": 1, "grammar": 1, "prune": 1, "possibl": 1, "tree": 1, "coars": 2, "model": 2}, "marker": "Petrov and Klein, 2007b)", "article": "W11-2921", "vector_2": [4, 0.9485065054059006, 3, 1, 7, 1]}, {"label": "Neut", "current": "Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).", "context": ["1 Introduction and Motivation", "Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).", "One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011)."], "vector_1": {"rang": 2, "help": 1, "intellig": 1, "challeng": 1, "one": 1, "topic": 1, "see": 1, "motiv": 1, "abil": 1, "increasingli": 1, "sever": 1, "detect": 1, "hoo": 1, "learner": 1, "research": 1, "write": 1, "languag": 2, "recent": 1, "type": 1, "assist": 1, "learn": 1, "autom": 1, "ical": 1, "becom": 1, "strand": 1, "introduct": 1, "task": 1, "computerassist": 1, "focu": 1, "exampl": 1, "determin": 1, "error": 2, "popular": 1}, "marker": "(Attali and Burstein, 2006", "article": "W12-2011", "vector_2": [6, 0.028805456593891783, 5, 1, 0, 0]}, {"label": "Neut", "current": "Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.", "context": ["The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.", "Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.", "In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues."], "vector_1": {"interfer": 2, "pronoun": 1, "definit": 1, "strongli": 1, "davidsennielsen": 1, "spanish": 1, "research": 1, "languag": 2, "smith": 1, "would": 1, "transfer": 2, "tapper": 1, "swan": 1, "aart": 1, "sometim": 1, "littl": 1, "articl": 1, "write": 1, "overusedunderus": 1, "altenberg": 1, "speech": 1, "answer": 1, "probabl": 1, "po": 1, "contrast": 1, "relat": 1, "tongu": 3, "mother": 3, "usag": 1, "french": 1, "part": 1, "granger": 1, "known": 1, "neg": 1, "modifi": 1, "possess": 1, "reveal": 1, "grammat": 1, "word": 1, "work": 1, "harder": 1, "across": 1, "item": 1, "anoth": 1, "allow": 1, "english": 1}, "marker": "(2001)", "article": "P13-1112", "vector_2": [12, 0.04195675509891121, 4, 2, 0, 0]}, {"label": "Neut", "current": "The former includes Relational Markov Networks by Bunescu et al (2004) and skip-edge CRFs by Sutton et al (2004).", "context": ["Such systems may be classified into two kinds, one of them uses a single classifier which is optimized incorporating non-local features, and the other consists of pipeline of more than one classifiers.", "The former includes Relational Markov Networks by Bunescu et al (2004) and skip-edge CRFs by Sutton et al (2004).", "A major drawback of this kind of systems may be heavy computational cost of inference both for training and running the systems, because non-local dependency forces such models to use expensive approximate inference instead of dynamic-programming-based exact inference."], "vector_1": {"major": 1, "featur": 1, "comput": 1, "approxim": 1, "al": 2, "one": 2, "heavi": 1, "cost": 1, "et": 2, "use": 2, "depend": 1, "network": 1, "system": 3, "classifi": 3, "forc": 1, "optim": 1, "drawback": 1, "includ": 1, "instead": 1, "bunescu": 1, "infer": 3, "singl": 1, "model": 1, "pipelin": 1, "nonloc": 2, "run": 1, "dynamicprogrammingbas": 1, "skipedg": 1, "may": 2, "relat": 1, "train": 1, "two": 1, "exact": 1, "former": 1, "sutton": 1, "kind": 2, "consist": 1, "crf": 1, "expens": 1, "incorpor": 1, "markov": 1}, "marker": "(2004)", "article": "W07-1033", "vector_2": [3, 0.14565527065527065, 2, 2, 4, 0]}, {"label": "Neut", "current": "Since the governing category information is encoded in the path feature, it may be redundant; indeed this redundancy might explain why the governing category feature was used in Gildea & Jurafsky(2002) but not in Gildea and Palmer(2002).", "context": ["\"the international Olympic Conference held in Paris\" Figure 1 Example of DE construction", "Since the governing category information is encoded in the path feature, it may be redundant; indeed this redundancy might explain why the governing category feature was used in Gildea & Jurafsky(2002) but not in Gildea and Palmer(2002).", "Since the \"DE\" construction caused us to modify the feature for Chinese, we conducted several experiments to test whether the governing category feature is useful or whether it is redundant with the path and position features."], "vector_1": {"featur": 5, "encod": 1, "held": 1, "intern": 1, "sever": 1, "caus": 1, "use": 2, "gildea": 2, "modifi": 1, "explain": 1, "construct": 2, "categori": 3, "figur": 1, "conduct": 1, "test": 1, "experi": 1, "might": 1, "palmer": 1, "may": 1, "chines": 1, "de": 2, "confer": 1, "pari": 1, "govern": 3, "path": 2, "redund": 3, "olymp": 1, "sinc": 2, "jurafski": 1, "whether": 2, "us": 1, "inde": 1, "inform": 1, "exampl": 1, "posit": 1}, "marker": "(2002)", "article": "N04-1032", "vector_2": [2, 0.34782465174622035, 2, 4, 0, 0]}, {"label": "Neut", "current": "Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "context": ["In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.", "Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose."], "vector_1": {"corpu": 1, "domain": 1, "task": 1, "comput": 1, "techniqu": 1, "appli": 1, "use": 2, "tree": 2, "histor": 1, "research": 1, "other": 1, "cluster": 1, "corpusbas": 1, "statist": 1, "famili": 2, "reconstruct": 2, "linguist": 1, "method": 1, "languag": 2, "purpos": 1}, "marker": "Barbancon et al., 2007", "article": "P13-1112", "vector_2": [6, 0.9278944947094004, 7, 2, 2, 0]}, {"label": "Neut", "current": "Another possibility is to explore semi-supervised extensions to boosting (d'Alche Buc et al., 2002).", "context": ["It remains to be seen whether this approach would be effective for information extraction.", "Another possibility is to explore semi-supervised extensions to boosting (d'Alche Buc et al., 2002).", "Boosting is a highly effective ensemble learning technique, and BWI uses boosting to tune the weights of the learned patterns, so if we generalize boosting to handle unlabelled data, then the learned weights may well be more effective than those calculated by TPLEX."], "vector_1": {"weight": 2, "explor": 1, "seen": 1, "extract": 1, "highli": 1, "use": 1, "techniqu": 1, "would": 1, "semisupervis": 1, "anoth": 1, "tplex": 1, "calcul": 1, "may": 1, "pattern": 1, "boost": 4, "unlabel": 1, "handl": 1, "gener": 1, "ensembl": 1, "effect": 3, "extens": 1, "bwi": 1, "data": 1, "tune": 1, "possibl": 1, "whether": 1, "approach": 1, "well": 1, "inform": 1, "remain": 1, "learn": 3}, "marker": "Alche Buc et al., 2002)", "article": "W06-2204", "vector_2": [4, 0.9539508707886531, 1, 1, 0, 0]}, {"label": "Neut", "current": "We trained 45-state HMMs on all 49208 sentences, 11-state PCFGs on WSJ-10 (7424 sentences) and DMVs on WSJ-20 (25523 sentences) (Klein and Manning, 2004).", "context": ["We binarized the PCFG trees and created gold dependency trees according to the Collins head rules.", "We trained 45-state HMMs on all 49208 sentences, 11-state PCFGs on WSJ-10 (7424 sentences) and DMVs on WSJ-20 (25523 sentences) (Klein and Manning, 2004).", "We ran EM for 100 iterations with the parameters initialized uniformly (always plus a small amount of random noise)."], "vector_1": {"em": 1, "creat": 1, "ran": 1, "random": 1, "head": 1, "iter": 1, "small": 1, "pcfg": 2, "paramet": 1, "depend": 1, "gold": 1, "alway": 1, "collin": 1, "0": 1, "state": 2, "accord": 1, "sentenc": 3, "initi": 1, "binar": 1, "hmm": 1, "train": 1, "uniformli": 1, "nois": 1, "wsj": 2, "tree": 2, "rule": 1, "amount": 1, "dmv": 1, "plu": 1}, "marker": "(Klein and Manning, 2004)", "article": "P08-1100", "vector_2": [4, 0.1822023869898908, 1, 5, 2, 1]}, {"label": "Neut", "current": "Fletcher (2002) demonstrates methods for gathering and using Web corpora in a language-teaching context.", "context": ["Varantola (2000) shows how translators can use \"just-in-time\" sublanguage corpora to choose correct target language terms for areas in which they are not expert.", "Fletcher (2002) demonstrates methods for gathering and using Web corpora in a language-teaching context.", "2.2 The 100M Words of the BNC"], "vector_1": {"corpora": 2, "show": 1, "sublanguag": 1, "fletcher": 1, "languag": 1, "languageteach": 1, "web": 1, "use": 2, "expert": 1, "area": 1, "justintim": 1, "bnc": 1, "correct": 1, "translat": 1, "demonstr": 1, "varantola": 1, "term": 1, "word": 1, "target": 1, "gather": 1, "m": 1, "method": 1, "context": 1, "choos": 1}, "marker": "(2002)", "article": "J03-3001", "vector_2": [1, 0.26372680555907146, 2, 1, 0, 0]}, {"label": "Neut", "current": "by clue words such as \"so\", \"no\", \"okay\", purpose clauses) are hypothesized by the Speech Act Interpreter [Heeman, 1993] and used by the Dialogue Manager [Traum, 1993] to guide further interpretation.", "context": ["Those relations that are conventionally signalled by surface features (e.g.", "by clue words such as \"so\", \"no\", \"okay\", purpose clauses) are hypothesized by the Speech Act Interpreter [Heeman, 1993] and used by the Dialogue Manager [Traum, 1993] to guide further interpretation.", "In the case of more implicit relationships we often do not identify the precise relation, merely operating on the speech act level forms."], "vector_1": {"oper": 1, "featur": 1, "identifi": 1, "often": 1, "eg": 1, "guid": 1, "mere": 1, "surfac": 1, "form": 1, "convent": 1, "speech": 2, "use": 1, "claus": 1, "okay": 1, "relationship": 1, "relat": 2, "dialogu": 1, "clue": 1, "implicit": 1, "interpret": 2, "case": 1, "word": 1, "level": 1, "hypothes": 1, "manag": 1, "precis": 1, "act": 2, "purpos": 1, "signal": 1}, "marker": "Traum, 1993]", "article": "W93-0235", "vector_2": [0, 0.7326159688941134, 2, 1, 1, 0]}, {"label": "Neut", "current": "There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).", "context": ["We will present an alternative to pre-calculation: on-demand paraphrasing, as described in Marton (to appear 2012).", "There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).", "This enables constructing large vector representation, since there is no longer a need to compute a whole matrix."], "vector_1": {"represent": 1, "comput": 1, "suffix": 2, "ondemand": 2, "text": 1, "prefix": 1, "monolingu": 1, "done": 1, "paraphras": 1, "need": 1, "array": 1, "longer": 1, "describ": 1, "construct": 1, "matrix": 1, "larg": 1, "altern": 1, "resourc": 1, "precalcul": 1, "link": 1, "sinc": 1, "present": 1, "appear": 1, "search": 1, "enabl": 1, "tree": 1, "marton": 1, "vector": 1, "whole": 1}, "marker": "Gusfield, 1997", "article": "N12-4007", "vector_2": [15, 0.7857573928786964, 3, 6, 0, 0]}, {"label": "Pos", "current": "In the first maximum alignment based approach we will consider, the definitions of sz,pred and sz, are inspired by Mihalcea et al (2006) who normalize phrasal similarities according to the phrase length.", "context": ["3.2 Maximum alignment (precision-recall average)", "In the first maximum alignment based approach we will consider, the definitions of sz,pred and sz, are inspired by Mihalcea et al (2006) who normalize phrasal similarities according to the phrase length.", "1 Si,pred = 2 1precei,pred,fi,pred + recei,pred,fi,pred 1 =2 1precei,j,fi,j + recei,j ,fi,j"], "vector_1": {"szpred": 1, "inspir": 1, "al": 1, "sipr": 1, "et": 1, "phrase": 1, "definit": 1, "preceipredfipr": 1, "precisionrecal": 1, "receij": 1, "averag": 1, "fij": 1, "approach": 1, "receipredfipr": 1, "accord": 1, "normal": 1, "base": 1, "consid": 1, "mihalcea": 1, "sz": 1, "align": 2, "maximum": 2, "length": 1, "phrasal": 1, "preceijfij": 1, "similar": 1, "first": 1}, "marker": "(2006)", "article": "W14-4719", "vector_2": [8, 0.4131206560328016, 1, 5, 0, 0]}, {"label": "Neut", "current": " Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].", "context": ["Areas of investigation using bilingual corpora have included the following:", " Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].", "Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991]."], "vector_1": {"use": 1, "investig": 1, "area": 1, "sentenc": 1, "align": 1, "corpora": 1, "automat": 1, "disambigu": 1, "includ": 1, "bilingu": 1, "follow": 1, "wordsens": 1}, "marker": "Kay and Roscheisen, 1988, ", "article": "P93-1003", "vector_2": [5, 0.05210931869837574, 6, 10, 0, 0]}, {"label": "Neut", "current": "We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).", "context": ["The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).", "We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).", "Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario."], "vector_1": {"gaussian": 2, "one": 1, "paramet": 1, "awar": 1, "use": 3, "techniqu": 1, "exponenti": 1, "adapt": 3, "build": 1, "map": 2, "deriv": 1, "extend": 1, "memm": 1, "previou": 1, "differ": 1, "although": 1, "easili": 1, "present": 1, "formal": 1, "scenario": 1, "work": 2, "smooth": 2, "maxent": 2, "prior": 3, "crf": 1, "model": 4}, "marker": "(Goodman, 2004)", "article": "W04-3237", "vector_2": [0, 0.35134416936091056, 2, 1, 0, 0]}, {"label": "Neut", "current": "Reiter and Dale (2000) state that for example aggregating multiple sentences does not change the information they express, but improves the readability and fluency of the text.", "context": ["According to Deane and Sheehan (2003), it is possible to change the wording of a text without changing its difficulty.", "Reiter and Dale (2000) state that for example aggregating multiple sentences does not change the information they express, but improves the readability and fluency of the text.", "This is what we want to achieve: adding variation to the text without affecting its interpretation."], "vector_1": {"dean": 1, "ad": 1, "text": 3, "fluenci": 1, "aggreg": 1, "want": 1, "multipl": 1, "sheehan": 1, "readabl": 1, "state": 1, "without": 2, "variat": 1, "dale": 1, "difficulti": 1, "accord": 1, "sentenc": 1, "express": 1, "affect": 1, "interpret": 1, "word": 1, "possibl": 1, "reiter": 1, "inform": 1, "achiev": 1, "exampl": 1, "improv": 1, "chang": 3}, "marker": "(2000)", "article": "W11-1403", "vector_2": [11, 0.495495231966302, 2, 2, 0, 0]}, {"label": "CoCo", "current": "A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "context": ["6 Related Work", "A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization."], "vector_1": {"finegrain": 1, "none": 1, "relat": 2, "last": 1, "compar": 1, "directli": 1, "possibl": 1, "provid": 1, "natur": 1, "howev": 1, "work": 3, "parser": 1, "two": 1, "bodi": 1, "accumul": 1, "much": 1, "substanti": 1, "gpu": 1, "decad": 1, "parallel": 2, "languag": 1}, "marker": "Pontelli et al., 1998", "article": "W11-2921", "vector_2": [13, 0.8736092567868269, 4, 2, 0, 0]}, {"label": "Neut", "current": "(Niehues and Kolss, 2009).", "context": ["To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction.", "(Niehues and Kolss, 2009).", "When translating from English to German, most of the changes in word order consist in a shift to the right while typical word shifts in German to English translations take place in the reverse direction."], "vector_1": {"right": 1, "german": 3, "appli": 1, "longrang": 1, "direct": 2, "chang": 1, "differ": 1, "noncontinu": 1, "depend": 1, "take": 1, "type": 1, "reorder": 2, "translat": 3, "model": 1, "word": 2, "consist": 1, "shift": 2, "revers": 1, "rule": 1, "place": 1, "english": 3, "typic": 1, "order": 1}, "marker": "(Niehues and Kolss, 2009)", "article": "W10-1719", "vector_2": [1, 0.3186050992229144, 1, 2, 4, 0]}, {"label": "Pos", "current": "To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).", "context": ["The extracted translations may serve as training data for statistical machine translation systems.", "To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).", "We then added the extracted translation pairs as additional parallel training corpus."], "vector_1": {"corpu": 1, "ad": 1, "evalu": 1, "text": 1, "fbi": 1, "extract": 2, "baselin": 1, "smt": 1, "system": 2, "machin": 1, "may": 1, "effect": 1, "train": 3, "translat": 3, "pair": 1, "data": 1, "parallel": 2, "addit": 1, "serv": 1, "phrasebas": 1, "statist": 1, "purpos": 1, "chineseenglish": 1}, "marker": "(NIST, 2003)", "article": "P08-1113", "vector_2": [5, 0.9291700845251257, 3, 1, 0, 0]}, {"label": "Neut", "current": "Other assessments, such as the TOEFL Practice Online Speaking test, on the other hand, focus on more spontaneous, high-entropy responses (Zechner et al., 2007).", "context": ["Entropy in this context can be seen as a measure for how predictable the language in the expected spoken response is: Some tests, such as SET-10 (Bernstein 1999), are focused mostly on the lower entropy aspects of language, using tasks such as \"reading\" or \"repetition\", where the expected sequence of words is highly predictable.", "Other assessments, such as the TOEFL Practice Online Speaking test, on the other hand, focus on more spontaneous, high-entropy responses (Zechner et al., 2007).", "In this paper, we describe a spoken language test with heterogeneous task types, ranging from read speech to tasks that require candidates to give their opinions on an issue, whose goal is to assess communicative competence (Bachman, 1990; Bachman & Palmer, 1996); we call this test THT (Test with Heterogeneous Tasks)."], "vector_1": {"rang": 1, "set": 1, "entropi": 2, "give": 1, "predict": 2, "focus": 1, "call": 1, "assess": 2, "paper": 1, "expect": 2, "aspect": 1, "respons": 2, "seen": 1, "spontan": 1, "languag": 3, "highli": 1, "whose": 1, "use": 1, "describ": 1, "commun": 1, "heterogen": 2, "spoken": 2, "speech": 1, "test": 5, "compet": 1, "bernstein": 1, "type": 1, "onlin": 1, "read": 2, "sequenc": 1, "hand": 1, "speak": 1, "candid": 1, "mostli": 1, "repetit": 1, "requir": 1, "goal": 1, "measur": 1, "lower": 1, "task": 4, "word": 1, "practic": 1, "tht": 1, "focu": 1, "highentropi": 1, "issu": 1, "context": 1, "opinion": 1, "toefl": 1}, "marker": "(Zechner et al., 2007)", "article": "W08-0912", "vector_2": [1, 0.03805944221597549, 3, 3, 4, 1]}, {"label": "Neut", "current": "To extract (and then highlight) nominal terminology we use the NP extractor described in (Sheremetyeva, 2009).", "context": ["This task is performed based on the results of a shallow analysis performed by a hybrid NP extractor and NP and predicate term chunkers which in succession run on the same claim text.", "To extract (and then highlight) nominal terminology we use the NP extractor described in (Sheremetyeva, 2009).", "The extraction methodology combines statistical techniques, heuristics and a very shallow linguistic knowledge extracted from the main system lexicon (see Section 5.1)."], "vector_1": {"extractor": 2, "lexicon": 1, "claim": 1, "text": 1, "knowledg": 1, "terminolog": 1, "see": 1, "result": 1, "heurist": 1, "extract": 3, "use": 1, "describ": 1, "nomin": 1, "perform": 2, "section": 1, "hybrid": 1, "system": 1, "np": 3, "main": 1, "analysi": 1, "run": 1, "methodolog": 1, "shallow": 2, "base": 1, "techniqu": 1, "term": 1, "task": 1, "success": 1, "chunker": 1, "predic": 1, "combin": 1, "statist": 1, "highlight": 1, "linguist": 1}, "marker": "(Sheremetyeva, 2009)", "article": "W14-5605", "vector_2": [5, 0.6541683590576767, 1, 1, 4, 1]}, {"label": "Neut", "current": "We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).", "context": ["4 Approximation error", "We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).", "We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models."], "vector_1": {"em": 3, "point": 1, "predict": 1, "approxim": 2, "increas": 1, "inform": 1, "decreas": 1, "follow": 1, "pcfg": 1, "incorrect": 1, "supervis": 1, "log": 1, "author": 1, "perform": 1, "diverg": 1, "px": 1, "show": 1, "question": 1, "accuraci": 2, "start": 1, "figur": 1, "estim": 1, "analyz": 1, "similarli": 1, "discrep": 1, "trend": 1, "experi": 1, "run": 1, "optim": 1, "act": 1, "initi": 2, "hmm": 1, "believ": 1, "mani": 1, "bgen": 1, "argmax": 1, "contain": 1, "likelihood": 3, "discuss": 1, "e": 1, "valuabl": 1, "specif": 1, "confront": 1, "iter": 1, "p": 3, "bias": 1, "behav": 1, "dmv": 1, "error": 2, "found": 1, "model": 3, "surrog": 1}, "marker": "Haghighi and Klein, 2006)", "article": "P08-1100", "vector_2": [2, 0.3141630320857423, 3, 1, 6, 1]}, {"label": "Neut", "current": "D(B  ||B0) can then be computed by finding a maximum weighted bipartite matching on M using the O(K3) Hungarian algorithm (Kuhn, 1955).", "context": ["We can form a K xK matrix M, where each entry MZj is the distance between the parameters involving label i of B and label j of B0.", "D(B  ||B0) can then be computed by finding a maximum weighted bipartite matching on M using the O(K3) Hungarian algorithm (Kuhn, 1955).", "For models such as the HMM and PCFG, computing D is NP-hard, since the summation in d (1) contains both first-order terms which depend on one label (e.g., emission parameters) and higher-order terms which depend on more than one label (e.g., transitions or rewrites)."], "vector_1": {"xk": 1, "distanc": 1, "comput": 2, "weight": 1, "eg": 2, "emiss": 1, "one": 2, "bipartit": 1, "higherord": 1, "summat": 1, "hungarian": 1, "find": 1, "pcfg": 1, "paramet": 2, "involv": 1, "use": 1, "nphard": 1, "depend": 2, "matrix": 1, "label": 4, "rewrit": 1, "transit": 1, "match": 1, "firstord": 1, "form": 1, "db": 1, "hmm": 1, "sinc": 1, "mzj": 1, "term": 2, "b": 3, "ok": 1, "algorithm": 1, "entri": 1, "k": 1, "j": 1, "maximum": 1, "contain": 1, "model": 1}, "marker": "(Kuhn, 1955)", "article": "P08-1100", "vector_2": [53, 0.7707678263515569, 1, 1, 0, 0]}, {"label": "Neut", "current": "Heuristic Kernel Expansion (SVM-HKE) To make the weight vector sparse, Kudo and Matsumoto (2003) proposed a heuristic method that filters out less useful features whose absolute weight values are less than a pre-defined threshold .2 They reported that increased threshold value  resulted in a dramatically sparse feature space Fd, which had the side-effects of accuracy degradation and classifier speed-up.", "context": ["1) is O(|xd|), which is linear with respect to the number of active features in xd within the expanded feature space Fd.", "Heuristic Kernel Expansion (SVM-HKE) To make the weight vector sparse, Kudo and Matsumoto (2003) proposed a heuristic method that filters out less useful features whose absolute weight values are less than a pre-defined threshold .2 They reported that increased threshold value  resulted in a dramatically sparse feature space Fd, which had the side-effects of accuracy degradation and classifier speed-up.", "3 Proposed Method"], "vector_1": {"kernel": 1, "featur": 4, "weight": 2, "less": 2, "within": 1, "number": 1, "xd": 1, "matsumoto": 1, "increas": 1, "result": 1, "heurist": 2, "threshold": 2, "respect": 1, "predefin": 1, "sideeffect": 1, "whose": 1, "use": 1, "space": 2, "make": 1, "activ": 1, "classifi": 1, "accuraci": 1, "valu": 2, "svmhke": 1, "method": 2, "spars": 2, "kudo": 1, "linear": 1, "absolut": 1, "expans": 1, "speedup": 1, "fd": 2, "report": 1, "expand": 1, "dramat": 1, "filter": 1, "vector": 1, "oxd": 1, "degrad": 1, "propos": 2}, "marker": "(2003)", "article": "D09-1160", "vector_2": [6, 0.2591334679640169, 1, 7, 4, 0]}, {"label": "Neut", "current": "(Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993).", "context": ["We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component.", "(Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993).", "Questioning the traditional n-grams, Magerman already advocates a heavier reliance on contextual information."], "vector_1": {"featur": 2, "ad": 1, "wall": 1, "treebank": 1, "move": 1, "penn": 1, "syntax": 1, "magerman": 1, "street": 1, "significantli": 1, "beyond": 1, "languag": 1, "use": 1, "question": 1, "system": 1, "compon": 1, "advoc": 1, "decis": 1, "sentenc": 1, "spatter": 1, "heavier": 1, "simpl": 1, "introduc": 1, "machin": 1, "knowledg": 1, "deriv": 1, "extend": 1, "far": 1, "sequenc": 1, "journal": 1, "express": 1, "sophist": 1, "ngram": 1, "train": 1, "pars": 2, "background": 1, "particular": 1, "increas": 1, "tradit": 1, "relianc": 1, "work": 1, "tree": 1, "contextu": 1, "inform": 1, "alreadi": 1, "limit": 1, "learn": 1, "action": 2, "model": 1, "similar": 1}, "marker": "(Marcus et al., 1993)", "article": "P97-1062", "vector_2": [4, 0.8782098057611453, 2, 1, 1, 0]}, {"label": "Neut", "current": "In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).", "context": ["Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.", "In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).", "However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation."], "vector_1": {"corpu": 1, "featur": 1, "process": 1, "decreas": 1, "loglinear": 1, "cw": 2, "sever": 1, "leverag": 1, "accur": 1, "develop": 1, "semisupervis": 1, "rather": 1, "data": 2, "smt": 1, "figur": 1, "translat": 1, "previous": 1, "method": 1, "unlabel": 1, "machin": 1, "perplex": 1, "cascad": 1, "standard": 1, "train": 1, "multilevel": 1, "vise": 1, "segment": 2, "word": 1, "howev": 1, "manual": 1, "focu": 1, "structur": 1, "caus": 1, "achiev": 1, "statist": 1, "bilingu": 2, "model": 2, "align": 1, "fact": 1, "propos": 1}, "marker": "(Chung et al., 2009)", "article": "D15-1142", "vector_2": [6, 0.1979655444052471, 5, 2, 0, 0]}, {"label": "Neut", "current": "(Matsuo and Ishizuka, 2004) proposed a co-occurrence distribution based method using a clustering strategy for extracting keywords for a single document without relying on a large corpus, and reported promising results.", "context": ["However, we may not have a big background collection that matches the test domain for a reliable IDF estimate.", "(Matsuo and Ishizuka, 2004) proposed a co-occurrence distribution based method using a clustering strategy for extracting keywords for a single document without relying on a large corpus, and reported promising results.", "Web information has also been used as an additional knowledge source for keyword extraction."], "vector_1": {"corpu": 1, "domain": 1, "knowledg": 1, "cluster": 1, "result": 1, "extract": 2, "web": 1, "use": 2, "cooccurr": 1, "inform": 1, "also": 1, "reli": 1, "estim": 1, "larg": 1, "test": 1, "document": 1, "method": 1, "singl": 1, "strategi": 1, "sourc": 1, "distribut": 1, "may": 1, "big": 1, "base": 1, "promis": 1, "background": 1, "report": 1, "addit": 1, "keyword": 2, "howev": 1, "collect": 1, "without": 1, "match": 1, "idf": 1, "reliabl": 1, "propos": 1}, "marker": "(Matsuo and Ishizuka, 2004)", "article": "N09-1070", "vector_2": [5, 0.13410270461945134, 1, 1, 0, 0]}, {"label": "Neut", "current": "The former includes Relational Markov Networks by Bunescu et al (2004) and skip-edge CRFs by Sutton et al (2004).", "context": ["Such systems may be classified into two kinds, one of them uses a single classifier which is optimized incorporating non-local features, and the other consists of pipeline of more than one classifiers.", "The former includes Relational Markov Networks by Bunescu et al (2004) and skip-edge CRFs by Sutton et al (2004).", "A major drawback of this kind of systems may be heavy computational cost of inference both for training and running the systems, because non-local dependency forces such models to use expensive approximate inference instead of dynamic-programming-based exact inference."], "vector_1": {"major": 1, "featur": 1, "comput": 1, "approxim": 1, "al": 2, "one": 2, "heavi": 1, "cost": 1, "et": 2, "use": 2, "depend": 1, "network": 1, "system": 3, "classifi": 3, "forc": 1, "optim": 1, "drawback": 1, "includ": 1, "instead": 1, "bunescu": 1, "infer": 3, "singl": 1, "model": 1, "pipelin": 1, "nonloc": 2, "run": 1, "dynamicprogrammingbas": 1, "skipedg": 1, "may": 2, "relat": 1, "train": 1, "two": 1, "exact": 1, "former": 1, "sutton": 1, "kind": 2, "consist": 1, "crf": 1, "expens": 1, "incorpor": 1, "markov": 1}, "marker": "(2004)", "article": "W07-1033", "vector_2": [3, 0.14565527065527065, 2, 2, 4, 0]}, {"label": "Weak", "current": "Unlike the study in (Wan et al., 2007), this information does not yield any gain.", "context": ["The global information in the S-S graph (connecting a sentence to other sentences in the document) is propagated to the word nodes.", "Unlike the study in (Wan et al., 2007), this information does not yield any gain.", "We did find that the graph approach performed better in the development set, but it seems that it does not generalize to this test set."], "vector_1": {"propag": 1, "set": 2, "global": 1, "unlik": 1, "connect": 1, "seem": 1, "find": 1, "develop": 1, "perform": 1, "graph": 2, "better": 1, "test": 1, "document": 1, "approach": 1, "node": 1, "sentenc": 2, "gener": 1, "gain": 1, "word": 1, "ss": 1, "yield": 1, "inform": 2, "studi": 1}, "marker": "(Wan et al., 2007)", "article": "N09-1070", "vector_2": [2, 0.751942232681087, 1, 4, 0, 0]}, {"label": "Neut", "current": "The distance between each phrase pair is the average of the total distances between every word pair in the phrases (Wu and Palmer, 1994).", "context": ["The distance between the two LexicalEntrys is the distance between the two LexicalUnits if the LexicalUnits occur in Wordnet ontology; otherwise, it is the distance between the two Senses.", "The distance between each phrase pair is the average of the total distances between every word pair in the phrases (Wu and Palmer, 1994).", "If the distance between two words or phrases is 1.00, there is no similarity between these words or phrases, but if they have the same meaning, the distance is 0.00."], "vector_1": {"phrase": 4, "distanc": 7, "lexicalentri": 1, "lexicalunit": 2, "everi": 1, "ontolog": 1, "two": 4, "averag": 1, "word": 3, "sens": 1, "pair": 2, "otherwis": 1, "total": 1, "similar": 1, "occur": 1, "wordnet": 1, "mean": 1}, "marker": "(Wu and Palmer, 1994)", "article": "N13-1057", "vector_2": [19, 0.5464786919016351, 1, 1, 0, 0]}, {"label": "Neut", "current": "Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.", "context": ["In the following, we refer to our methods as \"SLBD\" (segmenter leveraging bilingual data).", "Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.", "Moreover, we also evaluated the performance of our sub-models by"], "vector_1": {"evalu": 2, "zc": 1, "follow": 1, "cw": 1, "leverag": 1, "supervis": 1, "perform": 1, "ie": 1, "moreov": 1, "segment": 2, "peng": 1, "also": 1, "method": 2, "refer": 1, "asahara": 1, "whose": 1, "initi": 1, "train": 1, "slbd": 1, "data": 2, "manual": 1, "zhao": 1, "stateoftheart": 1, "bilingu": 1, "model": 1, "submodel": 1}, "marker": "(Zhang and Clark, 2007)", "article": "D15-1142", "vector_2": [8, 0.8553618522450936, 4, 2, 0, 0]}, {"label": "Neut", "current": "The categorical distributional compositional model of meaning of Coecke et al (2010) combines the modularity of formal semantic models with the empirical nature of vector space models of lexical semantics.", "context": ["1 Background", "The categorical distributional compositional model of meaning of Coecke et al (2010) combines the modularity of formal semantic models with the empirical nature of vector space models of lexical semantics.", "The meaning of a sentence is defined to be the application of its grammatical structure- represented in a type-logical model-to the kronecker product of the meanings of its words, as computed in a distributional model."], "vector_1": {"semant": 2, "composit": 1, "natur": 1, "al": 1, "repres": 1, "empir": 1, "et": 1, "comput": 1, "space": 1, "modelto": 1, "typelog": 1, "coeck": 1, "applic": 1, "product": 1, "distribut": 2, "sentenc": 1, "kroneck": 1, "lexic": 1, "modular": 1, "background": 1, "categor": 1, "formal": 1, "grammat": 1, "word": 1, "structur": 1, "defin": 1, "vector": 1, "combin": 1, "model": 4, "mean": 3}, "marker": "(2010)", "article": "W11-2507", "vector_2": [1, 0.08285109386026818, 1, 4, 9, 1]}, {"label": "Neut", "current": "Since its introduction, the C-test has been researched from many angles and has been adapted for over 20 languages (see Grotjahn et al (2002) for an overview).", "context": ["For each gap, the smaller half of the word is provided and the missing part has to be completed by the learner.", "Since its introduction, the C-test has been researched from many angles and has been adapted for over 20 languages (see Grotjahn et al (2002) for an overview).", "2.1 C-Tests vs Cloze Tests"], "vector_1": {"smaller": 1, "overview": 1, "al": 1, "see": 1, "vs": 1, "et": 1, "miss": 1, "languag": 1, "learner": 1, "cloze": 1, "research": 1, "adapt": 1, "test": 1, "complet": 1, "grotjahn": 1, "gap": 1, "part": 1, "half": 1, "sinc": 1, "ctest": 2, "introduct": 1, "word": 1, "provid": 1, "angl": 1, "mani": 1}, "marker": "(2002)", "article": "Q14-1040", "vector_2": [12, 0.08937134532505109, 1, 1, 13, 0]}, {"label": "Neut", "current": "Jia et al (2009) also developed a noisv channel model for the English-Chinese language pair using Moses, an SMT tool.", "context": ["On the other hand, statistical machine transliteration svstems tvpicallv use procedures familiar from statistical machine translation, including character alignments and subsequent training on the aligned data.", "Jia et al (2009) also developed a noisv channel model for the English-Chinese language pair using Moses, an SMT tool.", "Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses."], "vector_1": {"mose": 2, "tvpicallv": 1, "evalu": 1, "al": 2, "procedur": 1, "et": 2, "giza": 1, "languag": 1, "use": 3, "develop": 1, "jia": 1, "smt": 1, "also": 1, "includ": 1, "subsequ": 1, "channel": 1, "machin": 3, "svstem": 1, "familiar": 1, "tool": 1, "differ": 1, "hand": 1, "noisv": 1, "englishchines": 1, "train": 1, "translat": 1, "transliter": 2, "pair": 1, "data": 1, "kind": 1, "hindiurdu": 1, "align": 2, "charact": 1, "statist": 3, "malik": 1, "model": 2}, "marker": "(2009)", "article": "W14-5502", "vector_2": [5, 0.22205929523896806, 2, 1, 0, 0]}, {"label": "Pos", "current": "Parameters We used the parameters described by Mitchell and Lapata (2008) for the noun and verb vectors.", "context": ["The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011).", "Parameters We used the parameters described by Mitchell and Lapata (2008) for the noun and verb vectors.", "All vectors were built from a lemmatised version of the BNC."], "vector_1": {"use": 1, "vector": 2, "develop": 1, "describ": 1, "lapata": 1, "lemmatis": 1, "dataset": 1, "mitchel": 1, "built": 1, "verb": 1, "version": 1, "bnc": 1, "noun": 1, "experi": 1, "paramet": 2}, "marker": "(2008)", "article": "W11-2507", "vector_2": [3, 0.4980239943542696, 2, 7, 0, 0]}, {"label": "Neut", "current": "Many experiments and extensions still remain to be made: improved dynamic programming interpretation of bottomup parsers, more extensive experimental measurements with a variety of languages and parsing schemata, or generalization of this approach to more complex situations, such as word lattice parsing [21,30], or even handling of \"secondary\" language features.", "context": ["The approach taken here supports both theoretical analysis and actual experimentation, both for the computational behavior of parsers and for the structure of the resulting shared forest.", "Many experiments and extensions still remain to be made: improved dynamic programming interpretation of bottomup parsers, more extensive experimental measurements with a variety of languages and parsing schemata, or generalization of this approach to more complex situations, such as word lattice parsing [21,30], or even handling of \"secondary\" language features.", "Early research in that latter direction is promising: our framework and the corresponding paradigm for parser construction have been extended to full first-order Horn clauses [17,18], and are hence applicable to unification based grammatical formalisms [27]."], "vector_1": {"program": 1, "featur": 1, "comput": 1, "parser": 3, "share": 1, "direct": 1, "horn": 1, "result": 1, "claus": 1, "still": 1, "dynam": 1, "even": 1, "unif": 1, "support": 1, "complex": 1, "theoret": 1, "research": 1, "experiment": 2, "forest": 1, "varieti": 1, "taken": 1, "languag": 2, "experi": 1, "approach": 2, "firstord": 1, "analysi": 1, "applic": 1, "full": 1, "construct": 1, "extend": 1, "handl": 1, "henc": 1, "gener": 1, "lattic": 1, "situat": 1, "secondari": 1, "extens": 2, "base": 1, "pars": 2, "improv": 1, "word": 1, "interpret": 1, "formal": 1, "measur": 1, "grammat": 1, "made": 1, "actual": 1, "framework": 1, "correspond": 1, "structur": 1, "promis": 1, "remain": 1, "schemata": 1, "behavior": 1, "earli": 1, "mani": 1, "bottomup": 1, "paradigm": 1, "latter": 1}, "marker": "[21]", "article": "P89-1018", "vector_2": [3, 0.9706128180268129, 5, 1, 0, 0]}, {"label": "Pos", "current": "In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008).", "context": ["To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).", "In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008).", "The compositional model is based on phrase words vectors addition, where each vector is composed of the collocation pointwise mutual information of the word up to a window of 3 words left and right of the main word."], "vector_1": {"semant": 1, "right": 1, "composit": 2, "creat": 1, "inspir": 1, "repres": 1, "pointwis": 1, "go": 1, "phrase": 2, "beyond": 1, "use": 2, "multipl": 1, "tensor": 1, "lapata": 2, "sadrzadeh": 1, "compos": 1, "window": 1, "complex": 1, "grefenstett": 1, "main": 1, "product": 1, "variou": 1, "colloc": 1, "lexic": 1, "base": 1, "mitchel": 2, "addit": 2, "account": 1, "word": 5, "specif": 1, "work": 4, "inform": 1, "vector": 3, "widdow": 1, "model": 4, "left": 1, "order": 1, "mutual": 1}, "marker": "(2008)", "article": "S13-2019", "vector_2": [5, 0.5289151540734487, 4, 3, 0, 0]}, {"label": "Neut", "current": "Our basic parsing and interactive training paradigm is based on (Simmons and Yu, 1992).", "context": ["7 Related Work", "Our basic parsing and interactive training paradigm is based on (Simmons and Yu, 1992).", "We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component."], "vector_1": {"featur": 2, "ad": 1, "move": 1, "syntax": 1, "significantli": 1, "beyond": 1, "languag": 1, "interact": 1, "compon": 1, "knowledg": 1, "basic": 1, "simpl": 1, "introduc": 1, "machin": 1, "express": 1, "extend": 1, "far": 1, "relat": 1, "sophist": 1, "train": 1, "pars": 2, "background": 1, "particular": 1, "increas": 1, "work": 2, "base": 1, "limit": 1, "learn": 1, "action": 1, "paradigm": 1}, "marker": "(Simmons and Yu, 1992)", "article": "P97-1062", "vector_2": [5, 0.8736578755342437, 1, 1, 0, 0]}, {"label": "Neut", "current": "The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between human-human and human-computer dialogues (Doran et al., 2001).", "context": ["A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses.", "The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between human-human and human-computer dialogues (Doran et al., 2001).", "While there is a large degree of overlap across schemes, the set of DA types will differ due to differences in the nature of the communicative goals; thus information-seeking versus task-oriented dialogues differ in the set of speech acts and their relative frequencies."], "vector_1": {"repair": 1, "major": 1, "set": 2, "natur": 1, "eg": 1, "informationseek": 1, "humancomput": 1, "due": 1, "overlap": 1, "ground": 1, "dialogu": 2, "goal": 1, "commun": 1, "interact": 1, "frequenc": 1, "system": 1, "classifi": 1, "speech": 4, "rel": 1, "larg": 1, "scheme": 1, "type": 2, "cite": 1, "contrast": 1, "recogn": 1, "distribut": 1, "gener": 2, "differ": 3, "da": 1, "taskori": 1, "comparison": 1, "specif": 1, "analys": 1, "thu": 1, "work": 1, "focu": 2, "across": 1, "humanhuman": 1, "degre": 1, "versu": 1, "act": 4}, "marker": "(Traum and Heeman, 1996", "article": "W09-3953", "vector_2": [13, 0.10271662940931307, 3, 2, 0, 0]}, {"label": "Neut", "current": "Banea et al (2008) follow this approach, translating an annotated corpus via MT.", "context": ["The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language.", "Banea et al (2008) follow this approach, translating an annotated corpus via MT.", "Balamurali et al (2012) use linked Wordnets to"], "vector_1": {"corpu": 3, "via": 1, "al": 2, "corpusbas": 1, "et": 2, "banea": 1, "follow": 1, "languag": 2, "use": 1, "transfer": 2, "classifi": 1, "build": 1, "approach": 2, "sourc": 1, "balamurali": 1, "train": 1, "link": 1, "translat": 1, "target": 2, "consist": 1, "annot": 1, "mt": 1, "wordnet": 1}, "marker": "(2008)", "article": "P15-2128", "vector_2": [7, 0.3842511013215859, 2, 1, 4, 0]}, {"label": "Neut", "current": "A set of controlled experiments of this form is described in Grefenstette (1999).", "context": ["The phrase work group is 15 times more frequent than any other and is also the best translation among the tested possibilities.", "A set of controlled experiments of this form is described in Grefenstette (1999).", "In Grefenstette's study, a good translation was found in 87% of ambiguous cases from German to English and 86% of ambiguous cases from Spanish to English."], "vector_1": {"control": 1, "among": 1, "set": 1, "german": 1, "spanish": 1, "phrase": 1, "best": 1, "group": 1, "describ": 1, "also": 1, "grefenstett": 2, "experi": 1, "good": 1, "form": 1, "english": 2, "test": 1, "translat": 2, "case": 2, "possibl": 1, "work": 1, "ambigu": 2, "time": 1, "found": 1, "studi": 1, "frequent": 1}, "marker": "(1999)", "article": "J03-3001", "vector_2": [4, 0.5846897703971851, 1, 1, 0, 0]}, {"label": "Neut", "current": "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "context": ["Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).", "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations."], "vector_1": {"among": 1, "semant": 1, "wup": 1, "synonymi": 1, "past": 1, "packag": 1, "set": 1, "wordnet": 2, "use": 1, "instrument": 1, "perform": 1, "databas": 1, "lin": 1, "re": 1, "other": 1, "languag": 1, "test": 1, "import": 1, "jcn": 1, "method": 2, "variou": 1, "relat": 1, "path": 1, "becom": 1, "lch": 1, "measur": 2, "word": 2, "princeton": 1, "also": 1, "wordnetsimilar": 2, "implement": 1, "similar": 2, "propos": 1}, "marker": "(Resnik, 1995)", "article": "W14-0118", "vector_2": [19, 0.04608496916189224, 8, 1, 0, 0]}, {"label": "Neut", "current": "In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).", "context": ["Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.", "In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).", "However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation."], "vector_1": {"corpu": 1, "featur": 1, "process": 1, "decreas": 1, "loglinear": 1, "cw": 2, "sever": 1, "leverag": 1, "accur": 1, "develop": 1, "semisupervis": 1, "rather": 1, "data": 2, "smt": 1, "figur": 1, "translat": 1, "previous": 1, "method": 1, "unlabel": 1, "machin": 1, "perplex": 1, "cascad": 1, "standard": 1, "train": 1, "multilevel": 1, "vise": 1, "segment": 2, "word": 1, "howev": 1, "manual": 1, "focu": 1, "structur": 1, "caus": 1, "achiev": 1, "statist": 1, "bilingu": 2, "model": 2, "align": 1, "fact": 1, "propos": 1}, "marker": "(Xu et al., 2008)", "article": "D15-1142", "vector_2": [7, 0.1979655444052471, 5, 3, 5, 0]}, {"label": "Neut", "current": "Corpus statistics reveal that on average a monosyllabic word is followed by another monosyllabic word 85% of time (Yang, 2004), and thus learners that use only local transitional probabilities without any global optimization are unlikely to succeed.", "context": ["While the pseudo-words used in infant studies measuring the ability to use transitional probability information are uniformly three-syllables long, much of child-directed English consists of sequences of monosyllabic words.", "Corpus statistics reveal that on average a monosyllabic word is followed by another monosyllabic word 85% of time (Yang, 2004), and thus learners that use only local transitional probabilities without any global optimization are unlikely to succeed.", "This problem does not affect online approaches that use global information, such as computing the maximum likelihood of the corpus incrementally (Venkataraman, 2001)."], "vector_1": {"corpu": 2, "infant": 1, "comput": 1, "increment": 1, "learner": 1, "transit": 2, "global": 2, "unlik": 1, "abil": 1, "affect": 1, "follow": 1, "probabl": 2, "use": 4, "likelihood": 1, "anoth": 1, "threesyl": 1, "long": 1, "much": 1, "local": 1, "onlin": 1, "monosyllab": 3, "reveal": 1, "optim": 1, "sequenc": 1, "time": 1, "succeed": 1, "word": 3, "uniformli": 1, "problem": 1, "averag": 1, "measur": 1, "pseudoword": 1, "consist": 1, "approach": 1, "thu": 1, "maximum": 1, "childdirect": 1, "inform": 2, "without": 1, "statist": 1, "english": 1, "studi": 1}, "marker": "(Yang, 2004)", "article": "W10-2912", "vector_2": [6, 0.20989807270212574, 2, 9, 0, 0]}, {"label": "Neut", "current": "There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).", "context": ["We will present an alternative to pre-calculation: on-demand paraphrasing, as described in Marton (to appear 2012).", "There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).", "This enables constructing large vector representation, since there is no longer a need to compute a whole matrix."], "vector_1": {"represent": 1, "comput": 1, "suffix": 2, "ondemand": 2, "text": 1, "prefix": 1, "monolingu": 1, "done": 1, "paraphras": 1, "need": 1, "array": 1, "longer": 1, "describ": 1, "construct": 1, "matrix": 1, "larg": 1, "altern": 1, "resourc": 1, "precalcul": 1, "link": 1, "sinc": 1, "present": 1, "appear": 1, "search": 1, "enabl": 1, "tree": 1, "marton": 1, "vector": 1, "whole": 1}, "marker": "(Manber and Myers, 1993", "article": "N12-4007", "vector_2": [19, 0.7857573928786964, 3, 6, 0, 0]}, {"label": "Pos", "current": "The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.", "context": ["Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank.", "The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.", "While often ignored, the grammar constant |G |typically dominates the runtime in practice."], "vector_1": {"constitu": 1, "constant": 1, "often": 1, "weight": 1, "cfg": 1, "treebank": 1, "parser": 1, "find": 1, "dynam": 1, "use": 1, "tree": 1, "program": 1, "syntact": 1, "sentenc": 1, "time": 1, "given": 1, "g": 1, "ogn": 1, "pars": 1, "domin": 1, "practic": 1, "grammar": 2, "like": 1, "algorithm": 1, "contextfre": 1, "cki": 1, "n": 1, "employ": 1, "length": 1, "ignor": 1, "runtim": 1, "learn": 1, "typic": 1}, "marker": "Kasami, 1965", "article": "W11-2921", "vector_2": [46, 0.04133616063247729, 3, 1, 0, 0]}, {"label": "Neut", "current": "For instance, the resulting word graph can be used in the prediction engine of a CAT system (Och et al., 2003).", "context": ["Thus, the integration of MT models to ASR word graphs can be considered as an N-best rescoring but with very large value for N. Another advantage of working with ASR word graphs is the capability to pass on the word graphs for further processing.", "For instance, the resulting word graph can be used in the prediction engine of a CAT system (Och et al., 2003).", "The remaining part is structured as follows: in Section 2, a general model for an automatic text dictation system in the computer-assisted translation framework will be described."], "vector_1": {"advantag": 1, "process": 1, "text": 1, "automat": 1, "result": 1, "pass": 1, "follow": 1, "cat": 1, "rescor": 1, "use": 1, "engin": 1, "describ": 1, "graph": 4, "section": 1, "system": 2, "predict": 1, "larg": 1, "asr": 2, "gener": 1, "capabl": 1, "part": 1, "translat": 1, "consid": 1, "nbest": 1, "valu": 1, "dictat": 1, "word": 4, "framework": 1, "computerassist": 1, "thu": 1, "work": 1, "structur": 1, "n": 1, "mt": 1, "remain": 1, "instanc": 1, "integr": 1, "anoth": 1, "model": 2}, "marker": "(Och et al., 2003)", "article": "P06-2061", "vector_2": [3, 0.1519924812030075, 1, 4, 14, 1]}, {"label": "Neut", "current": "Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).", "context": ["Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).", "Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).", "In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features."], "vector_1": {"classif": 1, "set": 1, "pair": 1, "featur": 1, "need": 1, "extract": 1, "pmi": 2, "select": 1, "supervis": 1, "use": 3, "neg": 1, "top": 1, "classifi": 1, "research": 1, "also": 2, "score": 2, "treat": 1, "document": 1, "approach": 2, "final": 1, "machin": 1, "candid": 2, "solv": 1, "highest": 1, "averag": 1, "task": 1, "word": 2, "algorithm": 1, "keyword": 3, "k": 1, "exampl": 1, "learn": 3, "posit": 1}, "marker": "Turney, 2000", "article": "N09-1070", "vector_2": [9, 0.15750620547371136, 6, 2, 3, 0]}, {"label": "Neut", "current": "formation to detect word boundaries (Jusczyk et al., 1999), we believe that it is reasonable to assume that identifying syllabic stress is a task an infant learner can perform at the developmental stage of word segmentation.", "context": ["90", "formation to detect word boundaries (Jusczyk et al., 1999), we believe that it is reasonable to assume that identifying syllabic stress is a task an infant learner can perform at the developmental stage of word segmentation.", "4 A Simple Algorithm for Word Segmentation"], "vector_1": {"development": 1, "stress": 1, "detect": 1, "word": 3, "algorithm": 1, "format": 1, "perform": 1, "segment": 2, "boundari": 1, "learner": 1, "infant": 1, "syllab": 1, "reason": 1, "task": 1, "believ": 1, "identifi": 1, "simpl": 1, "assum": 1, "stage": 1}, "marker": "(Jusczyk et al., 1999)", "article": "W10-2912", "vector_2": [11, 0.38197179710754986, 1, 2, 1, 0]}, {"label": "Neut", "current": "These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).", "context": ["To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.", "These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).", "In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed."], "vector_1": {"scarciti": 1, "predict": 1, "number": 1, "monolingu": 1, "year": 1, "largescal": 1, "extract": 1, "sever": 1, "baselin": 1, "mutual": 1, "techniqu": 1, "semisupervis": 1, "cw": 1, "characterbas": 1, "label": 1, "also": 1, "cotrain": 1, "addit": 1, "approach": 3, "unlabel": 1, "updat": 1, "wordbas": 1, "investig": 1, "distribut": 1, "use": 1, "intens": 1, "address": 1, "segment": 2, "recent": 1, "attempt": 1, "corpora": 2, "manual": 2, "data": 1, "employ": 1, "inform": 1, "either": 1, "learn": 1, "model": 2}, "marker": "(Jiao et al., 2006)", "article": "D15-1142", "vector_2": [9, 0.09189300270575744, 5, 2, 0, 0]}, {"label": "Neut", "current": "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "context": ["Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.", "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort."], "vector_1": {"control": 1, "creat": 1, "al": 1, "assess": 1, "exist": 1, "expect": 1, "need": 1, "et": 1, "probabl": 1, "use": 1, "develop": 2, "pay": 1, "system": 1, "textual": 1, "genpex": 1, "also": 1, "larg": 2, "statement": 1, "test": 3, "instead": 1, "difficulti": 1, "exercis": 1, "variat": 1, "given": 1, "initi": 1, "effect": 1, "mention": 1, "holl": 1, "know": 1, "onetim": 1, "littl": 1, "effort": 1, "afterward": 1, "valu": 1, "count": 1, "great": 2, "level": 1, "invest": 1, "materi": 1, "item": 1, "amount": 2, "learn": 1}, "marker": "Deane and Sheehan, 2003", "article": "W11-1403", "vector_2": [8, 0.9378107997425846, 7, 4, 1, 0]}, {"label": "Neut", "current": "Additionally, there are significant challenges in reconstructing stress from an acoustic signal (Van Kuijk and Boves, 1999).", "context": ["We later discuss a manipulation of the corpus used by Yang (2004) to address this concern.", "Additionally, there are significant challenges in reconstructing stress from an acoustic signal (Van Kuijk and Boves, 1999).", "For a child learner to use the algorithm presented here, she would need to have mechanisms for detecting stress in the speech signal and categorizing the gradient stress in utterances into a discrete level for each syllable."], "vector_1": {"corpu": 1, "challeng": 1, "address": 1, "signific": 1, "need": 1, "utter": 1, "concern": 1, "use": 2, "detect": 1, "would": 1, "gradient": 1, "learner": 1, "discret": 1, "speech": 1, "manipul": 1, "mechan": 1, "child": 1, "reconstruct": 1, "categor": 1, "discuss": 1, "present": 1, "addit": 1, "stress": 3, "acoust": 1, "algorithm": 1, "level": 1, "signal": 2, "later": 1, "syllabl": 1, "yang": 1}, "marker": "(Van Kuijk and Boves, 1999)", "article": "W10-2912", "vector_2": [11, 0.36131573408701406, 2, 1, 0, 0]}, {"label": "Neut", "current": "(Cohen Priva et al., 2010)).", "context": ["This paper presents a new modality for studying MWE production, keystroke dynamics, which allows for large-scale, low-cost, high-precision metrics (cf.", "(Cohen Priva et al., 2010)).", "Keystroke dynamics looks at the speed at which a users hands move across a keyboard (Bergadano et al., 2002)."], "vector_1": {"metric": 1, "move": 1, "cf": 1, "paper": 1, "lowcost": 1, "keyboard": 1, "largescal": 1, "speed": 1, "dynam": 2, "mwe": 1, "new": 1, "across": 1, "product": 1, "allow": 1, "hand": 1, "user": 1, "keystrok": 2, "present": 1, "look": 1, "modal": 1, "highprecis": 1, "studi": 1}, "marker": "(Cohen Priva et al., 2010)", "article": "W15-0914", "vector_2": [5, 0.05840635826370491, 2, 2, 0, 0]}, {"label": "Neut", "current": "Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments.", "context": ["Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality.", "Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments.", "Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders."], "vector_1": {"wang": 1, "directli": 1, "qualiti": 1, "decreas": 1, "phrase": 1, "differ": 1, "sever": 1, "use": 1, "smt": 1, "waibel": 1, "accuraci": 1, "add": 1, "decod": 1, "languag": 1, "reorder": 1, "higher": 1, "perplex": 1, "intuit": 1, "extens": 1, "base": 1, "translat": 3, "increas": 1, "sinc": 1, "addit": 1, "word": 1, "ibm": 1, "constraint": 1, "align": 1, "phrasebas": 1, "mt": 1, "achiev": 1, "model": 4, "linguist": 1, "order": 1, "propos": 2}, "marker": "(1998)", "article": "N04-1023", "vector_2": [6, 0.13661792507426984, 1, 1, 0, 0]}, {"label": "Neut", "current": "Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).", "context": ["Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).", "Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).", "In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features."], "vector_1": {"classif": 1, "set": 1, "pair": 1, "featur": 1, "need": 1, "extract": 1, "pmi": 2, "select": 1, "supervis": 1, "use": 3, "neg": 1, "top": 1, "classifi": 1, "research": 1, "also": 2, "score": 2, "treat": 1, "document": 1, "approach": 2, "final": 1, "machin": 1, "candid": 2, "solv": 1, "highest": 1, "averag": 1, "task": 1, "word": 2, "algorithm": 1, "keyword": 3, "k": 1, "exampl": 1, "learn": 3, "posit": 1}, "marker": "Turney, 2002", "article": "N09-1070", "vector_2": [7, 0.15750620547371136, 6, 2, 3, 0]}, {"label": "Neut", "current": "Cognitive literature suggests that limited memory during learning may be essential to a learner in its early stages (Elman, 1993).", "context": ["But this slower learning is unlikely to be a concern for a child learner who would be exposed to much larger amounts of data than the corpora here provide.", "Cognitive literature suggests that limited memory during learning may be essential to a learner in its early stages (Elman, 1993).", "But we do not see any notable improvement as a result of the probabilistic memory model used in our experiments, although the learner does do better in the Reduced Stress condition with Probabilistic Memory than Perfect Memory."], "vector_1": {"corpora": 1, "slower": 1, "unlik": 1, "result": 1, "cognit": 1, "concern": 1, "perfect": 1, "use": 1, "expos": 1, "memori": 4, "suggest": 1, "learner": 3, "better": 1, "much": 1, "condit": 1, "experi": 1, "notabl": 1, "earli": 1, "see": 1, "may": 1, "probabilist": 2, "although": 1, "child": 1, "data": 1, "stage": 1, "reduc": 1, "stress": 1, "essenti": 1, "would": 1, "provid": 1, "larger": 1, "literatur": 1, "amount": 1, "limit": 1, "learn": 2, "improv": 1, "model": 1}, "marker": "(Elman, 1993)", "article": "W10-2912", "vector_2": [17, 0.9070627499323491, 1, 1, 0, 0]}, {"label": "Neut", "current": "Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).", "context": ["2.", "Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).", "3."], "vector_1": {"negra": 1, "corpu": 1, "use": 1, "stanford": 1, "german": 1, "sentenc": 1, "train": 1, "parser": 1, "version": 1, "pars": 1, "utter": 1, "segment": 1}, "marker": "(Rafferty and Manning, 2008)", "article": "W13-5507", "vector_2": [5, 0.3097002665108977, 3, 1, 2, 0]}, {"label": "Neut", "current": "We define a list of downward-monotone and non-monotone expressions, and for each item we specify its arity and a Tregex pattern (Levy and Andrew, 2006) which permits us to identify its occurrences.", "context": ["Unlike the categorial grammar parses assumed by Sanchez Valencia, the nesting of constituents in phrase-structure parses does not always correspond to the composition of semantic functions, which introduces a number of complications.", "We define a list of downward-monotone and non-monotone expressions, and for each item we specify its arity and a Tregex pattern (Levy and Andrew, 2006) which permits us to identify its occurrences.", "We also specify, for each argument, both the monotonicity and another Tregex pattern which helps us to determine the sentence span over which the monotonicity is projected."], "vector_1": {"sanchez": 1, "semant": 1, "constitu": 1, "composit": 1, "help": 1, "phrasestructur": 1, "number": 1, "unlik": 1, "downwardmonoton": 1, "identifi": 1, "assum": 1, "span": 1, "item": 1, "pattern": 2, "tregex": 2, "categori": 1, "alway": 1, "valencia": 1, "complic": 1, "introduc": 1, "function": 1, "ariti": 1, "monoton": 2, "sentenc": 1, "nest": 1, "express": 1, "specifi": 2, "argument": 1, "pars": 2, "nonmonoton": 1, "also": 1, "grammar": 1, "occurr": 1, "list": 1, "correspond": 1, "us": 2, "project": 1, "defin": 1, "anoth": 1, "determin": 1, "permit": 1}, "marker": "(Levy and Andrew, 2006)", "article": "W07-1431", "vector_2": [1, 0.3676450418323961, 1, 1, 0, 0]}, {"label": "Neut", "current": "The graphs of EM performance from iteration 1 illustrate the same 'classical' and 'initial' patterns observed by Elworthy (1994).", "context": ["In most cases, these results are significant, even when we manually select the best model (iteration) for EM.", "The graphs of EM performance from iteration 1 illustrate the same 'classical' and 'initial' patterns observed by Elworthy (1994).", "When EM is initialized from a relatively poor model, such as that built from S (Figure 2), a 'classical'"], "vector_1": {"em": 3, "classic": 2, "illustr": 1, "signific": 1, "result": 1, "best": 1, "even": 1, "built": 1, "perform": 1, "graph": 1, "figur": 1, "select": 1, "rel": 1, "poor": 1, "initi": 2, "elworthi": 1, "case": 1, "manual": 1, "iter": 2, "pattern": 1, "model": 2, "observ": 1}, "marker": "(1994)", "article": "W07-2203", "vector_2": [13, 0.7845114308721073, 1, 1, 0, 0]}, {"label": "Neut", "current": "Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).", "context": ["1 Introduction", "Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).", "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004)."], "vector_1": {"among": 1, "wup": 1, "past": 1, "packag": 1, "wordnet": 1, "use": 1, "lin": 1, "re": 1, "other": 1, "jcn": 1, "method": 2, "variou": 1, "path": 1, "lch": 1, "introduct": 1, "measur": 1, "word": 1, "princeton": 1, "wordnetsimilar": 1, "implement": 1, "similar": 1, "propos": 1}, "marker": "(Fellbaum, 1998)", "article": "W14-0118", "vector_2": [16, 0.04555273786043017, 8, 3, 0, 0]}, {"label": "CoCo", "current": "Data preparation method and error counting scheme are almost similar to that of similarity-based methods (Dagan et al., 1999) (Lee and Pereira, 1999).", "context": ["Each method is presented with a noun and two verbs, deciding which verb is more likely to have the noun as a direct object.", "Data preparation method and error counting scheme are almost similar to that of similarity-based methods (Dagan et al., 1999) (Lee and Pereira, 1999).", "Performance is measured by the error rate, defined as error rate = T-1(0 of incorrect choices) where T is the size of test set."], "vector_1": {"set": 1, "almost": 1, "direct": 1, "rate": 2, "size": 1, "incorrect": 1, "decid": 1, "perform": 1, "two": 1, "test": 1, "scheme": 1, "method": 3, "choic": 1, "object": 1, "verb": 2, "data": 1, "present": 1, "count": 1, "measur": 1, "noun": 2, "like": 1, "prepar": 1, "defin": 1, "similaritybas": 1, "t": 1, "error": 3, "similar": 1}, "marker": "(Dagan et al., 1999)", "article": "P00-1072", "vector_2": [1, 0.6823113677085407, 2, 8, 0, 0]}, {"label": "Pos", "current": "Portage implements a dynamic programming beam search decoding algorithm similar to that of Koehn (2004), in which translation hypotheses are constructed by combining in various ways the target-language part of phrase pairs whose sourcelanguage part matches the input.", "context": ["Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P(t1s).", "Portage implements a dynamic programming beam search decoding algorithm similar to that of Koehn (2004), in which translation hypotheses are constructed by combining in various ways the target-language part of phrase pairs whose sourcelanguage part matches the input.", "These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora."], "vector_1": {"text": 1, "pair": 3, "tabl": 1, "phrase": 3, "dynam": 1, "probabl": 1, "involv": 1, "whose": 1, "wordalign": 1, "pt": 1, "segment": 1, "smt": 1, "construct": 2, "current": 1, "program": 1, "decod": 2, "way": 1, "sourcelanguag": 1, "input": 1, "match": 2, "accord": 1, "sourc": 1, "koehn": 1, "variou": 1, "sentenc": 1, "beam": 1, "part": 2, "translat": 2, "phase": 1, "highest": 1, "come": 1, "implement": 1, "portag": 1, "search": 2, "central": 1, "contigu": 1, "algorithm": 1, "hypothes": 2, "corpora": 1, "collect": 1, "combin": 1, "larg": 1, "bilingu": 1, "model": 1, "similar": 1, "targetlanguag": 1}, "marker": "(2004)", "article": "N07-1064", "vector_2": [3, 0.42424018339125935, 1, 1, 1, 0]}, {"label": "Pos", "current": "For training, we use the maximum entropy software library Llama presented in (Haffner, 2006).", "context": ["Similarly, we call the reordering model, a 'context dependent block distortion model'.", "For training, we use the maximum entropy software library Llama presented in (Haffner, 2006).", "3.1.1 Context Dependent Block Translation Model"], "vector_1": {"softwar": 1, "use": 1, "depend": 2, "llama": 1, "librari": 1, "maximum": 1, "distort": 1, "train": 1, "call": 1, "block": 2, "context": 2, "similarli": 1, "entropi": 1, "model": 3, "present": 1, "reorder": 1, "translat": 1}, "marker": "(Haffner, 2006)", "article": "W10-3805", "vector_2": [4, 0.4214729019251295, 1, 2, 4, 0]}, {"label": "Neut", "current": "Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text.", "context": ["Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.", "Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text.", "She described a syntagmatic technique for identifying hyponymy relations in free text by using frequently occurring patterns like 'NP0 such as NP1, NP2, ...,NPn'."], "vector_1": {"set": 1, "identifi": 1, "primarili": 1, "ontolog": 2, "lexicosyntact": 1, "via": 1, "heurist": 1, "find": 1, "languag": 1, "use": 2, "describ": 1, "pattern": 3, "npn": 1, "occur": 1, "reli": 1, "varieti": 1, "np": 3, "hearst": 1, "approach": 1, "relat": 3, "express": 1, "free": 1, "hyponymi": 1, "techniqu": 1, "like": 1, "outlin": 1, "text": 2, "frequent": 1, "syntagmat": 1, "typic": 1, "linguist": 1, "fact": 1}, "marker": "(1992)", "article": "W12-5209", "vector_2": [20, 0.2848017494389135, 4, 2, 0, 0]}, {"label": "Neut", "current": "In all the evaluations reported in this paper we test our parser over a gold-standard set of relational dependencies compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth).", "context": ["King et al (2003) describe the development of the PARC 700 Dependency Bank, a goldstandard set of relational dependencies for 700 sentences (from the PTB) drawn at random from section 23 of the WSJ (the de facto standard test set for statistical parsing).", "In all the evaluations reported in this paper we test our parser over a gold-standard set of relational dependencies compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth).", "The Susanne Corpus is a (balanced) subset of the Brown Corpus which consists of 15 broad categories of American English texts."], "vector_1": {"corpu": 2, "subset": 1, "set": 3, "facto": 1, "evalu": 1, "text": 1, "random": 1, "parser": 2, "al": 1, "depbank": 1, "paper": 1, "et": 1, "compat": 1, "categori": 1, "develop": 1, "describ": 1, "broad": 1, "section": 1, "henceforth": 1, "test": 2, "drawn": 1, "balanc": 1, "brown": 1, "de": 1, "parc": 2, "deriv": 1, "sentenc": 1, "relat": 2, "susann": 1, "ptb": 1, "goldstandard": 2, "pars": 1, "report": 1, "depend": 4, "standard": 1, "bank": 2, "king": 1, "consist": 1, "wsj": 1, "american": 1, "statist": 1, "english": 1, "output": 1}, "marker": "(Briscoe and Carroll, 2006)", "article": "W07-2203", "vector_2": [1, 0.46959353009137694, 2, 2, 3, 1]}, {"label": "Neut", "current": "Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].", "context": [" Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].", "Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].", "Extracting word correspondences [Gale and Church, 1991a]."], "vector_1": {"word": 1, "sentenc": 1, "align": 1, "correspond": 1, "automat": 1, "disambigu": 1, "wordsens": 1, "extract": 1}, "marker": "Dagan et al., 1991, ", "article": "P93-1003", "vector_2": [2, 0.05604523532346582, 7, 10, 0, 0]}, {"label": "Pos", "current": "We have looked to the construct of genre (Martin, 1992) to provide this additional control, on two grounds: (1) since genres are distinguished by their communicative purposes, we can view each of the functional sections already identified as a distinct genre; (2) genre is presented as controlling text structure and realisation.", "context": ["In such cases, we must appeal to another source of control over the apparently available choices.", "We have looked to the construct of genre (Martin, 1992) to provide this additional control, on two grounds: (1) since genres are distinguished by their communicative purposes, we can view each of the functional sections already identified as a distinct genre; (2) genre is presented as controlling text structure and realisation.", "In Martin's view, genre is defined as a staged, goal-oriented social process realised through register, the context of situation, which in turn is realised in language to achieve the goals of a text."], "vector_1": {"control": 3, "identifi": 1, "process": 1, "text": 2, "alreadi": 1, "turn": 1, "stage": 1, "ground": 1, "goal": 1, "commun": 1, "distinct": 1, "anoth": 1, "section": 1, "goalori": 1, "two": 1, "construct": 1, "avail": 1, "must": 1, "languag": 1, "regist": 1, "appeal": 1, "function": 1, "sourc": 1, "social": 1, "situat": 1, "distinguish": 1, "appar": 1, "realis": 3, "sinc": 1, "present": 1, "addit": 1, "case": 1, "choic": 1, "look": 1, "provid": 1, "genr": 5, "structur": 1, "defin": 1, "achiev": 1, "context": 1, "martin": 1, "purpos": 1, "view": 2}, "marker": "(Martin, 1992)", "article": "P96-1026", "vector_2": [4, 0.629207664422579, 1, 1, 0, 0]}, {"label": "Neut", "current": "This effect is particularly visible when parsing incomplete sentences (14 Efficiency loss with increased look-ahead is mainly due to state splitting [6].", "context": ["Mar SBBL in appendix C shows a loss of efficiency with increasec look-ahead that is due exclusively to loss of sharing caused by ir relevant contextual distinctions.", "This effect is particularly visible when parsing incomplete sentences (14 Efficiency loss with increased look-ahead is mainly due to state splitting [6].", "This should favor LALR techniques over LR ones."], "vector_1": {"effici": 2, "show": 1, "ir": 1, "share": 1, "one": 1, "incomplet": 1, "particularli": 1, "visibl": 1, "mainli": 1, "techniqu": 1, "distinct": 1, "due": 2, "favor": 1, "state": 1, "lr": 1, "split": 1, "increasec": 1, "mar": 1, "sentenc": 1, "effect": 1, "sbbl": 1, "appendix": 1, "pars": 1, "increas": 1, "relev": 1, "loss": 3, "c": 1, "lookahead": 2, "contextu": 1, "caus": 1, "exclus": 1, "lalr": 1}, "marker": "[6]", "article": "P89-1018", "vector_2": [19, 0.7329076259570058, 1, 3, 0, 0]}, {"label": "Pos", "current": "Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.", "context": ["One obvious possibility is the extension of Tax and Duin's method to more than one nearest training neighbor for a more accurate estimate of local density.", "Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.", "Our immediate goal is to use unknown sense detection in combination with WSD, to filter out items that the WSD system cannot handle due to missing senses."], "vector_1": {"nearest": 1, "featur": 1, "detect": 2, "obviou": 1, "tax": 1, "one": 2, "miss": 1, "filter": 1, "use": 1, "accur": 1, "goal": 1, "unknown": 1, "system": 1, "estim": 1, "densiti": 1, "test": 1, "sens": 2, "immedi": 1, "local": 1, "method": 1, "neighbor": 1, "outlier": 1, "handl": 1, "duin": 1, "gener": 1, "sophist": 1, "extens": 1, "train": 1, "task": 1, "word": 1, "wsd": 2, "furthermor": 1, "possibl": 1, "approach": 1, "employ": 1, "item": 1, "vector": 1, "combin": 1, "due": 1, "context": 1, "cannot": 1}, "marker": "Markou and Singh, 2003b", "article": "N06-1017", "vector_2": [3, 0.9739488997649235, 3, 3, 3, 0]}, {"label": "Neut", "current": "Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).", "context": ["Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.", "Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).", "Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches."], "vector_1": {"earli": 1, "difficulti": 1, "procedur": 1, "immun": 1, "still": 1, "involv": 1, "perform": 1, "associ": 1, "smith": 2, "accuraci": 1, "also": 1, "new": 2, "approach": 2, "method": 1, "introduc": 1, "complet": 1, "function": 2, "core": 1, "relat": 1, "though": 1, "optim": 1, "object": 2, "train": 1, "nonconvex": 1, "bayesian": 1, "likelihood": 1, "induct": 1, "thu": 1, "eisner": 2, "improv": 2, "model": 1, "propos": 1}, "marker": "(Goldwater and Griffiths, 2007", "article": "P08-1100", "vector_2": [1, 0.04905838996517564, 5, 1, 0, 0]}, {"label": "Neut", "current": "In the TRAINS Conversation System implementation [Allen and Schubert, 1991], we take a fairly pragmatic approach towards rhetorical relations.", "context": ["Rhetorical Relations in the TRAINS System", "In the TRAINS Conversation System implementation [Allen and Schubert, 1991], we take a fairly pragmatic approach towards rhetorical relations.", "Those relations that are conventionally signalled by surface features (e.g."], "vector_1": {"surfac": 1, "pragmat": 1, "convers": 1, "convent": 1, "rhetor": 2, "signal": 1, "relat": 3, "system": 2, "fairli": 1, "train": 2, "take": 1, "implement": 1, "toward": 1, "approach": 1, "eg": 1, "featur": 1}, "marker": "Allen and Schubert, 1991]", "article": "W93-0235", "vector_2": [2, 0.7164300569671761, 1, 1, 0, 0]}, {"label": "Weak", "current": "We have tried using the trick as in (Li et al., 2002) to make data separable artificially, but the performance could not be improved with such features.", "context": ["If the number of the non-discriminative features is large enough, the data set becomes unsplittable.", "We have tried using the trick as in (Li et al., 2002) to make data separable artificially, but the performance could not be improved with such features.", "We achieve similar results with Algorithm 2, the ordinal regression with uneven margin."], "vector_1": {"featur": 2, "number": 1, "set": 1, "result": 1, "artifici": 1, "trick": 1, "use": 1, "uneven": 1, "perform": 1, "make": 1, "enough": 1, "larg": 1, "unsplitt": 1, "ordin": 1, "nondiscrimin": 1, "becom": 1, "data": 2, "tri": 1, "algorithm": 1, "margin": 1, "could": 1, "separ": 1, "achiev": 1, "regress": 1, "improv": 1, "similar": 1}, "marker": "(Li et al., 2002)", "article": "N04-1023", "vector_2": [2, 0.902233882480034, 1, 1, 2, 0]}, {"label": "Pos", "current": "In this paper, we use mean concreteness scores for words as published in the large-scale norming study by Brysbaert et al (2013).", "context": ["6 Experiment 2: Re-representing concreteness information", "In this paper, we use mean concreteness scores for words as published in the large-scale norming study by Brysbaert et al (2013).", "The dataset has a reasonable coverage for our data; thus, 78% of tokens in Set A have a concreteness rating."], "vector_1": {"set": 1, "al": 1, "dataset": 1, "rate": 1, "paper": 1, "coverag": 1, "largescal": 1, "et": 1, "use": 1, "score": 1, "publish": 1, "concret": 3, "experi": 1, "norm": 1, "reason": 1, "data": 1, "word": 1, "thu": 1, "inform": 1, "token": 1, "studi": 1, "mean": 1, "rerepres": 1, "brysbaert": 1}, "marker": "(2013)", "article": "W15-1402", "vector_2": [2, 0.3931980906921241, 1, 5, 0, 0]}, {"label": "Neut", "current": "Quotes provide additional context that were used by human annotators in a separate task for annotating agreement and disagreement (Misra and Walker, 2013).", "context": ["The annotations for stance were gathered using Amazon's Mechanical Turk service with an interface that allowed annotators to see complete discussions.", "Quotes provide additional context that were used by human annotators in a separate task for annotating agreement and disagreement (Misra and Walker, 2013).", "Responses can be labeled as either PRO or CON toward the topic."], "vector_1": {"pro": 1, "topic": 1, "see": 1, "human": 1, "respons": 1, "use": 2, "quot": 1, "label": 1, "interfac": 1, "disagr": 1, "complet": 1, "allow": 1, "agreement": 1, "amazon": 1, "mechan": 1, "discuss": 1, "addit": 1, "task": 1, "provid": 1, "servic": 1, "gather": 1, "annot": 4, "separ": 1, "stanc": 1, "turk": 1, "either": 1, "context": 1, "toward": 1, "con": 1}, "marker": "(Misra and Walker, 2013)", "article": "W14-2715", "vector_2": [1, 0.123380346290563, 1, 1, 0, 0]}, {"label": "Neut", "current": "Kaplan et al., 1989), there are advantages in treating the intermediate, transfer, stage independently.", "context": ["The first and last of these are to be performed by parsing and generation with natural language grammars, but, while proposals have been made to combine some of the three stages (e.g.", "Kaplan et al., 1989), there are advantages in treating the intermediate, transfer, stage independently.", "As an example, consider the FSs shown below:6"], "vector_1": {"advantag": 1, "natur": 1, "eg": 1, "independ": 1, "languag": 1, "shown": 1, "perform": 1, "transfer": 1, "intermedi": 1, "three": 1, "exampl": 1, "treat": 1, "gener": 1, "pars": 1, "consid": 1, "grammar": 1, "stage": 2, "made": 1, "last": 1, "below": 1, "fss": 1, "combin": 1, "first": 1, "propos": 1}, "marker": "Kaplan et al., 1989)", "article": "E91-1050", "vector_2": [2, 0.48235234371032454, 1, 1, 0, 0]}, {"label": "Neut", "current": "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "context": ["Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.", "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates."], "vector_1": {"set": 1, "profici": 1, "process": 1, "natur": 1, "close": 1, "languag": 2, "multipl": 1, "student": 1, "cloze": 1, "distractor": 1, "field": 1, "exercis": 3, "test": 2, "perspect": 1, "correct": 1, "difficulti": 1, "format": 1, "gener": 2, "previou": 1, "answer": 1, "candid": 1, "vocabulari": 1, "educ": 1, "choic": 2, "grammar": 1, "provid": 1, "approach": 1, "work": 1, "focu": 1, "discrimin": 1, "determin": 1, "usual": 1}, "marker": "(Perez-Beltrachini et al., 2012)", "article": "Q14-1040", "vector_2": [2, 0.14527649070270926, 7, 1, 0, 0]}, {"label": "Neut", "current": "We used a version of the corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006).", "context": ["Most of the emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely social emails.", "We used a version of the corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006).", "The annotator of the majority of the Loqui corpus also annotated the Enron corpus."], "vector_1": {"corpu": 3, "major": 1, "loqui": 1, "exchang": 1, "miss": 1, "concern": 1, "use": 1, "quot": 1, "messag": 1, "also": 2, "version": 1, "pure": 1, "schedul": 1, "email": 3, "solv": 1, "enron": 1, "restor": 1, "annot": 2, "inform": 1, "social": 1, "meet": 1, "problem": 1}, "marker": "(Yeh and Harnly, 2006)", "article": "W09-3953", "vector_2": [3, 0.4738377600121244, 1, 1, 0, 0]}, {"label": "Pos", "current": "The parser creates a packed parse forest represented as a graph-structured stack.2 The parse selection model ranks complete derivations in the parse forest by computing the product of the probabilities of the (shift/reduce) parse actions (given LR state and lookahead item) which created each derivation (Inui et al., 1997).", "context": ["The residue of features not incorporated into the backbone are unified on each reduce action and if unification fails the associated derivation paths also fail.", "The parser creates a packed parse forest represented as a graph-structured stack.2 The parse selection model ranks complete derivations in the parse forest by computing the product of the probabilities of the (shift/reduce) parse actions (given LR state and lookahead item) which created each derivation (Inui et al., 1997).", "Estimating action probabilities, consists of a) recording an action history for the correct derivation in the parse forest (for each sentence in a treebank), b) computing the frequency of each action over all action histories and c) normalizing these frequencies to determine probability distributions over conflicting (i.e."], "vector_1": {"featur": 1, "comput": 2, "creat": 2, "treebank": 1, "parser": 1, "unif": 1, "rank": 1, "repres": 1, "residu": 1, "fail": 2, "ie": 1, "select": 1, "probabl": 3, "given": 1, "record": 1, "also": 1, "state": 1, "estim": 1, "lr": 1, "forest": 3, "determin": 1, "distribut": 1, "correct": 1, "conflict": 1, "complet": 1, "product": 1, "deriv": 4, "normal": 1, "sentenc": 1, "graphstructur": 1, "pars": 5, "path": 1, "shiftreduc": 1, "associ": 1, "stack": 1, "reduc": 1, "c": 1, "b": 1, "backbon": 1, "consist": 1, "lookahead": 1, "histori": 2, "item": 1, "unifi": 1, "incorpor": 1, "frequenc": 2, "action": 6, "model": 1, "pack": 1}, "marker": "(Inui et al., 1997)", "article": "W07-2203", "vector_2": [10, 0.26285754297517766, 1, 3, 0, 0]}, {"label": "Pos", "current": "The training data of the reranker is created by the n-best tagger, and every set of 17 sections from the training set is used to train the n-best tagger for the remaining section (The same technique is used by previous studies to avoid the n-best tagger's 'unrealistically good' performance on the training set (Collins, 2000)).", "context": ["We divided the data into 20 contiguous and equally-sized sections, and used the first 18 sections for training, and the last 2 sections for testing while development (henceforth the training and development sets, respectively).", "The training data of the reranker is created by the n-best tagger, and every set of 17 sections from the training set is used to train the n-best tagger for the remaining section (The same technique is used by previous studies to avoid the n-best tagger's 'unrealistically good' performance on the training set (Collins, 2000)).", "Among the n-best sequences output by the MEMM tagger, the sequence with the highest F-score is used as the 'correct' sequence for training the reranker."], "vector_1": {"among": 1, "set": 4, "creat": 1, "unrealist": 1, "tagger": 4, "respect": 1, "use": 4, "develop": 2, "techniqu": 1, "contigu": 1, "divid": 1, "perform": 1, "section": 5, "henceforth": 1, "equallys": 1, "test": 1, "correct": 1, "fscore": 1, "good": 1, "everi": 1, "rerank": 2, "memm": 1, "sequenc": 3, "previou": 1, "train": 7, "highest": 1, "data": 2, "nbest": 4, "last": 1, "remain": 1, "output": 1, "studi": 1, "avoid": 1, "first": 1}, "marker": "(Collins, 2000)", "article": "W07-1033", "vector_2": [7, 0.6716524216524217, 1, 2, 0, 0]}, {"label": "Neut", "current": "ContextFree grammars can be represented by AND-OR graphs that are closely related to the syntax diagrams often used to describe the syntax of programming languages [37], and to the transition networks of Woods [22].", "context": ["We show here in section 4 that the CF grammar of all leftmost parses is just a theoretical formalization of the shared-forest graph.", "ContextFree grammars can be represented by AND-OR graphs that are closely related to the syntax diagrams often used to describe the syntax of programming languages [37], and to the transition networks of Woods [22].", "In the case of our grammar of leftmost parses, this AND-OR graph (which is acyclic when there is only finite ambiguity) is precisely the sharedforest graph."], "vector_1": {"andor": 2, "show": 1, "transit": 1, "cf": 1, "repres": 1, "close": 1, "languag": 1, "often": 1, "use": 1, "describ": 1, "graph": 4, "section": 1, "theoret": 1, "acycl": 1, "program": 1, "sharedforest": 2, "leftmost": 2, "relat": 1, "finit": 1, "diagram": 1, "pars": 2, "syntax": 2, "formal": 1, "case": 1, "wood": 1, "grammar": 3, "contextfre": 1, "ambigu": 1, "precis": 1, "network": 1}, "marker": "[22]", "article": "P89-1018", "vector_2": [14, 0.40433285413393066, 2, 1, 0, 0]}, {"label": "Neut", "current": "Moses (Koehn et al., 2007) is used as a decoder.", "context": ["A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language.", "Moses (Koehn et al., 2007) is used as a decoder.", "Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset."], "vector_1": {"mose": 1, "use": 2, "featur": 1, "develop": 1, "target": 1, "mert": 1, "smooth": 1, "dataset": 1, "minimum": 1, "rate": 1, "train": 2, "srilm": 1, "gram": 1, "error": 1, "tune": 1, "decod": 1, "model": 1, "kneserney": 1, "appli": 1, "languag": 2, "paramet": 1}, "marker": "(Koehn et al., 2007)", "article": "D15-1142", "vector_2": [8, 0.5498167619960955, 3, 1, 2, 0]}, {"label": "Neut", "current": "  While keystroke dynamics is concerned with a number of timing metrics, such as key holds (h in   We also did not remove any outliers, although this is common in keystroke dynamics (Epp et al., 2011; Zhong et al., 2012).", "context": ["4 Experiments", "  While keystroke dynamics is concerned with a number of timing metrics, such as key holds (h in   We also did not remove any outliers, although this is common in keystroke dynamics (Epp et al., 2011; Zhong et al., 2012).", "We feel it is difficult-toimpossible to discriminate between a true pause that is indicative of a subjects increased cognitive effort and any other type of pause, such as those caused by distraction or physical fatigue."], "vector_1": {"key": 1, "paus": 2, "feel": 1, "metric": 1, "concern": 1, "number": 1, "indic": 1, "increas": 1, "fatigu": 1, "cognit": 1, "dynam": 2, "subject": 1, "difficulttoimposs": 1, "effort": 1, "also": 1, "experi": 1, "type": 1, "outlier": 1, "although": 1, "keystrok": 2, "hold": 1, "true": 1, "physic": 1, "h": 1, "remov": 1, "distract": 1, "caus": 1, "discrimin": 1, "common": 1, "time": 1}, "marker": "Zhong et al., 2012)", "article": "W15-0914", "vector_2": [3, 0.49773792541267575, 2, 1, 0, 0]}, {"label": "Neut", "current": "Thus, to achieve good performance, it is important to increase the ratio of Compute to Global Memory Access (CGMA) (Kirk and Hwu, 2010), which can be done in part by cleverly utilizing the different types of shared onchip memory in each SM.", "context": ["Generally speaking, manycore architectures (like GPUs) have more ALUs in place of on-chip caches, making arithmetic operations relatively cheaper and global memory accesses relatively more expensive.", "Thus, to achieve good performance, it is important to increase the ratio of Compute to Global Memory Access (CGMA) (Kirk and Hwu, 2010), which can be done in part by cleverly utilizing the different types of shared onchip memory in each SM.", "Threads in a thread block are mapped onto the same SM and can cooperate with one another by sharing data through the on-chip shared memory of the SM (shown in Figure 5)."], "vector_1": {"oper": 1, "comput": 1, "cleverli": 1, "global": 2, "share": 3, "one": 1, "done": 1, "onchip": 3, "alu": 1, "speak": 1, "differ": 1, "ratio": 1, "memori": 4, "perform": 1, "anoth": 1, "make": 1, "access": 2, "figur": 1, "arithmet": 1, "rel": 2, "gpu": 1, "type": 1, "architectur": 1, "map": 1, "good": 1, "gener": 1, "shown": 1, "cheaper": 1, "util": 1, "part": 1, "import": 1, "increas": 1, "cooper": 1, "data": 1, "cach": 1, "like": 1, "thread": 2, "onto": 1, "thu": 1, "cgma": 1, "achiev": 1, "place": 1, "expens": 1, "sm": 3, "manycor": 1, "block": 1}, "marker": "(Kirk and Hwu, 2010)", "article": "W11-2921", "vector_2": [1, 0.3065263488572999, 1, 1, 0, 0]}, {"label": "Neut", "current": "Chiang also introduced the cube pruning algorithm for hierarchical search (Chiang, 2007).", "context": ["Hierarchical phrase-based translation (HPBT) was first proposed by Chiang (2005).", "Chiang also introduced the cube pruning algorithm for hierarchical search (Chiang, 2007).", "It is basically an adaptation of one of the k-best parsing algorithms by Huang and Chiang (2005)."], "vector_1": {"hierarch": 2, "search": 1, "cube": 1, "prune": 1, "algorithm": 2, "huang": 1, "introduc": 1, "phrasebas": 1, "one": 1, "also": 1, "kbest": 1, "translat": 1, "adapt": 1, "basic": 1, "chiang": 3, "pars": 1, "first": 1, "hpbt": 1, "propos": 1}, "marker": "(Chiang, 2007)", "article": "W13-0804", "vector_2": [6, 0.15397041319503485, 3, 3, 2, 0]}, {"label": "Neut", "current": "Negation and its scope has been studied extensively (Moilanen and Pulman, 2008; Pang and Lee, 2004; Choi and Cardie, 2009).", "context": ["Dragut et al (2012) examined inconsistency across lexicons.", "Negation and its scope has been studied extensively (Moilanen and Pulman, 2008; Pang and Lee, 2004; Choi and Cardie, 2009).", "Polar words can even carry an opposite sentiment in a new domain (Blitzer et al., 2007; Andreevskaia and Bergler, 2006; Schwartz et al., 2013; Wilson et al., 2005)."], "vector_1": {"even": 1, "polar": 1, "domain": 1, "lexicon": 1, "word": 1, "sentiment": 1, "carri": 1, "opposit": 1, "negat": 1, "al": 1, "new": 1, "extens": 1, "inconsist": 1, "scope": 1, "examin": 1, "dragut": 1, "et": 1, "studi": 1, "across": 1}, "marker": "(Moilanen and Pulman, 2008", "article": "W15-2911", "vector_2": [7, 0.18760928122489484, 8, 1, 0, 0]}, {"label": "Neut", "current": "For example, van Lohuizen (1999) reports a 1.8x speedup, while Manousopoulou et al (1997) claims a 7-8x speedup.", "context": ["The parallel parsers in past work are implemented on multicore systems, where the limited parallelization possibilities provided by the systems restrict the speedups that can be achieved.", "For example, van Lohuizen (1999) reports a 1.8x speedup, while Manousopoulou et al (1997) claims a 7-8x speedup.", "In contrast, our parallel parser is implemented on a manycore system with an abundant number of threads and pro"], "vector_1": {"claim": 1, "abund": 1, "pro": 1, "parser": 2, "speedup": 3, "restrict": 1, "et": 1, "van": 1, "system": 3, "multicor": 1, "contrast": 1, "manousopoul": 1, "al": 1, "lohuizen": 1, "report": 1, "number": 1, "parallel": 3, "past": 1, "thread": 1, "possibl": 1, "provid": 1, "work": 1, "achiev": 1, "exampl": 1, "limit": 1, "x": 2, "implement": 2, "manycor": 1}, "marker": "(1999)", "article": "W11-2921", "vector_2": [12, 0.880153930731171, 2, 3, 0, 0]}, {"label": "Pos", "current": "Inspired by the PCFG approximation idea (Fowler and Penn, 2010; Zhang and Krieger, 2011) for deep parsing, we study tree approximation approaches for graph spanning.", "context": ["In our work, we use a practical idea to indirectly profile the graph-based parsing techniques for dependency graph parsing.", "Inspired by the PCFG approximation idea (Fowler and Penn, 2010; Zhang and Krieger, 2011) for deep parsing, we study tree approximation approaches for graph spanning.", "This tree approximation technique can be applied to both transition-based and graph-based parsers."], "vector_1": {"appli": 1, "approxim": 3, "inspir": 1, "idea": 2, "deep": 1, "transitionbas": 1, "pcfg": 1, "use": 1, "depend": 1, "techniqu": 2, "graph": 2, "profil": 1, "approach": 1, "graphbas": 2, "parser": 1, "pars": 3, "span": 1, "practic": 1, "work": 1, "tree": 2, "indirectli": 1, "studi": 1}, "marker": "(Fowler and Penn, 2010", "article": "S14-2080", "vector_2": [4, 0.46343626988967124, 2, 2, 0, 0]}, {"label": "Neut", "current": "Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.", "context": ["They used automatically generated items similar to the exercises generated by Genpex, except that their exercises did not have variations in wording apart from context-related ones.", "Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.", "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009)."], "vector_1": {"control": 1, "al": 1, "automat": 1, "one": 1, "assess": 1, "exist": 1, "need": 1, "et": 1, "probabl": 1, "use": 2, "develop": 1, "except": 1, "textual": 1, "genpex": 2, "also": 1, "larg": 1, "statement": 1, "test": 1, "instead": 1, "difficulti": 1, "exercis": 3, "variat": 2, "gener": 2, "effect": 1, "mention": 1, "holl": 1, "know": 1, "given": 1, "valu": 1, "apart": 1, "count": 1, "great": 2, "word": 1, "level": 1, "materi": 1, "item": 1, "amount": 1, "contextrel": 1, "learn": 1, "similar": 1}, "marker": "(2009)", "article": "W11-1403", "vector_2": [2, 0.930497864622945, 7, 5, 1, 0]}, {"label": "Neut", "current": "Klein-Braley (1996) identifies additional error patterns related to production problems (right word stem in wrong form) and early closure, i.e.", "context": ["ines word frequency, word class, and constituent type of the gap for the C-test and finds high correlation only for the word frequency.", "Klein-Braley (1996) identifies additional error patterns related to production problems (right word stem in wrong form) and early closure, i.e.", "the solution works locally but not in the larger context."], "vector_1": {"constitu": 1, "identifi": 1, "kleinbraley": 1, "high": 1, "correl": 1, "right": 1, "ine": 1, "ie": 1, "find": 1, "pattern": 1, "solut": 1, "type": 1, "earli": 1, "product": 1, "form": 1, "relat": 1, "gap": 1, "closur": 1, "wrong": 1, "larger": 1, "stem": 1, "class": 1, "ctest": 1, "addit": 1, "word": 4, "frequenc": 2, "work": 1, "local": 1, "context": 1, "error": 1, "problem": 1}, "marker": "(1996)", "article": "Q14-1040", "vector_2": [18, 0.19258240090646056, 1, 2, 9, 0]}, {"label": "Pos", "current": "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "context": ["Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.", "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort."], "vector_1": {"control": 1, "creat": 1, "al": 1, "assess": 1, "exist": 1, "expect": 1, "need": 1, "et": 1, "probabl": 1, "use": 1, "develop": 2, "pay": 1, "system": 1, "textual": 1, "genpex": 1, "also": 1, "larg": 2, "statement": 1, "test": 3, "instead": 1, "difficulti": 1, "exercis": 1, "variat": 1, "given": 1, "initi": 1, "effect": 1, "mention": 1, "holl": 1, "know": 1, "onetim": 1, "littl": 1, "effort": 1, "afterward": 1, "valu": 1, "count": 1, "great": 2, "level": 1, "invest": 1, "materi": 1, "item": 1, "amount": 2, "learn": 1}, "marker": "Fairon and Williamson, 2002", "article": "W11-1403", "vector_2": [9, 0.9378107997425846, 7, 3, 0, 0]}, {"label": "Pos", "current": "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "context": ["Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.", "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort."], "vector_1": {"control": 1, "creat": 1, "al": 1, "assess": 1, "exist": 1, "expect": 1, "need": 1, "et": 1, "probabl": 1, "use": 1, "develop": 2, "pay": 1, "system": 1, "textual": 1, "genpex": 1, "also": 1, "larg": 2, "statement": 1, "test": 3, "instead": 1, "difficulti": 1, "exercis": 1, "variat": 1, "given": 1, "initi": 1, "effect": 1, "mention": 1, "holl": 1, "know": 1, "onetim": 1, "littl": 1, "effort": 1, "afterward": 1, "valu": 1, "count": 1, "great": 2, "level": 1, "invest": 1, "materi": 1, "item": 1, "amount": 2, "learn": 1}, "marker": "Holling et al., 2009)", "article": "W11-1403", "vector_2": [2, 0.9378107997425846, 7, 5, 1, 0]}, {"label": "Neut", "current": "We now present a simple algebraic approach to word segmentation based on the constraints suggested by Yang (2004).", "context": ["4 A Simple Algorithm for Word Segmentation", "We now present a simple algebraic approach to word segmentation based on the constraints suggested by Yang (2004).", "The learner we present is algebraic in that it has a lexicon which stores previously segmented words and identifies the input as a combination of words already in the lexicon and novel words."], "vector_1": {"lexicon": 2, "word": 5, "input": 1, "algorithm": 1, "algebra": 2, "simpl": 2, "constraint": 1, "learner": 1, "novel": 1, "approach": 1, "combin": 1, "store": 1, "base": 1, "suggest": 1, "alreadi": 1, "identifi": 1, "previous": 1, "segment": 3, "present": 2, "yang": 1}, "marker": "(2004)", "article": "W10-2912", "vector_2": [6, 0.38663218978321656, 1, 9, 0, 0]}, {"label": "CoCo", "current": "It also achieves a relative error rate reduction of 6.56% as compared with the previous work in (Wu and Wang, 2004).", "context": ["Our method achieves a relative error rate reduction of 17.43% as compared with the method directly combining the out-of-domain corpus and the in-domain corpus as training data.", "It also achieves a relative error rate reduction of 6.56% as compared with the previous work in (Wu and Wang, 2004).", "In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004), our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004)."], "vector_1": {"corpu": 3, "describ": 1, "work": 1, "directli": 1, "compar": 3, "indomain": 2, "previou": 1, "smallerscal": 1, "rate": 3, "method": 4, "outofdomain": 1, "also": 1, "achiev": 3, "train": 2, "combin": 1, "rel": 2, "error": 3, "model": 1, "data": 1, "reduct": 3, "addit": 1}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.9693470257701996, 3, 13, 0, 1]}, {"label": "Pos", "current": "To prune away those pairs, we used the log-likelihood-ratio algorithm (Dunning, 1993) to compute the degree of association between the verb and the noun in each pair.", "context": ["Secondly, those verbs tend not to occur in the modifier-head relation with a following noun and we gain very little in terms of disambiguation by storing those pairs in the knowledge base.", "To prune away those pairs, we used the log-likelihood-ratio algorithm (Dunning, 1993) to compute the degree of association between the verb and the noun in each pair.", "Pairs where there is high \"mutual information\" between the verb and noun would receive higher scores while pairs where the verb can co-occur with many different nouns would receive lower scores."], "vector_1": {"comput": 1, "knowledg": 1, "high": 1, "secondli": 1, "follow": 1, "differ": 1, "occur": 1, "modifierhead": 1, "use": 1, "would": 2, "receiv": 2, "away": 1, "littl": 1, "tend": 1, "lower": 1, "score": 2, "disambigu": 1, "store": 1, "higher": 1, "loglikelihoodratio": 1, "relat": 1, "mutual": 1, "cooccur": 1, "verb": 4, "gain": 1, "pair": 5, "associ": 1, "term": 1, "noun": 4, "prune": 1, "algorithm": 1, "inform": 1, "degre": 1, "base": 1, "mani": 1}, "marker": "(Dunning, 1993)", "article": "W03-1717", "vector_2": [10, 0.6590006020469596, 1, 1, 0, 0]}, {"label": "Neut", "current": "Head word heuristic (Cimiano, 2006) based pattern (NP)  (NP) is used to identify subsumption relation.", "context": ["Two patterns are used to detect subsumption and neighbor relations.", "Head word heuristic (Cimiano, 2006) based pattern (NP)  (NP) is used to identify subsumption relation.", "As per head word heuristic (NP1)(NP2) implies (NP2) subsumes (NP1NP2), e.g."], "vector_1": {"use": 2, "detect": 1, "word": 2, "impli": 1, "pattern": 2, "eg": 1, "subsumpt": 2, "two": 1, "per": 1, "subsum": 1, "base": 1, "npnp": 2, "neighbor": 1, "heurist": 2, "np": 3, "identifi": 1, "relat": 2, "head": 2}, "marker": "(Cimiano, 2006)", "article": "W12-5209", "vector_2": [6, 0.6288772515393911, 1, 1, 2, 0]}, {"label": "Neut", "current": "Of intentional actions, it is also possible to draw the distinction made in speech act theory between illocutionary acts, those in which part of the intended effect includes an awareness on the part of the hearer of this intention, and perlocutionary acts, in which it is only the effect that matters and not recognition of the intention [Austin, 1962].", "context": ["As with other actions, relations can be performed intentionally or incidentally.", "Of intentional actions, it is also possible to draw the distinction made in speech act theory between illocutionary acts, those in which part of the intended effect includes an awareness on the part of the hearer of this intention, and perlocutionary acts, in which it is only the effect that matters and not recognition of the intention [Austin, 1962].", "For non-illocutionary acts, the intention of the speaker is not relevant - these actions can be produced as side-effects of the speaker's intention, so that a determination of the intention is not necessary to determining whether the act was performed."], "vector_1": {"hearer": 1, "nonillocutionari": 1, "sideeffect": 1, "awar": 1, "recognit": 1, "perform": 2, "also": 1, "speech": 1, "includ": 1, "speaker": 2, "distinct": 1, "draw": 1, "intend": 1, "relat": 1, "effect": 2, "perlocutionari": 1, "incident": 1, "part": 2, "intent": 7, "theori": 1, "relev": 1, "made": 1, "possibl": 1, "whether": 1, "matter": 1, "necessari": 1, "determin": 2, "act": 5, "action": 3, "produc": 1, "illocutionari": 1}, "marker": "Austin, 1962]", "article": "W93-0235", "vector_2": [31, 0.4425354914549236, 1, 1, 0, 0]}, {"label": "Neut", "current": "Cube pruning (Chiang, 2007) is a widely used search strategy in state-of-the-art hierarchical decoders.", "context": ["1 Introduction", "Cube pruning (Chiang, 2007) is a widely used search strategy in state-of-the-art hierarchical decoders.", "Some alternatives and extensions to the classical algorithm as proposed by David Chiang have been presented in the literature since, e.g."], "vector_1": {"hierarch": 1, "wide": 1, "use": 1, "cube": 1, "prune": 1, "algorithm": 1, "classic": 1, "eg": 1, "altern": 1, "search": 1, "david": 1, "extens": 1, "decod": 1, "stateoftheart": 1, "propos": 1, "chiang": 1, "literatur": 1, "strategi": 1, "sinc": 1, "present": 1, "introduct": 1}, "marker": "(Chiang, 2007)", "article": "W13-0804", "vector_2": [6, 0.044932834551946944, 1, 3, 2, 0]}, {"label": "Neut", "current": "Varantola (2000) shows how translators can use \"just-in-time\" sublanguage corpora to choose correct target language terms for areas in which they are not expert.", "context": ["2000) are exploring the automatic population of existing ontologies using the Web as a source for new instances.", "Varantola (2000) shows how translators can use \"just-in-time\" sublanguage corpora to choose correct target language terms for areas in which they are not expert.", "Fletcher (2002) demonstrates methods for gathering and using Web corpora in a language-teaching context."], "vector_1": {"gather": 1, "show": 1, "ontolog": 1, "automat": 1, "exist": 1, "fletcher": 1, "explor": 1, "languag": 1, "languageteach": 1, "web": 2, "use": 3, "expert": 1, "area": 1, "method": 1, "justintim": 1, "new": 1, "correct": 1, "sourc": 1, "translat": 1, "demonstr": 1, "popul": 1, "varantola": 1, "term": 1, "target": 1, "corpora": 2, "sublanguag": 1, "instanc": 1, "context": 1, "choos": 1}, "marker": "(2000)", "article": "J03-3001", "vector_2": [3, 0.26129661038402147, 2, 1, 0, 0]}, {"label": "Neut", "current": "Our translation system implements a log-linear model in which a foreign language sentence fJ1 = f1, f2, ..., fJ is translated into another language eI1 = f1, f2, ..., eI by searching for the translation hypothesis oI1 maximizing a log-linear combination of several feature models (Brown et al., 1990):", "context": ["2 Ngram-based SMT System", "Our translation system implements a log-linear model in which a foreign language sentence fJ1 = f1, f2, ..., fJ is translated into another language eI1 = f1, f2, ..., eI by searching for the translation hypothesis oI1 maximizing a log-linear combination of several feature models (Brown et al., 1990):", "M oI1 = argmax { m=1 E amhm(ef, fJ1 ) eI l 1"], "vector_1": {"featur": 1, "ei": 3, "ngrambas": 1, "argmax": 1, "amhmef": 1, "maxim": 1, "loglinear": 2, "sever": 1, "anoth": 1, "smt": 1, "system": 2, "languag": 2, "hypothesi": 1, "sentenc": 1, "translat": 3, "fj": 3, "model": 2, "search": 1, "e": 1, "oi": 2, "f": 4, "m": 1, "l": 1, "foreign": 1, "combin": 1, "implement": 1}, "marker": "(Brown et al., 1990)", "article": "W08-0315", "vector_2": [18, 0.19301496792587314, 1, 1, 0, 0]}, {"label": "Neut", "current": "This process is known as wikification (Csomai and Mihalcea, 2008), and we used an implementation of Milne and Witten (2008).", "context": ["To compute document similarity, we first extract key representative Wikipedia concepts from a document to produce document concept vectors4.", "This process is known as wikification (Csomai and Mihalcea, 2008), and we used an implementation of Milne and Witten (2008).", "This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document."], "vector_1": {"concept": 4, "comput": 1, "weight": 1, "process": 1, "vi": 1, "repres": 1, "strongli": 1, "miln": 1, "identifi": 1, "extract": 1, "id": 2, "denot": 1, "use": 1, "wikipedia": 2, "articl": 1, "current": 1, "document": 5, "form": 1, "relat": 1, "wi": 1, "witten": 1, "key": 1, "known": 1, "wikif": 1, "vector": 2, "w": 2, "idi": 1, "implement": 1, "similar": 1, "produc": 2, "first": 1}, "marker": "(Csomai and Mihalcea, 2008)", "article": "W10-3506", "vector_2": [2, 0.7859435779296802, 2, 1, 0, 0]}, {"label": "Neut", "current": "In order to account for polysemy, we count the number of represented word senses for the solution in the lexical-semantic resource UBY (Gurevych et al., 2012).", "context": ["the word well has a high frequency, but it occurs only rarely in its sense fountain.", "In order to account for polysemy, we count the number of represented word senses for the solution in the lexical-semantic resource UBY (Gurevych et al., 2012).", "The two senses of well also differ in their word class."], "vector_1": {"number": 1, "two": 1, "high": 1, "occur": 1, "differ": 1, "solut": 1, "lexicalsemant": 1, "class": 1, "also": 1, "fountain": 1, "sens": 3, "polysemi": 1, "resourc": 1, "rare": 1, "repres": 1, "ubi": 1, "count": 1, "account": 1, "word": 3, "frequenc": 1, "well": 2, "order": 1}, "marker": "(Gurevych et al., 2012)", "article": "Q14-1040", "vector_2": [2, 0.40214070372093963, 1, 1, 4, 0]}, {"label": "Neut", "current": "For NLP practitioners, MWEs are notoriously difficult to detect and parse (Sag et al., 2002).", "context": ["For theoretical linguists, MWEs occupy a liminal space between the lexicon and syntax (Langacker, 2008).", "For NLP practitioners, MWEs are notoriously difficult to detect and parse (Sag et al., 2002).", "This paper presents a new modality for studying MWE production, keystroke dynamics, which allows for large-scale, low-cost, high-precision metrics (cf."], "vector_1": {"nlp": 1, "lexicon": 1, "metric": 1, "cf": 1, "syntax": 1, "paper": 1, "lowcost": 1, "largescal": 1, "dynam": 1, "occupi": 1, "detect": 1, "space": 1, "practition": 1, "theoret": 1, "mwe": 3, "notori": 1, "new": 1, "difficult": 1, "product": 1, "allow": 1, "pars": 1, "keystrok": 1, "present": 1, "limin": 1, "modal": 1, "highprecis": 1, "studi": 1, "linguist": 1}, "marker": "(Sag et al., 2002)", "article": "W15-0914", "vector_2": [13, 0.05343386998165885, 2, 1, 0, 0]}, {"label": "Neut", "current": "Based on POS annotation provided by the CLAWS tagger (Garside and Smith, 1997), USAS assigns a set of semantic tags to each item in running text and then attempts to disambiguate the tags in order to choose the most likely candidate in each context.", "context": ["The USAS system has been in development at Lancaster University since 19901.", "Based on POS annotation provided by the CLAWS tagger (Garside and Smith, 1997), USAS assigns a set of semantic tags to each item in running text and then attempts to disambiguate the tags in order to choose the most likely candidate in each context.", "Items can be single words or multiword expressions."], "vector_1": {"semant": 1, "set": 1, "text": 1, "tag": 2, "tagger": 1, "lancast": 1, "develop": 1, "usa": 2, "univers": 1, "system": 1, "disambigu": 1, "multiword": 1, "po": 1, "singl": 1, "run": 1, "express": 1, "base": 1, "word": 1, "sinc": 1, "attempt": 1, "like": 1, "provid": 1, "annot": 1, "assign": 1, "item": 2, "candid": 1, "context": 1, "choos": 1, "claw": 1, "order": 1}, "marker": "(Garside and Smith, 1997)", "article": "W03-1807", "vector_2": [6, 0.22911537830297063, 1, 1, 1, 0]}, {"label": "Neut", "current": "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "context": ["Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.", "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort."], "vector_1": {"control": 1, "creat": 1, "al": 1, "assess": 1, "exist": 1, "expect": 1, "need": 1, "et": 1, "probabl": 1, "use": 1, "develop": 2, "pay": 1, "system": 1, "textual": 1, "genpex": 1, "also": 1, "larg": 2, "statement": 1, "test": 3, "instead": 1, "difficulti": 1, "exercis": 1, "variat": 1, "given": 1, "initi": 1, "effect": 1, "mention": 1, "holl": 1, "know": 1, "onetim": 1, "littl": 1, "effort": 1, "afterward": 1, "valu": 1, "count": 1, "great": 2, "level": 1, "invest": 1, "materi": 1, "item": 1, "amount": 2, "learn": 1}, "marker": "Arendasy et al., 2006", "article": "W11-1403", "vector_2": [5, 0.9378107997425846, 7, 3, 0, 0]}, {"label": "Pos", "current": "We follow the -COREFERENCE setting from Barzilay and Lapata (2005) and perform heuristic coreference resolution by linking mentions which share a head noun.", "context": ["To construct a grid, we must first decide which textual units are to be considered \"entities\", and how the different mentions of an entity are to be linked.", "We follow the -COREFERENCE setting from Barzilay and Lapata (2005) and perform heuristic coreference resolution by linking mentions which share a head noun.", "Although some versions of the grid use an automatic coreference resolver, this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their target domains, and actually hurts for readability prediction."], "vector_1": {"domain": 1, "set": 1, "although": 1, "predict": 1, "share": 1, "automat": 1, "one": 1, "construct": 1, "heurist": 1, "fail": 1, "follow": 1, "unit": 1, "entiti": 2, "result": 2, "decid": 1, "lapata": 2, "perform": 1, "readabl": 1, "textual": 1, "corefer": 4, "version": 1, "use": 1, "head": 1, "resolut": 1, "differ": 1, "hurt": 1, "often": 1, "mention": 2, "grid": 2, "consid": 1, "improv": 2, "actual": 1, "must": 1, "noun": 1, "target": 1, "resolv": 1, "barzilay": 2, "link": 2, "first": 1}, "marker": "(2005)", "article": "P11-2022", "vector_2": [6, 0.27488875606030416, 2, 5, 8, 0]}, {"label": "Neut", "current": "An algorithm for producing collocational correspondences has also been described [Smadja, 1992].", "context": ["Single word correspondences have been investigated [Gale and Church, 1991a] using a statistic operating on contingency tables.", "An algorithm for producing collocational correspondences has also been described [Smadja, 1992].", "The algorithm involves several steps."], "vector_1": {"oper": 1, "involv": 1, "use": 1, "investig": 1, "word": 1, "describ": 1, "algorithm": 2, "correspond": 2, "also": 1, "step": 1, "sever": 1, "statist": 1, "colloc": 1, "tabl": 1, "produc": 1, "singl": 1, "conting": 1}, "marker": "Smadja, 1992]", "article": "P93-1003", "vector_2": [1, 0.10061533344420423, 2, 10, 0, 0]}, {"label": "Neut", "current": "We do another experiment using almost the same-scale in-domain training corpus as described in (Wu and Wang, 2004).", "context": ["In order to further compare our method with the method described in (Wu and Wang, 2004).", "We do another experiment using almost the same-scale in-domain training corpus as described in (Wu and Wang, 2004).", "From the in-domain training corpus, we randomly select about 500 sentence pairs to build the smaller training set."], "vector_1": {"corpu": 2, "randomli": 1, "use": 1, "samescal": 1, "smaller": 1, "describ": 2, "compar": 1, "almost": 1, "anoth": 1, "sentenc": 1, "method": 2, "train": 3, "build": 1, "indomain": 2, "pair": 1, "set": 1, "experi": 1, "order": 1, "select": 1}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.760705289672544, 2, 13, 0, 1]}, {"label": "Neut", "current": "Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).", "context": ["In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing.", "Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).", "The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models."], "vector_1": {"signific": 1, "result": 2, "year": 1, "perceptron": 1, "reduct": 1, "best": 1, "techniqu": 2, "support": 1, "especi": 1, "label": 1, "boost": 1, "machin": 2, "previou": 1, "recallprecis": 1, "variou": 1, "rerank": 4, "gener": 1, "error": 1, "pars": 3, "recent": 1, "algorithm": 1, "employ": 1, "discrimin": 1, "vector": 1, "learn": 1, "improv": 1, "model": 1}, "marker": "(Collins and Duffy, 2002)", "article": "N04-1023", "vector_2": [2, 0.24773332304487056, 3, 1, 2, 0]}, {"label": "Neut", "current": "Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).", "context": ["There is also a rich body of theoretical work on learning latent-variable models.", "Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).", "But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV."], "vector_1": {"em": 1, "certain": 1, "except": 1, "littl": 1, "famili": 1, "complex": 1, "special": 1, "provabl": 1, "capac": 2, "theoret": 2, "also": 1, "discret": 1, "pcfg": 1, "rich": 1, "dmv": 1, "dasgupta": 1, "gener": 2, "weak": 1, "bodi": 1, "hmm": 1, "understand": 1, "strong": 1, "constrain": 1, "schulman": 1, "latentvari": 1, "term": 2, "algorithm": 1, "work": 1, "alon": 1, "hiddenvari": 1, "learn": 2, "let": 1, "model": 3, "other": 1}, "marker": "Clark and Thollard, 2005", "article": "P08-1100", "vector_2": [3, 0.9644994421340907, 6, 1, 1, 0]}, {"label": "Neut", "current": "One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003).", "context": ["Two large margin approaches have been used.", "One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003).", "However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks."], "vector_1": {"show": 1, "global": 1, "rank": 2, "one": 1, "bias": 1, "perceptron": 1, "prank": 2, "use": 2, "multipl": 1, "boundari": 1, "two": 2, "larg": 1, "consecut": 1, "approach": 1, "everi": 1, "rerank": 1, "variant": 1, "repres": 1, "introduct": 1, "task": 1, "algorithm": 3, "howev": 1, "work": 1, "due": 1, "margin": 1, "section": 1}, "marker": "(Crammer and Singer, 2001", "article": "N04-1023", "vector_2": [3, 0.3057216713607778, 2, 1, 0, 0]}, {"label": "Neut", "current": "It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al (2008).", "context": ["Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years.", "It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al (2008).", "Headden III et al (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing."], "vector_1": {"cohen": 1, "al": 2, "year": 1, "et": 2, "depend": 2, "klein": 1, "smith": 1, "add": 1, "recent": 1, "approach": 1, "introduc": 2, "extend": 1, "lexic": 1, "pars": 1, "iii": 1, "man": 1, "grammar": 1, "unsupervis": 1, "smooth": 1, "valenc": 2, "dmv": 1, "popular": 1, "improv": 1, "model": 1, "headden": 1}, "marker": "(2004)", "article": "P13-1028", "vector_2": [9, 0.2191204935881926, 4, 3, 0, 0]}, {"label": "Neut", "current": "Extracting word correspondences [Gale and Church, 1991a].", "context": ["Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].", "Extracting word correspondences [Gale and Church, 1991a].", "Finding bilingual collocations [Smadja, 1992]."], "vector_1": {"colloc": 1, "word": 1, "correspond": 1, "disambigu": 1, "wordsens": 1, "bilingu": 1, "extract": 1, "find": 1}, "marker": "Gale and Church, 1991a]", "article": "P93-1003", "vector_2": [2, 0.05793004046787516, 5, 4, 2, 0]}, {"label": "Neut", "current": "Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.", "context": ["(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.", "Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.", "Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages."], "vector_1": {"yamada": 1, "treebank": 2, "parser": 2, "syntax": 1, "languag": 2, "use": 4, "gildea": 1, "selforgan": 1, "sourc": 2, "string": 1, "train": 1, "pars": 1, "implicit": 1, "target": 1, "knight": 1, "align": 2, "tree": 4, "statist": 2, "output": 1, "model": 3, "produc": 1, "propos": 2}, "marker": "(2001)", "article": "N04-1023", "vector_2": [3, 0.1735406458582507, 3, 1, 0, 0]}, {"label": "Neut", "current": "Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "context": ["2.2 Bilingual Semi-supervised CWS Methods", "Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004)."], "vector_1": {"dictionari": 1, "individu": 1, "focus": 1, "one": 1, "cw": 1, "leverag": 2, "semisupervis": 1, "perform": 1, "segment": 2, "smt": 1, "construct": 1, "label": 2, "better": 1, "consecut": 1, "approach": 1, "method": 1, "unlabel": 1, "either": 1, "machin": 1, "map": 1, "form": 1, "sequenc": 1, "chines": 2, "previou": 1, "train": 1, "translat": 1, "although": 1, "dataset": 1, "data": 1, "model": 1, "maximummatch": 1, "word": 2, "english": 1, "work": 1, "charact": 1, "achiev": 1, "statist": 1, "bilingu": 2, "studi": 1}, "marker": "(Chang et al., 2008)", "article": "D15-1142", "vector_2": [7, 0.2968798164194952, 7, 2, 0, 0]}, {"label": "Neut", "current": "Ninomiya et al (1997) propose a parallel CKY parser on a distributed-memory parallel machine consisting of 256 nodes, where each node contains a single processor.", "context": ["Their approach thus cannot be applied to real-world, state-of-theart grammars.", "Ninomiya et al (1997) propose a parallel CKY parser on a distributed-memory parallel machine consisting of 256 nodes, where each node contains a single processor.", "Using their parallel language, they parallelize over cells in the chart, assigning each chart cell to each node in the machine."], "vector_1": {"node": 3, "appli": 1, "parser": 1, "al": 1, "et": 1, "languag": 1, "use": 1, "cell": 2, "approach": 1, "singl": 1, "machin": 2, "chart": 2, "realworld": 1, "cannot": 1, "parallel": 4, "ninomiya": 1, "grammar": 1, "consist": 1, "thu": 1, "cki": 1, "assign": 1, "stateoftheart": 1, "contain": 1, "distributedmemori": 1, "processor": 1, "propos": 1}, "marker": "(1997)", "article": "W11-2921", "vector_2": [14, 0.9184010052619178, 1, 1, 0, 0]}, {"label": "Pos", "current": "The first type are CoNLL treebanks from the year 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007), which we use for inference and for evaluation.", "context": ["We use two types of resources in our experiments.", "The first type are CoNLL treebanks from the year 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007), which we use for inference and for evaluation.", "As is the standard practice in unsupervised parsing evaluation, we removed all punctuation marks from the trees."], "vector_1": {"conll": 1, "use": 2, "resourc": 1, "practic": 1, "evalu": 2, "treebank": 1, "tree": 1, "two": 1, "standard": 1, "mark": 1, "punctuat": 1, "pars": 1, "year": 1, "unsupervis": 1, "experi": 1, "type": 2, "infer": 1, "remov": 1, "first": 1}, "marker": "(Buchholz and Marsi, 2006)", "article": "P13-1028", "vector_2": [7, 0.6633801112992983, 2, 1, 0, 0]}, {"label": "Neut", "current": "This becomes important in native language identification) which is useful for improving grammatical error correction systems (Chodorow et al., 2010) or for providing more targeted feedback to language learners.", "context": ["One of the major contributions of this work is to reveal and visualize a language family tree preserved in non-native texts, by examining the hypothesis.", "This becomes important in native language identification) which is useful for improving grammatical error correction systems (Chodorow et al., 2010) or for providing more targeted feedback to language learners.", "As we will see in Sect."], "vector_1": {"major": 1, "feedback": 1, "text": 1, "one": 1, "see": 1, "famili": 1, "languag": 3, "use": 1, "learner": 1, "system": 1, "identif": 1, "import": 1, "correct": 1, "contribut": 1, "hypothesi": 1, "preserv": 1, "nativ": 1, "visual": 1, "examin": 1, "becom": 1, "reveal": 1, "grammat": 1, "target": 1, "provid": 1, "nonn": 1, "work": 1, "tree": 1, "sect": 1, "error": 1, "improv": 1}, "marker": "(Chodorow et al., 2010)", "article": "P13-1112", "vector_2": [3, 0.11663855236926851, 1, 1, 0, 0]}, {"label": "Neut", "current": "Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).", "context": ["For example, the word happens to have a similar 2 score with Shapiro as the word  (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.", "Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).", "They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model."], "vector_1": {"corpu": 1, "shapiro": 2, "partial": 1, "dictionari": 1, "deal": 1, "represent": 1, "fluenci": 1, "phonet": 1, "happen": 2, "total": 1, "special": 1, "use": 1, "cooccurr": 1, "reli": 1, "score": 1, "approach": 1, "pinyin": 1, "chines": 1, "previou": 1, "train": 1, "translat": 1, "transliter": 3, "known": 1, "parallel": 1, "convert": 1, "word": 3, "algorithm": 1, "parenthet": 1, "exampl": 1, "statist": 1, "bilingu": 1, "model": 1, "unrel": 1, "similar": 1}, "marker": "Wu and Chang, 2007)", "article": "P08-1113", "vector_2": [1, 0.5502692678055565, 3, 2, 0, 0]}, {"label": "Pos", "current": "In section 5, we report the results using the Dutch wordnet Cornetto 2.1 (Vossen et al., 2013).", "context": ["Section 3 explains how we created the Dutch gold standard and section 4 the WordnetTools implementation of the similarity functions.", "In section 5, we report the results using the Dutch wordnet Cornetto 2.1 (Vossen et al., 2013).", "2 Related work"], "vector_1": {"function": 1, "use": 1, "work": 1, "creat": 1, "gold": 1, "section": 3, "relat": 1, "wordnettool": 1, "standard": 1, "result": 1, "cornetto": 1, "dutch": 2, "report": 1, "implement": 1, "similar": 1, "explain": 1, "wordnet": 1}, "marker": "(Vossen et al., 2013)", "article": "W14-0118", "vector_2": [1, 0.12382204689896997, 1, 3, 9, 0]}, {"label": "Pos", "current": "However, Jakschik et al (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps.", "context": ["As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition.", "However, Jakschik et al (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps.", "This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice."], "vector_1": {"featur": 1, "consid": 1, "cohen": 1, "al": 1, "indic": 1, "prefix": 1, "significantli": 1, "abil": 1, "et": 1, "skill": 2, "open": 1, "find": 1, "given": 1, "multipl": 1, "transform": 1, "recognit": 2, "requir": 2, "test": 2, "easier": 1, "jakschik": 1, "recept": 1, "product": 2, "option": 1, "read": 1, "variant": 1, "gap": 1, "reflect": 1, "examin": 1, "solv": 1, "true": 1, "ctest": 4, "reduc": 1, "choic": 2, "provid": 1, "howev": 1, "extent": 1}, "marker": "(2010)", "article": "Q14-1040", "vector_2": [4, 0.13600955020941666, 2, 1, 0, 0]}, {"label": "Neut", "current": "The feature set is based on Florian et al (2002) but contains additional syntax-related features.", "context": ["for verb targets, the target voice.", "The feature set is based on Florian et al (2002) but contains additional syntax-related features.", "Each word-related feature is represented as four features for word, lemma, part of speech, and named entity."], "vector_1": {"set": 1, "repres": 1, "target": 2, "name": 1, "florian": 1, "word": 1, "voic": 1, "al": 1, "four": 1, "lemma": 1, "wordrel": 1, "verb": 1, "speech": 1, "base": 1, "contain": 1, "entiti": 1, "et": 1, "part": 1, "syntaxrel": 1, "featur": 4, "addit": 1}, "marker": "(2002)", "article": "N06-1017", "vector_2": [4, 0.38024586689275114, 1, 1, 0, 0]}, {"label": "CoCo", "current": "Training employed conventional Stochastic Gradient Descent (Rumelhart et al., 1985) with mini-batch size 1 and random uniform initialization similar to (Glorot and Bengio, 2010).", "context": ["The parameters of subspace model in Equation 2, S and C were estimated to minimize the negative log-likelihood of the correct class.", "Training employed conventional Stochastic Gradient Descent (Rumelhart et al., 1985) with mini-batch size 1 and random uniform initialization similar to (Glorot and Bengio, 2010).", "After some initial experiments, it was determined that a learning rate of 0.01 and selecting the model with the best accuracy on the 20% set after 8 iterations led to the best results."], "vector_1": {"set": 1, "random": 1, "rate": 1, "result": 1, "accuraci": 1, "loglikelihood": 1, "select": 1, "size": 1, "00": 1, "descent": 1, "convent": 1, "minim": 1, "gradient": 1, "neg": 1, "uniform": 1, "0": 1, "estim": 1, "best": 2, "paramet": 1, "correct": 1, "led": 1, "subspac": 1, "initi": 2, "train": 1, "class": 1, "c": 1, "experi": 1, "iter": 1, "employ": 1, "minibatch": 1, "equat": 1, "determin": 1, "learn": 1, "model": 2, "stochast": 1, "similar": 1}, "marker": "(Rumelhart et al., 1985)", "article": "S15-2109", "vector_2": [30, 0.6982390975374879, 2, 2, 0, 0]}, {"label": "Pos", "current": "The best results reported on these data sets are 69.0% on SENSEVAL-2 data (Mihalcea and Moldovan, 2002),", "context": ["A baseline, computed using the most frequent sense in WordNet, is also indicated.", "The best results reported on these data sets are 69.0% on SENSEVAL-2 data (Mihalcea and Moldovan, 2002),", "55"], "vector_1": {"result": 1, "use": 1, "set": 1, "comput": 1, "sensev": 1, "best": 1, "indic": 1, "also": 1, "frequent": 1, "report": 1, "sens": 1, "data": 2, "wordnet": 1, "baselin": 1}, "marker": "(Mihalcea and Moldovan, 2002)", "article": "P05-3014", "vector_2": [3, 0.817652329749104, 1, 2, 2, 1]}, {"label": "Neut", "current": "Our approach for this task is a supervised approached based on two main components: first, the availability of the phrases most frequent collocating expressions in a large corpus, and more specifically the top 1000 phrases by frequency in Web 1TB corpus (Brants and Franz, 2006).", "context": ["For example, the phrase big picture is used literally in the sentence Click here for a bigger picture and figuratively in To solve this problem, you have to look at the bigger picture.", "Our approach for this task is a supervised approached based on two main components: first, the availability of the phrases most frequent collocating expressions in a large corpus, and more specifically the top 1000 phrases by frequency in Web 1TB corpus (Brants and Franz, 2006).", "For example, for the phrase big picture, we collect the top 1000 phrases that come before and after the phrase in a corpus, those includes look at the, see the, understand the ....."], "vector_1": {"corpu": 3, "compon": 1, "tb": 1, "see": 1, "phrase": 6, "click": 1, "supervis": 1, "use": 1, "pictur": 4, "top": 2, "two": 1, "avail": 1, "figur": 1, "larg": 1, "includ": 1, "sentenc": 1, "main": 1, "approach": 2, "liter": 1, "web": 1, "big": 2, "express": 1, "colloc": 1, "base": 1, "understand": 1, "solv": 1, "come": 1, "bigger": 2, "task": 1, "look": 2, "specif": 1, "frequenc": 1, "collect": 1, "exampl": 2, "problem": 1, "frequent": 1, "first": 1}, "marker": "(Brants and Franz, 2006)", "article": "S13-2019", "vector_2": [7, 0.7817644575770367, 1, 2, 0, 0]}, {"label": "Pos", "current": "PMG is based on the Conditional Random Fields model (Lafferty et al., 1999) which has been found to provide the highest accuracy.", "context": ["This phrasing model is then applied in segmenting any arbitrary SL text being input to the PRESEMT system for translation.", "PMG is based on the Conditional Random Fields model (Lafferty et al., 1999) which has been found to provide the highest accuracy.", "The SL text segmented into phrases by PMG is then input to the 1st translation phase."], "vector_1": {"appli": 1, "random": 1, "pmg": 2, "presemt": 1, "phrase": 2, "system": 1, "accuraci": 1, "field": 1, "text": 2, "input": 2, "base": 1, "translat": 2, "condit": 1, "phase": 1, "highest": 1, "segment": 2, "arbitrari": 1, "provid": 1, "st": 1, "sl": 2, "found": 1, "model": 2}, "marker": "(Lafferty et al., 1999)", "article": "W12-0108", "vector_2": [13, 0.43893219938577843, 1, 1, 0, 0]}, {"label": "Neut", "current": "Most recently (Shinmori et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while (Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and reformatting the original text so as to emphasis segments with the identified border marker.", "context": ["For example, in one of the earlier works a rule-based technique was developed for decomposing the complex sentence of a claim into a set of simple sentences while preserving the initial content (Sheremetyeva, 2003).", "Most recently (Shinmori et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while (Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and reformatting the original text so as to emphasis segments with the identified border marker.", "This approach does not involve any syntactic restructuring, just visualization of claim segments."], "vector_1": {"origin": 1, "claim": 4, "work": 1, "emphasi": 1, "text": 2, "decompos": 1, "one": 1, "set": 1, "marker": 1, "phrase": 1, "border": 2, "involv": 2, "develop": 1, "techniqu": 1, "suggest": 1, "section": 1, "identifi": 1, "content": 1, "complex": 1, "reformat": 1, "explanatori": 1, "restructur": 1, "syntact": 1, "preserv": 1, "sentenc": 2, "earlier": 1, "initi": 1, "visual": 1, "approach": 2, "segment": 3, "recent": 1, "simpl": 1, "align": 1, "rulebas": 1, "descript": 1, "exampl": 1, "highlight": 1, "propos": 1}, "marker": "(Ferraro et al., 2014)", "article": "W14-5605", "vector_2": [0, 0.27787367993501216, 3, 1, 0, 0]}, {"label": "Neut", "current": "Finding these is also necessary to maximize coreference recall (Elsner and Charniak, 2010).", "context": ["This enables the model to pick up premodifiers in phrases like \"a Bush spokesman\", which do not head NPs in the Penn Treebank.", "Finding these is also necessary to maximize coreference recall (Elsner and Charniak, 2010).", "We give nonhead mentions the role X."], "vector_1": {"give": 1, "treebank": 1, "maxim": 1, "phrase": 1, "find": 1, "premodifi": 1, "corefer": 1, "spokesman": 1, "also": 1, "bush": 1, "role": 1, "np": 1, "penn": 1, "head": 1, "nonhead": 1, "recal": 1, "mention": 1, "enabl": 1, "like": 1, "necessari": 1, "pick": 1, "x": 1, "model": 1}, "marker": "(Elsner and Charniak, 2010)", "article": "P11-2022", "vector_2": [1, 0.5074051935976622, 1, 1, 7, 0]}, {"label": "Pos", "current": "This makes them more versatile; the Distributional Memory framework by Baroni and Lenci (2010) is applicable to a wide range of tasks.", "context": ["The advantage of syntactic models is that they incorporate a richer, structured notion of context.", "This makes them more versatile; the Distributional Memory framework by Baroni and Lenci (2010) is applicable to a wide range of tasks.", "It is also able - at least in principle - to capture more fine-grained types of semantic similarity such as predicateargument plausibility (Erk et al., 2010)."], "vector_1": {"rang": 1, "advantag": 1, "captur": 1, "semant": 1, "baroni": 1, "finegrain": 1, "versatil": 1, "distribut": 1, "lenci": 1, "memori": 1, "make": 1, "abl": 1, "least": 1, "also": 1, "type": 1, "applic": 1, "syntact": 1, "framework": 1, "plausibl": 1, "wide": 1, "task": 1, "incorpor": 1, "predicateargu": 1, "richer": 1, "structur": 1, "context": 1, "model": 1, "principl": 1, "similar": 1, "notion": 1}, "marker": "(2010)", "article": "P13-2128", "vector_2": [3, 0.10386035162360008, 2, 3, 0, 0]}, {"label": "Pos", "current": "An alternative solution to this second step was suggested in (Mihalcea and Faruque, 2004), using semantic generalizations learned from dependencies identified between nodes in a conceptual network.", "context": ["The words that are not covered by these models (typically about 10-15% of the words in the test corpus) are assigned with the most frequent sense in WordNet.", "An alternative solution to this second step was suggested in (Mihalcea and Faruque, 2004), using semantic generalizations learned from dependencies identified between nodes in a conceptual network.", "Their approach however, although slightly more accurate, conflicted with our goal of creating an efficient WSD system, and therefore we opted for the simpler backoff method that employs WordNet sense frequencies."], "vector_1": {"corpu": 1, "node": 1, "semant": 1, "identifi": 1, "creat": 1, "slightli": 1, "backoff": 1, "second": 1, "effici": 1, "wordnet": 2, "use": 1, "accur": 1, "depend": 1, "network": 1, "suggest": 1, "solut": 1, "system": 1, "test": 1, "sens": 2, "therefor": 1, "approach": 1, "method": 1, "conflict": 1, "altern": 1, "gener": 1, "simpler": 1, "step": 1, "although": 1, "typic": 1, "goal": 1, "word": 2, "wsd": 1, "opt": 1, "frequenc": 1, "howev": 1, "cover": 1, "employ": 1, "conceptu": 1, "learn": 1, "model": 1, "assign": 1, "frequent": 1}, "marker": "(Mihalcea and Faruque, 2004)", "article": "P05-3014", "vector_2": [1, 0.4682646356033453, 1, 2, 2, 1]}, {"label": "Neut", "current": "We test whether the WSD system built into SHALMANESER (Erk, 2005) can distinguish known sense items from unknown sense items reliably by its confidence scores.", "context": ["Modeling.", "We test whether the WSD system built into SHALMANESER (Erk, 2005) can distinguish known sense items from unknown sense items reliably by its confidence scores.", "The system extracts a rich feature set, which forms the basis of all three experiments in this paper:"], "vector_1": {"featur": 1, "shalmanes": 1, "distinguish": 1, "set": 1, "extract": 1, "built": 1, "unknown": 1, "system": 2, "score": 1, "rich": 1, "test": 1, "sens": 2, "experi": 1, "confid": 1, "form": 1, "known": 1, "basi": 1, "wsd": 1, "whether": 1, "reliabl": 1, "item": 2, "paper": 1, "three": 1, "model": 1}, "marker": "(Erk, 2005)", "article": "N06-1017", "vector_2": [1, 0.3538864696134726, 1, 3, 0, 1]}, {"label": "Neut", "current": "For each sentence pair, there are two different word alignment results, from which the final alignment links are selected according to their translation probabilities in the dictionary D. The selection order is similar to that in the competitive linking algorithm (Melamed, 1997).", "context": ["The detailed algorithm is shown in Figure 2.", "For each sentence pair, there are two different word alignment results, from which the final alignment links are selected according to their translation probabilities in the dictionary D. The selection order is similar to that in the competitive linking algorithm (Melamed, 1997).", "The difference is that we allow many-to-one an We compare our method with four other methods."], "vector_1": {"manytoon": 1, "two": 1, "four": 1, "result": 1, "select": 2, "probabl": 1, "shown": 1, "compar": 1, "detail": 1, "figur": 1, "final": 1, "competit": 1, "accord": 1, "sentenc": 1, "differ": 2, "link": 2, "translat": 1, "dictionari": 1, "pair": 1, "word": 1, "algorithm": 2, "align": 2, "method": 2, "allow": 1, "similar": 1, "order": 1}, "marker": "(Melamed, 1997)", "article": "P05-1058", "vector_2": [8, 0.5042821158690176, 1, 1, 0, 0]}, {"label": "Pos", "current": "The taggers are robust and operate with a low error rate [Kupiec, 19921.", "context": ["Each tagger contains a hidden Markov model (HMM), which is trained using samples of raw text from the Hansards for each language.", "The taggers are robust and operate with a low error rate [Kupiec, 19921.", "Simple noun phrases (excluding pronouns and digits) are then extracted from the sentences by finite-state recognizers that are specified by regular expressions defined in terms of part-ofspeech categories."], "vector_1": {"oper": 1, "pronoun": 1, "text": 1, "noun": 1, "raw": 1, "rate": 1, "tagger": 2, "phrase": 1, "extract": 1, "languag": 1, "use": 1, "categori": 1, "low": 1, "hidden": 1, "simpl": 1, "digit": 1, "express": 1, "recogn": 1, "sentenc": 1, "contain": 1, "specifi": 1, "hmm": 1, "train": 1, "partofspeech": 1, "robust": 1, "model": 1, "term": 1, "hansard": 1, "exclud": 1, "finitest": 1, "defin": 1, "regular": 1, "error": 1, "sampl": 1, "markov": 1}, "marker": "Kupiec, 1992", "article": "P93-1003", "vector_2": [1, 0.19829258828094684, 1, 10, 0, 0]}, {"label": "Neut", "current": "Researchers including Wong and Dras (2009), Wong et al (2011; 2012), and Koppel et al (2005) work on native language identification and show that machine learning-based methods are effective.", "context": ["6 Implications for Work in Related Domains", "Researchers including Wong and Dras (2009), Wong et al (2011; 2012), and Koppel et al (2005) work on native language identification and show that machine learning-based methods are effective.", "Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation."], "vector_1": {"domain": 1, "show": 2, "al": 2, "expect": 1, "et": 2, "languag": 1, "implic": 1, "use": 2, "perform": 2, "research": 1, "identif": 1, "includ": 1, "better": 1, "method": 1, "machin": 1, "relat": 1, "effect": 1, "nativ": 1, "koppel": 1, "grammat": 1, "learningbas": 1, "contrari": 1, "work": 2, "inform": 1, "achiev": 1, "wong": 3, "determin": 1, "error": 2, "dra": 2, "improv": 1, "propos": 1}, "marker": "(2009)", "article": "P13-1112", "vector_2": [4, 0.8258549302254256, 3, 3, 0, 0]}, {"label": "CoCo", "current": "It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively.", "context": ["New measures for computing similarity between individual concepts (inter-concept similarity, such as \"France\" and \"Great Britain\"), as well as between documents (inter-document similarity) are proposed and tested.", "It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively.", "Our methods outperform WLM in computing inter-concept similarity, and match ESA for inter-document similarity."], "vector_1": {"semant": 1, "concept": 1, "comput": 2, "explicit": 1, "individu": 1, "dataset": 1, "britain": 1, "respect": 1, "art": 1, "techniqu": 1, "compar": 2, "wikipedia": 1, "achiev": 1, "accuraci": 1, "current": 1, "linkbas": 1, "state": 1, "esa": 2, "test": 1, "new": 1, "document": 1, "method": 2, "match": 1, "analysi": 1, "franc": 1, "interdocu": 3, "demonstr": 1, "measur": 2, "great": 1, "outperform": 1, "well": 1, "interconcept": 3, "wlm": 2, "similar": 7, "propos": 2}, "marker": "(Gabrilovich and Markovitch, 2007)", "article": "W10-3506", "vector_2": [3, 0.0773820763624004, 2, 6, 1, 0]}, {"label": "Neut", "current": "Kessler et al (1997) categorize four types of genre-revealing cues: structural cues (e.g., part-of-speech (POS) tag counts), lexical cues (specific words), character-level cues (e.g., punctuation marks), and derivative cues (ratios and variation measures based on other types of cues).", "context": ["Karlgren and Cutting (1994) were among the first to use simple document statistics, such as common word frequencies, first-person pronoun count, and average sentence length.", "Kessler et al (1997) categorize four types of genre-revealing cues: structural cues (e.g., part-of-speech (POS) tag counts), lexical cues (specific words), character-level cues (e.g., punctuation marks), and derivative cues (ratios and variation measures based on other types of cues).", "Dewdney et al (2001) compare a large number of document features and show that these outperform bag-of-words approaches, which are traditionally used in topic-based text classifica"], "vector_1": {"among": 1, "featur": 1, "pronoun": 1, "show": 1, "text": 1, "eg": 2, "al": 2, "four": 1, "tag": 1, "type": 2, "classifica": 1, "et": 2, "karlgren": 1, "genrerev": 1, "use": 2, "cut": 1, "ratio": 1, "compar": 1, "topicbas": 1, "mark": 1, "kessler": 1, "cue": 6, "approach": 1, "larg": 1, "variat": 1, "firstperson": 1, "document": 2, "simpl": 1, "po": 1, "statist": 1, "bagofword": 1, "deriv": 1, "sentenc": 1, "lexic": 1, "number": 1, "punctuat": 1, "base": 1, "partofspeech": 1, "categor": 1, "averag": 1, "count": 2, "measur": 1, "word": 2, "tradit": 1, "specif": 1, "frequenc": 1, "structur": 1, "length": 1, "dewdney": 1, "common": 1, "outperform": 1, "characterlevel": 1, "first": 1}, "marker": "(1997)", "article": "W15-2518", "vector_2": [18, 0.2312828479820183, 3, 1, 0, 0]}, {"label": "Pos", "current": "More specifically, we use the W x LW matricization of DM.DE, the German version (Pado and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010).", "context": ["The syntactic distributional model that we use represents target words by pairs of dependency relations and context words.", "More specifically, we use the W x LW matricization of DM.DE, the German version (Pado and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010).", "DM.DE was created on the basis of the 884M-token SDEWAC web corpus (Faa et al., 2010), lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010)."], "vector_1": {"corpu": 1, "sdewac": 1, "creat": 1, "german": 2, "toolkit": 1, "repres": 1, "tag": 1, "dmde": 2, "distribut": 2, "web": 1, "use": 2, "depend": 1, "memori": 1, "lw": 1, "version": 1, "matric": 1, "syntact": 1, "context": 1, "relat": 1, "mtoken": 1, "pars": 1, "mate": 1, "pair": 1, "basi": 1, "word": 2, "target": 1, "specif": 1, "lemmat": 1, "w": 1, "x": 1, "model": 1}, "marker": "(Pado and Utt, 2012)", "article": "P13-2128", "vector_2": [1, 0.5767377838953889, 4, 2, 3, 0]}, {"label": "Neut", "current": "Traditional readability formulas normally take into account the number of words per sentence or/and the number of \"hard\", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).", "context": ["Anybody who has seen patent claims at least once will find it unnecessary to calculate claim readability indices to get persuaded that the claim text is extremely low readable.", "Traditional readability formulas normally take into account the number of words per sentence or/and the number of \"hard\", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).", "Both the first and the second ratio will be equal to the number of words in a claim sentence where practically all words are \"hard\" terms, some of them used for the first time."], "vector_1": {"claim": 4, "text": 1, "hard": 2, "number": 3, "indic": 1, "orand": 1, "second": 1, "per": 2, "seen": 1, "find": 1, "use": 1, "long": 1, "ratio": 1, "patent": 1, "normal": 1, "readabl": 3, "persuad": 1, "least": 1, "low": 2, "formula": 1, "take": 1, "get": 1, "sentenc": 3, "term": 1, "tradit": 1, "account": 1, "word": 4, "practic": 1, "frequenc": 1, "equal": 1, "unnecessari": 1, "calcul": 1, "time": 1, "extrem": 1, "anybodi": 1, "first": 2}, "marker": "Greenfield, 2004)", "article": "W14-5605", "vector_2": [10, 0.11372867587327376, 3, 5, 0, 0]}, {"label": "Pos", "current": "We use the logistic regression classifier as implemented in the SKLL package (Blanchard et al., 2013), which is based on scikitlearn (Pedregosa et al., 2011), with F1 optimization (\"metaphor\" class).", "context": ["In this study, each content-word token in a text is an instance that is classified as either a metaphor or not a metaphor.", "We use the logistic regression classifier as implemented in the SKLL package (Blanchard et al., 2013), which is based on scikitlearn (Pedregosa et al., 2011), with F1 optimization (\"metaphor\" class).", "Performance will be evaluated using Precision, Recall, and F-1 score, for the positive (\"metaphor\") class."], "vector_1": {"contentword": 1, "evalu": 1, "text": 1, "packag": 1, "implement": 1, "use": 2, "metaphor": 4, "perform": 1, "classifi": 2, "score": 1, "scikitlearn": 1, "recal": 1, "skll": 1, "optim": 1, "posit": 1, "base": 1, "logist": 1, "class": 2, "f": 2, "precis": 1, "token": 1, "instanc": 1, "either": 1, "regress": 1, "studi": 1}, "marker": "(Blanchard et al., 2013)", "article": "W15-1402", "vector_2": [2, 0.15125298329355608, 2, 2, 8, 0]}, {"label": "Neut", "current": "The FraCaS test suite (Cooper et al., 1996) was developed as part of a collaborative research effort in computational semantics.", "context": ["4 Experiments with the FraCaS test suite", "The FraCaS test suite (Cooper et al., 1996) was developed as part of a collaborative research effort in computational semantics.", "It contains 346 inference problems reminiscent of a textbook on formal semantics."], "vector_1": {"reminisc": 1, "semant": 2, "develop": 1, "problem": 1, "contain": 1, "36": 1, "textbook": 1, "research": 1, "collabor": 1, "part": 1, "suit": 2, "test": 2, "fraca": 2, "comput": 1, "experi": 1, "effort": 1, "infer": 1, "formal": 1}, "marker": "(Cooper et al., 1996)", "article": "W07-1431", "vector_2": [11, 0.5981003977506515, 1, 1, 0, 0]}, {"label": "Pos", "current": "2The shortest path is based on an implementation of Dijkstras graph search algorithm (Dijkstra, 1959)", "context": ["1The weight can be seen as the cost of traversing an edge; hence a lower weight is assigned to a highly contributory relation.", "2The shortest path is based on an implementation of Dijkstras graph search algorithm (Dijkstra, 1959)", "109"], "vector_1": {"travers": 1, "algorithm": 1, "edg": 1, "weight": 2, "graph": 1, "relat": 1, "henc": 1, "lower": 1, "search": 1, "dijkstra": 1, "cost": 1, "implement": 1, "09": 1, "base": 1, "path": 1, "seen": 1, "the": 2, "contributori": 1, "assign": 1, "shortest": 1, "highli": 1}, "marker": "(Dijkstra, 1959)", "article": "S13-2019", "vector_2": [54, 0.3808719773261774, 1, 1, 0, 0]}, {"label": "Neut", "current": "(Milne et al., 2007) proposed a system called \"KORU\" for query expansion using Wikipedia's most relevant articles to user's query.", "context": ["Synonyms from WordNet are used to expand the question in order to extract the most semantically relevant passages to the question.", "(Milne et al., 2007) proposed a system called \"KORU\" for query expansion using Wikipedia's most relevant articles to user's query.", "The system allows the user to refine the set of Wikipedia pages to be used for expansion."], "vector_1": {"semant": 1, "set": 1, "wikipedia": 2, "extract": 1, "wordnet": 1, "use": 3, "passag": 1, "question": 2, "system": 2, "articl": 1, "call": 1, "queri": 2, "expans": 2, "user": 2, "refin": 1, "koru": 1, "relev": 2, "expand": 1, "synonym": 1, "page": 1, "allow": 1, "order": 1, "propos": 1}, "marker": "(Milne et al., 2007)", "article": "W14-3611", "vector_2": [7, 0.2967125936669084, 1, 1, 0, 0]}, {"label": "Neut", "current": "This type of memory function is a simplified representation of models of humans' memory recall capabilities (Anderson et al., 1998; Gillund and Shiffrin, 1984).", "context": ["pr(word) is the probability of a word being retrieved,  is a constant, and c(word) is the number of times the word has been identified in segmentations thus far.", "This type of memory function is a simplified representation of models of humans' memory recall capabilities (Anderson et al., 1998; Gillund and Shiffrin, 1984).", "This memory function for the value of  = 0.05, the value used in our experiments, is given in Figure 1."], "vector_1": {"represent": 1, "identifi": 1, "number": 1, "prword": 1, "human": 1, "constant": 1, "probabl": 1, "use": 1, "cword": 1, "memori": 3, "figur": 1, "experi": 1, "type": 1, "function": 2, "simplifi": 1, "recal": 1, "far": 1, "given": 1, "capabl": 1, "word": 2, "segment": 1, "valu": 2, "retriev": 1, "thu": 1, "time": 1, "model": 1}, "marker": "(Anderson et al., 1998", "article": "W10-2912", "vector_2": [12, 0.5198893532577649, 2, 1, 0, 0]}, {"label": "Neut", "current": "Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system.", "context": ["In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language.", "Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system.", "Sag et al (2001b) introduced Head-driven Phrase Structure Grammar for analyzing MWEs."], "vector_1": {"identifi": 2, "sag": 1, "it": 1, "headdriven": 1, "dufour": 1, "michiel": 1, "phrase": 1, "et": 1, "languag": 1, "project": 1, "use": 1, "system": 1, "idiom": 1, "analyz": 1, "multiword": 1, "wehrli": 1, "introduc": 1, "mwe": 1, "gener": 1, "express": 1, "french": 1, "al": 1, "framework": 1, "translat": 1, "dictionari": 1, "compound": 1, "defi": 1, "grammar": 2, "structur": 1, "employ": 1, "mt": 1, "english": 1, "englishfrench": 1}, "marker": "(1998)", "article": "W03-1807", "vector_2": [5, 0.1859921221073363, 3, 1, 0, 0]}, {"label": "Neut", "current": "We set n = 3 (i.e., trigram language model) following Kita's work and use Kneser-Ney (KN) smoothing (Kneser and Ney, 1995) to estimate its conditional probabilities.", "context": ["Now, the language model Mi can be built from Di.", "We set n = 3 (i.e., trigram language model) following Kita's work and use Kneser-Ney (KN) smoothing (Kneser and Ney, 1995) to estimate its conditional probabilities.", "With Mi and Di, we can naturally apply Kita's method to our task."], "vector_1": {"kita": 2, "set": 1, "appli": 1, "trigram": 1, "follow": 1, "ie": 1, "languag": 2, "probabl": 1, "use": 1, "built": 1, "estim": 1, "natur": 1, "mi": 2, "method": 1, "di": 2, "condit": 1, "task": 1, "work": 1, "smooth": 1, "kn": 1, "n": 1, "model": 2, "kneserney": 1}, "marker": "(Kneser and Ney, 1995)", "article": "P13-1112", "vector_2": [18, 0.29280785155650974, 1, 1, 0, 0]}, {"label": "Pos", "current": "After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).", "context": ["Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.", "After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).", "However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al."], "vector_1": {"entropi": 1, "tackl": 1, "global": 1, "share": 3, "al": 1, "indic": 1, "attain": 1, "fail": 1, "et": 1, "seem": 1, "research": 1, "improv": 1, "best": 2, "entiti": 1, "use": 2, "develop": 1, "perform": 3, "make": 1, "particip": 1, "system": 3, "long": 1, "label": 1, "field": 1, "score": 1, "treatment": 1, "postprocess": 1, "import": 1, "might": 1, "random": 1, "zhou": 1, "optim": 1, "reach": 1, "jnlpba": 1, "like": 1, "memm": 1, "extens": 1, "promis": 1, "condit": 1, "implicitli": 1, "markov": 1, "enabl": 1, "comparison": 1, "task": 4, "name": 1, "local": 1, "howev": 1, "maximum": 1, "inform": 1, "achiev": 1, "crf": 2, "mani": 2, "model": 2}, "marker": "(Lafferty et al., 2001)", "article": "W07-1033", "vector_2": [6, 0.07043368154479265, 4, 2, 4, 0]}, {"label": "Neut", "current": "The corpus used to collect the features and their frequencies is the Web 1TB corpus (Brants and Franz, 2006).", "context": ["The compositional model is based on phrase words vectors addition, where each vector is composed of the collocation pointwise mutual information of the word up to a window of 3 words left and right of the main word.", "The corpus used to collect the features and their frequencies is the Web 1TB corpus (Brants and Franz, 2006).", "For the Interview to Formal Meeting example, the vector of the word interview is first created from the corpus of the top 1000 words collocating interview between the window of 1 to 3 words with their frequencies."], "vector_1": {"corpu": 3, "right": 1, "composit": 1, "creat": 1, "featur": 1, "pointwis": 1, "phrase": 1, "web": 1, "colloc": 2, "inform": 1, "top": 1, "compos": 1, "window": 2, "exampl": 1, "interview": 3, "main": 1, "tb": 1, "use": 1, "base": 1, "formal": 1, "mutual": 1, "addit": 1, "word": 7, "frequenc": 2, "000": 1, "collect": 1, "vector": 3, "meet": 1, "model": 1, "first": 1, "left": 1}, "marker": "(Brants and Franz, 2006)", "article": "S13-2019", "vector_2": [7, 0.5538201772899958, 1, 2, 0, 0]}, {"label": "Neut", "current": "Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.", "context": ["Researchers including Wong and Dras (2009), Wong et al (2011; 2012), and Koppel et al (2005) work on native language identification and show that machine learning-based methods are effective.", "Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.", "Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11."], "vector_1": {"show": 3, "predict": 1, "text": 1, "al": 2, "expect": 1, "et": 2, "differ": 1, "languag": 4, "better": 1, "use": 2, "perform": 3, "research": 2, "identif": 2, "also": 1, "speaker": 2, "includ": 1, "method": 2, "machin": 2, "sourc": 1, "emphas": 1, "relat": 1, "effect": 1, "nativ": 3, "task": 1, "translat": 2, "although": 1, "koppel": 1, "given": 1, "grammat": 1, "learningbas": 2, "contrari": 1, "target": 1, "nonn": 1, "work": 1, "rather": 1, "inform": 1, "achiev": 1, "wong": 3, "determin": 1, "error": 2, "dra": 2, "improv": 1, "typic": 1, "propos": 1}, "marker": "(2009)", "article": "P13-1112", "vector_2": [4, 0.8277871492102439, 5, 3, 0, 0]}, {"label": "Neut", "current": "One hitch is that the number of parses may be exponential in the size of the input sentence, or even infinite for cyclic grammars or incomplete sentences [16].", "context": ["is then to produce all possible parses, according to the CF backbone, and then select among them on the basis of the complete features information.", "One hitch is that the number of parses may be exponential in the size of the input sentence, or even infinite for cyclic grammars or incomplete sentences [16].", "However chart parsing techniques have been developed that produce an encoding of all possible parses as a data structure with a size polynomial in the length of the input sentence."], "vector_1": {"among": 1, "featur": 1, "encod": 1, "number": 1, "hitch": 1, "incomplet": 1, "select": 1, "size": 2, "infinit": 1, "develop": 1, "techniqu": 1, "exponenti": 1, "input": 2, "complet": 1, "even": 1, "accord": 1, "may": 1, "sentenc": 3, "chart": 1, "cf": 1, "polynomi": 1, "pars": 4, "one": 1, "data": 1, "basi": 1, "grammar": 1, "backbon": 1, "possibl": 2, "howev": 1, "cyclic": 1, "structur": 1, "inform": 1, "length": 1, "produc": 2}, "marker": "[16]", "article": "P89-1018", "vector_2": [1, 0.07926849653973454, 1, 4, 7, 1]}, {"label": "Pos", "current": "For estimating the STOP probabilities (Section 3), we use the Wikipedia articles from W2C corpus (Majlis and Zabokrtsky, 2012), which provide sufficient amount of data for our purposes.", "context": ["In case a punctuation node was not a leaf, its children are attached to the parent of the removed node.", "For estimating the STOP probabilities (Section 3), we use the Wikipedia articles from W2C corpus (Majlis and Zabokrtsky, 2012), which provide sufficient amount of data for our purposes.", "Statistics across languages are shown in Table 1."], "vector_1": {"corpu": 1, "tabl": 1, "children": 1, "languag": 1, "probabl": 1, "use": 1, "leaf": 1, "suffici": 1, "section": 1, "wikipedia": 1, "articl": 1, "attach": 1, "estim": 1, "across": 1, "node": 2, "wc": 1, "parent": 1, "stop": 1, "shown": 1, "punctuat": 1, "data": 1, "case": 1, "provid": 1, "remov": 1, "amount": 1, "statist": 1, "purpos": 1}, "marker": "(Majlis and Zabokrtsky, 2012)", "article": "P13-1028", "vector_2": [1, 0.6720904911686426, 1, 1, 8, 0]}, {"label": "Neut", "current": "We use the training data and computed the three features (Frequent Collocation (FC), Semantic Relatedness word Before (SRB), and Semantic Relatedness word After (SRA), and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999) to learn a set of rule that differentiate between a figurative and literal phrase use.", "context": ["The data set contains a total of 1114 training instances, and 518 test instances.", "We use the training data and computed the three features (Frequent Collocation (FC), Semantic Relatedness word Before (SRB), and Semantic Relatedness word After (SRA), and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999) to learn a set of rule that differentiate between a figurative and literal phrase use.", "This method resulted in a set of rules that can be summarized as follows: if FC is equal to 0 and SRB < 75% then it is used literally in this context, else if FC is equal to 0 and SRA < 75% then it is is also used literally, otherwise it is used figuratively."], "vector_1": {"semant": 2, "set": 3, "comput": 1, "cohen": 1, "related": 2, "jrip": 1, "result": 1, "follow": 1, "phrase": 1, "total": 1, "featur": 1, "summar": 1, "use": 6, "weka": 1, "three": 1, "also": 1, "figur": 2, "instanc": 2, "test": 1, "els": 1, "method": 1, "liter": 3, "learn": 2, "colloc": 1, "differenti": 1, "train": 2, "ripper": 1, "data": 2, "srb": 2, "sra": 2, "word": 2, "algorithm": 1, "equal": 2, "rule": 3, "fc": 3, "context": 1, "contain": 1, "otherwis": 1, "implement": 1, "frequent": 1}, "marker": "(Witten et al., 1999)", "article": "S13-2019", "vector_2": [14, 0.9035156485557498, 2, 2, 0, 0]}, {"label": "Neut", "current": "Kukich (1992), citing a number of studies, reports that typically 80% of misspelled words contain a single error of one of the unit operations, although", "context": ["For edit distance thresholds 1, 2, and 3, we selected 1,000 words at random from each word list and perturbed them by random insertions, deletions, replacements, and transpositions, so that each misspelled word had the required edit distance from the correct form.", "Kukich (1992), citing a number of studies, reports that typically 80% of misspelled words contain a single error of one of the unit operations, although", "84"], "vector_1": {"oper": 1, "distanc": 2, "perturb": 1, "replac": 1, "random": 2, "number": 1, "one": 1, "threshold": 1, "select": 1, "delet": 1, "unit": 1, "cite": 1, "singl": 1, "form": 1, "error": 1, "misspel": 2, "000": 1, "although": 1, "report": 1, "typic": 1, "requir": 1, "insert": 1, "word": 4, "transposit": 1, "edit": 2, "list": 1, "correct": 1, "contain": 1, "studi": 1, "kukich": 1}, "marker": "(1992)", "article": "J96-1003", "vector_2": [4, 0.7539378224048991, 1, 1, 0, 0]}, {"label": "Neut", "current": "provide the learner the most flexibility, allowing the learner to build structure from the individual phonemes and apply distributions at each level of abstraction (Johnson and Goldwater, 2009).", "context": ["Modeling the corpus using hierarchical grammars that can model the input at varying levels (word collocations, words, syllables, onsets, etc.)", "provide the learner the most flexibility, allowing the learner to build structure from the individual phonemes and apply distributions at each level of abstraction (Johnson and Goldwater, 2009).", "While this results in state-of-the-art performance for segmentation performed at the phoneme level, this approach requires significant computational resources as each additional level of representation increases the complexity of learning."], "vector_1": {"corpu": 1, "represent": 1, "comput": 1, "appli": 1, "individu": 1, "signific": 1, "result": 1, "phonem": 2, "flexibl": 1, "vari": 1, "hierarch": 1, "use": 1, "perform": 2, "learner": 2, "onset": 1, "complex": 1, "build": 1, "input": 1, "approach": 1, "abstract": 1, "resourc": 1, "distribut": 1, "colloc": 1, "increas": 1, "word": 2, "segment": 1, "requir": 1, "addit": 1, "grammar": 1, "level": 4, "provid": 1, "structur": 1, "syllabl": 1, "etc": 1, "stateoftheart": 1, "allow": 1, "learn": 1, "model": 2}, "marker": "(Johnson and Goldwater, 2009)", "article": "W10-2912", "vector_2": [1, 0.1521994046724195, 1, 2, 2, 0]}, {"label": "CoCo", "current": "The decoding strategy in DTM2 (Ittycheriah and Roukos, 2007) is similar to a phrase-based decoder except that the score of a particular translation block is obtained from the maximum entropy model using the set of feature functions.", "context": ["The DTM2 model differs from other phrasebased SMT models in that it avoids the redundancy present in other systems by extracting from a word aligned parallel corpora a set of minimal phrases such that no two phrases overlap with each other (Hassan et al., 2009).", "The decoding strategy in DTM2 (Ittycheriah and Roukos, 2007) is similar to a phrase-based decoder except that the score of a particular translation block is obtained from the maximum entropy model using the set of feature functions.", "In our approach, instead of providing the complete scoring function ourselves, we compute the parameters needed by a phrase based decoder, which in turn uses these parameters appropriately."], "vector_1": {"set": 2, "entropi": 1, "obtain": 1, "two": 1, "featur": 1, "need": 1, "phrase": 3, "comput": 1, "extract": 1, "paramet": 2, "differ": 1, "minim": 1, "avoid": 1, "smt": 1, "system": 1, "overlap": 1, "score": 2, "decod": 3, "instead": 1, "approach": 1, "except": 1, "strategi": 1, "function": 2, "use": 2, "base": 1, "translat": 1, "particular": 1, "redund": 1, "parallel": 1, "present": 1, "appropri": 1, "word": 1, "dtm": 2, "provid": 1, "align": 1, "corpora": 1, "maximum": 1, "phrasebas": 2, "turn": 1, "model": 3, "complet": 1, "similar": 1, "block": 1}, "marker": "(Ittycheriah and Roukos, 2007)", "article": "W10-3805", "vector_2": [3, 0.31731035300628535, 2, 3, 2, 0]}, {"label": "Neut", "current": "Similar methods have also been used in the field of intrinsic plagiarism detection, which involves segmenting a text and then identifying outlier segments (Stamatatos, 2009; Stein et al., 2010).", "context": ["The vectorization methods employ a variety of lexical and syntactic stylistic features, while the outlier detection methods use a variety of similarity/distance measures such as cosine and Euclidean distance.", "Similar methods have also been used in the field of intrinsic plagiarism detection, which involves segmenting a text and then identifying outlier segments (Stamatatos, 2009; Stein et al., 2010).", "3 Proximity Methods"], "vector_1": {"similaritydist": 1, "identifi": 1, "text": 1, "featur": 1, "involv": 1, "use": 2, "detect": 2, "field": 1, "proxim": 1, "also": 1, "plagiar": 1, "varieti": 2, "method": 4, "syntact": 1, "outlier": 2, "lexic": 1, "distanc": 1, "segment": 2, "measur": 1, "stylist": 1, "intrins": 1, "employ": 1, "euclidean": 1, "vector": 1, "cosin": 1, "similar": 1}, "marker": "(Stamatatos, 2009", "article": "D13-1151", "vector_2": [4, 0.2822397086557846, 2, 1, 1, 0]}, {"label": "Neut", "current": "Information Quality Dimensions (Source: Strong, Lee, Wang, 1997, p.39)", "context": ["Table 1.", "Information Quality Dimensions (Source: Strong, Lee, Wang, 1997, p.39)", "Empirical attempts to assess quality have primarily focused on counting hyperlinks in a networked environment."], "vector_1": {"dimens": 1, "count": 1, "sourc": 1, "attempt": 1, "network": 1, "primarili": 1, "p": 1, "hyperlink": 1, "qualiti": 2, "focus": 1, "assess": 1, "inform": 1, "environ": 1, "empir": 1, "tabl": 1}, "marker": "Strong, Lee, Wang, 1997, ", "article": "N03-2033", "vector_2": [6, 0.24273086764363805, 1, 1, 1, 0]}, {"label": "Neut", "current": "Finkel et al (2008).1 The grammars in our experiments have on the order of thousands of nonterminals and millions of productions.", "context": ["Figure 2: The chart that visualizes the bottom-up process of CKY parsing for the sentence \"I love you .\"", "Finkel et al (2008).1 The grammars in our experiments have on the order of thousands of nonterminals and millions of productions.", "Figure 1(a) shows a constituency parse tree."], "vector_1": {"constitu": 1, "love": 1, "show": 1, "process": 1, "tree": 1, "al": 1, "et": 1, "figur": 2, "experi": 1, "product": 1, "sentenc": 1, "thousand": 1, "million": 1, "chart": 1, "visual": 1, "pars": 2, "finkel": 1, "a": 1, "grammar": 1, "cki": 1, "nontermin": 1, "bottomup": 1, "order": 1}, "marker": "(2008)", "article": "W11-2921", "vector_2": [3, 0.14298803633602974, 1, 1, 0, 0]}, {"label": "Neut", "current": "Transition-based models are widely used for dependency tree parsing, and they can be adapted to graph parsing (Sagae and Tsujii, 2008; Titov et al., 2009).", "context": ["We explore two kinds of basic models: One is transition-based, and the other is tree approximation.", "Transition-based models are widely used for dependency tree parsing, and they can be adapted to graph parsing (Sagae and Tsujii, 2008; Titov et al., 2009).", "Here we implement 5 transitionbased models for dependency graph parsing, each of which is based on different transition system."], "vector_1": {"wide": 1, "kind": 1, "depend": 2, "system": 1, "transit": 1, "graph": 2, "approxim": 1, "use": 1, "tree": 2, "two": 1, "one": 1, "base": 1, "pars": 3, "adapt": 1, "explor": 1, "basic": 1, "transitionbas": 3, "model": 3, "differ": 1, "implement": 1}, "marker": "(Sagae and Tsujii, 2008", "article": "S14-2080", "vector_2": [6, 0.13527748516937407, 2, 1, 1, 0]}, {"label": "Neut", "current": "Finally, Gurevych (2005) translated the datasets into German.", "context": ["Because of the fact that the Pearson correlation with the original datasets was 0.86, only one translator translated the datasets into Arabic and Romanian.", "Finally, Gurevych (2005) translated the datasets into German.", "However, no instructions, as to how it was done, were provided."], "vector_1": {"origin": 1, "pearson": 1, "german": 1, "romanian": 1, "howev": 1, "gurevych": 1, "one": 1, "arab": 1, "provid": 1, "instruct": 1, "correl": 1, "done": 1, "translat": 3, "dataset": 3, "final": 1, "fact": 1}, "marker": "(2005)", "article": "W14-0118", "vector_2": [9, 0.22892207507592122, 1, 1, 0, 0]}, {"label": "Neut", "current": "In previous work, Harsch and Hartig (2010) examine dependencies between individual gaps using a Rasch testlet model and find that some gaps strongly depend on each other, while others can be solved independently.", "context": ["This macro-level dimension assesses the dependency of the current gap on previous gaps: can it be solved, even if the previous gap has not been solved?", "In previous work, Harsch and Hartig (2010) examine dependencies between individual gaps using a Rasch testlet model and find that some gaps strongly depend on each other, while others can be solved independently.", "At the same time, fertility is set to fall as women leave childbirth la and la ."], "vector_1": {"dimens": 1, "set": 1, "individu": 1, "independ": 1, "strongli": 1, "find": 1, "even": 1, "use": 1, "depend": 3, "la": 2, "harsch": 1, "current": 1, "other": 1, "leav": 1, "testlet": 1, "hartig": 1, "previou": 3, "rasch": 1, "gap": 5, "examin": 1, "fall": 1, "solv": 3, "fertil": 1, "childbirth": 1, "women": 1, "assess": 1, "work": 1, "macrolevel": 1, "time": 1, "model": 1}, "marker": "(2010)", "article": "Q14-1040", "vector_2": [4, 0.6094328551484127, 1, 1, 0, 0]}, {"label": "Neut", "current": "Each pulse in the Spreading Activation (SA) process consists of three stages: 1) pre-adjustment, 2) spreading, and 3) post-adjustment (Crestani, 1997).", "context": ["4 Adapting Spreading Activation for Wikipedia's Hyperlink Structure", "Each pulse in the Spreading Activation (SA) process consists of three stages: 1) pre-adjustment, 2) spreading, and 3) post-adjustment (Crestani, 1997).", "During pre- and post-adjustment, some form of activation decay is optionally applied to the active nodes."], "vector_1": {"pre": 1, "node": 1, "form": 1, "consist": 1, "hyperlink": 1, "process": 1, "activ": 4, "preadjust": 1, "wikipedia": 1, "three": 1, "structur": 1, "spread": 3, "decay": 1, "adapt": 1, "option": 1, "appli": 1, "postadjust": 2, "sa": 1, "puls": 1, "stage": 1}, "marker": "(Crestani, 1997)", "article": "W10-3506", "vector_2": [13, 0.2772763321877714, 1, 3, 0, 0]}, {"label": "Neut", "current": "According to Deane and Sheehan (2003), it is possible to change the wording of a text without changing its difficulty.", "context": ["When introducing variation in the narrative form of the exercise, it is important that variations of the same exercise should all have the same meaning and approximately the same difficulty.", "According to Deane and Sheehan (2003), it is possible to change the wording of a text without changing its difficulty.", "Reiter and Dale (2000) state that for example aggregating multiple sentences does not change the information they express, but improves the readability and fluency of the text."], "vector_1": {"dean": 1, "text": 2, "approxim": 1, "fluenci": 1, "aggreg": 1, "multipl": 1, "sheehan": 1, "readabl": 1, "narr": 1, "state": 1, "without": 1, "exercis": 2, "sentenc": 1, "import": 1, "introduc": 1, "dale": 1, "difficulti": 2, "accord": 1, "form": 1, "variat": 2, "express": 1, "word": 1, "possibl": 1, "reiter": 1, "inform": 1, "exampl": 1, "improv": 1, "chang": 3, "mean": 1}, "marker": "(2003)", "article": "W11-1403", "vector_2": [8, 0.4891476042824548, 2, 4, 1, 0]}, {"label": "Neut", "current": "One can prove that Ak is the best approximation to A for any unitray invariant norm(Michael W. Berry and Jessup, 1999) three variant probability estimation methods in dimension-reduced space.", "context": ["Now, we suggest 21n other words, the projection into the reduced space is chosen such that the representations in the original space are changed as little as possible when measured by the sum of the squares of the differences.", "One can prove that Ak is the best approximation to A for any unitray invariant norm(Michael W. Berry and Jessup, 1999) three variant probability estimation methods in dimension-reduced space.", "First is estimating p(y Ix) by computing distance between given word x and predicting word y in reduced space."], "vector_1": {"origin": 1, "represent": 1, "distanc": 1, "comput": 1, "ix": 1, "predict": 1, "approxim": 1, "normmichael": 1, "one": 1, "best": 1, "probabl": 1, "differ": 1, "squar": 1, "prove": 1, "chosen": 1, "suggest": 1, "sum": 1, "py": 1, "three": 1, "littl": 1, "method": 1, "estim": 2, "ak": 1, "unitray": 1, "dimensionreduc": 1, "variant": 1, "given": 1, "reduc": 2, "measur": 1, "word": 3, "possibl": 1, "space": 4, "n": 1, "project": 1, "w": 1, "x": 1, "chang": 1, "invari": 1, "first": 1}, "marker": "Berry and Jessup, 1999)", "article": "P00-1072", "vector_2": [1, 0.3853772645829186, 1, 3, 0, 0]}, {"label": "Neut", "current": "(Dahlmann and Adolphs, 2007, p. 55) In many ways, the present study can, and should, be viewed as an extension of Dahlmann and Adolphs study.", "context": ["They conclude that ...where pauses occur they give valuable indications of possible [MWE] boundaries.", "(Dahlmann and Adolphs, 2007, p. 55) In many ways, the present study can, and should, be viewed as an extension of Dahlmann and Adolphs study.", "If we view keystroke dynamics as a reflection of many speech production principles in the typing process, then this is a reasonable extension."], "vector_1": {"paus": 1, "give": 1, "process": 1, "indic": 1, "occur": 1, "dynam": 1, "boundari": 1, "dahlmann": 1, "speech": 1, "way": 1, "type": 1, "product": 1, "mwe": 1, "conclud": 1, "adolph": 1, "reflect": 1, "extens": 2, "reason": 1, "keystrok": 1, "present": 1, "valuabl": 1, "possibl": 1, "p": 1, "mani": 2, "studi": 2, "principl": 1, "view": 2}, "marker": "(Dahlmann and Adolphs, 2007, ", "article": "W15-0914", "vector_2": [8, 0.225881393927043, 1, 6, 0, 0]}, {"label": "Neut", "current": "Guindon & Shuldberg, 1987; Ringle & Halstead-Nussloch, 1989; Sidner & Forlines, 2002).", "context": ["Several studies have previously found that users are able to interact successfully using constrained or subset languages (e.g.", "Guindon & Shuldberg, 1987; Ringle & Halstead-Nussloch, 1989; Sidner & Forlines, 2002).", "As far as we know, no studies have been done comparing constrained, \"universal\" languages and natural language interfaces directly as we have done in this study."], "vector_1": {"subset": 1, "use": 1, "compar": 1, "success": 1, "far": 1, "eg": 1, "interact": 1, "univers": 1, "abl": 1, "know": 1, "interfac": 1, "directli": 1, "languag": 3, "user": 1, "done": 2, "natur": 1, "found": 1, "studi": 3, "constrain": 2, "sever": 1, "previous": 1}, "marker": "Guindon & Shuldberg, 1987", "article": "N04-4019", "vector_2": [17, 0.2148906585766654, 3, 1, 0, 0]}, {"label": "Neut", "current": "and a different level of measurement, following (Stevens, 1946).", "context": ["Figure 4: UML class diagram (simplified) of the FiESTA data model.", "and a different level of measurement, following (Stevens, 1946).", "Scales can be left independent, or a synchronisation betweeen them can be expressed (e. g., a linear transformation between a video-frame-based scale and a millisecond-based one, or a manual alignment using explicit alignment points)."], "vector_1": {"point": 1, "explicit": 1, "independ": 1, "one": 1, "betweeen": 1, "follow": 1, "differ": 1, "scale": 2, "millisecondbas": 1, "transform": 1, "figur": 1, "uml": 1, "simplifi": 1, "linear": 1, "express": 1, "fiesta": 1, "use": 1, "diagram": 1, "videoframebas": 1, "data": 1, "class": 1, "measur": 1, "e": 1, "g": 1, "level": 1, "align": 2, "manual": 1, "synchronis": 1, "model": 1, "left": 1}, "marker": "(Stevens, 1946)", "article": "W13-5507", "vector_2": [67, 0.5243328100470958, 1, 1, 0, 0]}, {"label": "Neut", "current": "Racing for model selection (Maron and Moore, 1994; Moore and Lee, 1994) works as follows: we are given a collection of N,,t models and Nd data points, and we must find the  model that minimizes the mean e j = 1 i ej(i),", "context": ["Prior to considering the SMT case, we review one of these methods in the case of leave-one-out cross validation (LOOCV).", "Racing for model selection (Maron and Moore, 1994; Moore and Lee, 1994) works as follows: we are given a collection of N,,t models and Nd data points, and we must find the  model that minimizes the mean e j = 1 i ej(i),", "Nd"], "vector_1": {"j": 1, "point": 1, "loocv": 1, "one": 1, "leaveoneout": 1, "follow": 1, "find": 1, "select": 1, "given": 1, "eji": 1, "prior": 1, "review": 1, "smt": 1, "nd": 2, "cross": 1, "valid": 1, "nt": 1, "method": 1, "minim": 1, "consid": 1, "data": 1, "must": 1, "case": 2, "e": 1, "work": 1, "collect": 1, "race": 1, "model": 3, "mean": 1}, "marker": "(Maron and Moore, 1994", "article": "W12-3159", "vector_2": [18, 0.47133837751165003, 2, 2, 0, 0]}, {"label": "Neut", "current": "LALR(1) and LALR(2) [6], weak precedence [12], LL(0) top-down (recursive descent), LR(0), LR(1) [1] ...).", "context": ["(e.g.", "LALR(1) and LALR(2) [6], weak precedence [12], LL(0) top-down (recursive descent), LR(0), LR(1) [1] ...).", "2. to study the computational behavior of the generated code, and the optimization techniques that could be used on the lin code  and more generally chart parser code  with respect to code size, execution speed and better sharing in the parse forest."], "vector_1": {"code": 4, "comput": 1, "execut": 1, "eg": 1, "parser": 1, "share": 1, "respect": 1, "speed": 1, "size": 1, "use": 1, "descent": 1, "lin": 1, "better": 1, "lr": 2, "forest": 1, "topdown": 1, "optim": 1, "gener": 2, "weak": 1, "chart": 1, "pars": 1, "recurs": 1, "techniqu": 1, "could": 1, "preced": 1, "behavior": 1, "studi": 1, "ll": 1, "lalr": 2}, "marker": "[1]", "article": "P89-1018", "vector_2": [17, 0.636722276085721, 3, 4, 0, 0]}, {"label": "Neut", "current": "Given that the category distribution is generally heavily skewed towards the non-metaphor category (see Table 1), we experimented with cost-sensitive machine learning techniques to try to correct for the imbalanced class distribution (Yang et al., 2014; Muller et al., 2014).", "context": ["5 Experiment 1: Re-weighting of Examples", "Given that the category distribution is generally heavily skewed towards the non-metaphor category (see Table 1), we experimented with cost-sensitive machine learning techniques to try to correct for the imbalanced class distribution (Yang et al., 2014; Muller et al., 2014).", "The first technique uses AutoWeight (as implemented in the auto flag in scikitlearn toolkit), where we assign weights that are inversely proportional to the class frequencies.2 Table 2 shows the results."], "vector_1": {"weight": 1, "show": 1, "heavili": 1, "toolkit": 1, "see": 1, "result": 1, "tabl": 2, "proport": 1, "given": 1, "techniqu": 2, "skew": 1, "invers": 1, "categori": 2, "costsensit": 1, "scikitlearn": 1, "experi": 2, "correct": 1, "machin": 1, "distribut": 2, "auto": 1, "gener": 1, "use": 1, "nonmetaphor": 1, "flag": 1, "imbalanc": 1, "class": 2, "tri": 1, "reweight": 1, "frequenc": 1, "autoweight": 1, "exampl": 1, "learn": 1, "implement": 1, "toward": 1, "assign": 1, "first": 1}, "marker": "(Yang et al., 2014", "article": "W15-1402", "vector_2": [1, 0.1896048793423495, 2, 1, 0, 0]}, {"label": "CoCo", "current": "The presented work is similar to that of Agirre et al (2009), but is applied to a fully statistical MT system.", "context": ["Weller et al (2014) use noun class information as tree labels in syntactic SMT to model selectional preferences of prepositions.", "The presented work is similar to that of Agirre et al (2009), but is applied to a fully statistical MT system.", "The main difference is that Agirre et al (2009) use linguistic information to select appropriate translation rules, whereas we generate prepositions in a post-processing step."], "vector_1": {"preposit": 2, "appli": 1, "prefer": 1, "al": 3, "et": 3, "select": 2, "use": 2, "weller": 1, "mt": 1, "smt": 1, "system": 1, "label": 1, "postprocess": 1, "main": 1, "syntact": 1, "gener": 1, "wherea": 1, "differ": 1, "step": 1, "translat": 1, "class": 1, "present": 1, "agirr": 2, "appropri": 1, "noun": 1, "work": 1, "tree": 1, "rule": 1, "inform": 2, "statist": 1, "fulli": 1, "model": 1, "linguist": 1, "similar": 1}, "marker": "(2009)", "article": "W15-4923", "vector_2": [6, 0.14290369887567214, 3, 3, 0, 0]}, {"label": "Neut", "current": "Wu and Chang (2007), Kwok et al (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration.", "context": ["Their work was concentrated on terminologies, and assumed the English terms were given as input.", "Wu and Chang (2007), Kwok et al (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration.", "It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by."], "vector_1": {"lexicon": 1, "kwok": 1, "hard": 1, "al": 1, "terminolog": 1, "largescal": 1, "et": 1, "assum": 2, "concentr": 1, "given": 2, "engin": 1, "also": 1, "build": 1, "way": 1, "input": 2, "difficult": 1, "may": 1, "search": 1, "wu": 1, "translat": 1, "transliter": 1, "come": 1, "truli": 1, "term": 3, "name": 1, "work": 1, "focu": 1, "employ": 1, "english": 3, "chang": 1}, "marker": "(2007)", "article": "P08-1113", "vector_2": [1, 0.9475373586789828, 2, 2, 0, 0]}, {"label": "Neut", "current": "3Similar baselines for comparison have been used for many classification problems (Duda and Hart, 1973), e.g., part-of-speech tagging (Church, 1988; Allen, 1995).", "context": ["2This test was suggested by Judith Klavans (personal communication).", "3Similar baselines for comparison have been used for many classification problems (Duda and Hart, 1973), e.g., part-of-speech tagging (Church, 1988; Allen, 1995).", "159"], "vector_1": {"comparison": 1, "use": 1, "partofspeech": 1, "commun": 1, "suggest": 1, "eg": 1, "classif": 1, "klavan": 1, "person": 1, "tag": 1, "thi": 1, "test": 1, "mani": 1, "problem": 1, "similar": 1, "judith": 1, "baselin": 1}, "marker": "(Duda and Hart, 1973)", "article": "W97-0318", "vector_2": [24, 0.7149963002548714, 3, 1, 0, 0]}, {"label": "Neut", "current": "Riloff et al (2013) identify sarcasm that arises from the contrast between a positive sentiment referring to a negative situation.", "context": ["They concluded that user-labelled sarcastic tweets can be noisy and constitute the hardest form of sarcasm.", "Riloff et al (2013) identify sarcasm that arises from the contrast between a positive sentiment referring to a negative situation.", "Reyes et al (2012) involved in their work features that make use of contextual imbalance, natural language concepts, syntactical and morphological aspects of a tweet."], "vector_1": {"featur": 1, "identifi": 1, "natur": 1, "al": 2, "userlabel": 1, "concept": 1, "aspect": 1, "et": 2, "constitut": 1, "languag": 1, "involv": 1, "morpholog": 1, "use": 1, "rey": 1, "sentiment": 1, "neg": 1, "tweet": 2, "sarcast": 1, "aris": 1, "contrast": 1, "refer": 1, "syntact": 1, "form": 1, "conclud": 1, "situat": 1, "noisi": 1, "imbal": 1, "work": 1, "contextu": 1, "hardest": 1, "riloff": 1, "sarcasm": 2, "posit": 1, "make": 1}, "marker": "(2013)", "article": "S15-2120", "vector_2": [2, 0.2737241670181358, 2, 1, 0, 0]}, {"label": "Pos", "current": "In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.", "context": ["6, this paper reveals several crucial findings that contribute to improving native language identification.", "In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.", "The rest of this paper is structured as follows."], "vector_1": {"show": 1, "rest": 1, "one": 1, "paper": 3, "famili": 1, "follow": 1, "find": 2, "languag": 2, "crucial": 1, "identif": 1, "sever": 1, "contribut": 2, "nativ": 1, "reconstruct": 1, "addit": 1, "reveal": 1, "task": 1, "central": 1, "could": 1, "tree": 1, "histor": 1, "structur": 1, "improv": 1, "linguist": 1}, "marker": "Barbancon et al., 2007", "article": "P13-1112", "vector_2": [6, 0.12737310228492563, 5, 2, 2, 0]}, {"label": "Neut", "current": "Difficulty prediction of C-tests has up to now only been performed on the paragraph level (KleinBraley, 1984; Traxel and Dresemann, 2010).", "context": ["Having introduced all four dimensions of C-test difficulty, we now report on the results of the actual difficulty prediction.", "Difficulty prediction of C-tests has up to now only been performed on the paragraph level (KleinBraley, 1984; Traxel and Dresemann, 2010).", "In this article, we go beyond paragraphs and predict the difficulty of gaps."], "vector_1": {"dimens": 1, "difficulti": 4, "actual": 1, "level": 1, "predict": 3, "articl": 1, "four": 1, "perform": 1, "gap": 1, "paragraph": 2, "introduc": 1, "go": 1, "report": 1, "beyond": 1, "ctest": 2, "result": 1}, "marker": "Traxel and Dresemann, 2010)", "article": "Q14-1040", "vector_2": [4, 0.6746049410193634, 2, 1, 0, 0]}, {"label": "Neut", "current": "The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003).", "context": ["Considerable efforts have been made in the NLP community in the study of Chinese word segmentation.", "The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003).", "Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et"], "vector_1": {"nlp": 1, "featur": 1, "consider": 1, "design": 1, "et": 1, "supervis": 1, "use": 1, "commun": 1, "effort": 1, "system": 1, "label": 1, "treat": 1, "approach": 1, "asahara": 1, "model": 1, "linear": 1, "sequenc": 1, "chines": 1, "previou": 1, "task": 1, "address": 1, "segment": 2, "problem": 1, "care": 1, "made": 1, "word": 2, "statist": 1, "popular": 1, "studi": 1, "first": 1, "propos": 1}, "marker": "(Xue et al., 2003)", "article": "D15-1142", "vector_2": [12, 0.24170291468301539, 2, 4, 0, 0]}, {"label": "Neut", "current": "Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "context": ["We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.", "Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed."], "vector_1": {"often": 1, "process": 1, "natur": 1, "parser": 2, "focus": 1, "finegrain": 1, "exploit": 1, "result": 1, "massiv": 1, "languag": 1, "differ": 1, "anoth": 1, "priorit": 1, "intermedi": 1, "sentenc": 1, "previou": 1, "speedup": 1, "pars": 1, "inher": 1, "parallel": 2, "agendabas": 2, "refin": 1, "work": 1, "iter": 1, "order": 1, "queue": 1, "achiev": 1, "maintain": 1, "combin": 1, "whole": 1, "magnitud": 1}, "marker": "(van Lohuizen, 1999", "article": "W11-2921", "vector_2": [12, 0.8912013403492238, 4, 3, 0, 0]}, {"label": "Neut", "current": "The Berkeley FrameNet project (Baker et al., 1998) is building a semantic lexicon for English describing the frames and linking them to the words and expressions that can evoke them.", "context": ["Each frame provides its specific set of semantic roles.", "The Berkeley FrameNet project (Baker et al., 1998) is building a semantic lexicon for English describing the frames and linking them to the words and expressions that can evoke them.", "These can be verbs as well as nouns, adjectives, prepositions, adverbs, and multiword expressions."], "vector_1": {"preposit": 1, "semant": 2, "set": 1, "frame": 2, "lexicon": 1, "describ": 1, "adverb": 1, "adject": 1, "role": 1, "build": 1, "multiword": 1, "evok": 1, "express": 2, "verb": 1, "link": 1, "berkeley": 1, "noun": 1, "word": 1, "specif": 1, "provid": 1, "well": 1, "project": 1, "english": 1, "framenet": 1}, "marker": "(Baker et al., 1998)", "article": "N06-1017", "vector_2": [8, 0.20555705422174264, 1, 2, 0, 0]}, {"label": "Pos", "current": "Furthermore, we describe a software toolchain for easy extraction of RDF data from existing information structures, such as classes or database records, and delivery of this data via web applications and services based on the popular framework Rails (Ruby et al., 2011).", "context": ["source more widely available and to enable a long and successful lifecycle for the resource.", "Furthermore, we describe a software toolchain for easy extraction of RDF data from existing information structures, such as classes or database records, and delivery of this data via web applications and services based on the popular framework Rails (Ruby et al., 2011).", "This tool chain is designed to be easy to integrate with existing libraries in a plugin-like fashion, in order to reduce the effort of integrating existing systems into Linked Data networks and infrastructures."], "vector_1": {"softwar": 1, "via": 1, "chain": 1, "fashion": 1, "exist": 3, "pluginlik": 1, "effort": 1, "extract": 1, "librari": 1, "web": 1, "describ": 1, "easi": 2, "record": 1, "databas": 1, "system": 1, "long": 1, "avail": 1, "lifecycl": 1, "infrastructur": 1, "furthermor": 1, "deliveri": 1, "applic": 1, "sourc": 1, "resourc": 1, "tool": 1, "rail": 1, "framework": 1, "base": 1, "link": 1, "data": 3, "class": 1, "network": 1, "reduc": 1, "wide": 1, "toolchain": 1, "enabl": 1, "success": 1, "servic": 1, "structur": 1, "inform": 1, "rdf": 1, "integr": 2, "popular": 1, "design": 1, "order": 1}, "marker": "(Ruby et al., 2011)", "article": "W13-5507", "vector_2": [2, 0.11003614325873462, 1, 1, 0, 0]}, {"label": "Neut", "current": "Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.", "context": ["The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.", "Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.", "In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues."], "vector_1": {"interfer": 2, "pronoun": 1, "definit": 1, "strongli": 1, "davidsennielsen": 1, "spanish": 1, "research": 1, "languag": 2, "smith": 1, "would": 1, "transfer": 2, "tapper": 1, "swan": 1, "aart": 1, "sometim": 1, "littl": 1, "articl": 1, "write": 1, "overusedunderus": 1, "altenberg": 1, "speech": 1, "answer": 1, "probabl": 1, "po": 1, "contrast": 1, "relat": 1, "tongu": 3, "mother": 3, "usag": 1, "french": 1, "part": 1, "granger": 1, "known": 1, "neg": 1, "modifi": 1, "possess": 1, "reveal": 1, "grammat": 1, "word": 1, "work": 1, "harder": 1, "across": 1, "item": 1, "anoth": 1, "allow": 1, "english": 1}, "marker": "(1998)", "article": "P13-1112", "vector_2": [15, 0.04195675509891121, 4, 2, 1, 0]}, {"label": "Neut", "current": "The other approach is to reduce the ranking problem to a classification problem by using the method of pairwise samples (Herbrich et al., 2000).", "context": ["However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks.", "The other approach is to reduce the ranking problem to a classification problem by using the method of pairwise samples (Herbrich et al., 2000).", "The underlying assumption is that the samples of consecutive ranks are separable."], "vector_1": {"underli": 1, "classif": 1, "show": 1, "global": 1, "rank": 3, "sampl": 2, "prank": 1, "use": 1, "section": 1, "due": 1, "consecut": 1, "approach": 1, "method": 1, "assumpt": 1, "rerank": 1, "introduct": 1, "reduc": 1, "task": 1, "algorithm": 1, "howev": 1, "work": 1, "separ": 1, "pairwis": 1, "problem": 2}, "marker": "(Herbrich et al., 2000)", "article": "N04-1023", "vector_2": [4, 0.31266638373394035, 1, 2, 1, 0]}, {"label": "Neut", "current": "In Direct Translation Model (DTM) proposed for statistical machine translation by (Papineni et al., 1998; Och and Ney, 2002), the authors present a discriminative set-up for natural language understanding (and MT).", "context": ["In this section, we present approaches that are directly related to our approach.", "In Direct Translation Model (DTM) proposed for statistical machine translation by (Papineni et al., 1998; Och and Ney, 2002), the authors present a discriminative set-up for natural language understanding (and MT).", "They use a slightly modified equation (in comparison to IBM models) as shown in equation 1."], "vector_1": {"discrimin": 1, "directli": 1, "natur": 1, "direct": 1, "languag": 1, "use": 1, "author": 1, "section": 1, "approach": 2, "equat": 2, "machin": 1, "relat": 1, "shown": 1, "understand": 1, "translat": 2, "modifi": 1, "present": 2, "comparison": 1, "ibm": 1, "dtm": 1, "setup": 1, "slightli": 1, "mt": 1, "statist": 1, "model": 2, "propos": 1}, "marker": "(Papineni et al., 1998", "article": "W10-3805", "vector_2": [12, 0.2487646756532395, 2, 1, 4, 0]}, {"label": "Pos", "current": "1We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005).", "context": ["Note that these meetings are research discussions, and that the annotators may not be very familiar with", "1We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005).", "the topics discussed and often had trouble deciding the important sentences or keywords."], "vector_1": {"topic": 2, "we": 1, "previou": 1, "often": 1, "sentenc": 1, "may": 1, "familiar": 1, "use": 1, "annot": 1, "troubl": 1, "research": 1, "note": 1, "decid": 1, "summar": 1, "keyword": 1, "import": 1, "meet": 2, "work": 1, "segment": 1, "discuss": 2, "select": 1}, "marker": "(Galley et al., 2003", "article": "N09-1070", "vector_2": [6, 0.298829824957287, 2, 1, 0, 0]}, {"label": "Neut", "current": "Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages.", "context": ["Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.", "Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages.", "The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models."], "vector_1": {"yamada": 1, "treebank": 1, "parser": 2, "subtre": 1, "languag": 2, "involv": 1, "use": 4, "gildea": 1, "reorder": 1, "sourc": 2, "string": 1, "handl": 1, "clone": 1, "earlier": 1, "train": 1, "pars": 1, "translat": 1, "treebas": 1, "case": 1, "target": 1, "possibl": 1, "knight": 1, "align": 4, "tree": 5, "statist": 2, "output": 1, "model": 4, "produc": 1, "propos": 2}, "marker": "(2003)", "article": "N04-1023", "vector_2": [1, 0.17566264130560594, 2, 1, 0, 0]}, {"label": "Neut", "current": "Our future plans include further sophistication of features, such as the use of external gazetteers which is reported to improve the F-score by 1.0 and 2.7 points in (Zhou and Su, 2004) and (Friedrich et al., 2006), respectively.", "context": ["Our system showed clear improvement over many of the machine-learning-based systems reported to date, and also proved comparable to the existing state-ofthe-art systems that use rule-based post-processing.", "Our future plans include further sophistication of features, such as the use of external gazetteers which is reported to improve the F-score by 1.0 and 2.7 points in (Zhou and Su, 2004) and (Friedrich et al., 2006), respectively.", "We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase."], "vector_1": {"gazett": 1, "featur": 2, "show": 1, "appli": 1, "elabor": 1, "readili": 1, "date": 1, "exist": 1, "expect": 1, "respect": 1, "machinelearningbas": 1, "use": 2, "compar": 1, "prove": 1, "point": 1, "system": 3, "avail": 1, "also": 1, "includ": 1, "postprocess": 1, "fscore": 1, "futur": 1, "qualifi": 1, "string": 1, "rerank": 2, "sophist": 1, "candid": 1, "plan": 1, "improv": 2, "extern": 1, "algorithm": 1, "dictionarybas": 1, "clear": 1, "rulebas": 1, "stringmatch": 1, "accommod": 1, "phase": 1, "stateoftheart": 1, "report": 2, "mani": 1, "architectur": 1}, "marker": "(Zhou and Su, 2004)", "article": "W07-1033", "vector_2": [3, 0.9664055080721747, 2, 5, 0, 0]}, {"label": "Neut", "current": "Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al., 2011).", "context": ["While features can be based on individual phenomena of any type, we base our extracted features largely upon learner errors.", "Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al., 2011).", "To detect errors, we align the learner sentence with a gold standard and extract the features."], "vector_1": {"upon": 1, "impact": 1, "shown": 1, "featur": 3, "detect": 1, "sentenc": 1, "level": 1, "predict": 1, "extract": 2, "individu": 1, "phenomena": 1, "standard": 1, "signific": 1, "base": 2, "larg": 1, "learner": 3, "error": 3, "gold": 1, "type": 1, "align": 1}, "marker": "(Yannakoudakis et al., 2011)", "article": "W12-2011", "vector_2": [1, 0.288694010535611, 1, 3, 1, 0]}, {"label": "Neut", "current": "It has been observed for related systems that a combination of separately trained features in the machine learning component can lead to an overall improvement in system performance, in particular if features from a more \"informed\" component and shallow ones are combined (Hickl et al., 2006; Bos and Markert, 2006).", "context": ["As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements.", "It has been observed for related systems that a combination of separately trained features in the machine learning component can lead to an overall improvement in system performance, in particular if features from a more \"informed\" component and shallow ones are combined (Hickl et al., 2006; Bos and Markert, 2006).", "We provide a detailed analysis of our system's behaviour on different training and test sets."], "vector_1": {"analysi": 1, "featur": 2, "one": 1, "set": 1, "systemat": 1, "concentr": 1, "separ": 1, "overal": 1, "describ": 1, "lead": 1, "perform": 1, "shallow": 1, "system": 5, "compon": 2, "futur": 1, "test": 1, "machin": 1, "earlier": 1, "detail": 1, "relat": 1, "spot": 1, "differ": 1, "behaviour": 2, "extens": 1, "train": 2, "promis": 1, "particular": 1, "discuss": 1, "provid": 1, "observ": 1, "aim": 1, "inform": 1, "combin": 2, "learn": 1, "improv": 2, "anchor": 1}, "marker": "(Hickl et al., 2006", "article": "W07-1402", "vector_2": [1, 0.06106830214394297, 2, 2, 0, 0]}, {"label": "Neut", "current": " a target language model (a 4-gram model of words, estimated with Kneser-Ney smoothing);  a POS target language model (a 4-gram model of tags with Good-Turing discounting (TPOS));  a word bonus model, which is used to compensate the system's preference for short output sentences;  a source-to-target lexicon model and a target-tosource lexicon model, these models use word-toword IBM Model 1 probabilities (Och and Ney, 2004) to estimate the lexical weights for each tuple in the translation table.", "context": ["The TALP-UPC 2008 translation system, besides the bilingual translation model, which consists of a 4-gram LM of tuples with Kneser-Ney discounting (estimated with SRI Language Modeling Toolkit1), implements a log-linear combination of five additional feature models:", " a target language model (a 4-gram model of words, estimated with Kneser-Ney smoothing);  a POS target language model (a 4-gram model of tags with Good-Turing discounting (TPOS));  a word bonus model, which is used to compensate the system's preference for short output sentences;  a source-to-target lexicon model and a target-tosource lexicon model, these models use word-toword IBM Model 1 probabilities (Och and Ney, 2004) to estimate the lexical weights for each tuple in the translation table.", "Decisions on the particular LM configuration and smoothing technique were taken on the minimalperplexity and maximal-BLEU bases."], "vector_1": {"featur": 1, "maximalbleu": 1, "weight": 1, "compens": 1, "toolkit": 1, "tag": 1, "tabl": 1, "loglinear": 1, "languag": 3, "probabl": 1, "use": 2, "techniqu": 1, "sentenc": 1, "ibm": 1, "lm": 2, "system": 2, "configur": 1, "estim": 3, "besid": 1, "decis": 1, "prefer": 1, "goodtur": 1, "taken": 1, "minimalperplex": 1, "po": 1, "lexicon": 2, "tupl": 2, "lexic": 1, "discount": 2, "base": 1, "five": 1, "translat": 3, "particular": 1, "wordtoword": 1, "implement": 1, "addit": 1, "short": 1, "word": 2, "target": 2, "consist": 1, "sourcetotarget": 1, "tpo": 1, "sri": 1, "smooth": 2, "bonu": 1, "talpupc": 1, "targettosourc": 1, "combin": 1, "gram": 3, "bilingu": 1, "output": 1, "model": 12, "kneserney": 2}, "marker": "(Och and Ney, 2004)", "article": "W08-0315", "vector_2": [4, 0.2991446899501069, 1, 1, 1, 0]}, {"label": "Neut", "current": "Wordnets of Indian languages are linked with each other and English WordNet through a common index (Bhattacharyya, 2010), which makes it possible to share concept definitions across languages.", "context": ["The patterns used in the system are generic and can be used across languages.", "Wordnets of Indian languages are linked with each other and English WordNet through a common index (Bhattacharyya, 2010), which makes it possible to share concept definitions across languages.", "Following are the major features of the proposed system:"], "vector_1": {"make": 1, "index": 1, "use": 2, "concept": 1, "possibl": 1, "pattern": 1, "gener": 1, "share": 1, "system": 2, "propos": 1, "indian": 1, "link": 1, "wordnet": 2, "english": 1, "follow": 1, "definit": 1, "major": 1, "featur": 1, "across": 2, "languag": 3, "common": 1}, "marker": "(Bhattacharyya, 2010)", "article": "W12-5209", "vector_2": [2, 0.19681187776946538, 1, 3, 0, 0]}, {"label": "Neut", "current": "According to the IBM models (Brown et al., 1993), the statistical word alignment model can be generally represented as in Equation (1).", "context": ["2 Statistical Word Alignment", "According to the IBM models (Brown et al., 1993), the statistical word alignment model can be generally represented as in Equation (1).", "f e , ) = | a '  |e)  p(a' ,  |) f e (1) , a ( f p ( a p"], "vector_1": {"accord": 1, "word": 2, "ibm": 1, "f": 3, "align": 2, "repres": 1, "p": 2, "pa": 1, "statist": 2, "e": 3, "model": 2, "gener": 1, "equat": 1}, "marker": "(Brown et al., 1993)", "article": "P05-1058", "vector_2": [12, 0.21364076729316023, 1, 4, 0, 0]}, {"label": "Neut", "current": "Caraballo and Charniak (1999) have shown that the term frequency is a good indicator of determining specificity of a term.", "context": ["'Terms with high information content remain lower in the concept hierarchy and terms with low information content remain higher in the concept hierarchy' (Resnik, 1999).", "Caraballo and Charniak (1999) have shown that the term frequency is a good indicator of determining specificity of a term.", "We divide the initial set of terms into different partitions based on the term frequency and then construct k-partite graph by finding subsumption relation between the terms of different partitions."], "vector_1": {"concept": 2, "indic": 1, "high": 1, "set": 1, "charniak": 1, "hierarchi": 2, "find": 1, "differ": 2, "divid": 1, "graph": 1, "caraballo": 1, "subsumpt": 1, "construct": 1, "content": 2, "low": 1, "higher": 1, "good": 1, "partit": 2, "relat": 1, "lower": 1, "initi": 1, "kpartit": 1, "base": 1, "shown": 1, "term": 7, "specif": 1, "frequenc": 2, "inform": 2, "remain": 2, "determin": 1}, "marker": "(1999)", "article": "W12-5209", "vector_2": [13, 0.14686079300224433, 2, 3, 2, 0]}, {"label": "Neut", "current": "As with many exams, the main purpose is to \"reduce the number of students who attend an oral interview\" (Fulcher, 1997).", "context": ["The system is intended to semi-automatically place incoming students into the appropriate Hebrew course, i.e., level.", "As with many exams, the main purpose is to \"reduce the number of students who attend an oral interview\" (Fulcher, 1997).", "2 The Data"], "vector_1": {"reduc": 1, "main": 1, "purpos": 1, "appropri": 1, "intend": 1, "exam": 1, "attend": 1, "level": 1, "semiautomat": 1, "oral": 1, "system": 1, "number": 1, "cours": 1, "place": 1, "data": 1, "student": 2, "hebrew": 1, "interview": 1, "incom": 1, "mani": 1, "ie": 1}, "marker": "(Fulcher, 1997)", "article": "W12-2011", "vector_2": [15, 0.13361347096617032, 1, 3, 0, 0]}, {"label": "Neut", "current": "We have drawn from the Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) a collection of British newspaper reports on court stories to evaluate our approach.", "context": ["multiword expressions.", "We have drawn from the Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) a collection of British newspaper reports on court stories to evaluate our approach.", "Our experiment shows that it is efficient in identifying MWEs, in particular MWEs of low frequencies."], "vector_1": {"corpu": 1, "newspap": 1, "court": 1, "mwe": 2, "evalu": 1, "show": 1, "frequenc": 1, "express": 1, "meter": 1, "collect": 1, "stori": 1, "effici": 1, "british": 1, "multiword": 1, "particular": 1, "report": 1, "drawn": 1, "identifi": 1, "experi": 1, "approach": 1, "low": 1}, "marker": "Clough et al., 2002)", "article": "W03-1807", "vector_2": [1, 0.09597078614803874, 2, 3, 2, 0]}, {"label": "Neut", "current": "For example, a. purpose clause is useful for the domain plan recognizer [Ferguson and Allen, 1993] in incorporating new content into an existing (partial) plan, but in the absence of such a cue, the recognizer will still try to connect the new content to previous content.", "context": ["This is particularly true for subject matter relations.", "For example, a. purpose clause is useful for the domain plan recognizer [Ferguson and Allen, 1993] in incorporating new content into an existing (partial) plan, but in the absence of such a cue, the recognizer will still try to connect the new content to previous content.", "It would then be possible to deduce the relations that this item holds with previous items, but we currently see no need to do this."], "vector_1": {"domain": 1, "partial": 1, "see": 1, "particularli": 1, "connect": 1, "need": 1, "still": 1, "subject": 1, "current": 1, "use": 1, "would": 1, "item": 2, "content": 3, "cue": 1, "new": 2, "deduc": 1, "relat": 2, "recogn": 2, "incorpor": 1, "claus": 1, "plan": 2, "hold": 1, "true": 1, "tri": 1, "possibl": 1, "previou": 2, "matter": 1, "exampl": 1, "absenc": 1, "exist": 1, "purpos": 1}, "marker": "Ferguson and Allen, 1993]", "article": "W93-0235", "vector_2": [0, 0.7827109141875396, 1, 1, 0, 0]}, {"label": "CoCo", "current": "Dataset The dataset is built using the same guidelines as Mitchell and Lapata (2008), using transitive verbs obtained from CELEX1 paired with subjects and objects.", "context": ["This degree decreases for the pair 'the child met the house' and 'the child satisfied the house'.", "Dataset The dataset is built using the same guidelines as Mitchell and Lapata (2008), using transitive verbs obtained from CELEX1 paired with subjects and objects.", "We first picked 10 transitive verbs from the most frequent verbs of the BNC."], "vector_1": {"guidelin": 1, "transit": 2, "obtain": 1, "dataset": 2, "decreas": 1, "subject": 1, "use": 2, "built": 1, "lapata": 1, "0": 1, "bnc": 1, "celex": 1, "object": 1, "satisfi": 1, "met": 1, "verb": 3, "child": 2, "pair": 2, "mitchel": 1, "hous": 2, "degre": 1, "pick": 1, "frequent": 1, "first": 1}, "marker": "(2008)", "article": "W11-2507", "vector_2": [3, 0.6225123500352858, 1, 7, 0, 0]}, {"label": "Neut", "current": "Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).", "context": ["1 Introduction", "Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).", "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links."], "vector_1": {"machin": 1, "measur": 1, "word": 1, "align": 2, "mani": 1, "intermedi": 1, "research": 1, "employ": 1, "build": 1, "recent": 1, "result": 1, "translat": 1, "year": 1, "link": 1, "statist": 2, "introduct": 1, "first": 1, "associ": 1, "model": 1, "propos": 1}, "marker": "(Brown et al., 1993)", "article": "P05-1058", "vector_2": [12, 0.03820964929277272, 7, 4, 0, 0]}, {"label": "Neut", "current": "There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).", "context": ["3.6 Large Margin Classifiers", "There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).", "The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization."], "vector_1": {"quit": 1, "svm": 2, "linear": 2, "superior": 1, "perform": 1, "boost": 1, "winnow": 1, "maxim": 1, "classifi": 3, "separ": 1, "larg": 2, "perceptron": 1, "sampl": 1, "abil": 1, "margin": 3}, "marker": "(Krauth and Mezard, 1987)", "article": "N04-1023", "vector_2": [17, 0.5118253019020795, 4, 1, 0, 0]}, {"label": "Pos", "current": "Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).", "context": ["In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.", "Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).", "For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option."], "vector_1": {"advantag": 1, "almost": 1, "kleinbraley": 1, "stabl": 1, "indic": 1, "restrict": 1, "prefix": 1, "correl": 1, "thorough": 1, "empir": 1, "raatz": 1, "follow": 2, "guess": 1, "languag": 1, "automat": 2, "score": 1, "space": 1, "solut": 2, "cloze": 2, "valid": 1, "overcom": 1, "test": 4, "analys": 1, "singl": 1, "altern": 1, "option": 1, "benefici": 1, "regard": 1, "weak": 1, "given": 1, "theori": 1, "ctest": 3, "case": 1, "enabl": 1, "provid": 1, "approach": 1, "reliabl": 1, "properti": 1, "without": 1, "principl": 1, "order": 1, "propos": 1}, "marker": "Jafarpur, 1995)", "article": "Q14-1040", "vector_2": [19, 0.12160330210630678, 4, 1, 0, 0]}, {"label": "Neut", "current": "Patterns are extracted in training from the crossed links found in the word alignment, in other words, found in translation tuples (as no word within a tuple can be linked to a word out of it (Crego and Marino, 2006)).", "context": ["where t1, ..., tn is a sequence of POS tags (relating a sequence of source words), and i1, ..., in indicates which order of the source words generate monotonically the target words.", "Patterns are extracted in training from the crossed links found in the word alignment, in other words, found in translation tuples (as no word within a tuple can be linked to a word out of it (Crego and Marino, 2006)).", "Having all the instances of rewrite patterns, a score for each pattern on the basis of relative frequency is calculated as shown below:"], "vector_1": {"within": 1, "monoton": 1, "indic": 1, "tag": 1, "extract": 1, "shown": 1, "pattern": 3, "cross": 1, "tn": 1, "rewrit": 1, "score": 1, "rel": 1, "tupl": 2, "po": 1, "sourc": 2, "sequenc": 2, "gener": 1, "relat": 1, "train": 1, "link": 2, "translat": 1, "basi": 1, "word": 7, "target": 1, "i": 1, "frequenc": 1, "align": 1, "calcul": 1, "instanc": 1, "t": 1, "found": 2, "order": 1}, "marker": "(Crego and Marino, 2006)", "article": "W08-0315", "vector_2": [2, 0.4520313613684961, 1, 4, 0, 1]}, {"label": "Neut", "current": "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "context": ["Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).", "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training."], "vector_1": {"corpu": 1, "satisfactori": 1, "result": 2, "year": 1, "largescal": 1, "intermedi": 1, "research": 1, "build": 1, "method": 1, "machin": 1, "train": 1, "link": 1, "translat": 1, "associ": 1, "requir": 1, "recent": 1, "measur": 1, "word": 1, "align": 2, "employ": 1, "achiev": 1, "statist": 2, "bilingu": 1, "mani": 1, "model": 1, "propos": 1, "order": 1, "first": 1}, "marker": "Tufis and Barbu, 2002)", "article": "P05-1058", "vector_2": [3, 0.039255958147645806, 7, 1, 0, 0]}, {"label": "Neut", "current": "In contrast with deictic usage, previous studies (Chang, 1980; Chang, 1984) assumed that anaphoric demonstratives show only a two-way distinction between proximal forms i and distal forms ku.", "context": ["Thus, distinct usage of ce and ku is associated with how the speaker allocates the deictic center and contextual space, i.e., the speakercentered space vs. the speaker- and the hearer- centered space.", "In contrast with deictic usage, previous studies (Chang, 1980; Chang, 1984) assumed that anaphoric demonstratives show only a two-way distinction between proximal forms i and distal forms ku.", "However, it is still controversial as to whether the boundaries between anaphora and deixis are clear cut."], "vector_1": {"show": 1, "distal": 1, "hearer": 1, "ce": 1, "clear": 1, "vs": 1, "howev": 1, "still": 1, "deictic": 2, "anaphora": 1, "cut": 1, "space": 3, "distinct": 2, "assum": 1, "boundari": 1, "proxim": 1, "speaker": 2, "anaphor": 1, "contrast": 1, "form": 2, "previou": 1, "usag": 2, "speakercent": 1, "associ": 1, "studi": 1, "demonstr": 1, "alloc": 1, "center": 2, "deixi": 1, "whether": 1, "thu": 1, "contextu": 1, "ku": 2, "ie": 1, "twoway": 1, "controversi": 1}, "marker": "(Chang, 1980", "article": "W10-1824", "vector_2": [30, 0.13838980416537147, 2, 1, 0, 0]}, {"label": "Neut", "current": "Besides Joubarne and Inkpen (2011), other studies have made an effort to translate the original datasets by Rubenstein & Goodenough and by Miller & Charles.", "context": ["The difference between these correlations was relatively small, which is why they claim that it is possible to use the original scores from the English gold standard in other languages.", "Besides Joubarne and Inkpen (2011), other studies have made an effort to translate the original datasets by Rubenstein & Goodenough and by Miller & Charles.", "Hassan and Mihalcea (2009) translated these datasets into Spanish, Arabic, and Romanian."], "vector_1": {"origin": 2, "claim": 1, "gold": 1, "goodenough": 1, "joubarn": 1, "rubenstein": 1, "dataset": 2, "correl": 1, "spanish": 1, "languag": 1, "differ": 1, "romanian": 1, "inkpen": 1, "score": 1, "besid": 1, "rel": 1, "miller": 1, "use": 1, "standard": 1, "arab": 1, "hassan": 1, "translat": 2, "effort": 1, "mihalcea": 1, "made": 1, "possibl": 1, "english": 1, "small": 1, "studi": 1, "charl": 1}, "marker": "(2011)", "article": "W14-0118", "vector_2": [3, 0.20321843398766476, 2, 2, 0, 0]}, {"label": "Pos", "current": "Since it is impractical to calculate derivatives of this function, we use derivative-free optimization methods such as the downhill simplex method (Helder and Mead, 1965) and Powell's method (Powell, 1964), which generally handle such difficult search conditions relatively well.", "context": ["which takes source sentences and a set of weights as inputs, and outputs the evaluation score (e.g., BLEU score) computed on the decoded sentences.", "Since it is impractical to calculate derivatives of this function, we use derivative-free optimization methods such as the downhill simplex method (Helder and Mead, 1965) and Powell's method (Powell, 1964), which generally handle such difficult search conditions relatively well.", "This approach confers several benefits over MERT."], "vector_1": {"bleu": 1, "set": 1, "comput": 1, "derivativefre": 1, "evalu": 1, "eg": 1, "mert": 1, "downhil": 1, "sever": 1, "use": 1, "weight": 1, "benefit": 1, "handl": 1, "score": 2, "decod": 1, "take": 1, "rel": 1, "input": 1, "approach": 1, "method": 3, "impract": 1, "function": 1, "sourc": 1, "deriv": 1, "optim": 1, "sentenc": 2, "gener": 1, "confer": 1, "simplex": 1, "condit": 1, "powel": 1, "sinc": 1, "search": 1, "well": 1, "calcul": 1, "output": 1, "difficult": 1}, "marker": "(Helder and Mead, 1965)", "article": "W12-3159", "vector_2": [47, 0.08803928446159243, 2, 2, 0, 0]}, {"label": "Neut", "current": "A quote from the most recent Switchboard labeling standard (Hamaker et al., 1998) gives the flavor: 20.", "context": ["An alternative approach is seen in some schemes used for labeling corpora for purposes of training and evaluating speech recognizers.", "A quote from the most recent Switchboard labeling standard (Hamaker et al., 1998) gives the flavor: 20.", "Hesitation Sounds: Use \"uh\" or \"oh\" for hesitations consisting of a vowel sound, and \"urn\" or \"hm\" for hesitations with a nasal sound, depending upon which transcription the actual sound is closest to."], "vector_1": {"evalu": 1, "give": 1, "hm": 1, "seen": 1, "flavor": 1, "use": 2, "depend": 1, "quot": 1, "label": 2, "speech": 1, "scheme": 1, "approach": 1, "hesit": 3, "altern": 1, "recogn": 1, "closest": 1, "upon": 1, "nasal": 1, "standard": 1, "train": 1, "vowel": 1, "switchboard": 1, "transcript": 1, "recent": 1, "sound": 4, "actual": 1, "consist": 1, "oh": 1, "urn": 1, "corpora": 1, "uh": 1, "purpos": 1}, "marker": "(Hamaker et al., 1998)", "article": "W00-1004", "vector_2": [2, 0.32251888097010956, 1, 1, 0, 0]}, {"label": "Neut", "current": "The toolkit was used to evaluate the different measures against a gold standard of English words created by Rubenstein and Goodenough (1965) and Miller and Charles (1991).", "context": ["WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.", "The toolkit was used to evaluate the different measures against a gold standard of English words created by Rubenstein and Goodenough (1965) and Miller and Charles (1991).", "The evaluation results tell us something about the capacity of WordNet to mimic human judgements of similarity but also about the different methods in relation to each other."], "vector_1": {"semant": 1, "set": 1, "evalu": 2, "synonymi": 1, "goodenough": 1, "rubenstein": 1, "toolkit": 1, "result": 1, "human": 1, "wordnet": 2, "use": 1, "gold": 1, "instrument": 1, "perform": 1, "databas": 1, "someth": 1, "capac": 1, "also": 2, "languag": 1, "creat": 1, "miller": 1, "test": 1, "import": 1, "tell": 1, "relat": 2, "differ": 2, "standard": 1, "judgement": 1, "charl": 1, "becom": 1, "measur": 2, "word": 2, "us": 1, "method": 1, "mimic": 1, "wordnetsimilar": 1, "english": 1, "similar": 2}, "marker": "(1991)", "article": "W14-0118", "vector_2": [23, 0.05381797689489997, 2, 1, 1, 0]}, {"label": "Pos", "current": "However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification.", "context": ["E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007).", "However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification.", "In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy."], "vector_1": {"classif": 1, "featur": 5, "effici": 1, "weight": 2, "text": 1, "eregular": 1, "number": 1, "activ": 1, "restrict": 1, "zero": 1, "loglinear": 1, "assum": 1, "select": 1, "concern": 1, "even": 1, "tsujii": 1, "irrelev": 2, "appear": 1, "confirm": 1, "solut": 1, "laplacian": 1, "accuraci": 1, "end": 1, "exhaust": 1, "spars": 1, "ellm": 2, "kazama": 1, "regard": 1, "use": 1, "hand": 1, "train": 1, "pars": 1, "conjunct": 1, "report": 1, "depend": 1, "major": 1, "exactli": 1, "categor": 1, "must": 1, "reduc": 1, "task": 3, "rare": 1, "greatli": 1, "provid": 1, "howev": 1, "later": 1, "prior": 1, "cannot": 1, "expens": 1, "model": 1, "order": 1}, "marker": "(2005)", "article": "D09-1160", "vector_2": [4, 0.07251698182485772, 7, 1, 5, 0]}, {"label": "Pos", "current": "This simplified version does not take word classes into account as described in (Brown et al., 1993).", "context": ["In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999), which is shown in Equation (2).", "This simplified version does not take word classes into account as described in (Brown et al., 1993).", "p(a,  |e) =  Pr( ,  |) f ( , )   e   l m , , 1 j = j : 0 a j =  n(i  |ei ) i i =1 i=1 m m e) m 0 l l |  ,  e)Pr(=  p(a, | f ( , )     m 0 = 0   p   20 0 p1  ! )"], "vector_1": {"ei": 1, "version": 1, "paper": 1, "pr": 1, "ni": 1, "use": 1, "describ": 1, "pa": 2, "take": 1, "simplifi": 2, "shown": 1, "e": 3, "class": 1, "account": 1, "word": 1, "ibm": 1, "f": 2, "i": 1, "j": 3, "epr": 1, "l": 3, "p": 2, "equat": 1, "model": 1}, "marker": "(Brown et al., 1993)", "article": "P05-1058", "vector_2": [12, 0.21995737260220888, 2, 4, 0, 0]}, {"label": "Neut", "current": "In (Specia, 2010) text simplification is developed in the Statistical Machine Translation framework, given a parallel corpus of original and simplified texts, aligned at the sentence level.", "context": ["2003).", "In (Specia, 2010) text simplification is developed in the Statistical Machine Translation framework, given a parallel corpus of original and simplified texts, aligned at the sentence level.", "In (Poornima et al.2011) a rule based technique is proposed to simplify the complex sentences based on connectives like relative pronouns, coordinating and subordinating conjunctions."], "vector_1": {"corpu": 1, "origin": 1, "pronoun": 1, "text": 2, "al": 1, "connect": 1, "et": 1, "given": 1, "develop": 1, "techniqu": 1, "coordin": 1, "complex": 1, "rel": 1, "machin": 1, "simplifi": 2, "sentenc": 2, "conjunct": 1, "framework": 1, "base": 2, "translat": 1, "parallel": 1, "like": 1, "simplif": 1, "poornima": 1, "align": 1, "level": 1, "rule": 1, "statist": 1, "subordin": 1, "propos": 1}, "marker": "(Specia, 2010)", "article": "W14-5605", "vector_2": [4, 0.2378655564581641, 1, 1, 0, 0]}, {"label": "Pos", "current": "We performed the experiments with the weka toolkit (Hall et al., 2009), using a filter to convert strings into word vectors, and two learning algorithms: SVMs and bagging with Fast Decision Tree Learner as base algorithm.", "context": ["bels.", "We performed the experiments with the weka toolkit (Hall et al., 2009), using a filter to convert strings into word vectors, and two learning algorithms: SVMs and bagging with Fast Decision Tree Learner as base algorithm.", "Figure 4 represents the experiments conducted with the EN test set."], "vector_1": {"en": 1, "toolkit": 1, "repres": 1, "set": 1, "use": 1, "weka": 1, "perform": 1, "learner": 1, "two": 1, "fast": 1, "figur": 1, "decis": 1, "conduct": 1, "test": 1, "experi": 2, "string": 1, "base": 1, "convert": 1, "svm": 1, "word": 1, "bel": 1, "algorithm": 2, "tree": 1, "filter": 1, "bag": 1, "vector": 1, "learn": 1}, "marker": "(Hall et al., 2009)", "article": "P15-2128", "vector_2": [6, 0.7904185022026432, 1, 1, 0, 0]}, {"label": "Neut", "current": "As noted by Fountain and Lapata (2012), 'Most of the existing approaches construct flat structure rather than a taxonomy.", "context": ["Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.", "As noted by Fountain and Lapata (2012), 'Most of the existing approaches construct flat structure rather than a taxonomy.", "Also, the automatically constructed ontologies often create false association between terms and result in erroneous concept hierarchy (Zhou, 2007)."], "vector_1": {"major": 1, "concept": 3, "identifi": 1, "often": 1, "creat": 2, "ontolog": 2, "challeng": 1, "automat": 1, "exist": 1, "result": 1, "hierarchi": 2, "taxonomi": 1, "involv": 1, "lapata": 1, "rather": 1, "top": 1, "construct": 2, "note": 1, "also": 1, "fountain": 1, "approach": 1, "flat": 1, "good": 1, "fals": 1, "associ": 1, "term": 1, "task": 1, "level": 1, "erron": 1, "structur": 1, "learn": 1}, "marker": "(2012)", "article": "W12-5209", "vector_2": [0, 0.11601542268515855, 2, 3, 0, 0]}, {"label": "Neut", "current": "Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting.", "context": ["The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs).", "Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting.", "These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models."], "vector_1": {"em": 2, "suggest": 1, "set": 1, "somewhat": 1, "tagger": 1, "result": 1, "use": 2, "techniqu": 1, "semisupervis": 1, "perform": 1, "anoth": 1, "baumwelch": 2, "necessarili": 1, "merialdo": 1, "hidden": 1, "contrast": 1, "deploy": 1, "gener": 1, "hmm": 2, "elworthi": 1, "train": 2, "partofspeech": 1, "data": 1, "markov": 1, "demonstr": 1, "pereira": 1, "algorithm": 1, "fairli": 1, "unsupervis": 1, "requir": 1, "schabe": 1, "yield": 1, "instanc": 1, "determin": 1, "neg": 1, "ioa": 1, "improv": 1, "model": 2, "forwardbackward": 1}, "marker": "(1994)", "article": "W07-2203", "vector_2": [13, 0.15628610440079824, 3, 4, 0, 0]}, {"label": "Neut", "current": "While the roots of natural logic can be traced back to Aristotle's syllogisms, the modern conception of natural logic began with George Lakoff, who proposed \"a logic for natural language\" which could \"characterize all the valid inferences that can be made in natural language\" (Lakoff, 1970).", "context": ["6 Related work", "While the roots of natural logic can be traced back to Aristotle's syllogisms, the modern conception of natural logic began with George Lakoff, who proposed \"a logic for natural language\" which could \"characterize all the valid inferences that can be made in natural language\" (Lakoff, 1970).", "The study of natural logic was formalized by Johan van Benthem, who crucially connected it with categorial grammar (van Benthem, 1986), and later was brought to fruition by Victor Sanchez Valencia, who first gave a precise definition of a calculus of mono"], "vector_1": {"sanchez": 1, "concept": 1, "benthem": 1, "natur": 5, "modern": 1, "back": 1, "brought": 1, "connect": 1, "gave": 1, "languag": 2, "victor": 1, "definit": 1, "van": 1, "began": 1, "character": 1, "calculu": 1, "georg": 1, "categori": 1, "valid": 1, "valencia": 1, "fruition": 1, "infer": 1, "first": 1, "crucial": 1, "syllog": 1, "trace": 1, "relat": 1, "lakoff": 1, "formal": 1, "made": 1, "grammar": 1, "mono": 1, "could": 1, "work": 1, "later": 1, "precis": 1, "aristotl": 1, "logic": 4, "studi": 1, "johan": 1, "root": 1, "propos": 1}, "marker": "(Lakoff, 1970)", "article": "W07-1431", "vector_2": [37, 0.9343025648059251, 2, 1, 0, 0]}, {"label": "Neut", "current": "Since links relate one article to its neighbours, and by extension to their neighbours, we extract and process this hyperlink structure (using SA) as an Associative Network (AN) (Berger et al., 2004) of concepts and links relating them to one another.", "context": ["In Wikipedia, users create links between articles which are seen to be related to some degree.", "Since links relate one article to its neighbours, and by extension to their neighbours, we extract and process this hyperlink structure (using SA) as an Associative Network (AN) (Berger et al., 2004) of concepts and links relating them to one another.", "The SA algorithm can briefly be described as an iterative process of propagating real-valued energy from one or more source nodes, via weighted links over an associative network (each such a propagation is called a pulse)."], "vector_1": {"propag": 2, "concept": 1, "creat": 1, "process": 2, "one": 3, "via": 1, "iter": 1, "seen": 1, "energi": 1, "extract": 1, "use": 1, "network": 2, "anoth": 1, "wikipedia": 1, "articl": 2, "call": 1, "node": 1, "sourc": 1, "relat": 3, "weight": 1, "neighbour": 2, "extens": 1, "briefli": 1, "link": 4, "user": 1, "associ": 2, "sinc": 1, "describ": 1, "algorithm": 1, "hyperlink": 1, "structur": 1, "degre": 1, "puls": 1, "sa": 2, "realvalu": 1}, "marker": "(Berger et al., 2004)", "article": "W10-3506", "vector_2": [6, 0.16620718305071944, 1, 2, 0, 0]}, {"label": "Neut", "current": "At the other extreme, textual inference can be approached as deduction, building on work in formal computational semantics to translate sentences into first-order logic (FOL), and then applying a theorem prover or a model builder (Akhmatova, 2005; Fowler et al., 2005).", "context": ["Because it drops important qualifiers in a negative context, the hypothesis does not follow; yet both the lexical content and the predicate-argument structure of the hypothesis closely match the premise.", "At the other extreme, textual inference can be approached as deduction, building on work in formal computational semantics to translate sentences into first-order logic (FOL), and then applying a theorem prover or a model builder (Akhmatova, 2005; Fowler et al., 2005).", "However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and so on."], "vector_1": {"semant": 1, "work": 1, "comput": 1, "founder": 1, "appli": 1, "intension": 1, "certain": 1, "prover": 1, "deduct": 1, "close": 1, "follow": 1, "yet": 1, "languag": 1, "accur": 1, "neg": 1, "textual": 1, "tend": 1, "content": 1, "idiom": 1, "causal": 1, "build": 1, "theorem": 1, "fol": 1, "import": 1, "premis": 1, "approach": 2, "infer": 1, "match": 1, "firstord": 1, "difficulti": 1, "qualifi": 1, "modal": 1, "hypothesi": 2, "sentenc": 1, "relat": 1, "lexic": 1, "translat": 2, "quantifi": 1, "natur": 1, "proposit": 1, "formal": 1, "foltricki": 1, "drop": 1, "builder": 1, "predicateargu": 1, "attitud": 1, "structur": 1, "tempor": 1, "issu": 1, "includ": 1, "context": 1, "logic": 1, "extrem": 1, "model": 1, "howev": 1}, "marker": "(Akhmatova, 2005", "article": "W07-1431", "vector_2": [2, 0.07152653956933205, 2, 1, 0, 0]}, {"label": "Weak", "current": "Carroll and Charniak (1992) report that EM fared poorly with local optima.", "context": ["Figures 4(f,g) show how both likelihood and accuracy, which both start quite low, improve substantially over time for the HMM on artificial data.", "Carroll and Charniak (1992) report that EM fared poorly with local optima.", "We do not claim that there are no local optima, but only that the likelihood surface that EM is optimizing can become smoother with more examples."], "vector_1": {"em": 2, "fare": 1, "show": 1, "carrol": 1, "poorli": 1, "substanti": 1, "charniak": 1, "artifici": 1, "quit": 1, "surfac": 1, "likelihood": 2, "smoother": 1, "accuraci": 1, "start": 1, "figur": 1, "low": 1, "local": 2, "optima": 2, "optim": 1, "hmm": 1, "fg": 1, "report": 1, "becom": 1, "data": 1, "claim": 1, "exampl": 1, "time": 1, "improv": 1}, "marker": "(1992)", "article": "P08-1100", "vector_2": [16, 0.9359637556209217, 1, 2, 0, 0]}, {"label": "Neut", "current": "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).", "context": ["Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of \"normality\" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).", "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).", "One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set."], "vector_1": {"set": 4, "one": 1, "densiti": 1, "around": 1, "detect": 5, "network": 1, "compar": 1, "boundari": 1, "normal": 1, "classifi": 1, "write": 1, "estim": 1, "includ": 1, "deciph": 1, "test": 1, "new": 1, "probabl": 2, "approach": 2, "belong": 1, "applic": 1, "accord": 1, "deriv": 2, "outlier": 4, "object": 2, "hand": 1, "intrus": 1, "train": 2, "standard": 1, "model": 1, "fault": 1, "typic": 1, "nonoutli": 1}, "marker": "Scholkopf et al., 2000)", "article": "N06-1017", "vector_2": [6, 0.4826390226983699, 8, 1, 0, 0]}, {"label": "Pos", "current": "Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).", "context": ["2.", "Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).", "3."], "vector_1": {"negra": 1, "corpu": 1, "use": 1, "stanford": 1, "german": 1, "sentenc": 1, "train": 1, "parser": 1, "version": 1, "pars": 1, "utter": 1, "segment": 1}, "marker": "(Klein and Manning, 2002", "article": "W13-5507", "vector_2": [11, 0.3097002665108977, 3, 1, 2, 0]}, {"label": "CoCo", "current": "The intent is to provide a uniform framework for the construction and experimentation of chart parsers, somewhat as systems like MCHART [29], but with a more systematic theoretical foundation.", "context": ["This was noted by Shell [26] and is implicit in his use of \"2form\" grammars.", "The intent is to provide a uniform framework for the construction and experimentation of chart parsers, somewhat as systems like MCHART [29], but with a more systematic theoretical foundation.", "The kernel of the system is a virtual parsing machine with a stack and a set of primitive commands corresponding essentially to the operation of a practical Push-Down Transducer."], "vector_1": {"oper": 1, "kernel": 1, "set": 1, "primit": 1, "parser": 1, "foundat": 1, "mchart": 1, "theoret": 1, "pushdown": 1, "stack": 1, "systemat": 1, "use": 1, "transduc": 1, "system": 2, "construct": 1, "uniform": 1, "note": 1, "experiment": 1, "machin": 1, "shell": 1, "form": 1, "chart": 1, "framework": 1, "intent": 1, "somewhat": 1, "essenti": 1, "implicit": 1, "practic": 1, "grammar": 1, "like": 1, "virtual": 1, "provid": 1, "correspond": 1, "command": 1, "pars": 1}, "marker": "[29]", "article": "P89-1018", "vector_2": [2, 0.5829627896091739, 2, 1, 3, 0]}, {"label": "Pos", "current": "We used six meetings as our development set (the same six meetings as the test set in (Murray et al., 2005)) to optimize our keyword extraction methods, and the remaining 21 meetings for final testing in Section 5.", "context": ["The average length of the topics (measured using the number of dialog acts) among all the meetings is 172.5, with a high standard deviation of 236.8.", "We used six meetings as our development set (the same six meetings as the test set in (Murray et al., 2005)) to optimize our keyword extraction methods, and the remaining 21 meetings for final testing in Section 5.", "One example of the annotated keywords for a topic segment is:"], "vector_1": {"among": 1, "set": 2, "number": 1, "one": 1, "high": 1, "extract": 1, "use": 2, "develop": 1, "section": 1, "six": 2, "length": 1, "topic": 2, "deviat": 1, "test": 2, "method": 1, "optim": 1, "standard": 1, "segment": 1, "averag": 1, "measur": 1, "keyword": 2, "annot": 1, "final": 1, "remain": 1, "exampl": 1, "dialog": 1, "act": 1, "meet": 4}, "marker": "(Murray et al., 2005)", "article": "N09-1070", "vector_2": [4, 0.28148673479255987, 1, 3, 0, 0]}, {"label": "Neut", "current": "Given the fact that this was also the case for the Spanish and English intuitions, as discussed by Hassan and Mihalcea (2009), it might be the case the people with different mother tongues have a shared sense of similarity of meaning.", "context": ["Firstly, the correlations between the English and Dutch gold standards are very high.", "Given the fact that this was also the case for the Spanish and English intuitions, as discussed by Hassan and Mihalcea (2009), it might be the case the people with different mother tongues have a shared sense of similarity of meaning.", "It should be noted that all speakers from the different languages share a similar Western background."], "vector_1": {"gold": 1, "share": 2, "high": 1, "correl": 1, "spanish": 1, "differ": 2, "languag": 1, "given": 1, "note": 1, "also": 1, "speaker": 1, "dutch": 1, "sens": 1, "might": 1, "peopl": 1, "english": 2, "intuit": 1, "standard": 1, "western": 1, "background": 1, "discuss": 1, "mihalcea": 1, "case": 2, "firstli": 1, "tongu": 1, "mother": 1, "hassan": 1, "similar": 2, "fact": 1, "mean": 1}, "marker": "(2009)", "article": "W14-0118", "vector_2": [5, 0.7841645533953226, 1, 3, 0, 0]}, {"label": "Neut", "current": "These weights are initially added only to the S-W graph, as in (Wan et al., 2007); then that graph is normalized and transposed to create the W-S graph.", "context": ["S-W and W-S Graphs: The weight for an edge between a sentence and a word is the TF of the word in the sentence multiplied by the word's IDF value.", "These weights are initially added only to the S-W graph, as in (Wan et al., 2007); then that graph is normalized and transposed to create the W-S graph.", "S-S Graph: The sentence node uses a vector"], "vector_1": {"node": 1, "use": 1, "word": 3, "edg": 1, "weight": 2, "normal": 1, "multipli": 1, "sentenc": 3, "sw": 2, "transpos": 1, "initi": 1, "idf": 1, "vector": 1, "ws": 2, "creat": 1, "graph": 5, "tf": 1, "ss": 1, "valu": 1, "ad": 1}, "marker": "(Wan et al., 2007)", "article": "N09-1070", "vector_2": [2, 0.5753844170078334, 1, 4, 0, 0]}, {"label": "Pos", "current": "When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage.", "context": ["To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs.", "When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage.", "We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al (2009))."], "vector_1": {"kernel": 2, "featur": 2, "trie": 1, "approxim": 1, "random": 1, "al": 1, "budget": 1, "increment": 1, "et": 1, "speed": 1, "use": 3, "techniqu": 2, "memori": 1, "fstri": 1, "solut": 1, "classifi": 2, "mix": 1, "exampl": 1, "build": 1, "succinct": 1, "input": 1, "method": 2, "applic": 1, "spars": 1, "run": 1, "gener": 1, "usag": 1, "base": 1, "user": 1, "memoryeffici": 1, "kashima": 1, "data": 1, "resourcetight": 1, "requir": 1, "reduc": 1, "survey": 1, "cellphon": 1, "provid": 1, "realtim": 1, "combin": 1, "structur": 1, "vector": 1, "environ": 1, "implement": 1}, "marker": "(Ganchev and Dredze, 2008)", "article": "D09-1160", "vector_2": [1, 0.9730738632886604, 7, 1, 0, 0]}, {"label": "Neut", "current": "Friedrich et al (2006) used CRFs with features from the external gazetteer.", "context": ["Okanohara et al (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state.", "Friedrich et al (2006) used CRFs with features from the external gazetteer.", "Current state-of-the-art for the shared-task is achieved by Tsai et al (2006), whose improvement depends on careful design of features including the normalization of numeric expressions, and use of post-processing by automatically extracted patterns."], "vector_1": {"gazett": 1, "featur": 2, "appli": 1, "al": 3, "automat": 1, "design": 1, "et": 3, "extract": 1, "whose": 1, "use": 2, "multipl": 1, "pattern": 1, "current": 1, "friedrich": 1, "state": 1, "treat": 1, "sharedtask": 1, "okanohara": 1, "singl": 1, "includ": 1, "postprocess": 1, "normal": 1, "numer": 1, "express": 1, "semicrf": 1, "extern": 1, "depend": 1, "care": 1, "word": 1, "tsai": 1, "correspond": 1, "achiev": 1, "crf": 1, "stateoftheart": 1, "improv": 1}, "marker": "(2006)", "article": "W07-1033", "vector_2": [1, 0.27469135802469136, 4, 2, 0, 0]}, {"label": "Pos", "current": "These findings are consistent with those of Snow et al (2008) in showing that AMT judgements can be as reliable as those of expert judges.", "context": ["The majority judgements of these annotators are the same as those obtained from AMT on the development data, giving us confidence in the reliability of the AMT judgements.", "These findings are consistent with those of Snow et al (2008) in showing that AMT judgements can be as reliable as those of expert judges.", "Finally, we remove a small number of items from the testing dataset which were difficult to paraphrase due to ellipsis of the verb participating in the target construction, or an extra negation in the verb phrase."], "vector_1": {"major": 1, "give": 1, "negat": 1, "al": 1, "dataset": 1, "ellipsi": 1, "paraphras": 1, "phrase": 1, "et": 1, "find": 1, "extra": 1, "develop": 1, "expert": 1, "show": 1, "particip": 1, "snow": 1, "construct": 1, "test": 1, "final": 1, "confid": 1, "obtain": 1, "judgement": 3, "verb": 2, "number": 1, "data": 1, "amt": 3, "target": 1, "consist": 1, "reliabl": 2, "judg": 1, "annot": 1, "us": 1, "item": 1, "due": 1, "small": 1, "difficult": 1, "remov": 1}, "marker": "(2008)", "article": "W10-2109", "vector_2": [2, 0.3891259543092688, 1, 1, 2, 0]}, {"label": "Neut", "current": "(Turney, 2002) selected a set of keywords first and then determined whether to add another keyword hypothesis based on its PMI (point-wise mutual information) score to the current selected keywords.", "context": ["Web information has also been used as an additional knowledge source for keyword extraction.", "(Turney, 2002) selected a set of keywords first and then determined whether to add another keyword hypothesis based on its PMI (point-wise mutual information) score to the current selected keywords.", "The preselected keywords can be generated using basic extraction algorithms such as TFIDF."], "vector_1": {"set": 1, "knowledg": 1, "pointwis": 1, "extract": 2, "pmi": 1, "select": 2, "web": 1, "use": 2, "anoth": 1, "current": 1, "also": 1, "add": 1, "score": 1, "basic": 1, "sourc": 1, "hypothesi": 1, "gener": 1, "mutual": 1, "preselect": 1, "base": 1, "addit": 1, "algorithm": 1, "keyword": 5, "whether": 1, "tfidf": 1, "inform": 2, "determin": 1, "first": 1}, "marker": "(Turney, 2002)", "article": "N09-1070", "vector_2": [7, 0.1435479191515425, 1, 2, 3, 0]}, {"label": "Neut", "current": "In addition, we applied phrase table smoothing as described in Foster et al (2006).", "context": ["The phrase table was trained using the Moses training scripts, but for the German to English system we used a different phrase extraction method described in detail in Section 4.2.", "In addition, we applied phrase table smoothing as described in Foster et al (2006).", "Furthermore, we extended the translation model by additional features for unaligned words and introduced bilingual language models."], "vector_1": {"phrase": 3, "featur": 1, "german": 1, "appli": 1, "al": 1, "detail": 1, "tabl": 2, "et": 1, "extract": 1, "languag": 1, "use": 2, "describ": 2, "script": 1, "section": 1, "system": 1, "foster": 1, "method": 1, "introduc": 1, "unalign": 1, "mose": 1, "extend": 1, "bilingu": 1, "differ": 1, "train": 2, "translat": 1, "addit": 2, "word": 1, "furthermor": 1, "smooth": 1, "english": 1, "model": 2}, "marker": "(2006)", "article": "W10-1719", "vector_2": [4, 0.3575236015670156, 1, 1, 0, 0]}, {"label": "Neut", "current": "Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.", "context": ["Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques.", "Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.", "Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text."], "vector_1": {"set": 1, "primarili": 1, "ontolog": 3, "lexicosyntact": 1, "via": 1, "heurist": 2, "find": 1, "languag": 1, "use": 1, "techniqu": 1, "divid": 1, "pattern": 2, "hybrid": 1, "three": 1, "categori": 1, "reli": 1, "varieti": 1, "hearst": 1, "approach": 2, "express": 1, "relat": 2, "base": 1, "outlin": 1, "text": 1, "statist": 1, "learn": 1, "typic": 1, "linguist": 1, "fact": 1}, "marker": "(Hearst, 1992", "article": "W12-5209", "vector_2": [20, 0.27904701617080047, 4, 2, 0, 0]}, {"label": "Neut", "current": "In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).", "context": ["Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.", "In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).", "However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation."], "vector_1": {"corpu": 1, "featur": 1, "process": 1, "decreas": 1, "loglinear": 1, "cw": 2, "sever": 1, "leverag": 1, "accur": 1, "develop": 1, "semisupervis": 1, "rather": 1, "data": 2, "smt": 1, "figur": 1, "translat": 1, "previous": 1, "method": 1, "unlabel": 1, "machin": 1, "perplex": 1, "cascad": 1, "standard": 1, "train": 1, "multilevel": 1, "vise": 1, "segment": 2, "word": 1, "howev": 1, "manual": 1, "focu": 1, "structur": 1, "caus": 1, "achiev": 1, "statist": 1, "bilingu": 2, "model": 2, "align": 1, "fact": 1, "propos": 1}, "marker": "(Ma and Way, 2009)", "article": "D15-1142", "vector_2": [6, 0.1979655444052471, 5, 3, 0, 0]}, {"label": "Neut", "current": "Sigott (1995) exam", "context": ["Brown (1989) identifies the word class, the local word frequency, and readability measures as factors correlating with cloze gap difficulty.", "Sigott (1995) exam", "2It should be noted, that their definition of \"vocabulary\" is very wide."], "vector_1": {"brown": 1, "definit": 1, "identifi": 1, "exam": 1, "measur": 1, "frequenc": 1, "readabl": 1, "it": 1, "cloze": 1, "gap": 1, "note": 1, "correl": 1, "sigott": 1, "vocabulari": 1, "factor": 1, "word": 2, "difficulti": 1, "local": 1, "class": 1, "wide": 1}, "marker": "(1995)", "article": "Q14-1040", "vector_2": [19, 0.18361896283107054, 2, 7, 1, 0]}, {"label": "Pos", "current": "However, Structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2005) is also a compelling method which has the potential to handle the interdependence between labeling and sequencing, due to its ability to handle dependencies among features and prediction results within the structure.", "context": ["Taskar, 2007) such as link prediction.", "However, Structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2005) is also a compelling method which has the potential to handle the interdependence between labeling and sequencing, due to its ability to handle dependencies among features and prediction results within the structure.", "sequence labeling (Tsochantaridis et al., 2005)."], "vector_1": {"among": 1, "svm": 1, "depend": 1, "abil": 1, "predict": 2, "sequenc": 2, "howev": 1, "interdepend": 1, "compel": 1, "due": 1, "structur": 2, "label": 2, "within": 1, "also": 1, "featur": 1, "link": 1, "result": 1, "potenti": 1, "handl": 2, "method": 1}, "marker": "(Taskar et al., 2003", "article": "W09-3953", "vector_2": [6, 0.5928086992763233, 4, 1, 0, 0]}, {"label": "Neut", "current": "This corresponds to the fact that noun-noun compounds are less common in the Italic languages than in English and that instead, the of-phrase (NN of NN) is preferred (Swan and Smith, 2001).", "context": ["4.", "This corresponds to the fact that noun-noun compounds are less common in the Italic languages than in English and that instead, the of-phrase (NN of NN) is preferred (Swan and Smith, 2001).", "For"], "vector_1": {"nn": 2, "less": 1, "nounnoun": 1, "ofphras": 1, "english": 1, "ital": 1, "correspond": 1, "common": 1, "prefer": 1, "compound": 1, "instead": 1, "languag": 1, "fact": 1}, "marker": "(Swan and Smith, 2001)", "article": "P13-1112", "vector_2": [12, 0.7140622603895108, 1, 1, 0, 0]}, {"label": "Pos", "current": "Most of the features we use are described in more detail in (Toutanova et al., 2005).", "context": ["Figure 2: An example tree with semantic role annotations.", "Most of the features we use are described in more detail in (Toutanova et al., 2005).", "Here we briefly describe these features and introduce several new joint features (denoted by *)."], "vector_1": {"briefli": 1, "denot": 1, "semant": 1, "featur": 3, "describ": 2, "use": 1, "tree": 1, "annot": 1, "detail": 1, "joint": 1, "figur": 1, "exampl": 1, "role": 1, "sever": 1, "new": 1, "introduc": 1}, "marker": "(Toutanova et al., 2005)", "article": "W05-0623", "vector_2": [0, 0.5598320503848845, 1, 5, 0, 0]}, {"label": "Neut", "current": "The Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) built in Sheffield was used to evaluate our approach.", "context": ["We use an English semantic tagger (USAS) developed at Lancaster University to identify multiword units which depict single semantic concepts.", "The Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) built in Sheffield was used to evaluate our approach.", "In our evaluation, this approach extracted a total of 4,195 MWE candidates, of which, after manual checking, 3,792 were accepted as valid MWEs, producing a precision of 90.39% and an estimated recall of 39.38%."], "vector_1": {"corpu": 1, "semant": 2, "concept": 1, "identifi": 1, "evalu": 2, "meter": 1, "tagger": 1, "total": 1, "extract": 1, "check": 1, "unit": 1, "use": 2, "lancast": 1, "develop": 1, "built": 1, "usa": 1, "univers": 1, "valid": 1, "multiword": 1, "approach": 2, "singl": 1, "recal": 1, "mwe": 2, "sheffield": 1, "depict": 1, "accept": 1, "manual": 1, "precis": 1, "candid": 1, "estim": 1, "english": 1, "produc": 1}, "marker": "(Gaizauskas et al., 2001", "article": "W03-1807", "vector_2": [2, 0.03692762186115214, 2, 2, 3, 0]}, {"label": "Neut", "current": "A well-known study evaluated usage of Twitter in the aftermath of the 2010 earthquake in Japan (Sakaki et al., 2010).", "context": ["More work has been done in the latter category: analysis of social phenomena in a non-English context.", "A well-known study evaluated usage of Twitter in the aftermath of the 2010 earthquake in Japan (Sakaki et al., 2010).", "Another Japanese-oriented study evaluated the impact of television on tweeted content (Akioka et al., 2010)."], "vector_1": {"evalu": 2, "twitter": 1, "phenomena": 1, "nonenglish": 1, "done": 1, "japan": 1, "impact": 1, "content": 1, "televis": 1, "anoth": 1, "latter": 1, "categori": 1, "wellknown": 1, "analysi": 1, "usag": 1, "japaneseori": 1, "aftermath": 1, "work": 1, "earthquak": 1, "context": 1, "social": 1, "studi": 2, "tweet": 1}, "marker": "(Sakaki et al., 2010)", "article": "D13-1114", "vector_2": [3, 0.16237106387611247, 2, 2, 0, 0]}, {"label": "Neut", "current": "Kim et al (Kim et al., 2004) compare the 8 systems participated in the shared task.", "context": ["The difference of publication years between the training and test sets reflects the organizer's intention to see the entity recognizers' portability with regard to the differences of the articles' publication years.", "Kim et al (Kim et al., 2004) compare the 8 systems participated in the shared task.", "The systems use various classification models including CRFs, hidden Markov models (HMMs), support vector machines (SVMs), and MEMMs, with various features and external resources."], "vector_1": {"classif": 1, "set": 1, "share": 1, "al": 1, "see": 1, "featur": 1, "year": 2, "et": 1, "entiti": 1, "differ": 2, "compar": 1, "support": 1, "particip": 1, "system": 2, "articl": 1, "includ": 1, "test": 1, "hidden": 1, "public": 2, "machin": 1, "recogn": 1, "variou": 2, "memm": 1, "regard": 1, "use": 1, "reflect": 1, "hmm": 1, "train": 1, "intent": 1, "kim": 1, "extern": 1, "resourc": 1, "markov": 1, "svm": 1, "task": 1, "organ": 1, "vector": 1, "crf": 1, "model": 2, "portabl": 1}, "marker": "(Kim et al., 2004)", "article": "W07-1033", "vector_2": [3, 0.24200696422918644, 1, 3, 3, 0]}, {"label": "Neut", "current": "Georgescul et al (2009) note that while WindowDiff technically penalizes false positives and false negatives equally, false positives are in fact more likely; a false positive error occurs anywhere were there are more computed boundaries than boundaries in the reference, while a false negative error can only occur when a boundary is missed.", "context": ["Tasks requiring a uniform theme in a segment might tolerate false positives, while tasks requiring complete ideas or complete themes might accept false negatives.", "Georgescul et al (2009) note that while WindowDiff technically penalizes false positives and false negatives equally, false positives are in fact more likely; a false positive error occurs anywhere were there are more computed boundaries than boundaries in the reference, while a false negative error can only occur when a boundary is missed.", "Consider figure 1, only 3 of the 8 windows contain a boundary; only those 3 windows may have false negatives (a missed boundary), while all other windows may contain false positives (too many boundaries)."], "vector_1": {"comput": 1, "georgescul": 1, "al": 1, "accept": 1, "et": 1, "miss": 2, "occur": 2, "neg": 4, "boundari": 6, "figur": 1, "uniform": 1, "note": 1, "may": 2, "theme": 2, "window": 3, "might": 2, "complet": 2, "windowdiff": 1, "fals": 9, "contain": 2, "idea": 1, "consid": 1, "toler": 1, "segment": 1, "technic": 1, "requir": 2, "task": 2, "like": 1, "anywher": 1, "equal": 1, "error": 2, "posit": 5, "mani": 1, "refer": 1, "fact": 1, "penal": 1}, "marker": "(2009)", "article": "N12-1038", "vector_2": [3, 0.2723895038315659, 1, 1, 0, 0]}, {"label": "Neut", "current": "(Siddharthan, 2002) describe the implementation of the three stages - analysis, transforma", "context": ["Automated text simplification tools are trying to achieve this purpose by combining linguistic and statistical techniques and penalize writers for polysyllabic words and long, complex sentences.", "(Siddharthan, 2002) describe the implementation of the three stages - analysis, transforma", "42"], "vector_1": {"transforma": 1, "text": 1, "techniqu": 1, "writer": 1, "three": 1, "long": 1, "complex": 1, "stage": 1, "analysi": 1, "sentenc": 1, "tool": 1, "autom": 1, "describ": 1, "tri": 1, "purpos": 1, "word": 1, "simplif": 1, "polysyllab": 1, "achiev": 1, "combin": 1, "statist": 1, "implement": 1, "linguist": 1, "penal": 1}, "marker": "(Siddharthan, 2002)", "article": "W14-5605", "vector_2": [12, 0.22192323314378554, 1, 1, 0, 0]}, {"label": "Neut", "current": "A supervised approach to keyword extraction was used in (Liu et al., 2008).", "context": ["However, for the less well structured meeting domain (lack of title and paragraph), these kinds of features may not be indicative.", "A supervised approach to keyword extraction was used in (Liu et al., 2008).", "Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reasonable performance for this task."], "vector_1": {"domain": 1, "featur": 1, "less": 1, "lack": 1, "indic": 1, "set": 1, "seem": 1, "extract": 1, "even": 1, "supervis": 2, "use": 1, "perform": 1, "titl": 1, "approach": 2, "though": 1, "may": 1, "big": 1, "reason": 1, "data": 1, "kind": 1, "task": 1, "keyword": 1, "howev": 1, "well": 1, "structur": 1, "achiev": 1, "paragraph": 1, "learn": 1, "meet": 1, "studi": 1}, "marker": "(Liu et al., 2008)", "article": "N09-1070", "vector_2": [1, 0.1853260694368331, 1, 4, 0, 1]}, {"label": "Pos", "current": "In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.", "context": ["6, this paper reveals several crucial findings that contribute to improving native language identification.", "In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.", "The rest of this paper is structured as follows."], "vector_1": {"show": 1, "rest": 1, "one": 1, "paper": 3, "famili": 1, "follow": 1, "find": 2, "languag": 2, "crucial": 1, "identif": 1, "sever": 1, "contribut": 2, "nativ": 1, "reconstruct": 1, "addit": 1, "reveal": 1, "task": 1, "central": 1, "could": 1, "tree": 1, "histor": 1, "structur": 1, "improv": 1, "linguist": 1}, "marker": "(Enright and Kondrak, 2011", "article": "P13-1112", "vector_2": [2, 0.12737310228492563, 5, 2, 0, 0]}, {"label": "Neut", "current": "Okanohara et al (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state.", "context": ["After the shared task, several researchers tackled the problem using the CRFs and their extensions.", "Okanohara et al (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state.", "Friedrich et al (2006) used CRFs with features from the external gazetteer."], "vector_1": {"gazett": 1, "featur": 1, "appli": 1, "share": 1, "al": 2, "et": 2, "sever": 1, "use": 2, "multipl": 1, "tackl": 1, "research": 1, "friedrich": 1, "state": 1, "treat": 1, "okanohara": 1, "singl": 1, "extens": 1, "semicrf": 1, "extern": 1, "task": 1, "word": 1, "correspond": 1, "crf": 2, "problem": 1}, "marker": "(Sarawagi and Cohen, 2004)", "article": "W07-1033", "vector_2": [3, 0.27148622981956316, 3, 1, 0, 0]}, {"label": "Pos", "current": "We use an annotation tool especially for short text (Ferragina and Scaiella, 2012) called Tagme3 to recognize entities and observe only 16% of all the queries are exactly an entity itself, which means most of queries do have refiner words to convey information need.", "context": ["We then move on to map queries to Freebase and empirically filter sessions that are less entity-centric.", "We use an annotation tool especially for short text (Ferragina and Scaiella, 2012) called Tagme3 to recognize entities and observe only 16% of all the queries are exactly an entity itself, which means most of queries do have refiner words to convey information need.", "To ensure the precision of recognized entities, we set a significant threshold and bottom line threshold , queries should have at least one recognized entity with a likelihood above significant level, and those below bottom line are ignored."], "vector_1": {"set": 1, "convey": 1, "less": 1, "queri": 4, "move": 1, "one": 1, "ensur": 1, "signific": 2, "session": 1, "empir": 1, "need": 1, "entiti": 4, "precis": 1, "use": 1, "bottom": 2, "least": 1, "call": 1, "threshold": 2, "text": 1, "ignor": 1, "map": 1, "recogn": 3, "especi": 1, "tool": 1, "freebas": 1, "refin": 1, "tagm": 1, "line": 2, "exactli": 1, "likelihood": 1, "short": 1, "word": 1, "level": 1, "annot": 1, "filter": 1, "inform": 1, "entitycentr": 1, "observ": 1, "mean": 1}, "marker": "(Ferragina and Scaiella, 2012)", "article": "D14-1114", "vector_2": [2, 0.565085546475996, 1, 1, 0, 0]}, {"label": "Neut", "current": " Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].", "context": ["Areas of investigation using bilingual corpora have included the following:", " Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].", "Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991]."], "vector_1": {"use": 1, "investig": 1, "area": 1, "sentenc": 1, "align": 1, "corpora": 1, "automat": 1, "disambigu": 1, "includ": 1, "bilingu": 1, "follow": 1, "wordsens": 1}, "marker": "Brown et al., 1991a, ", "article": "P93-1003", "vector_2": [2, 0.05210931869837574, 6, 10, 0, 0]}, {"label": "Pos", "current": "For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (Petrov and Klein, 2007).", "context": ["Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the grow-diag-final-and heuristic (Koehn et al., 2005).", "For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (Petrov and Klein, 2007).", "Except for minor bug fixes, our method for extracting and scoring a translation grammar remains the same as in our WMT 2010 submission."], "vector_1": {"mgiza": 1, "parser": 1, "heurist": 1, "extract": 1, "use": 1, "fix": 1, "except": 1, "wmt": 1, "score": 1, "unidirect": 1, "submiss": 1, "symmetr": 1, "method": 1, "minor": 1, "gener": 1, "french": 1, "pars": 1, "translat": 1, "berkeley": 1, "grammar": 2, "bug": 1, "word": 1, "provid": 1, "align": 1, "growdiagfinaland": 1, "tree": 1, "remain": 1, "statist": 1, "english": 1}, "marker": "(Petrov and Klein, 2007)", "article": "W11-2143", "vector_2": [4, 0.23622305529522025, 3, 1, 1, 0]}, {"label": "Pos", "current": "The Susanne Treebank (Sampson, 1995) is utilized to create fully annotated training data.", "context": ["3.2 The Susanne Treebank and Baseline Training Data", "The Susanne Treebank (Sampson, 1995) is utilized to create fully annotated training data.", "25"], "vector_1": {"creat": 1, "treebank": 2, "annot": 1, "susann": 2, "util": 1, "train": 2, "fulli": 1, "data": 2, "baselin": 1}, "marker": "(Sampson, 1995)", "article": "W07-2203", "vector_2": [12, 0.38745930049364563, 1, 1, 0, 0]}, {"label": "Pos", "current": "The basic motivation for this approach is to benefit from the CF parsing technology whose development over 30 years has lead to powerful and efficient parsers [1,7].", "context": ["In [28], Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that \"split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing\".", "The basic motivation for this approach is to benefit from the CF parsing technology whose development over 30 years has lead to powerful and efficient parsers [1,7].", "A parser that takes into account only an approximation of the grammatical features will often find ambiguities it cannot resolve in the analyzed sentences'."], "vector_1": {"tunabl": 1, "set": 1, "effici": 1, "often": 1, "give": 1, "approxim": 1, "parser": 2, "domain": 1, "restrict": 1, "featur": 1, "motiv": 1, "grammat": 1, "year": 1, "ambigu": 1, "find": 1, "infinit": 1, "whose": 1, "use": 1, "develop": 1, "equival": 1, "split": 1, "basic": 1, "approach": 3, "take": 1, "power": 1, "sentenc": 1, "variant": 1, "finit": 1, "cf": 1, "base": 1, "pars": 2, "analyz": 1, "class": 1, "account": 1, "lead": 1, "technolog": 1, "resolv": 1, "nontermin": 1, "shieber": 1, "benefit": 1, "cannot": 1, "survey": 1, "exist": 1}, "marker": "[7]", "article": "P89-1018", "vector_2": [1, 0.05476246197051252, 3, 2, 0, 0]}, {"label": "CoCo", "current": "Unlike in (Marecek and Zabokrtsky, 2012), where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming.", "context": ["We employ the Gibbs sampling algorithm (Gilks et al., 1996).", "Unlike in (Marecek and Zabokrtsky, 2012), where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming.", "The algorithm works as follows:"], "vector_1": {"given": 1, "gibb": 1, "edg": 1, "algorithm": 2, "possibl": 1, "sentenc": 1, "individu": 1, "work": 1, "tree": 1, "use": 1, "employ": 1, "unlik": 1, "program": 1, "sampl": 3, "follow": 1, "whole": 1, "dynam": 1}, "marker": "(Marecek and Zabokrtsky, 2012)", "article": "P13-1028", "vector_2": [1, 0.5251028308734575, 2, 8, 1, 0]}, {"label": "Neut", "current": "Our architecture is based on a Support Vector Machine classifier, following Pradhan et al (2003).", "context": ["Most constituents are not arguments of the verb, and so the most common label is NULL.", "Our architecture is based on a Support Vector Machine classifier, following Pradhan et al (2003).", "Since SVMs are binary classifiers, we represent this 1-of-19 classification problem (18 roles plus NULL) by training 19 binary one-versus-all classifiers."], "vector_1": {"classif": 1, "constitu": 1, "train": 1, "al": 1, "repres": 1, "follow": 1, "et": 1, "null": 2, "support": 1, "classifi": 3, "label": 1, "binari": 2, "role": 1, "9": 1, "8": 1, "of9": 1, "oneversusal": 1, "architectur": 1, "machin": 1, "plu": 1, "argument": 1, "verb": 1, "base": 1, "pradhan": 1, "sinc": 1, "svm": 1, "vector": 1, "common": 1, "problem": 1}, "marker": "(2003)", "article": "N04-1032", "vector_2": [1, 0.20228428071565327, 1, 3, 0, 0]}, {"label": "Neut", "current": "Traditional readability formulas normally take into account the number of words per sentence or/and the number of \"hard\", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).", "context": ["Anybody who has seen patent claims at least once will find it unnecessary to calculate claim readability indices to get persuaded that the claim text is extremely low readable.", "Traditional readability formulas normally take into account the number of words per sentence or/and the number of \"hard\", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).", "Both the first and the second ratio will be equal to the number of words in a claim sentence where practically all words are \"hard\" terms, some of them used for the first time."], "vector_1": {"claim": 4, "text": 1, "hard": 2, "number": 3, "indic": 1, "orand": 1, "second": 1, "per": 2, "seen": 1, "find": 1, "use": 1, "long": 1, "ratio": 1, "patent": 1, "normal": 1, "readabl": 3, "persuad": 1, "least": 1, "low": 2, "formula": 1, "take": 1, "get": 1, "sentenc": 3, "term": 1, "tradit": 1, "account": 1, "word": 4, "practic": 1, "frequenc": 1, "equal": 1, "unnecessari": 1, "calcul": 1, "time": 1, "extrem": 1, "anybodi": 1, "first": 2}, "marker": "Brown, 1998", "article": "W14-5605", "vector_2": [16, 0.11372867587327376, 3, 5, 0, 0]}, {"label": "CoCo", "current": "Similar to our approach, Electric Knowledge transforms a natural language question into a series of increasingly more general keyword queries (Bierner, 2001).", "context": ["User questions are compared against those in the database, and links to webpages for the closest matches are returned.", "Similar to our approach, Electric Knowledge transforms a natural language question into a series of increasingly more general keyword queries (Bierner, 2001).", "However, their query formulation process utilizes hard-crafted regular expressions, while we adopt a more general machine learning approach for transformation rule application."], "vector_1": {"process": 1, "queri": 2, "knowledg": 1, "increasingli": 1, "languag": 1, "compar": 1, "databas": 1, "question": 2, "transform": 2, "formul": 1, "electr": 1, "approach": 2, "match": 1, "machin": 1, "applic": 1, "return": 1, "closest": 1, "webpag": 1, "gener": 2, "express": 1, "util": 1, "regular": 1, "link": 1, "user": 1, "natur": 1, "keyword": 1, "howev": 1, "adopt": 1, "rule": 1, "learn": 1, "similar": 1, "hardcraft": 1, "seri": 1}, "marker": "(Bierner, 2001)", "article": "W02-1024", "vector_2": [1, 0.16571449365015828, 1, 1, 0, 0]}, {"label": "Neut", "current": "However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse.", "context": ["The worst size complexity of such a chart is only a square function of the size of the input2.", "However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse.", "Furthermore it makes the extraction of parse-trees a trivial matter."], "vector_1": {"constitu": 2, "often": 1, "advantag": 1, "share": 1, "one": 1, "parsetre": 1, "ambigu": 2, "extract": 1, "sever": 1, "size": 2, "explicitli": 1, "squar": 1, "distinct": 1, "trivial": 1, "make": 1, "particip": 1, "complex": 2, "way": 1, "input": 1, "function": 1, "subtre": 1, "fragment": 1, "sentenc": 1, "relat": 1, "chart": 2, "worst": 1, "retain": 1, "associ": 1, "case": 1, "actual": 1, "practic": 1, "algorithm": 1, "possibl": 1, "howev": 1, "structur": 2, "nontermin": 1, "matter": 1, "instanc": 1, "common": 1, "furthermor": 1, "pars": 3, "produc": 1}, "marker": "[7]", "article": "P89-1018", "vector_2": [1, 0.11443950386145565, 5, 2, 0, 0]}, {"label": "Neut", "current": "See Goldsmith and Xanthos (2009) for an excellent survey on how to do this (something which falls under learning phonological categories rather than morphology learning).", "context": ["Xanthos (2007), on the other hand, starts out only by assuming that there exists a distinction between root and pattern graphemes and subsequently learns which graphemes are which.", "See Goldsmith and Xanthos (2009) for an excellent survey on how to do this (something which falls under learning phonological categories rather than morphology learning).", "Basically, it is possible only because there are systematic combination constraints between different phonemes (approximated by graphemes); for example, vowels and consonants alternate in a very non-random manner."], "vector_1": {"nonrandom": 1, "phonolog": 1, "graphem": 3, "approxim": 1, "see": 1, "exist": 1, "manner": 1, "phonem": 1, "assum": 1, "categori": 1, "differ": 1, "rather": 1, "pattern": 1, "someth": 1, "excel": 1, "morpholog": 1, "start": 1, "basic": 1, "combin": 1, "goldsmith": 1, "subsequ": 1, "systemat": 1, "altern": 1, "conson": 1, "hand": 1, "xantho": 2, "vowel": 1, "fall": 1, "possibl": 1, "constraint": 1, "distinct": 1, "exampl": 1, "survey": 1, "learn": 3, "root": 1}, "marker": "(2009)", "article": "J11-2002", "vector_2": [2, 0.7938520136933485, 2, 2, 8, 0]}, {"label": "Neut", "current": "Second, Irvine et al (2013) have shown that including relevant training data in a mixture modeling approach solves many coverage errors, but also introduces substantial amounts of new scoring errors.", "context": ["The reason we choose an instanceweighting method rather than a mixture modeling approach is twofold: First, mixture modeling approaches intrinsically depend on subcorpus boundaries, which resemble provenance or require manual labeling.", "Second, Irvine et al (2013) have shown that including relevant training data in a mixture modeling approach solves many coverage errors, but also introduces substantial amounts of new scoring errors.", "With phrase-pair weighting we aim to optimize phrase translation selection while keeping our training data fixed, and we can thus compare the impact of several methodological variants on genre adaptation for SMT."], "vector_1": {"phrase": 1, "weight": 1, "genr": 1, "al": 1, "proven": 1, "second": 1, "substanti": 1, "phrasepair": 1, "et": 1, "select": 1, "impact": 1, "shown": 1, "twofold": 1, "compar": 1, "rather": 1, "resembl": 1, "boundari": 1, "smt": 1, "coverag": 1, "label": 1, "also": 1, "score": 1, "includ": 1, "sever": 1, "new": 1, "approach": 3, "method": 1, "introduc": 1, "adapt": 1, "optim": 1, "methodolog": 1, "error": 2, "variant": 1, "mixtur": 3, "reason": 1, "train": 2, "translat": 1, "solv": 1, "depend": 1, "data": 2, "relev": 1, "requir": 1, "irvin": 1, "instanceweight": 1, "thu": 1, "manual": 1, "intrins": 1, "keep": 1, "aim": 1, "amount": 1, "choos": 1, "mani": 1, "model": 3, "subcorpu": 1, "fix": 1, "first": 1}, "marker": "(2013)", "article": "W15-2518", "vector_2": [2, 0.27907976068489077, 1, 1, 1, 0]}, {"label": "Pos", "current": "This knowledge is structured following the methodology described in (Sheremetyeva, 1999; Sheremetyeva, 2003) and is mostly coded in the system lexicon as well as in analysis and generation rules.", "context": ["Micro-level simplification at each of its stages is done by means of a specific combination of rulebased and statistical techniques and relies on linguistic knowledge of different depth.", "This knowledge is structured following the methodology described in (Sheremetyeva, 1999; Sheremetyeva, 2003) and is mostly coded in the system lexicon as well as in analysis and generation rules.", "Different modules of the micro-level simplification component use specific parts and types of linguistic knowledge included in the lexicon and their own specific sets of rules."], "vector_1": {"code": 1, "set": 1, "modul": 1, "knowledg": 3, "lexicon": 2, "done": 1, "rule": 2, "follow": 1, "differ": 2, "techniqu": 1, "system": 1, "compon": 1, "reli": 1, "includ": 1, "stage": 1, "microlevel": 2, "gener": 1, "analysi": 1, "methodolog": 1, "use": 1, "part": 1, "mostli": 1, "describ": 1, "specif": 3, "simplif": 2, "type": 1, "rulebas": 1, "well": 1, "structur": 1, "depth": 1, "combin": 1, "statist": 1, "linguist": 2, "mean": 1}, "marker": "Sheremetyeva, 2003)", "article": "W14-5605", "vector_2": [11, 0.5623730706742486, 2, 2, 3, 1]}, {"label": "Neut", "current": "We extended the standard ID3 model (Quinlan, 1986) to more general hybrid decision structures.", "context": ["A set of parse examples, as already described in the previous section, is then fed into an 1D3-based learning routine that generates a decision structure, which can then 'classify' any given parse state by proposing what parse action to perform next.", "We extended the standard ID3 model (Quinlan, 1986) to more general hybrid decision structures.", "In our tests, the best performing structure was a decision list (Rivest, 1987) of hierarchical decision trees, whose simplified basic structure is illustrated in figure 3."], "vector_1": {"routin": 1, "set": 1, "illustr": 1, "tree": 1, "alreadi": 1, "id": 1, "best": 1, "hierarch": 1, "whose": 1, "fed": 1, "describ": 1, "perform": 2, "section": 1, "hybrid": 1, "figur": 1, "classifi": 1, "next": 1, "state": 1, "decis": 4, "basic": 1, "test": 1, "simplifi": 1, "extend": 1, "gener": 2, "previou": 1, "given": 1, "standard": 1, "pars": 3, "dbase": 1, "list": 1, "structur": 4, "exampl": 1, "learn": 1, "action": 1, "model": 1, "propos": 1}, "marker": "(Quinlan, 1986)", "article": "P97-1062", "vector_2": [11, 0.4817749053129018, 2, 1, 0, 0]}, {"label": "CoCo", "current": "In the future we also intend to extract usages of the target construction from datasets of less formal text, such as blogs (e.g., Burton et al., 2009).", "context": ["While we do not adopt the view of some that usages of the target construction having the \"no\" interpretation are errors, it could be the case that such usages are more frequent in less formal text.", "In the future we also intend to extract usages of the target construction from datasets of less formal text, such as blogs (e.g., Burton et al., 2009).", "Constructions other than No X is too Y to Z exhibit a similar ambiguity."], "vector_1": {"exhibit": 1, "less": 2, "text": 2, "eg": 1, "dataset": 1, "extract": 1, "construct": 3, "blog": 1, "also": 1, "futur": 1, "intend": 1, "usag": 3, "interpret": 1, "formal": 2, "case": 1, "target": 2, "could": 1, "adopt": 1, "ambigu": 1, "error": 1, "x": 1, "z": 1, "similar": 1, "frequent": 1, "view": 1}, "marker": "Burton et al., 2009)", "article": "W10-2109", "vector_2": [1, 0.890562861040959, 1, 1, 0, 0]}, {"label": "Neut", "current": "Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "context": ["In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.", "Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose."], "vector_1": {"corpu": 1, "domain": 1, "task": 1, "comput": 1, "techniqu": 1, "appli": 1, "use": 2, "tree": 2, "histor": 1, "research": 1, "other": 1, "cluster": 1, "corpusbas": 1, "statist": 1, "famili": 2, "reconstruct": 2, "linguist": 1, "method": 1, "languag": 2, "purpos": 1}, "marker": "(Enright and Kondrak, 2011", "article": "P13-1112", "vector_2": [2, 0.9278944947094004, 7, 2, 0, 0]}, {"label": "CoCo", "current": "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "context": ["The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\"", "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level."], "vector_1": {"featur": 1, "profici": 1, "focus": 1, "connect": 1, "touch": 1, "skill": 1, "languag": 3, "involv": 1, "rather": 1, "question": 1, "construct": 1, "valid": 1, "advoc": 1, "factor": 1, "test": 2, "instead": 1, "analys": 1, "tightli": 1, "difficulti": 2, "gener": 1, "argu": 1, "gap": 1, "vocabulari": 1, "earliest": 1, "ctest": 4, "reduc": 1, "measur": 2, "search": 1, "grammar": 1, "level": 3, "aim": 1, "paragraph": 1, "combin": 1, "determin": 1, "model": 1, "other": 1}, "marker": "Sigott, 1995", "article": "Q14-1040", "vector_2": [19, 0.1619488901928252, 6, 7, 1, 0]}, {"label": "Pos", "current": "The ngram contexts achieved the best F-Scores fairly consistently for all parsers, vindicating our appeal to the psycholinguistic research of Cartwright and Brent (1997), Mintz (2003) and Redington et al (1998).", "context": ["Surprisingly, the Skip-gram model retrained on biomedical data (SG-bio) fared worse than the original (SG-news), due probably in large part to the fact that the original training data was almost 100 times larger than our 1.2B word corpus.", "The ngram contexts achieved the best F-Scores fairly consistently for all parsers, vindicating our appeal to the psycholinguistic research of Cartwright and Brent (1997), Mintz (2003) and Redington et al (1998).", "Turning now to each parser individually, the baseline performance of the Berkeley Parser proved difficult to exceed, with only the 2gram distributional contexts giving any improvement."], "vector_1": {"skipgram": 1, "origin": 2, "exceed": 1, "give": 1, "almost": 1, "individu": 1, "parser": 3, "al": 1, "biomed": 1, "cartwright": 1, "et": 1, "surprisingli": 1, "best": 1, "probabl": 1, "prove": 1, "perform": 1, "due": 1, "research": 1, "vindic": 1, "larg": 1, "brent": 1, "mintz": 1, "sgnew": 1, "appeal": 1, "fscore": 1, "difficult": 1, "fare": 1, "distribut": 1, "corpu": 1, "redington": 1, "sgbio": 1, "retrain": 1, "fairli": 1, "ngram": 1, "train": 1, "berkeley": 1, "part": 1, "data": 2, "b": 1, "word": 1, "consist": 1, "psycholinguist": 1, "larger": 1, "turn": 1, "achiev": 1, "wors": 1, "gram": 1, "context": 2, "time": 1, "improv": 1, "model": 1, "baselin": 1, "fact": 1}, "marker": "(1997)", "article": "W15-2610", "vector_2": [18, 0.7871614351154426, 3, 5, 2, 0]}, {"label": "Neut", "current": "Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.", "context": ["Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques.", "Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.", "Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text."], "vector_1": {"set": 1, "primarili": 1, "ontolog": 3, "lexicosyntact": 1, "via": 1, "heurist": 2, "find": 1, "languag": 1, "use": 1, "techniqu": 1, "divid": 1, "pattern": 2, "hybrid": 1, "three": 1, "categori": 1, "reli": 1, "varieti": 1, "hearst": 1, "approach": 2, "express": 1, "relat": 2, "base": 1, "outlin": 1, "text": 1, "statist": 1, "learn": 1, "typic": 1, "linguist": 1, "fact": 1}, "marker": "Berland and Charniak, 1999", "article": "W12-5209", "vector_2": [13, 0.27904701617080047, 4, 3, 2, 0]}, {"label": "Neut", "current": "An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al (2009).", "context": ["The corpus is a collection of 2,000 introductory sections from Wikipedia articles about individual people in which all mentions of person entities have been annotated.", "An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al (2009).", "Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b)."], "vector_1": {"corpu": 1, "evalu": 1, "individu": 1, "al": 1, "msr": 1, "belz": 1, "result": 1, "year": 1, "et": 1, "entiti": 1, "develop": 1, "person": 1, "section": 1, "wikipedia": 1, "system": 1, "articl": 1, "submiss": 1, "main": 1, "challeng": 1, "indepth": 1, "peopl": 1, "gener": 1, "previou": 1, "subject": 1, "mention": 1, "along": 1, "task": 1, "extens": 1, "provid": 1, "descript": 1, "annot": 1, "collect": 1, "grec": 1, "introductori": 1, "refer": 1}, "marker": "(2009)", "article": "W10-4231", "vector_2": [1, 0.13363390441839496, 3, 2, 0, 0]}, {"label": "Pos", "current": "The third line in Table 4 shows the result reported by Zeng et al (2014) when only word embeddings and WPEs are used as input to the network (similar to our CNN+Softmax).", "context": ["CR-CNN outperforms CNN+Softmax in both precision and recall, and improves the F1 by 1.6.", "The third line in Table 4 shows the result reported by Zeng et al (2014) when only word embeddings and WPEs are used as input to the network (similar to our CNN+Softmax).", "We believe that the word embeddings employed by them is the main reason their result is much worse than that of CNN+Softmax."], "vector_1": {"show": 1, "al": 1, "zeng": 1, "result": 2, "tabl": 1, "et": 1, "employ": 1, "use": 1, "network": 1, "much": 1, "6": 1, "input": 1, "main": 1, "crcnn": 1, "recal": 1, "reason": 1, "believ": 1, "report": 1, "wpe": 1, "line": 1, "word": 2, "third": 1, "f": 1, "outperform": 1, "precis": 1, "wors": 1, "cnnsoftmax": 3, "improv": 1, "embed": 2, "similar": 1}, "marker": "(2014)", "article": "P15-1061", "vector_2": [1, 0.6972541421699626, 1, 12, 0, 0]}, {"label": "Neut", "current": "A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) but previously discussed by Halle and Vergnaud (1987), is as follows: Unique Stress Constraint (USC): A word can bear at most one primary stress.", "context": ["It is important to identify such constraints to see to what extent they complement, or even replace, domain neutral learning mechanisms.", "A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) but previously discussed by Halle and Vergnaud (1987), is as follows: Unique Stress Constraint (USC): A word can bear at most one primary stress.", "A simple example of how adult learners might use the USC is upon hearing novel names or words."], "vector_1": {"domain": 1, "identifi": 1, "vergnaud": 1, "replac": 1, "one": 1, "see": 1, "particularli": 1, "follow": 1, "primari": 1, "previous": 1, "even": 1, "use": 2, "usc": 2, "learner": 1, "discuss": 1, "exampl": 1, "import": 1, "simpl": 1, "introduc": 1, "complement": 1, "upon": 1, "bear": 1, "neutral": 1, "mechan": 1, "adult": 1, "extent": 1, "segment": 2, "hall": 1, "stress": 2, "novel": 1, "word": 4, "name": 1, "constraint": 3, "might": 1, "uniqu": 1, "yang": 1, "learn": 1, "hear": 1, "problem": 1}, "marker": "(2004)", "article": "W10-2912", "vector_2": [6, 0.2741212904777654, 2, 9, 0, 0]}, {"label": "Neut", "current": "For example, (Gollins and Sanderson, 2001) use lexical triangulation to translate in parallel across multiple intermediate languages and", "context": ["Existing algorithms for creating new bilingual dictionaries use intermediate languages or intermediate dictionaries to find chains of words with the same meaning.", "For example, (Gollins and Sanderson, 2001) use lexical triangulation to translate in parallel across multiple intermediate languages and", "1http://www.ethnologue.com/ 2http://dsal.uchicago.edu/dictionaries/list.html"], "vector_1": {"creat": 1, "exist": 1, "httpdsaluchicagoedudictionarieslisthtml": 1, "find": 1, "languag": 2, "use": 2, "multipl": 1, "chain": 1, "intermedi": 3, "new": 1, "across": 1, "triangul": 1, "lexic": 1, "translat": 1, "dictionari": 2, "httpwwwethnologuecom": 1, "parallel": 1, "word": 1, "algorithm": 1, "exampl": 1, "bilingu": 1, "mean": 1}, "marker": "(Gollins and Sanderson, 2001)", "article": "N13-1057", "vector_2": [12, 0.09282863396420754, 1, 1, 0, 0]}, {"label": "Pos", "current": "Our approach is integrated into an English-German morphology-aware SMT system which first translates into a lemmatized representation with a component to generate fully inflected forms in a second step, an approach similar to the work by Toutanova et al (2008) and Fraser et al (2012).", "context": ["3 Methodology", "Our approach is integrated into an English-German morphology-aware SMT system which first translates into a lemmatized representation with a component to generate fully inflected forms in a second step, an approach similar to the work by Toutanova et al (2008) and Fraser et al (2012).", "The inflection requires the modeling of the grammatical case of noun phrases (among other features), which corresponds to determining the syntactic function1."], "vector_1": {"represent": 1, "featur": 1, "among": 1, "fraser": 1, "al": 2, "correspond": 1, "second": 1, "phrase": 1, "et": 2, "smt": 1, "system": 1, "compon": 1, "approach": 2, "inflect": 2, "function": 1, "syntact": 1, "form": 1, "methodolog": 1, "step": 1, "englishgerman": 1, "translat": 1, "morphologyawar": 1, "requir": 1, "case": 1, "grammat": 1, "noun": 1, "work": 1, "toutanova": 1, "lemmat": 1, "integr": 1, "determin": 1, "gener": 1, "fulli": 1, "model": 1, "similar": 1, "first": 1}, "marker": "(2008)", "article": "W15-4923", "vector_2": [7, 0.18966921948834936, 2, 2, 0, 0]}, {"label": "Neut", "current": "Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.", "context": ["3 Constraining the Learning Space", "Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.", "If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand."], "vector_1": {"effici": 1, "modern": 1, "fail": 1, "equip": 1, "children": 2, "space": 2, "suggest": 1, "research": 1, "machin": 1, "knowledg": 1, "hand": 1, "succeed": 1, "model": 1, "like": 1, "task": 2, "essenti": 1, "algorithm": 1, "specif": 2, "constraint": 2, "realist": 1, "learn": 6, "domainneutr": 1, "constrain": 1}, "marker": "(Gold, 1967", "article": "W10-2912", "vector_2": [43, 0.2615833308277459, 3, 1, 0, 0]}, {"label": "Neut", "current": "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "context": ["Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.", "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc."], "vector_1": {"phrase": 1, "domain": 1, "en": 1, "aspect": 2, "modelis": 2, "celui": 1, "al": 1, "ce": 1, "accessibilitd": 1, "distanc": 1, "travaux": 1, "il": 1, "semblent": 1, "et": 2, "eu": 1, "sen": 1, "diver": 1, "le": 2, "la": 2, "associ": 1, "montrant": 1, "capter": 1, "densitd": 1, "dictionnair": 1, "dernier": 1, "preter": 1, "structur": 2, "effet": 1, "de": 5, "lexic": 1, "mot": 2, "compri": 1, "du": 2, "langu": 1, "mond": 1, "merveil": 1, "nombreux": 2, "pour": 2, "etc": 1, "gaum": 1, "entr": 1, "leur": 1, "lexical": 1, "dynamiqu": 1, "graph": 1, "pertin": 1, "ou": 1, "moyenn": 1, "se": 1}, "marker": "Mihalcea et Radev, 2011", "article": "W14-6700", "vector_2": [3, 0.2616651418115279, 11, 4, 0, 1]}, {"label": "Neut", "current": "These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003).", "context": ["There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.", "These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003).", "One of the most pressing problems is the restriction to a single, flat stream or sequence ofprimary data (called \"text\" in some approaches), or a single, flat timeline."], "vector_1": {"stream": 1, "text": 1, "timelin": 1, "one": 1, "restrict": 1, "establish": 1, "commun": 1, "ofprimari": 1, "also": 1, "powla": 1, "complex": 1, "call": 1, "approach": 2, "singl": 2, "difficult": 1, "flat": 2, "sequenc": 1, "laf": 1, "express": 1, "link": 1, "theori": 1, "press": 1, "multimod": 1, "data": 3, "problem": 1, "made": 1, "structur": 1, "shortcom": 2, "found": 1, "model": 1}, "marker": "(Chiarcos, 2012)", "article": "W13-5507", "vector_2": [1, 0.37815340805374026, 2, 1, 1, 0]}, {"label": "Pos", "current": "As a baseline, we use the best feature set from Beigman Klebanov et al (2014).", "context": ["4 Baseline System", "As a baseline, we use the best feature set from Beigman Klebanov et al (2014).", "Specifically, the baseline contains the following families of features:"], "vector_1": {"use": 1, "featur": 2, "specif": 1, "system": 1, "klebanov": 1, "al": 1, "beigman": 1, "set": 1, "follow": 1, "famili": 1, "contain": 1, "et": 1, "best": 1, "baselin": 3}, "marker": "(2014)", "article": "W15-1402", "vector_2": [1, 0.17117475470697427, 1, 8, 5, 0]}, {"label": "Neut", "current": "Moreover, the bilingual unlabeled data is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014).", "context": ["Table 1: Statistics of training and testing datasets", "Moreover, the bilingual unlabeled data is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014).", "There are in total 2,215,000 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous."], "vector_1": {"corpu": 1, "domain": 1, "spoken": 1, "dataset": 1, "tabl": 1, "onlin": 1, "news": 1, "total": 1, "concentr": 1, "differ": 1, "miscellan": 1, "data": 1, "moreov": 1, "larg": 1, "includ": 1, "test": 1, "unlabel": 1, "resourc": 1, "form": 1, "sentenc": 1, "train": 1, "inhous": 1, "chineseenglish": 2, "pair": 1, "22000": 1, "law": 1, "crawl": 1, "parallel": 1, "novel": 1, "statist": 1, "bilingu": 1}, "marker": "(Tian et al., 2014)", "article": "D15-1142", "vector_2": [1, 0.8300852827345275, 1, 1, 0, 0]}, {"label": "Neut", "current": "Terms are filtered out using weirdness measure (Ahmad et al., 1999).", "context": ["Relevance of the key term in the corpus is calculated by counting the frequency of the term.", "Terms are filtered out using weirdness measure (Ahmad et al., 1999).", "Feature vector for each term is created by including co-occurring nouns, verbs and adjectives."], "vector_1": {"corpu": 1, "count": 1, "term": 4, "featur": 1, "noun": 1, "creat": 1, "measur": 1, "frequenc": 1, "use": 1, "cooccur": 1, "filter": 1, "calcul": 1, "vector": 1, "includ": 1, "key": 1, "verb": 1, "adject": 1, "weird": 1, "relev": 1}, "marker": "(Ahmad et al., 1999)", "article": "W12-5209", "vector_2": [13, 0.513494849513725, 1, 3, 0, 0]}, {"label": "Neut", "current": "More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.", "context": ["For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.", "More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.", "Among them, the 'Recently, native language identification has drawn the attention of NLP researchers."], "vector_1": {"kita": 1, "nlp": 1, "among": 1, "metric": 1, "al": 1, "cluster": 1, "chrietien": 1, "famili": 1, "batagelj": 1, "et": 1, "languag": 3, "use": 1, "ellegard": 1, "research": 1, "identif": 1, "drawn": 1, "method": 2, "kroeber": 1, "nativ": 1, "attent": 1, "reconstruct": 1, "recent": 2, "measur": 1, "tree": 1, "instanc": 1, "statist": 1, "similar": 1, "propos": 2}, "marker": "(1999)", "article": "P13-1112", "vector_2": [14, 0.1641159331390891, 4, 3, 0, 0]}, {"label": "Pos", "current": "We used a version of the corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006).", "context": ["Most of the emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely social emails.", "We used a version of the corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006).", "The annotator of the majority of the Loqui corpus also annotated the Enron corpus."], "vector_1": {"corpu": 3, "major": 1, "loqui": 1, "exchang": 1, "miss": 1, "concern": 1, "use": 1, "quot": 1, "messag": 1, "also": 2, "version": 1, "pure": 1, "schedul": 1, "email": 3, "solv": 1, "enron": 1, "restor": 1, "annot": 2, "inform": 1, "social": 1, "meet": 1, "problem": 1}, "marker": "(Yeh and Harnly, 2006)", "article": "W09-3953", "vector_2": [3, 0.4738377600121244, 1, 1, 0, 0]}, {"label": "Neut", "current": "We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).", "context": ["4 Approximation error", "We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).", "We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models."], "vector_1": {"em": 3, "point": 1, "predict": 1, "approxim": 2, "increas": 1, "inform": 1, "decreas": 1, "follow": 1, "pcfg": 1, "incorrect": 1, "supervis": 1, "log": 1, "author": 1, "perform": 1, "diverg": 1, "px": 1, "show": 1, "question": 1, "accuraci": 2, "start": 1, "figur": 1, "estim": 1, "analyz": 1, "similarli": 1, "discrep": 1, "trend": 1, "experi": 1, "run": 1, "optim": 1, "act": 1, "initi": 2, "hmm": 1, "believ": 1, "mani": 1, "bgen": 1, "argmax": 1, "contain": 1, "likelihood": 3, "discuss": 1, "e": 1, "valuabl": 1, "specif": 1, "confront": 1, "iter": 1, "p": 3, "bias": 1, "behav": 1, "dmv": 1, "error": 2, "found": 1, "model": 3, "surrog": 1}, "marker": "Smith and Eisner, 2005", "article": "P08-1100", "vector_2": [3, 0.3141630320857423, 3, 2, 1, 0]}, {"label": "Neut", "current": "Elming (2006) suggests using tranformation-based learning to automatically acquire error-correcting rules from such data; however, the proposed method only applies to lexical choice errors.", "context": ["Allen and Hogan (2000) sketch the outline of such an automated post-editing (APE) system, which would automatically learn post-editing rules from a tri-parallel corpus of source, raw MT and post-edited text.", "Elming (2006) suggests using tranformation-based learning to automatically acquire error-correcting rules from such data; however, the proposed method only applies to lexical choice errors.", "Knight and Chander (1994) also argue in favor of using a separate APE module, which is then portable across multiple MT systems and language pairs, and suggest that the post-editing task could be performed using statistical machine translation techniques."], "vector_1": {"corpu": 1, "machin": 1, "sketch": 1, "chander": 1, "text": 1, "modul": 1, "automat": 2, "raw": 1, "allen": 1, "postedit": 4, "tranformationbas": 1, "languag": 1, "ape": 2, "use": 3, "multipl": 1, "acquir": 1, "would": 1, "elm": 1, "perform": 1, "suggest": 2, "system": 2, "also": 1, "appli": 1, "method": 1, "hogan": 1, "errorcorrect": 1, "sourc": 1, "error": 1, "argu": 1, "lexic": 1, "autom": 1, "translat": 1, "pair": 1, "triparallel": 1, "data": 1, "techniqu": 1, "choic": 1, "task": 1, "knight": 1, "howev": 1, "outlin": 1, "favor": 1, "rule": 2, "across": 1, "separ": 1, "mt": 2, "statist": 1, "learn": 2, "portabl": 1, "could": 1, "propos": 1}, "marker": "(2006)", "article": "N07-1064", "vector_2": [1, 0.13140575316128078, 3, 1, 0, 0]}, {"label": "Neut", "current": "For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.", "context": ["Thirdly, many (lexical) parameter estimates do not generalize well between domains.", "For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.", "Tadayoshi et al (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format."], "vector_1": {"corpu": 2, "domain": 3, "manual": 1, "parser": 2, "ptb": 1, "al": 1, "biomed": 1, "et": 1, "select": 2, "paramet": 2, "thirdli": 1, "gildea": 1, "collin": 1, "accuraci": 1, "estim": 1, "adapt": 1, "test": 2, "correct": 1, "brown": 1, "contribut": 1, "format": 1, "gener": 1, "lexic": 1, "bilex": 1, "retrain": 1, "train": 1, "pars": 1, "tadayoshi": 1, "improv": 1, "report": 1, "data": 2, "wsjderiv": 1, "deriv": 1, "augment": 1, "wsj": 1, "well": 1, "yield": 1, "instanc": 1, "statist": 1, "mani": 1, "model": 1, "genia": 1}, "marker": "(2001)", "article": "W07-2203", "vector_2": [6, 0.06809508805097504, 3, 2, 0, 0]}, {"label": "Pos", "current": "We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel.", "context": ["4.2 Link scoring", "We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel.", "2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005)."], "vector_1": {"x": 1, "competit": 1, "use": 1, "ratio": 1, "vogel": 1, "algorithm": 1, "possibl": 1, "005": 1, "train": 1, "zhang": 1, "weight": 1, "discrimin": 1, "score": 3, "link": 4, "t": 1, "although": 1, "choic": 1, "mani": 1, "modifi": 1, "loglikelihood": 1}, "marker": "(Gale and Church, 1991)", "article": "P08-1113", "vector_2": [17, 0.4981276079746068, 3, 1, 0, 0]}, {"label": "Neut", "current": "There is only little work investigating this correspondence formally (see (Hawkins and Filipovic, 2010; Alexopoulou et al., 2010) for discussion) and only on error-annotated English learner corpora.", "context": ["To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels.", "There is only little work investigating this correspondence formally (see (Hawkins and Filipovic, 2010; Alexopoulou et al., 2010) for discussion) and only on error-annotated English learner corpora.", "For this reason, we follow a data-driven approach to learn the correspondence between errors and levels, based on exercises from written placement exams."], "vector_1": {"corpora": 1, "datadriven": 1, "process": 1, "automat": 1, "see": 1, "learn": 1, "need": 1, "follow": 1, "learner": 2, "littl": 1, "written": 1, "approach": 1, "decis": 1, "exercis": 1, "type": 1, "investig": 1, "exam": 1, "error": 2, "reason": 1, "base": 1, "understand": 1, "discuss": 1, "formal": 1, "placement": 1, "level": 2, "frequenc": 1, "work": 1, "well": 1, "correspond": 3, "english": 1, "errorannot": 1, "model": 1}, "marker": "(Hawkins and Filipovic, 2010", "article": "W12-2011", "vector_2": [2, 0.060747236685850005, 2, 1, 2, 0]}, {"label": "Neut", "current": "The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output.", "context": ["In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques.", "The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output.", "A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors."], "vector_1": {"rulebas": 1, "show": 1, "idea": 1, "automat": 1, "second": 1, "tag": 2, "tagger": 1, "explor": 1, "dynam": 1, "quit": 1, "use": 3, "built": 1, "sensit": 1, "recognit": 2, "program": 1, "disambigu": 1, "languag": 2, "speech": 2, "approach": 3, "capit": 2, "sentenc": 1, "gener": 2, "punctuat": 2, "consid": 1, "pair": 1, "robust": 1, "techniqu": 1, "case": 1, "word": 1, "describ": 1, "possibl": 1, "outperform": 1, "larger": 1, "context": 1, "error": 2, "output": 1, "model": 2, "assign": 1}, "marker": "(Kim and Woodland, 2004)", "article": "W04-3237", "vector_2": [0, 0.268080960184242, 2, 2, 0, 0]}, {"label": "Neut", "current": "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "context": ["Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.", "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates."], "vector_1": {"set": 1, "profici": 1, "process": 1, "natur": 1, "close": 1, "languag": 2, "multipl": 1, "student": 1, "cloze": 1, "distractor": 1, "field": 1, "exercis": 3, "test": 2, "perspect": 1, "correct": 1, "difficulti": 1, "format": 1, "gener": 2, "previou": 1, "answer": 1, "candid": 1, "vocabulari": 1, "educ": 1, "choic": 2, "grammar": 1, "provid": 1, "approach": 1, "work": 1, "focu": 1, "discrimin": 1, "determin": 1, "usual": 1}, "marker": "Agarwal and Mannem, 2011", "article": "Q14-1040", "vector_2": [3, 0.14527649070270926, 7, 1, 0, 0]}, {"label": "Neut", "current": "Of these, LDA with 10 dimensions yields the best translation performance, which is consistent with findings in a related topic adaptation approach by Eidelman et al (2012).", "context": ["2010), with varying numbers of latent dimensions (5, 10, 20, and 50).", "Of these, LDA with 10 dimensions yields the best translation performance, which is consistent with findings in a related topic adaptation approach by Eidelman et al (2012).", "The LDA features in this VSM variant are inferred from the source side of the training data."], "vector_1": {"dimens": 2, "lda": 2, "featur": 1, "number": 1, "topic": 1, "et": 1, "vari": 1, "vsm": 1, "best": 1, "latent": 1, "perform": 1, "find": 1, "0": 1, "adapt": 1, "approach": 1, "infer": 1, "sourc": 1, "relat": 1, "variant": 1, "al": 1, "train": 1, "translat": 1, "data": 1, "consist": 1, "yield": 1, "side": 1, "eidelman": 1}, "marker": "(2012)", "article": "W15-2518", "vector_2": [3, 0.5213697815092718, 1, 2, 0, 0]}, {"label": "Neut", "current": "Zadrozny (1995) proposes Q-Complexity, which roughly corresponds to vocabulary size, and is essentially the number of questions that can be asked about a database.", "context": ["Little work has been reported on measures of the relationship between dialogue complexity and the semantic structure of a DS application's database.", "Zadrozny (1995) proposes Q-Complexity, which roughly corresponds to vocabulary size, and is essentially the number of questions that can be asked about a database.", "Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space."], "vector_1": {"semant": 2, "consid": 1, "number": 2, "ask": 1, "qcomplex": 1, "zadrozni": 1, "size": 1, "bierman": 1, "dialogu": 1, "describ": 1, "space": 1, "databas": 2, "question": 1, "littl": 1, "complex": 1, "distinguish": 1, "attribut": 1, "applic": 1, "everi": 1, "relationship": 2, "object": 1, "vocabulari": 1, "report": 1, "bit": 1, "ds": 1, "requir": 1, "measur": 2, "essenti": 1, "work": 1, "correspond": 1, "structur": 1, "pollard": 1, "similar": 1, "roughli": 1, "propos": 1}, "marker": "(1995)", "article": "W12-1635", "vector_2": [17, 0.270250998288648, 2, 1, 0, 0]}, {"label": "Neut", "current": "Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).", "context": ["Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model.", "Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).", "To ensure a fair comparison, we performed the evaluation in two steps."], "vector_1": {"featur": 1, "fair": 1, "zeng": 1, "ensur": 1, "monolingu": 1, "loglinear": 2, "sentencelevel": 1, "sever": 1, "use": 1, "outer": 1, "evalu": 1, "compar": 1, "semisupervis": 1, "perform": 1, "sun": 1, "moreov": 1, "two": 1, "next": 1, "inner": 1, "method": 2, "includ": 1, "rerank": 1, "effect": 1, "enhanc": 1, "step": 1, "candid": 1, "therebi": 1, "slbd": 1, "segment": 1, "demonstr": 1, "comparison": 1, "achiev": 1, "stateoftheart": 1, "x": 1, "model": 2, "produc": 1}, "marker": "(Sun et al., 2012)", "article": "D15-1142", "vector_2": [3, 0.9019077302462581, 3, 1, 0, 0]}, {"label": "Neut", "current": "features potentially generalize across languages (Petrenz and Webber, 2012), we compute the document-level feature values wi(d) on the source as well as the target sides of our bitext, and we examine whether both are equally suitable for translation model genre adaptation.", "context": ["seven features are most discriminative between the genres NW and UG, and are used in the genrespecific VSM approaches.", "features potentially generalize across languages (Petrenz and Webber, 2012), we compute the document-level feature values wi(d) on the source as well as the target sides of our bitext, and we examine whether both are equally suitable for translation model genre adaptation.", "3.4 Genre adaptation with LDA"], "vector_1": {"seven": 1, "featur": 3, "comput": 1, "genr": 3, "lda": 1, "wid": 1, "vsm": 1, "languag": 1, "use": 1, "bitext": 1, "suitabl": 1, "approach": 1, "across": 1, "nw": 1, "sourc": 1, "genrespecif": 1, "gener": 1, "adapt": 2, "translat": 1, "examin": 1, "model": 1, "valu": 1, "target": 1, "documentlevel": 1, "whether": 1, "well": 1, "equal": 1, "discrimin": 1, "potenti": 1, "ug": 1, "side": 1}, "marker": "(Petrenz and Webber, 2012)", "article": "W15-2518", "vector_2": [3, 0.46514395266585135, 1, 3, 0, 0]}, {"label": "Neut", "current": "These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).", "context": ["To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.", "These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).", "In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed."], "vector_1": {"scarciti": 1, "predict": 1, "number": 1, "monolingu": 1, "year": 1, "largescal": 1, "extract": 1, "sever": 1, "baselin": 1, "mutual": 1, "techniqu": 1, "semisupervis": 1, "cw": 1, "characterbas": 1, "label": 1, "also": 1, "cotrain": 1, "addit": 1, "approach": 3, "unlabel": 1, "updat": 1, "wordbas": 1, "investig": 1, "distribut": 1, "use": 1, "intens": 1, "address": 1, "segment": 2, "recent": 1, "attempt": 1, "corpora": 2, "manual": 2, "data": 1, "employ": 1, "inform": 1, "either": 1, "learn": 1, "model": 2}, "marker": "(Liang et al., 2005)", "article": "D15-1142", "vector_2": [10, 0.09189300270575744, 5, 1, 0, 0]}, {"label": "Neut", "current": "The baseline systems for the translation directions German-English and English-German are both developed using Discriminative Word Alignment (Niehues and Vogel, 2008) and the Moses Toolkit (Koehn et al., 2007) for extracting phrase pairs and generating the phrase table from the discriminative word alignments.", "context": ["2 Baseline System", "The baseline systems for the translation directions German-English and English-German are both developed using Discriminative Word Alignment (Niehues and Vogel, 2008) and the Moses Toolkit (Koehn et al., 2007) for extracting phrase pairs and generating the phrase table from the discriminative word alignments.", "The difficult reordering between German and English was modeled using POS-based reordering rules."], "vector_1": {"german": 1, "direct": 1, "tabl": 1, "phrase": 2, "extract": 1, "posbas": 1, "baselin": 2, "use": 2, "develop": 1, "system": 2, "reorder": 2, "difficult": 1, "mose": 1, "germanenglish": 1, "gener": 1, "toolkit": 1, "englishgerman": 1, "translat": 1, "pair": 1, "word": 2, "align": 2, "rule": 1, "discrimin": 2, "english": 1, "model": 1}, "marker": "(Koehn et al., 2007)", "article": "W10-1719", "vector_2": [3, 0.11559951191317193, 2, 1, 1, 0]}, {"label": "Neut", "current": "Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).", "context": ["2 Related Work and Overview", "Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).", "Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage."], "vector_1": {"wikipedia": 3, "semant": 3, "larger": 1, "comput": 2, "creat": 1, "overview": 1, "ir": 1, "cost": 1, "cognit": 1, "network": 1, "memori": 1, "activ": 1, "due": 1, "coverag": 2, "linkbas": 1, "spread": 1, "knowledg": 1, "esa": 1, "appli": 1, "hurdl": 1, "method": 2, "analysi": 1, "regard": 1, "variou": 1, "relat": 1, "wordnetbas": 1, "foremost": 1, "knowledgebas": 1, "biggest": 1, "base": 2, "theori": 1, "although": 1, "arguabl": 1, "associ": 1, "model": 1, "earlier": 1, "recent": 1, "measur": 1, "success": 1, "level": 1, "outperform": 1, "work": 1, "explicit": 1, "conceptu": 2, "adequ": 1, "text": 1, "wlm": 1, "found": 1, "sa": 1, "similar": 1}, "marker": "(Collins and Loftus, 1975)", "article": "W10-3506", "vector_2": [35, 0.11850900713773178, 6, 1, 0, 0]}, {"label": "CoCo", "current": "(Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics.", "context": ["(Ren et al., 2014) uses an unsupervised heterogeneous clustering.", "(Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics.", "We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or strongly, in a holistical view."], "vector_1": {"origin": 1, "featur": 1, "captur": 1, "certain": 1, "mine": 1, "indic": 1, "topic": 1, "cluster": 2, "motiv": 1, "aspect": 1, "phrase": 1, "taxonomi": 1, "entiti": 1, "use": 1, "latent": 1, "heterogen": 1, "broad": 1, "split": 1, "queri": 2, "holist": 1, "subtop": 1, "around": 1, "relationship": 1, "gener": 1, "intent": 6, "modifi": 1, "relev": 1, "strongli": 1, "name": 1, "unsupervis": 1, "tree": 1, "weakli": 1, "graph": 1, "link": 1, "model": 2, "similar": 2, "view": 1}, "marker": "(Yin and Shah, 2010)", "article": "D14-1114", "vector_2": [4, 0.9244445863125639, 3, 3, 0, 0]}, {"label": "Neut", "current": "(Shen and Joshi, 2004).", "context": ["In addition, generalization performance in the Top Twenty set is better than the Large Set due to the smaller set of \"better\" features, cf.", "(Shen and Joshi, 2004).", "If the number of the non-discriminative features is large enough, the data set becomes unsplittable."], "vector_1": {"set": 4, "number": 1, "enough": 1, "perform": 1, "data": 1, "gener": 1, "due": 1, "cf": 1, "better": 2, "nondiscrimin": 1, "smaller": 1, "larg": 2, "unsplitt": 1, "twenti": 1, "becom": 1, "featur": 2, "top": 1, "addit": 1}, "marker": "(Shen and Joshi, 2004)", "article": "N04-1023", "vector_2": [0, 0.8945175353987422, 1, 4, 2, 1]}, {"label": "Neut", "current": "We base our experiments on SemCor (Miller et al., 1993), a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers.", "context": ["SENSELEARNER is attempting to learn general semantic models for various word categories, starting with a relatively small sense-annotated corpus.", "We base our experiments on SemCor (Miller et al., 1993), a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers.", "The input to the disambiguation algorithm consists of raw text."], "vector_1": {"corpu": 1, "semant": 2, "text": 1, "dataset": 1, "raw": 1, "senselearn": 1, "tag": 1, "senseannot": 1, "lexicograph": 1, "categori": 1, "start": 1, "content": 1, "semcor": 1, "disambigu": 1, "rel": 1, "input": 1, "experi": 1, "variou": 1, "gener": 1, "base": 1, "word": 2, "attempt": 1, "consist": 1, "algorithm": 1, "manual": 1, "annot": 1, "train": 1, "balanc": 1, "learn": 1, "small": 1, "model": 1}, "marker": "(Miller et al., 1993)", "article": "P05-3014", "vector_2": [12, 0.3274342891278375, 1, 1, 1, 0]}, {"label": "Neut", "current": "We do not introduce the CRF-based CWS model in detail here, but more information can be obtained from (Lafferty et al., 2001) and (Xue et al., 2003).", "context": ["yj1 and yj represent the tags of the previous and current characters, respectively.", "We do not introduce the CRF-based CWS model in detail here, but more information can be obtained from (Lafferty et al., 2001) and (Xue et al., 2003).", "3.2 Phrase-level Features"], "vector_1": {"yj": 2, "crfbase": 1, "phraselevel": 1, "previou": 1, "obtain": 1, "detail": 1, "charact": 1, "current": 1, "inform": 1, "tag": 1, "respect": 1, "model": 1, "repres": 1, "cw": 1, "featur": 1, "introduc": 1}, "marker": "(Xue et al., 2003)", "article": "D15-1142", "vector_2": [12, 0.36729801006952767, 2, 4, 0, 0]}, {"label": "Pos", "current": "One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).", "context": ["Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).", "One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).", "Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011)."], "vector_1": {"rang": 2, "help": 1, "intellig": 1, "challeng": 1, "one": 1, "topic": 1, "see": 1, "abil": 1, "morphologicallycomplex": 1, "increasingli": 1, "sever": 1, "detect": 2, "hoo": 1, "learner": 1, "research": 1, "write": 1, "languag": 3, "type": 1, "assist": 1, "learn": 1, "autom": 1, "ical": 1, "becom": 1, "strand": 1, "recent": 1, "task": 1, "rare": 1, "computerassist": 1, "work": 1, "focu": 1, "exampl": 1, "determin": 1, "error": 3, "popular": 1}, "marker": "(Dale and Kilgarriff, 2011)", "article": "W12-2011", "vector_2": [1, 0.03127188575256539, 6, 1, 0, 0]}, {"label": "Neut", "current": "To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and Ramat, 2006).", "context": ["2 Approach", "To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and Ramat, 2006).", "If the reconstructed tree is sufficiently similar to the original Indo-European family tree, it will support the hypothesis."], "vector_1": {"origin": 1, "whose": 1, "famili": 2, "hypothesi": 2, "text": 1, "tongu": 1, "mother": 1, "tree": 3, "indoeuropean": 2, "one": 1, "written": 1, "suffici": 1, "speaker": 1, "examin": 1, "english": 2, "reconstruct": 2, "similar": 1, "approach": 1, "support": 1, "languag": 2, "nonn": 1}, "marker": "(Beekes, 2011", "article": "P13-1112", "vector_2": [2, 0.14506977457445178, 2, 1, 0, 0]}, {"label": "Neut", "current": "In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.", "context": ["These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).", "In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.", "However, because monolingual unlabeled data contain limited natural segmenting information, in most semisupervised methods, the objective function tends to be optimized based on the personal experience and knowledge of the researchers."], "vector_1": {"predict": 1, "natur": 1, "knowledg": 1, "monolingu": 2, "largescal": 1, "tend": 1, "extract": 1, "research": 1, "sever": 1, "baselin": 1, "mutual": 1, "techniqu": 1, "semisupervis": 1, "data": 2, "characterbas": 1, "label": 1, "also": 1, "cotrain": 1, "experi": 1, "approach": 2, "method": 1, "unlabel": 2, "updat": 1, "function": 1, "wordbas": 1, "object": 1, "distribut": 1, "optim": 1, "contain": 1, "use": 1, "base": 1, "segment": 2, "addit": 1, "attempt": 1, "howev": 1, "corpora": 1, "manual": 1, "employ": 1, "inform": 2, "person": 1, "limit": 1, "either": 1, "learn": 1, "model": 2}, "marker": "(Zeng et al., 2013b)", "article": "D15-1142", "vector_2": [2, 0.09548926259547214, 5, 4, 2, 0]}, {"label": "CoCo", "current": "This stands in contrast to the analysis of Wason and Reich (1979), which presumes that people are applying some higher-level reasoning to \"correct\" an ill-formed statement in the case of the \"no\" in", "context": ["Note that such a constructional analysis of this phenomenon assumes that both interpretations of these sentences are linguistically valid, given the appropriate lexical instantiation.", "This stands in contrast to the analysis of Wason and Reich (1979), which presumes that people are applying some higher-level reasoning to \"correct\" an ill-formed statement in the case of the \"no\" in", "67"], "vector_1": {"phenomenon": 1, "instanti": 1, "appli": 1, "assum": 1, "given": 1, "illform": 1, "construct": 1, "note": 1, "valid": 1, "statement": 1, "peopl": 1, "presum": 1, "correct": 1, "contrast": 1, "analysi": 2, "sentenc": 1, "higherlevel": 1, "lexic": 1, "reason": 1, "wason": 1, "interpret": 1, "case": 1, "appropri": 1, "reich": 1, "stand": 1, "linguist": 1}, "marker": "(1979)", "article": "W10-2109", "vector_2": [31, 0.8493425063136811, 1, 7, 0, 0]}, {"label": "Neut", "current": "Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "context": ["2.2 Bilingual Semi-supervised CWS Methods", "Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004)."], "vector_1": {"dictionari": 1, "individu": 1, "focus": 1, "one": 1, "cw": 1, "leverag": 2, "semisupervis": 1, "perform": 1, "segment": 2, "smt": 1, "construct": 1, "label": 2, "better": 1, "consecut": 1, "approach": 1, "method": 1, "unlabel": 1, "either": 1, "machin": 1, "map": 1, "form": 1, "sequenc": 1, "chines": 2, "previou": 1, "train": 1, "translat": 1, "although": 1, "dataset": 1, "data": 1, "model": 1, "maximummatch": 1, "word": 2, "english": 1, "work": 1, "charact": 1, "achiev": 1, "statist": 1, "bilingu": 2, "studi": 1}, "marker": "(Xu et al., 2008)", "article": "D15-1142", "vector_2": [7, 0.2968798164194952, 7, 3, 5, 0]}, {"label": "Neut", "current": "[Cutting et al., 1992]).", "context": ["This situation is very similar to that involved in training HMM text taggers, where joint probabilities are computed that a particular word corresponds to a particular part-ofspeech, and the rest of the words in the sentence are also generated (e.g.", "[Cutting et al., 1992]).", "CONCLUSION"], "vector_1": {"involv": 1, "also": 1, "comput": 1, "sentenc": 1, "joint": 1, "text": 1, "gener": 1, "situat": 1, "correspond": 1, "rest": 1, "hmm": 1, "train": 1, "tagger": 1, "partofspeech": 1, "particular": 2, "word": 2, "similar": 1, "eg": 1, "conclus": 1, "probabl": 1}, "marker": "Cutting et al., 1992]", "article": "P93-1003", "vector_2": [1, 0.9634126060202893, 1, 10, 0, 0]}, {"label": "Weak", "current": "Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007), however, have until recently completely ignored stress.", "context": ["Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and \"algebraic\" (as opposed to \"statistical\") approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012).", "Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007), however, have until recently completely ignored stress.", "The sole exception in this respect is Doyle and Levy (2013) who added stress cues to the Bigram model (Goldwater et al., 2009), demonstrating that this leads to an improvement in segmentation performance."], "vector_1": {"comput": 1, "ad": 1, "oppos": 1, "respect": 1, "bigram": 1, "sever": 1, "use": 2, "network": 1, "lead": 1, "perform": 1, "except": 1, "cue": 1, "role": 1, "doyl": 1, "approach": 1, "complet": 1, "investig": 1, "algebra": 1, "sole": 1, "levi": 1, "bayesian": 1, "segment": 3, "model": 3, "demonstr": 1, "recent": 1, "stress": 3, "word": 2, "neural": 1, "howev": 1, "ignor": 1, "statist": 1, "improv": 1, "studi": 1}, "marker": "(Brent, 1999", "article": "Q14-1008", "vector_2": [15, 0.04721109236609026, 9, 4, 0, 0]}, {"label": "Neut", "current": "Pereira et al (1993) proposed a divisive clustering method to induce noun hierarchy from an encyclopedia.", "context": ["Verb-wise similarity of two nouns is calculated as the minimum shared weight and the similarity of two nouns is the sum of all verb-wise similarities.", "Pereira et al (1993) proposed a divisive clustering method to induce noun hierarchy from an encyclopedia.", "Hybrid approaches leverage the strengths of both statistical and heuristic based approaches and often use evidences from existing knowledge bases such as wordnet, wikipedia, etc."], "vector_1": {"often": 1, "weight": 1, "wikipedia": 1, "knowledg": 1, "share": 1, "al": 1, "cluster": 1, "minimum": 1, "exist": 1, "heurist": 1, "hierarchi": 1, "et": 1, "wordnet": 1, "leverag": 1, "use": 1, "strength": 1, "sum": 1, "hybrid": 1, "two": 2, "divis": 1, "approach": 2, "method": 1, "evid": 1, "verbwis": 2, "induc": 1, "base": 2, "noun": 3, "etc": 1, "calcul": 1, "encyclopedia": 1, "statist": 1, "pereira": 1, "similar": 3, "propos": 1}, "marker": "(1993)", "article": "W12-5209", "vector_2": [19, 0.34355757610634746, 1, 1, 0, 0]}, {"label": "Neut", "current": "[Cohen and Levesque, 1990; Grosz and Sidner, 1986; Lochbaum; hughes and McCoy]) is centered: intuitively, all that is needed for successful communication is that the hearer understand the speaker's end intentions, not that the act types themselves be recognized.", "context": ["It is on the following point that the main criticism of bounded sets of speech acts or rhetorical relations (e.g.", "[Cohen and Levesque, 1990; Grosz and Sidner, 1986; Lochbaum; hughes and McCoy]) is centered: intuitively, all that is needed for successful communication is that the hearer understand the speaker's end intentions, not that the act types themselves be recognized.", "This intuition, along with the lack of general agreement on the precise set of acts or relations lead some to reject the utility of relations altogether and concentrate only On intentions."], "vector_1": {"set": 2, "point": 1, "rhetor": 1, "eg": 1, "hearer": 1, "bound": 1, "critic": 1, "need": 1, "follow": 1, "concentr": 1, "end": 1, "lead": 1, "commun": 1, "hugh": 1, "speaker": 1, "reject": 1, "speech": 1, "mccoy": 1, "main": 1, "type": 1, "lochbaum": 1, "lack": 1, "recogn": 1, "altogeth": 1, "gener": 1, "relat": 3, "intuit": 2, "agreement": 1, "util": 1, "understand": 1, "along": 1, "center": 1, "success": 1, "precis": 1, "act": 3, "intent": 2}, "marker": "Grosz and Sidner, 1986", "article": "W93-0235", "vector_2": [7, 0.543358350664617, 2, 1, 0, 0]}, {"label": "Neut", "current": "Due to its popularity, both our work and that of (Lita et al., 2003) uses the 1-gram capitalizer as a baseline.", "context": ["2As with everything in natural language, it is not hard to find exceptions to this \"rule\".", "Due to its popularity, both our work and that of (Lita et al., 2003) uses the 1-gram capitalizer as a baseline.", "The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well."], "vector_1": {"evalu": 1, "natur": 1, "hard": 1, "indic": 1, "due": 1, "consequ": 1, "as": 1, "find": 1, "languag": 1, "baselin": 2, "use": 3, "perform": 1, "except": 1, "capit": 1, "000": 1, "word": 1, "algorithm": 2, "work": 2, "well": 1, "rule": 1, "gram": 2, "popular": 1, "everyth": 1, "microsoft": 1}, "marker": "(Lita et al., 2003)", "article": "W04-3237", "vector_2": [1, 0.24735373577217767, 2, 3, 0, 0]}, {"label": "Pos", "current": "We used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead.", "context": ["Then these alignments are used to extract the phrase pairs.", "We used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead.", "This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++2 Toolkit and POS information."], "vector_1": {"vogel": 1, "toolkit": 1, "phrase": 1, "extract": 1, "probabl": 1, "use": 3, "fertil": 1, "describ": 1, "inform": 1, "dwa": 1, "instead": 1, "po": 1, "handalign": 1, "gener": 2, "lexic": 1, "train": 1, "niehu": 1, "pair": 1, "data": 1, "word": 1, "align": 3, "well": 1, "discrimin": 1, "amount": 1, "small": 1, "model": 2, "pgiza": 1}, "marker": "(2008)", "article": "W10-1719", "vector_2": [2, 0.3931667844069103, 1, 3, 5, 0]}, {"label": "Neut", "current": "This type of memory function is a simplified representation of models of humans' memory recall capabilities (Anderson et al., 1998; Gillund and Shiffrin, 1984).", "context": ["pr(word) is the probability of a word being retrieved,  is a constant, and c(word) is the number of times the word has been identified in segmentations thus far.", "This type of memory function is a simplified representation of models of humans' memory recall capabilities (Anderson et al., 1998; Gillund and Shiffrin, 1984).", "This memory function for the value of  = 0.05, the value used in our experiments, is given in Figure 1."], "vector_1": {"represent": 1, "identifi": 1, "number": 1, "prword": 1, "human": 1, "constant": 1, "probabl": 1, "use": 1, "cword": 1, "memori": 3, "figur": 1, "experi": 1, "type": 1, "function": 2, "simplifi": 1, "recal": 1, "far": 1, "given": 1, "capabl": 1, "word": 2, "segment": 1, "valu": 2, "retriev": 1, "thu": 1, "time": 1, "model": 1}, "marker": "Gillund and Shiffrin, 1984)", "article": "W10-2912", "vector_2": [26, 0.5198893532577649, 2, 1, 0, 0]}, {"label": "Neut", "current": "The most similar work to ours is Yeh (2009) in which the authors derive a graph structure from the inter-article links in Wikipedia pages, and then perform random walks over the graph to compute relatedness.", "context": ["Text is categorised as vectors in this concept space and similarity is computed as the cosine similarity of their ESA vectors.", "The most similar work to ours is Yeh (2009) in which the authors derive a graph structure from the inter-article links in Wikipedia pages, and then perform random walks over the graph to compute relatedness.", "In Wikipedia, users create links between articles which are seen to be related to some degree."], "vector_1": {"yeh": 1, "concept": 1, "comput": 2, "creat": 1, "text": 1, "random": 1, "related": 1, "walk": 1, "seen": 1, "work": 1, "author": 1, "perform": 1, "graph": 2, "wikipedia": 2, "articl": 1, "esa": 1, "deriv": 1, "relat": 1, "categoris": 1, "link": 2, "user": 1, "space": 1, "interarticl": 1, "structur": 1, "degre": 1, "vector": 2, "cosin": 1, "similar": 3, "page": 1}, "marker": "(2009)", "article": "W10-3506", "vector_2": [1, 0.1535556478718985, 1, 1, 0, 0]}, {"label": "Neut", "current": "  Several natural language parser start with a pure ContextFree (CF) backbone that makes a first sketch of the structure of the analyzed sentence, before it is handed to a more elaborate analyzer (possibly a coroutine), that takes into account the finer grammatical structure to filter out undesirable parses (see for example [24,28]).", "context": ["In this paper we shall call shared forests such data struc", "  Several natural language parser start with a pure ContextFree (CF) backbone that makes a first sketch of the structure of the analyzed sentence, before it is handed to a more elaborate analyzer (possibly a coroutine), that takes into account the finer grammatical structure to filter out undesirable parses (see for example [24,28]).", "In [28], Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that \"split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing\"."], "vector_1": {"sketch": 1, "set": 1, "give": 1, "natur": 1, "parser": 1, "share": 1, "cf": 1, "restrict": 1, "see": 1, "paper": 1, "exist": 1, "domain": 1, "take": 1, "languag": 1, "sever": 1, "use": 1, "equival": 1, "shieber": 1, "make": 1, "start": 1, "tunabl": 1, "call": 1, "forest": 1, "pure": 1, "approach": 2, "analyz": 2, "infinit": 1, "shall": 1, "sentenc": 1, "variant": 1, "grammat": 1, "hand": 1, "base": 1, "pars": 2, "finit": 1, "elabor": 1, "struc": 1, "data": 1, "class": 1, "undesir": 1, "account": 1, "backbon": 1, "possibl": 1, "contextfre": 1, "split": 1, "structur": 2, "nontermin": 1, "filter": 1, "coroutin": 1, "exampl": 1, "survey": 1, "finer": 1, "first": 1}, "marker": "[24]", "article": "P89-1018", "vector_2": [13, 0.046370900337668414, 3, 2, 0, 0]}, {"label": "Neut", "current": "jMWE has reported an F1 measure of 83.4 in detecting continuous, unbroken MWEs in the Semcor (Mihalcea, 1998) Brown Concordance (Finlayson and Kulkarni, 2011).", "context": ["For the present studies we only looked at contiguous MWEs.", "jMWE has reported an F1 measure of 83.4 in detecting continuous, unbroken MWEs in the Semcor (Mihalcea, 1998) Brown Concordance (Finlayson and Kulkarni, 2011).", "Contiguous MWEs should show more signs of being a cohesive lexical unit, although non-contiguous MWEs should still exhibit some degree of the same."], "vector_1": {"exhibit": 1, "concord": 1, "jmwe": 1, "continu": 1, "sign": 1, "cohes": 1, "still": 1, "unit": 1, "detect": 1, "contigu": 2, "show": 1, "semcor": 1, "mwe": 4, "unbroken": 1, "brown": 1, "noncontigu": 1, "lexic": 1, "although": 1, "report": 1, "present": 1, "measur": 1, "look": 1, "f": 1, "degre": 1, "studi": 1}, "marker": "(Finlayson and Kulkarni, 2011)", "article": "W15-0914", "vector_2": [4, 0.47731811697574894, 2, 2, 0, 0]}, {"label": "Neut", "current": "However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse.", "context": ["The worst size complexity of such a chart is only a square function of the size of the input2.", "However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse.", "Furthermore it makes the extraction of parse-trees a trivial matter."], "vector_1": {"constitu": 2, "often": 1, "advantag": 1, "share": 1, "one": 1, "parsetre": 1, "ambigu": 2, "extract": 1, "sever": 1, "size": 2, "explicitli": 1, "squar": 1, "distinct": 1, "trivial": 1, "make": 1, "particip": 1, "complex": 2, "way": 1, "input": 1, "function": 1, "subtre": 1, "fragment": 1, "sentenc": 1, "relat": 1, "chart": 2, "worst": 1, "retain": 1, "associ": 1, "case": 1, "actual": 1, "practic": 1, "algorithm": 1, "possibl": 1, "howev": 1, "structur": 2, "nontermin": 1, "matter": 1, "instanc": 1, "common": 1, "furthermor": 1, "pars": 3, "produc": 1}, "marker": "[31]", "article": "P89-1018", "vector_2": [5, 0.11443950386145565, 5, 4, 0, 0]}, {"label": "Neut", "current": "In (Liu et al., 2008), a lenient metric is used which accounts for some inflection of words.", "context": ["Therefore, when the system only generates five keywords, the upper bound of the recall rate may not be 100%.", "In (Liu et al., 2008), a lenient metric is used which accounts for some inflection of words.", "Since that is highly correlated with the results using exact word match, we report results based on strict matching in the following experiments."], "vector_1": {"upper": 1, "metric": 1, "bound": 1, "rate": 1, "correl": 1, "result": 2, "follow": 1, "highli": 1, "use": 2, "system": 1, "strict": 1, "lenient": 1, "therefor": 1, "inflect": 1, "match": 2, "recal": 1, "may": 1, "gener": 1, "base": 1, "five": 1, "report": 1, "exact": 1, "sinc": 1, "account": 1, "experi": 1, "word": 2, "keyword": 1}, "marker": "(Liu et al., 2008)", "article": "N09-1070", "vector_2": [1, 0.6241578285677444, 1, 4, 0, 1]}, {"label": "Neut", "current": "A forward link (Flink) is the analog of a \"first pair-part\" of an adjacency pair (Sacks et al., 1974), and is similarly restricted to specific speech act types.", "context": ["DFU Links, or simply Links, correspond to adjacency pairs, but need not be adjacent.", "A forward link (Flink) is the analog of a \"first pair-part\" of an adjacency pair (Sacks et al., 1974), and is similarly restricted to specific speech act types.", "All Request-Information and Request-Action DFUs are assigned Flinks."], "vector_1": {"pairpart": 1, "simpli": 1, "specif": 1, "requestinform": 1, "dfu": 2, "flink": 2, "correspond": 1, "forward": 1, "restrict": 1, "need": 1, "adjac": 3, "requestact": 1, "link": 3, "act": 1, "similarli": 1, "pair": 2, "speech": 1, "type": 1, "assign": 1, "analog": 1, "first": 1}, "marker": "(Sacks et al., 1974)", "article": "W09-3953", "vector_2": [35, 0.3708180199295268, 1, 2, 0, 0]}, {"label": "Neut", "current": "Hassan and Mihalcea (2009) translated these datasets into Spanish, Arabic, and Romanian.", "context": ["Besides Joubarne and Inkpen (2011), other studies have made an effort to translate the original datasets by Rubenstein & Goodenough and by Miller & Charles.", "Hassan and Mihalcea (2009) translated these datasets into Spanish, Arabic, and Romanian.", "For Spanish, native speakers, who were highly proficient in English, were asked to translate the datasets."], "vector_1": {"origin": 1, "profici": 1, "goodenough": 1, "joubarn": 1, "rubenstein": 1, "dataset": 3, "spanish": 2, "highli": 1, "romanian": 1, "inkpen": 1, "besid": 1, "miller": 1, "speaker": 1, "arab": 1, "nativ": 1, "translat": 3, "ask": 1, "effort": 1, "mihalcea": 1, "made": 1, "english": 1, "hassan": 1, "studi": 1, "charl": 1}, "marker": "(2009)", "article": "W14-0118", "vector_2": [5, 0.20788328480636173, 2, 3, 0, 0]}, {"label": "Neut", "current": "2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005).", "context": ["We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel.", "2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005).", "The T2 statistics for a pair of words e; and fj is computed as"], "vector_1": {"comput": 1, "although": 1, "vogel": 1, "loglikelihood": 1, "use": 1, "ratio": 1, "weight": 1, "score": 2, "statist": 1, "competit": 1, "zhang": 1, "train": 1, "link": 3, "005": 1, "mani": 1, "pair": 1, "e": 1, "fj": 1, "modifi": 1, "choic": 1, "word": 1, "algorithm": 1, "possibl": 1, "discrimin": 1, "t": 2, "x": 1}, "marker": "(Taskar et al, 2005)", "article": "P08-1113", "vector_2": [3, 0.49680801740432967, 3, 1, 0, 0]}, {"label": "Neut", "current": "We here used the automatic text planner of the claim generator that was developed as a module of a patent MT system (Sheremetyeva, 2007).", "context": ["Simplification of a claim text into a diagram is performed based of the internal claim representation as shown in Section 5.3.", "We here used the automatic text planner of the claim generator that was developed as a module of a patent MT system (Sheremetyeva, 2007).", "Figure 4."], "vector_1": {"figur": 1, "claim": 3, "shown": 1, "planner": 1, "represent": 1, "system": 1, "simplif": 1, "perform": 1, "text": 2, "section": 1, "use": 1, "automat": 1, "mt": 1, "diagram": 1, "patent": 1, "intern": 1, "base": 1, "develop": 1, "modul": 1, "gener": 1}, "marker": "(Sheremetyeva, 2007)", "article": "W14-5605", "vector_2": [7, 0.8297877741673436, 1, 1, 4, 1]}, {"label": "Neut", "current": "The microaveraged F1 score for the baseline system using this evaluation scheme is 75.61%, which - over similar sets of relational dependencies - is broadly comparable to recent evaluation results published by King and collaborators with their state-of-theart parsing system (Briscoe et al., 2006).", "context": ["The microaveraged precision, recall and F1 scores are calculated from the counts for all relations in the hierarchy which subsume the parser output.", "The microaveraged F1 score for the baseline system using this evaluation scheme is 75.61%, which - over similar sets of relational dependencies - is broadly comparable to recent evaluation results published by King and collaborators with their state-of-theart parsing system (Briscoe et al., 2006).", "3The pipeline is the same as that used for creating S though we do not automatically map the bracketing to be more consistent with the system grammar, instead, we simply removed unary brackets."], "vector_1": {"unari": 1, "set": 1, "simpli": 1, "evalu": 2, "parser": 1, "automat": 1, "microaverag": 2, "collabor": 1, "result": 1, "hierarchi": 1, "baselin": 1, "use": 2, "depend": 1, "creat": 1, "compar": 1, "system": 3, "publish": 1, "score": 2, "instead": 1, "scheme": 1, "map": 1, "pipelin": 1, "recal": 1, "756": 1, "though": 1, "subsum": 1, "relat": 2, "pars": 1, "broadli": 1, "recent": 1, "count": 1, "king": 1, "grammar": 1, "consist": 1, "f": 2, "remov": 1, "precis": 1, "calcul": 1, "bracket": 2, "stateoftheart": 1, "output": 1, "the": 1, "similar": 1}, "marker": "(Briscoe et al., 2006)", "article": "W07-2203", "vector_2": [1, 0.5063543745404895, 1, 2, 3, 1]}, {"label": "Pos", "current": "We use the Moses statistical MT (SMT) toolkit (Koehn et al., 2007) to perform the translation.", "context": ["That is, the text between the relevant segment boundaries is not reordered nor mixed with the text outside these boundaries.3 Thus the text in the target language segment comes only from the corresponding source language segment.", "We use the Moses statistical MT (SMT) toolkit (Koehn et al., 2007) to perform the translation.", "In Moses, these reordering constraints are implemented with the zone and wall tags, as indicated in Figure 3."], "vector_1": {"wall": 1, "text": 3, "toolkit": 1, "indic": 1, "tag": 1, "languag": 2, "use": 1, "zone": 1, "perform": 1, "segment": 3, "boundari": 2, "smt": 1, "mix": 1, "figur": 1, "reorder": 2, "mose": 2, "sourc": 1, "translat": 1, "come": 1, "relev": 1, "target": 1, "constraint": 1, "thu": 1, "correspond": 1, "mt": 1, "statist": 1, "outsid": 1, "implement": 1}, "marker": "(Koehn et al., 2007)", "article": "P15-2128", "vector_2": [8, 0.5573788546255507, 1, 1, 1, 0]}, {"label": "Neut", "current": "Both text and hypothesis are tagged and lemmatised using Tree Tagger (Schmid, 1994), taking only nouns, non-auxiliary verbs, adjectives and adverbs into account.", "context": ["The shallow system measures the relative number of words in the hypothesis that also occur in the text.", "Both text and hypothesis are tagged and lemmatised using Tree Tagger (Schmid, 1994), taking only nouns, non-auxiliary verbs, adjectives and adverbs into account.", "Training a decision tree on the relative word-overlap as single feature yields a system which performs comparable to earlier word-overlap based systems, achieving an accuracy of 60.6 % if trained and tested on the RTE 2 development and test set, respectively (using Weka's J48 classifier), or 57.5 % if we use Weka's LogitBoost classifier."], "vector_1": {"rte": 1, "wordoverlap": 2, "featur": 1, "set": 1, "text": 2, "number": 1, "tag": 1, "tagger": 1, "weka": 2, "respect": 1, "occur": 1, "use": 3, "develop": 1, "adverb": 1, "perform": 1, "system": 3, "classifi": 2, "accuraci": 1, "also": 1, "adject": 1, "take": 1, "rel": 2, "test": 2, "singl": 1, "decis": 1, "earlier": 1, "hypothesi": 2, "lemmatis": 1, "shallow": 1, "verb": 1, "base": 1, "noun": 1, "measur": 1, "account": 1, "word": 1, "compar": 1, "j": 1, "tree": 2, "yield": 1, "achiev": 1, "train": 2, "nonauxiliari": 1, "logitboost": 1}, "marker": "(Schmid, 1994)", "article": "W07-1402", "vector_2": [13, 0.422445877234366, 1, 1, 0, 0]}, {"label": "CoCo", "current": "F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF", "context": ["Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.", "F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF", "Table 7: Performance comparison on the test set."], "vector_1": {"gazett": 1, "set": 1, "evalu": 1, "al": 4, "paper": 1, "tagger": 1, "tabl": 2, "et": 4, "best": 1, "25": 1, "zhou": 1, "perform": 1, "friedrich": 1, "postprocess": 2, "test": 1, "okanohara": 1, "method": 1, "fscore": 2, "variou": 1, "rerank": 2, "memm": 1, "hmm": 1, "train": 1, "semicrf": 1, "11": 1, "comparison": 2, "svm": 1, "tsai": 1, "crf": 1, "crfgazett": 1}, "marker": "(2006)", "article": "W07-1033", "vector_2": [1, 0.9417141500474834, 4, 6, 0, 0]}, {"label": "Neut", "current": "Previous studies applied this hypothesis to silent reading (Fodor, 2002).", "context": ["The Implicit Prosody Hypothesis, for example, posits that a silent prosodic contour is projected onto a stimulus, and may help a reader resolve syntactic ambiguity (Fodor, 2002).", "Previous studies applied this hypothesis to silent reading (Fodor, 2002).", "The present study, in turn, applies this same principle to (silent) typing: Language users take advantage of prosodic contours to help organize and make sense of language stimulus, whether in the form of words they are perceiving or words they are producing."], "vector_1": {"help": 2, "advantag": 1, "appli": 2, "prosodi": 1, "contour": 2, "languag": 2, "silent": 3, "make": 1, "read": 1, "take": 1, "reader": 1, "sens": 1, "type": 1, "syntact": 1, "hypothesi": 2, "may": 1, "previou": 1, "user": 1, "implicit": 1, "present": 1, "word": 2, "organ": 1, "prosod": 2, "whether": 1, "perceiv": 1, "resolv": 1, "ambigu": 1, "project": 1, "turn": 1, "exampl": 1, "form": 1, "stimulu": 2, "posit": 1, "studi": 2, "principl": 1, "onto": 1, "produc": 1}, "marker": "(Fodor, 2002)", "article": "W15-0914", "vector_2": [13, 0.08669248013042592, 2, 2, 0, 0]}, {"label": "Pos", "current": "As our n-best tagger, we use a first order MEMM model (McCallum et al., 2000).", "context": ["3 N-best MEMM tagger", "As our n-best tagger, we use a first order MEMM model (McCallum et al., 2000).", "Though CRFs (Lafferty et al., 2001) can be regarded as improved version of MEMMs, we have chosen MEMMs because MEMMs are usually much faster to train compared to CRFs, which enables extensive feature selection."], "vector_1": {"featur": 1, "version": 1, "tagger": 2, "select": 1, "use": 1, "compar": 1, "chosen": 1, "much": 1, "though": 1, "memm": 5, "regard": 1, "extens": 1, "train": 1, "nbest": 2, "faster": 1, "enabl": 1, "crf": 2, "improv": 1, "model": 1, "first": 1, "order": 1, "usual": 1}, "marker": "(McCallum et al., 2000)", "article": "W07-1033", "vector_2": [7, 0.30903767014878125, 2, 2, 4, 0]}, {"label": "Neut", "current": "For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CRF model.", "context": ["To address this problem, a number of semisupervised CWS approaches have been intensively investigated in recent years.", "For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CRF model.", "(Zeng et al., 2013a) introduced a graph-based semi-supervised joint model of Chinese word segmentation and part-of-speech tagging and regularized the learning of a linear CRF model based on the label distributions derived from unlabeled data."], "vector_1": {"statisticsbas": 1, "featur": 1, "number": 1, "tag": 1, "result": 1, "year": 1, "cw": 1, "linear": 1, "semisupervis": 2, "data": 2, "label": 1, "distribut": 1, "approach": 1, "unlabel": 2, "graphbas": 1, "investig": 1, "deriv": 2, "chines": 1, "learn": 1, "enhanc": 1, "joint": 1, "intens": 1, "partofspeech": 1, "address": 1, "base": 1, "segment": 2, "model": 3, "regular": 1, "recent": 1, "word": 1, "exampl": 1, "crf": 2, "introduc": 1, "interpol": 1, "problem": 1}, "marker": "(Sun and Xu, 2011)", "article": "D15-1142", "vector_2": [4, 0.26430797684693635, 2, 3, 4, 0]}, {"label": "Neut", "current": "The ability to distinguish states, e.g., \"Mark seems happy,\" from events, e.g., \"Renee ran down the street,\" is a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse (Moens and Steedman, 1988; Dorr, 1992; Klavans, 1994).", "context": ["1 Introduction", "The ability to distinguish states, e.g., \"Mark seems happy,\" from events, e.g., \"Renee ran down the street,\" is a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse (Moens and Steedman, 1988; Dorr, 1992; Klavans, 1994).", "Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause."], "vector_1": {"stativ": 1, "rene": 1, "ran": 1, "certain": 1, "eg": 2, "aspectu": 1, "adverbi": 1, "compos": 1, "street": 1, "abil": 1, "identifi": 1, "seem": 1, "happi": 1, "event": 1, "distinct": 1, "three": 1, "mark": 1, "adjunct": 1, "state": 1, "discours": 1, "sentenc": 1, "claus": 1, "prerequisit": 1, "distinguish": 1, "class": 1, "interpret": 1, "introduct": 1, "furthermor": 1, "constraint": 1, "well": 1, "tempor": 2, "necessari": 1, "fundament": 1, "first": 1}, "marker": "(Moens and Steedman, 1988", "article": "W97-0318", "vector_2": [9, 0.03062566800953712, 3, 3, 0, 0]}, {"label": "Neut", "current": "Rada Mihalcea and Dan Moldovan (1999) used hit counts for carefully constructed search engine queries to identify rank orders for word sense frequencies, as an input to a word sense disambiguation engine.", "context": ["The Web walked in on ACL meetings starting in 1999.", "Rada Mihalcea and Dan Moldovan (1999) used hit counts for carefully constructed search engine queries to identify rank orders for word sense frequencies, as an input to a word sense disambiguation engine.", "Philip Resnik (1999) showed that parallel corpora-until then a promising research avenue but largely constrained to the English-French Canadian Hansard-could be found on the Web: We can grow our own parallel corpus using the many Web pages that exist in parallel in local and in major languages."], "vector_1": {"corporauntil": 1, "corpu": 1, "major": 1, "identifi": 1, "show": 1, "queri": 1, "rank": 1, "walk": 1, "avenu": 1, "exist": 1, "mani": 1, "languag": 1, "page": 1, "web": 3, "use": 2, "engin": 2, "rada": 1, "construct": 1, "research": 1, "start": 1, "disambigu": 1, "mihalcea": 1, "sens": 2, "grow": 1, "local": 1, "hit": 1, "found": 1, "dan": 1, "larg": 1, "promis": 1, "englishfrench": 1, "input": 1, "philip": 1, "parallel": 3, "moldovan": 1, "care": 1, "count": 1, "search": 1, "word": 2, "resnik": 1, "canadian": 1, "frequenc": 1, "acl": 1, "hansardcould": 1, "meet": 1, "constrain": 1, "order": 1}, "marker": "(1999)", "article": "J03-3001", "vector_2": [4, 0.18011290281750753, 2, 1, 0, 0]}, {"label": "CoCo", "current": "3This is, in essence, also the strategy chosen by Doyle and Levy (2013).", "context": ["This (partly) captures the tendency of English for stress-initial words and thus provide an additional cue for identifying words; and it is exactly the kind of preference infant learners of English seem to acquire (Jusczyk", "3This is, in essence, also the strategy chosen by Doyle and Levy (2013).", "96"], "vector_1": {"infant": 1, "identifi": 1, "captur": 1, "prefer": 1, "seem": 1, "acquir": 1, "chosen": 1, "learner": 1, "also": 1, "cue": 1, "doyl": 1, "essenc": 1, "kind": 1, "jusczyk": 1, "levi": 1, "partli": 1, "exactli": 1, "stressiniti": 1, "addit": 1, "tendenc": 1, "word": 2, "provid": 1, "thu": 1, "thi": 1, "english": 2, "strategi": 1}, "marker": "(2013)", "article": "Q14-1008", "vector_2": [1, 0.3897617039223956, 1, 9, 0, 0]}, {"label": "Neut", "current": "We will also cover pivot paraphrasing (Bannard and Callison-Burch, 2005).", "context": ["We focus on distributional paraphrasing (Pasca and Dienes 2005; Marton et al., 2009; Marton, to appear 2012).", "We will also cover pivot paraphrasing (Bannard and Callison-Burch, 2005).", "We will discuss several weaknesses of distributional paraphrasing, and where the stateof-the-art is."], "vector_1": {"stateoftheart": 1, "distribut": 2, "also": 1, "weak": 1, "cover": 1, "pasca": 1, "focu": 1, "marton": 1, "dien": 1, "paraphras": 3, "pivot": 1, "discuss": 1, "sever": 1, "appear": 1}, "marker": "(Bannard and Callison-Burch, 2005)", "article": "N12-4007", "vector_2": [7, 0.4981894990947495, 2, 6, 0, 0]}, {"label": "Neut", "current": "Further PAM details are reported in Tambouratzis et al (2011).", "context": ["For instance, based on a sentence pair from the parallel corpus, the SL sentence with structure A-B-C-D is transformed into A'-C'-D'-B', where X is a phrase in SL and X' is a phrase in TL.", "Further PAM details are reported in Tambouratzis et al (2011).", "The PAM output in terms of SL phrases is then handed over to the Phrasing model generator (PMG), which is trained to determine the phrasal structure of an input sentence."], "vector_1": {"corpu": 1, "abcd": 1, "train": 1, "tambouratzi": 1, "transform": 1, "pair": 1, "pmg": 1, "et": 1, "phrase": 4, "detail": 1, "tl": 1, "input": 1, "pam": 2, "sentenc": 3, "acdb": 1, "al": 1, "base": 1, "x": 2, "report": 1, "hand": 1, "parallel": 1, "phrasal": 1, "term": 1, "structur": 2, "instanc": 1, "determin": 1, "sl": 3, "gener": 1, "output": 1, "model": 1}, "marker": "(2011)", "article": "W12-0108", "vector_2": [1, 0.3971178832978975, 1, 1, 0, 1]}, {"label": "Neut", "current": "in Johansson, 2010).", "context": ["The notion of various schemata interacting is supported by Kellogg (1996), who proposes that resources from the central executive of Baddeleys model of the working-memory, e.g., Baddeley (1974), are needed to perform both lower-level writing processes such as spelling, grammar and motor movements and higher-level writing processes such as planning and revising. (qtd.", "in Johansson, 2010).", "By comparing the production rates of different types of lexical unit retrieved from working memory  MWES versus free expressions  along with varying the overarching cognitive task, we believe our experiment lends quantifiable support to this notion."], "vector_1": {"execut": 1, "process": 2, "retriev": 1, "eg": 1, "baddeley": 2, "motor": 1, "need": 1, "plan": 1, "vari": 1, "revis": 1, "unit": 1, "believ": 1, "higherlevel": 1, "compar": 1, "memori": 1, "perform": 1, "cognit": 1, "interact": 1, "support": 2, "write": 2, "mwe": 1, "quantifi": 1, "experi": 1, "type": 1, "movement": 1, "free": 1, "product": 1, "express": 1, "resourc": 1, "variou": 1, "lend": 1, "spell": 1, "differ": 1, "lexic": 1, "grammar": 1, "kellogg": 1, "lowerlevel": 1, "workingmemori": 1, "along": 1, "task": 1, "central": 1, "work": 1, "qtd": 1, "overarch": 1, "versu": 1, "schemata": 1, "model": 1, "rate": 1, "notion": 2, "propos": 1}, "marker": "in Johansson, 2010)", "article": "W15-0914", "vector_2": [5, 0.05840635826370491, 3, 1, 0, 0]}, {"label": "Neut", "current": "Yang (2004) introduced a simple incremental algorithm that relies on stress by embodying a Unique Stress Constraint (USC) that allows at most a single stressed syllable per word.", "context": ["They only reported a word-token f-score of 44% (roughly, segmentation accuracy: see Section 4), which is considerably below the performance of subsequent models, making a direct comparison complicated.", "Yang (2004) introduced a simple incremental algorithm that relies on stress by embodying a Unique Stress Constraint (USC) that allows at most a single stressed syllable per word.", "On pre-syllabified child directed speech, he reported a word token fscore of 85.6% for a non-statistical algorithm that exploits the USC."], "vector_1": {"consider": 1, "exploit": 1, "direct": 2, "see": 1, "increment": 1, "make": 1, "usc": 2, "singl": 1, "perform": 1, "section": 1, "per": 1, "accuraci": 1, "reli": 1, "speech": 1, "subsequ": 1, "complic": 1, "introduc": 1, "child": 1, "report": 2, "segment": 1, "wordtoken": 1, "comparison": 1, "stress": 3, "word": 2, "algorithm": 2, "constraint": 1, "embodi": 1, "simpl": 1, "uniqu": 1, "syllabl": 1, "presyllabifi": 1, "token": 1, "yang": 1, "fscore": 2, "allow": 1, "nonstatist": 1, "model": 1, "roughli": 1}, "marker": "(2004)", "article": "Q14-1008", "vector_2": [10, 0.14039434837621256, 1, 9, 3, 0]}, {"label": "Neut", "current": "(Crigonyt`e et al., 2010)), but note that our problem is more restricted in that we have the same number of words, and in most cases identical words.", "context": ["This method is reminiscent of alignment approaches in paraphrasing (e.g.", "(Crigonyt`e et al., 2010)), but note that our problem is more restricted in that we have the same number of words, and in most cases identical words.", "We use different distance and similarity metrics, to ensure robustness across different kinds of errors."], "vector_1": {"distanc": 1, "eg": 1, "number": 1, "restrict": 1, "paraphras": 1, "ensur": 1, "metric": 1, "use": 1, "note": 1, "approach": 1, "across": 1, "differ": 2, "robust": 1, "case": 1, "kind": 1, "ident": 1, "word": 2, "reminisc": 1, "align": 1, "method": 1, "error": 1, "problem": 1, "similar": 1, "crigonyt": 1}, "marker": "e et al., 2010)", "article": "W12-2011", "vector_2": [2, 0.32410706129533207, 1, 2, 0, 0]}, {"label": "Neut", "current": "This in itself is not surprising, given the fact that Fokkens et al (2013) showed that even replicating the results that Pedersen (2010) reports can be challenging.", "context": ["Finally, the differences between the scores from the WordNet::Similarity package and the WordNetTools show that we did not reproduce the results exactly.", "This in itself is not surprising, given the fact that Fokkens et al (2013) showed that even replicating the results that Pedersen (2010) reports can be challenging.", "They showed that even if the main properties are kept stable, such as software and versions of software, variations in minor properties can lead to completely different outcomes."], "vector_1": {"softwar": 2, "version": 1, "show": 3, "challeng": 1, "al": 1, "packag": 1, "result": 2, "stabl": 1, "et": 1, "surpris": 1, "even": 2, "differ": 2, "lead": 1, "pedersen": 1, "score": 1, "main": 1, "reproduc": 1, "final": 1, "minor": 1, "complet": 1, "fokken": 1, "variat": 1, "given": 1, "report": 1, "exactli": 1, "replic": 1, "kept": 1, "properti": 2, "wordnettool": 1, "outcom": 1, "wordnetsimilar": 1, "fact": 1}, "marker": "(2013)", "article": "W14-0118", "vector_2": [1, 0.899283053129207, 2, 3, 9, 0]}, {"label": "Pos", "current": "We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999), a memory-based learner (MBL), for both phases.", "context": ["6.1 Details of the experiments", "We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999), a memory-based learner (MBL), for both phases.", "We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001); allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space."], "vector_1": {"mbl": 2, "set": 1, "well": 1, "featur": 1, "suffer": 1, "use": 3, "space": 1, "learner": 1, "detail": 1, "fragment": 1, "timbl": 2, "experi": 1, "numer": 1, "shown": 1, "phase": 1, "data": 1, "class": 1, "work": 1, "memorybas": 1, "allow": 1, "small": 1, "textbas": 1}, "marker": "(Daelemans et al., 2010", "article": "W12-2011", "vector_2": [2, 0.7237294844858561, 3, 1, 1, 0]}, {"label": "Neut", "current": "This problem does not affect online approaches that use global information, such as computing the maximum likelihood of the corpus incrementally (Venkataraman, 2001).", "context": ["Corpus statistics reveal that on average a monosyllabic word is followed by another monosyllabic word 85% of time (Yang, 2004), and thus learners that use only local transitional probabilities without any global optimization are unlikely to succeed.", "This problem does not affect online approaches that use global information, such as computing the maximum likelihood of the corpus incrementally (Venkataraman, 2001).", "Since these approaches do not require each boundary be a local minimum, they are able to correctly handle a sequence of monosyllable words."], "vector_1": {"corpu": 2, "comput": 1, "abl": 1, "transit": 1, "global": 2, "unlik": 1, "minimum": 1, "increment": 1, "onlin": 1, "follow": 1, "probabl": 1, "use": 2, "correctli": 1, "anoth": 1, "boundari": 1, "learner": 1, "handl": 1, "requir": 1, "local": 2, "monosyllab": 2, "monosyl": 1, "optim": 1, "sequenc": 1, "succeed": 1, "affect": 1, "likelihood": 1, "sinc": 1, "averag": 1, "reveal": 1, "word": 3, "approach": 2, "thu": 1, "maximum": 1, "inform": 1, "without": 1, "statist": 1, "time": 1, "problem": 1}, "marker": "(Venkataraman, 2001)", "article": "W10-2912", "vector_2": [9, 0.2150996722691602, 2, 1, 0, 0]}, {"label": "Neut", "current": "Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).", "context": ["national project on item generation for testing student competencies in solving probability problems.", "Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).", "The goal of our item generation project is to develop a model to support optimal problem and test construction."], "vector_1": {"control": 1, "set": 1, "automat": 1, "predefin": 1, "paramet": 1, "develop": 1, "goal": 1, "support": 1, "construct": 2, "compet": 1, "way": 1, "test": 2, "probabl": 1, "difficulti": 1, "optim": 1, "gener": 3, "effect": 1, "nation": 1, "base": 1, "student": 1, "solv": 1, "model": 1, "task": 1, "project": 2, "item": 4, "mani": 1, "problem": 2}, "marker": "(Enright et al., 2002", "article": "W11-1403", "vector_2": [9, 0.05554905516878254, 4, 2, 0, 0]}, {"label": "Pos", "current": "We use probabilistic soft logic (PSL) (Bach et al., 2013) to model post stance by leveraging both these local linguistic features as well as the observed network structure of the posts to reason over the dataset.", "context": ["We identify both linguistic features of the posts and features that capture the underlying relationships between posts and users.", "We use probabilistic soft logic (PSL) (Bach et al., 2013) to model post stance by leveraging both these local linguistic features as well as the observed network structure of the posts to reason over the dataset.", "We evaluate our approach on 4FORUMS (Walker et al., 2012b), a collection of discussions from an online debate site on issues ranging from gun control to gay marriage."], "vector_1": {"underli": 1, "control": 1, "rang": 1, "featur": 3, "identifi": 1, "evalu": 1, "captur": 1, "site": 1, "dataset": 1, "onlin": 1, "leverag": 1, "use": 1, "relationship": 1, "debat": 1, "local": 1, "psl": 1, "probabilist": 1, "reason": 1, "user": 1, "post": 4, "discuss": 1, "network": 1, "gay": 1, "forum": 1, "approach": 1, "well": 1, "gun": 1, "structur": 1, "issu": 1, "collect": 1, "stanc": 1, "marriag": 1, "logic": 1, "model": 1, "linguist": 2, "soft": 1, "observ": 1}, "marker": "(Bach et al., 2013)", "article": "W14-2715", "vector_2": [1, 0.04192944234223904, 2, 3, 3, 0]}, {"label": "Neut", "current": "(Zeng et al., 2013a) introduced a graph-based semi-supervised joint model of Chinese word segmentation and part-of-speech tagging and regularized the learning of a linear CRF model based on the label distributions derived from unlabeled data.", "context": ["For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CRF model.", "(Zeng et al., 2013a) introduced a graph-based semi-supervised joint model of Chinese word segmentation and part-of-speech tagging and regularized the learning of a linear CRF model based on the label distributions derived from unlabeled data.", "However, because monolingual unlabeled data lack natural segmenting information, most previous semi-supervised CWS methods have required certain assumptions to be made regarding their objective functions based on the researchers' personal experiences."], "vector_1": {"statisticsbas": 1, "featur": 1, "person": 1, "natur": 1, "lack": 1, "assumpt": 1, "tag": 1, "result": 1, "research": 1, "cw": 1, "linear": 1, "semisupervis": 2, "segment": 3, "label": 1, "certain": 1, "distribut": 1, "experi": 1, "method": 1, "introduc": 1, "function": 1, "graphbas": 1, "previou": 1, "monolingu": 1, "deriv": 2, "chines": 1, "interpol": 1, "regard": 1, "enhanc": 1, "joint": 1, "regular": 1, "partofspeech": 1, "base": 2, "data": 3, "requir": 1, "made": 1, "word": 1, "howev": 1, "object": 1, "inform": 1, "exampl": 1, "crf": 2, "unlabel": 3, "learn": 1, "model": 3}, "marker": "(Zeng et al., 2013a)", "article": "D15-1142", "vector_2": [2, 0.2696852416344145, 2, 4, 2, 0]}, {"label": "Neut", "current": "[Bratman, 1990] discusses three roles that intention plays in deliberative behavior: serving as a motivation for planning, a \"filter of admissibility\" on plans and further intentions, and a controller of conduct, motivating execution monitoring and repair and replanning when necessary.", "context": ["Intentions are commitments towards a course of action.", "[Bratman, 1990] discusses three roles that intention plays in deliberative behavior: serving as a motivation for planning, a \"filter of admissibility\" on plans and further intentions, and a controller of conduct, motivating execution monitoring and repair and replanning when necessary.", "Rhetorical relations are thus actions in the world distinguished by conditions on their occurrence and effects, which will generally be changes to the conversational state and the beliefs of the conversants."], "vector_1": {"control": 1, "repair": 1, "delib": 1, "replan": 1, "execut": 1, "rhetor": 1, "motiv": 2, "cours": 1, "admiss": 1, "three": 1, "state": 1, "role": 1, "conduct": 1, "condit": 1, "occurr": 1, "play": 1, "belief": 1, "gener": 1, "relat": 1, "effect": 1, "intent": 3, "plan": 2, "distinguish": 1, "world": 1, "discuss": 1, "monitor": 1, "serv": 1, "convers": 2, "thu": 1, "filter": 1, "necessari": 1, "behavior": 1, "action": 2, "commit": 1, "chang": 1, "toward": 1}, "marker": "Bratman, 1990]", "article": "W93-0235", "vector_2": [3, 0.3863821322000181, 1, 1, 0, 0]}, {"label": "Neut", "current": "Standardized tests like GRE and GMAT too use such systems to complement human scorers while evaluating student essays automatically (Burstein, 2003; Rudner et al., 2005).", "context": ["Forms of text used for assessment include mathematical responses, short answers, essays and spoken responses among others (Williamson et al., 2010).", "Standardized tests like GRE and GMAT too use such systems to complement human scorers while evaluating student essays automatically (Burstein, 2003; Rudner et al., 2005).", "Zhang (2008) discusses proficiency classification for the Examination for the Certificate of Proficiency in English (ECPE) in detail, by comparing procedures based on four types of measurement models."], "vector_1": {"essay": 2, "among": 1, "classif": 1, "student": 1, "evalu": 1, "certif": 1, "spoken": 1, "text": 1, "automat": 1, "detail": 1, "assess": 1, "human": 1, "respons": 2, "profici": 2, "use": 2, "compar": 1, "measur": 1, "system": 1, "mathemat": 1, "includ": 1, "test": 1, "type": 1, "gre": 1, "form": 1, "zhang": 1, "complement": 1, "gmat": 1, "standard": 1, "answer": 1, "base": 1, "procedur": 1, "scorer": 1, "ecp": 1, "discuss": 1, "four": 1, "short": 1, "examin": 1, "like": 1, "english": 1, "model": 1, "other": 1}, "marker": "(Burstein, 2003", "article": "W13-1708", "vector_2": [10, 0.14197974713883396, 4, 2, 0, 0]}, {"label": "Pos", "current": "For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003).", "context": ["We used the original 37 sentence trial set for feature used these as our training and test sets, respectively.", "For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003).", "For this task we have used the same test data as the competition entrants, and therefore can directly compare our results."], "vector_1": {"origin": 1, "corpu": 1, "set": 3, "directli": 1, "featur": 1, "result": 1, "respect": 1, "paramet": 1, "naacl": 1, "use": 4, "compar": 1, "entrant": 1, "test": 2, "therefor": 1, "competit": 1, "sentenc": 2, "train": 1, "data": 1, "tune": 1, "task": 2, "trial": 2, "romanianenglish": 1}, "marker": "(Mihalcea and Pedersen, 2003)", "article": "P06-1009", "vector_2": [3, 0.6161623038431426, 1, 2, 3, 0]}, {"label": "Pos", "current": "By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%.", "context": ["More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.", "By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%.", "2 Ranking and Reranking"], "vector_1": {"bleu": 1, "featur": 1, "rank": 1, "dataset": 1, "best": 1, "baselin": 1, "och": 1, "differ": 1, "system": 1, "score": 1, "test": 1, "wellformed": 1, "function": 1, "syntact": 1, "rerank": 2, "gener": 1, "use": 1, "improv": 2, "list": 1, "mt": 2, "output": 1, "order": 1}, "marker": "(Papineni et al., 2001)", "article": "N04-1023", "vector_2": [3, 0.23125892202631274, 2, 1, 0, 0]}, {"label": "Neut", "current": "Like some other ranking approaches that only update two classes/examples at every training round (Weston et al., 2011; Gao et al., 2014), we can efficiently train the network for tasks which have a very large number of classes.", "context": ["We use stochastic gradient descent (SGD) to minimize the loss function with respect to .", "Like some other ranking approaches that only update two classes/examples at every training round (Weston et al., 2011; Gao et al., 2014), we can efficiently train the network for tasks which have a very large number of classes.", "This is an advantage over softmax classifiers."], "vector_1": {"effici": 1, "advantag": 1, "number": 1, "rank": 1, "respect": 1, "sgd": 1, "use": 1, "descent": 1, "minim": 1, "gradient": 1, "two": 1, "classifi": 1, "larg": 1, "classesexampl": 1, "updat": 1, "function": 1, "everi": 1, "train": 2, "class": 1, "network": 1, "loss": 1, "task": 1, "like": 1, "approach": 1, "softmax": 1, "stochast": 1, "round": 1}, "marker": "(Weston et al., 2011", "article": "P15-1061", "vector_2": [4, 0.3700227151256013, 2, 1, 2, 0]}, {"label": "Pos", "current": "In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al (2003) \"balance\" their corpus using Web documents.", "context": ["The Web is being used to address data sparseness for language modeling.", "In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al (2003) \"balance\" their corpus using Web documents.", "The information retrieval community now has a Web track as a component of its TREC evaluation initiative."], "vector_1": {"therein": 1, "preposit": 1, "evalu": 1, "al": 1, "volk": 1, "keller": 1, "phrase": 1, "languag": 1, "web": 3, "use": 2, "lapata": 1, "villasenorpineda": 1, "compon": 1, "attach": 1, "document": 1, "refer": 1, "spars": 1, "corpu": 1, "et": 1, "lexic": 1, "track": 1, "balanc": 1, "address": 1, "data": 1, "addit": 1, "commun": 1, "retriev": 1, "trec": 1, "gather": 1, "resolv": 1, "issu": 1, "inform": 1, "statist": 1, "model": 1, "initi": 1}, "marker": "(2001)", "article": "J03-3001", "vector_2": [2, 0.22021112320583247, 2, 1, 0, 0]}, {"label": "Neut", "current": " Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].", "context": ["Areas of investigation using bilingual corpora have included the following:", " Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].", "Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991]."], "vector_1": {"use": 1, "investig": 1, "area": 1, "sentenc": 1, "align": 1, "corpora": 1, "automat": 1, "disambigu": 1, "includ": 1, "bilingu": 1, "follow": 1, "wordsens": 1}, "marker": "Gale and Church, 1991b]", "article": "P93-1003", "vector_2": [2, 0.05210931869837574, 6, 2, 4, 0]}, {"label": "Neut", "current": "Two studies in this area are Sekine (1997) and Gildea (2001), both of which show substantial variation in model performance", "context": ["There is little work on assessing how well one language model fares when applied to a text type that is different from that of the training corpus.", "Two studies in this area are Sekine (1997) and Gildea (2001), both of which show substantial variation in model performance", "341"], "vector_1": {"corpu": 1, "show": 1, "appli": 1, "one": 1, "assess": 1, "substanti": 1, "languag": 1, "differ": 1, "gildea": 1, "area": 1, "perform": 1, "two": 1, "littl": 1, "sekin": 1, "type": 1, "fare": 1, "variat": 1, "train": 1, "studi": 1, "work": 1, "well": 1, "text": 1, "model": 2}, "marker": "(1997)", "article": "J03-3001", "vector_2": [6, 0.6984026529630661, 2, 2, 0, 0]}, {"label": "Neut", "current": "While it is not possible to devise a single transcription scheme which is perfect for all purposes (Barry and Fourcin, 1992), it is clear that the current schemes all have room for improvement.", "context": ["2. can represent all observed grunts, and 3. unambiguously represents all meaningful differences in sound.", "While it is not possible to devise a single transcription scheme which is perfect for all purposes (Barry and Fourcin, 1992), it is clear that the current schemes all have room for improvement.", "3 Proposal"], "vector_1": {"sound": 1, "perfect": 1, "differ": 1, "scheme": 2, "room": 1, "possibl": 1, "transcript": 1, "clear": 1, "repres": 2, "current": 1, "meaning": 1, "purpos": 1, "unambigu": 1, "improv": 1, "grunt": 1, "devis": 1, "observ": 1, "singl": 1, "propos": 1}, "marker": "(Barry and Fourcin, 1992)", "article": "W00-1004", "vector_2": [8, 0.6492926284437825, 1, 1, 0, 0]}, {"label": "Pos", "current": "An immediate benefit of this work is that (re)training parsers with incrementally-modified grammars based on different linguistic frameworks should be much more straightforward - see, for example Oepen et al (2002) for a good discussion of the problem.", "context": ["Mapping an incompatible annotated treebank to a compatible partially-bracketed corpus is relatively easy compared to mapping to a compatible fully-annotated corpus.", "An immediate benefit of this work is that (re)training parsers with incrementally-modified grammars based on different linguistic frameworks should be much more straightforward - see, for example Oepen et al (2002) for a good discussion of the problem.", "Furthermore, it suggests that it may be possible to usefully tune"], "vector_1": {"corpu": 2, "treebank": 1, "parser": 1, "al": 1, "see": 1, "et": 1, "incompat": 1, "oepen": 1, "compat": 2, "differ": 1, "straightforward": 1, "easi": 1, "suggest": 1, "much": 1, "rel": 1, "immedi": 1, "map": 2, "good": 1, "fullyannot": 1, "may": 1, "use": 1, "retrain": 1, "framework": 1, "base": 1, "grammar": 1, "discuss": 1, "tune": 1, "incrementallymodifi": 1, "compar": 1, "furthermor": 1, "possibl": 1, "work": 1, "annot": 1, "partiallybracket": 1, "benefit": 1, "exampl": 1, "problem": 1, "linguist": 1}, "marker": "(2002)", "article": "W07-2203", "vector_2": [5, 0.9279137345516928, 1, 1, 0, 0]}, {"label": "Pos", "current": "Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving.", "context": ["In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001).", "Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving.", "Therefore, we developed a model for C-test difficulty that incorporates features from both processing levels (see Figure 4)."], "vector_1": {"featur": 1, "process": 2, "share": 1, "terminolog": 1, "see": 1, "establish": 1, "develop": 1, "learner": 1, "detail": 1, "figur": 1, "ctest": 3, "therefor": 1, "analys": 1, "strategi": 3, "difficulti": 1, "solv": 2, "categor": 1, "discuss": 1, "requir": 1, "success": 1, "level": 1, "microlevel": 1, "macrolevel": 1, "incorpor": 1, "psycholinguist": 1, "model": 1, "order": 1}, "marker": "Grotjahn and Stemmer, 2002)", "article": "Q14-1040", "vector_2": [12, 0.3302510976670781, 3, 1, 3, 0]}, {"label": "Pos", "current": "(Koehn and Haddow, 2009) presents a post-editing environment using information from the phrasebased SMT system Moses.2 (Guerberof, 2009) compares the post-editing effort required for TM and MT output, respectively.", "context": ["Related research by (Simard and Isabelle., 2009) focuses on combining TM information into an SMT system for improving the performance of the MT when a close match already exists within the TM.", "(Koehn and Haddow, 2009) presents a post-editing environment using information from the phrasebased SMT system Moses.2 (Guerberof, 2009) compares the post-editing effort required for TM and MT output, respectively.", "(Tatsumi, 2009) studies the correlation between automatic evaluation scores and post-editing effort."], "vector_1": {"evalu": 1, "within": 1, "focus": 1, "automat": 1, "isabel": 1, "alreadi": 1, "exist": 1, "respect": 1, "close": 1, "postedit": 3, "use": 1, "score": 1, "compar": 1, "inform": 2, "perform": 1, "smt": 2, "system": 2, "research": 1, "tm": 3, "requir": 1, "combin": 1, "match": 1, "mose": 1, "relat": 1, "output": 1, "effort": 2, "present": 1, "phrasebas": 1, "mt": 2, "simard": 1, "correl": 1, "environ": 1, "improv": 1, "studi": 1}, "marker": "(Koehn and Haddow, 2009)", "article": "N13-3003", "vector_2": [4, 0.32553835800807535, 3, 1, 0, 0]}, {"label": "Neut", "current": "For example, Wu (1997) used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms, including multiword expressions.", "context": ["Lexical resources and parsers are used to obtain better coverage of the lexicon in MWE extraction.", "For example, Wu (1997) used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms, including multiword expressions.", "In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language."], "vector_1": {"lexicon": 1, "identifi": 2, "parser": 2, "obtain": 1, "dufour": 1, "michiel": 1, "extract": 1, "languag": 1, "use": 3, "coverag": 1, "better": 1, "includ": 1, "multiword": 2, "express": 2, "resourc": 1, "mwe": 1, "bilingu": 1, "lexic": 1, "englishchines": 1, "wu": 1, "base": 1, "french": 1, "dictionari": 1, "defi": 1, "term": 1, "grammar": 1, "english": 1, "project": 1, "exampl": 1, "transduct": 1, "stochast": 1, "translat": 1}, "marker": "(1997)", "article": "W03-1807", "vector_2": [6, 0.17655506318726408, 2, 1, 0, 0]}, {"label": "Neut", "current": "Moreover, as in the approach of Jurafsky (1996), the phenomenon we investigate here may be best considered within a constructional analysis (e.g., Langacker, 1987), in which both the syntactic construction and the particular lexical items contribute to the determination of the meaning of a usage.", "context": ["Although not intended as proposing a cognitive model, the work here can be seen as connected to these lines of research, in investigating the extent to which lexical factors can be used as proxies to more \"hidden\" features that underlie the appropriate interpretation of a pragmatically complex construction.", "Moreover, as in the approach of Jurafsky (1996), the phenomenon we investigate here may be best considered within a constructional analysis (e.g., Langacker, 1987), in which both the syntactic construction and the particular lexical items contribute to the determination of the meaning of a usage.", "We suggest that a clause of the form No Xis too Y to Z might be the (identical) surface expression of two underlying constructions-one with the \"every\" interpretation and one with the \"no\" interpretation-which place differing constraints on the semantics of the verb."], "vector_1": {"underli": 2, "semant": 1, "pragmat": 1, "although": 1, "eg": 1, "within": 1, "interpretationwhich": 1, "constructionson": 1, "one": 1, "featur": 1, "connect": 1, "seen": 1, "differ": 1, "cognit": 1, "best": 1, "phenomenon": 1, "use": 1, "suggest": 1, "moreov": 1, "z": 1, "two": 1, "construct": 3, "research": 1, "complex": 1, "factor": 1, "xi": 1, "everi": 1, "hidden": 1, "approach": 1, "analysi": 1, "syntact": 1, "investig": 2, "intend": 1, "proxi": 1, "form": 1, "may": 1, "claus": 1, "usag": 1, "surfac": 1, "lexic": 2, "ident": 1, "extent": 1, "particular": 1, "consid": 1, "line": 1, "interpret": 2, "jurafski": 1, "appropri": 1, "constraint": 1, "might": 1, "work": 1, "item": 1, "express": 1, "verb": 1, "place": 1, "determin": 1, "model": 1, "mean": 1, "contribut": 1, "propos": 1}, "marker": "(1996)", "article": "W10-2109", "vector_2": [14, 0.8142181195390287, 2, 2, 1, 0]}, {"label": "Weak", "current": "NETE mining from comparable corpora using phonetic mappings was proposed in (Tao et al., 2006), but the need for language specific knowledge restricts its applicability across languages.", "context": ["In addition, (Klementiev and Roth, 2006) may not scale for large corpora, as they examine every word in the target side as a potential transliteration equivalent.", "NETE mining from comparable corpora using phonetic mappings was proposed in (Tao et al., 2006), but the need for language specific knowledge restricts its applicability across languages.", "We proposed the idea of mining NETEs from multilingual articles with similar content in (Udupa, et al., 2008)."], "vector_1": {"multilingu": 1, "examin": 1, "applic": 1, "nete": 2, "knowledg": 1, "mine": 2, "phonet": 1, "restrict": 1, "need": 1, "languag": 2, "use": 1, "scale": 1, "equival": 1, "articl": 1, "content": 1, "larg": 1, "across": 1, "compar": 1, "map": 1, "everi": 1, "may": 1, "idea": 1, "transliter": 1, "addit": 1, "word": 1, "target": 1, "specif": 1, "corpora": 2, "potenti": 1, "similar": 1, "side": 1, "propos": 2}, "marker": "(Tao et al., 2006)", "article": "E09-1091", "vector_2": [3, 0.9414692275339969, 3, 2, 0, 0]}, {"label": "Weak", "current": "However, Doyle and Levy (2013) do not directly examine the probabilities assigned to the stress-templates; they only report that their model does slightly prefer stress-initial words over the baseline model by calculating the fraction of stress-initial word types in the output segmentations of their models.", "context": ["This allows the model to acquire knowledge about the stress patterns of its input by assigning different probabilities to the different stress-templates.", "However, Doyle and Levy (2013) do not directly examine the probabilities assigned to the stress-templates; they only report that their model does slightly prefer stress-initial words over the baseline model by calculating the fraction of stress-initial word types in the output segmentations of their models.", "They also demonstrate that stress cues do indeed aid segmentation, although their reported gain of 1% in token f-score is even smaller than that reported by Lignos (2011)."], "vector_1": {"smaller": 1, "although": 1, "directli": 1, "knowledg": 1, "prefer": 1, "stresstempl": 2, "baselin": 1, "even": 1, "differ": 2, "acquir": 1, "pattern": 1, "segment": 2, "token": 1, "also": 1, "cue": 1, "fraction": 1, "doyl": 1, "input": 1, "probabl": 2, "type": 1, "fscore": 1, "ligno": 1, "levi": 1, "gain": 1, "examin": 1, "aid": 1, "report": 3, "stressiniti": 2, "demonstr": 1, "stress": 2, "word": 2, "howev": 1, "slightli": 1, "inde": 1, "calcul": 1, "allow": 1, "output": 1, "model": 4, "assign": 2}, "marker": "(2013)", "article": "Q14-1008", "vector_2": [1, 0.17872205820328976, 2, 9, 0, 0]}, {"label": "Neut", "current": "Brown (1989) identifies the word class, the local word frequency, and readability measures as factors correlating with cloze gap difficulty.", "context": ["Previous work on gap difficulty is based on correlation analyses.", "Brown (1989) identifies the word class, the local word frequency, and readability measures as factors correlating with cloze gap difficulty.", "Sigott (1995) exam"], "vector_1": {"difficulti": 2, "previou": 1, "identifi": 1, "exam": 1, "factor": 1, "brown": 1, "local": 1, "work": 1, "readabl": 1, "cloze": 1, "gap": 2, "correl": 2, "sigott": 1, "measur": 1, "base": 1, "frequenc": 1, "word": 2, "analys": 1, "class": 1}, "marker": "(1989)", "article": "Q14-1040", "vector_2": [25, 0.18351779535843635, 2, 5, 0, 0]}, {"label": "Neut", "current": "In this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003).", "context": ["The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.", "In this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003).", "For the jth character cj in the sentence cJ1 = c1...cJ, the score can be calculated as follows:"], "vector_1": {"cj": 2, "featur": 1, "random": 1, "paper": 1, "manner": 1, "follow": 1, "cw": 2, "use": 1, "detect": 1, "field": 1, "score": 2, "treat": 1, "sentenc": 1, "condit": 1, "jth": 1, "sequenc": 1, "effect": 1, "oov": 1, "problem": 1, "demonstr": 1, "task": 2, "word": 1, "charact": 1, "defin": 1, "calcul": 1, "tag": 1, "crf": 1, "ccj": 1, "model": 2, "characterlevel": 1, "first": 1}, "marker": "(Xue et al., 2003)", "article": "D15-1142", "vector_2": [12, 0.3480152070418194, 3, 4, 0, 0]}, {"label": "Pos", "current": "If we succeed, then we may be able to hybridize with a broad-coverage RTE system to obtain better results than either system individually-the same strategy that was adopted by (Bos and Markert, 2006) for their FOL-based system.", "context": ["Rather, in applying NatLog to RTE, we hope to make reliable predictions on a subset of RTE problems, trading recall for precision.", "If we succeed, then we may be able to hybridize with a broad-coverage RTE system to obtain better results than either system individually-the same strategy that was adopted by (Bos and Markert, 2006) for their FOL-based system.", "For this purpose, we have chosen to use the Stanford RTE system described in (de Marneffe et al., 2006)."], "vector_1": {"rte": 4, "subset": 1, "chosen": 1, "predict": 1, "appli": 1, "obtain": 1, "trade": 1, "result": 1, "individuallyth": 1, "use": 1, "describ": 1, "rather": 1, "make": 1, "hybrid": 1, "system": 4, "better": 1, "abl": 1, "folbas": 1, "hope": 1, "strategi": 1, "natlog": 1, "recal": 1, "may": 1, "succeed": 1, "stanford": 1, "reliabl": 1, "adopt": 1, "precis": 1, "broadcoverag": 1, "either": 1, "problem": 1, "purpos": 1}, "marker": "(Bos and Markert, 2006)", "article": "W07-1431", "vector_2": [1, 0.7863118913729256, 2, 3, 0, 0]}, {"label": "Neut", "current": "Chekuri et al (2001) proposed an integer linear programming (ILP) formulation of the metric labeling problem, with both assignment cost and separation costs being modeled as binary variables of the linear cost function.", "context": ["the costs of selecting a pair of labels for two related objects3.", "Chekuri et al (2001) proposed an integer linear programming (ILP) formulation of the metric labeling problem, with both assignment cost and separation costs being modeled as binary variables of the linear cost function.", "Recently, Roth & Yih (2004) applied an ILP model to the task of the simultaneous assignment of semantic roles to the entities mentioned in a sentence and recognition of the relations holding between them."], "vector_1": {"semant": 1, "formul": 1, "metric": 1, "al": 1, "yih": 1, "cost": 4, "et": 1, "simultan": 1, "select": 1, "entiti": 1, "variabl": 1, "recognit": 1, "two": 1, "label": 2, "binari": 1, "program": 1, "role": 1, "appli": 1, "function": 1, "relat": 2, "linear": 2, "sentenc": 1, "object": 1, "mention": 1, "pair": 1, "hold": 1, "problem": 1, "recent": 1, "task": 1, "chekuri": 1, "integ": 1, "separ": 1, "roth": 1, "ilp": 2, "model": 2, "assign": 2, "propos": 1}, "marker": "(2001)", "article": "W05-0618", "vector_2": [4, 0.21889417027820496, 2, 2, 0, 0]}, {"label": "Pos", "current": "We have presented what is to our knowledge the first formalization and implementation of a type of rule and control regime intended for use in situations where it is desired to produce the effect of transforming one feature structure into another.9 The formalism described above has been implemented as part of ISSCO's ELUM, an enhanced PATR-II style (Shieber, 1986) unification grammar environment, based on the UD system presented by Johnson and Rosner (1989).", "context": ["Conclusion", "We have presented what is to our knowledge the first formalization and implementation of a type of rule and control regime intended for use in situations where it is desired to produce the effect of transforming one feature structure into another.9 The formalism described above has been implemented as part of ISSCO's ELUM, an enhanced PATR-II style (Shieber, 1986) unification grammar environment, based on the UD system presented by Johnson and Rosner (1989).", "ELU incorporates a parser and generator, and is primarily intended for use as a tool for research in machine translation."], "vector_1": {"control": 1, "featur": 1, "primarili": 1, "knowledg": 1, "unif": 1, "one": 1, "style": 1, "rule": 1, "desir": 1, "patrii": 1, "use": 2, "describ": 1, "anoth": 1, "transform": 1, "research": 1, "present": 2, "type": 1, "tool": 1, "elu": 1, "machin": 1, "parser": 1, "intend": 2, "issco": 1, "gener": 1, "situat": 1, "effect": 1, "enhanc": 1, "regim": 1, "part": 1, "translat": 1, "elum": 1, "conclus": 1, "formal": 2, "grammar": 1, "johnson": 1, "structur": 1, "rosner": 1, "base": 1, "environ": 1, "incorpor": 1, "ud": 1, "implement": 2, "system": 1, "produc": 1, "first": 1}, "marker": "(Shieber, 1986)", "article": "E91-1050", "vector_2": [5, 0.8549083337565385, 2, 1, 1, 0]}, {"label": "Pos", "current": "To rescore the N-best lists, we use the method of (Khadivi et al., 2005).", "context": ["4.2 N-best Rescoring", "To rescore the N-best lists, we use the method of (Khadivi et al., 2005).", "But the results shown here are different from that work due to a better optimization of the overall ASR system, using a"], "vector_1": {"asr": 1, "use": 2, "overal": 1, "system": 1, "due": 1, "optim": 1, "work": 1, "list": 1, "shown": 1, "method": 1, "better": 1, "result": 1, "differ": 1, "rescor": 2, "nbest": 2}, "marker": "(Khadivi et al., 2005)", "article": "P06-2061", "vector_2": [1, 0.4106766917293233, 1, 5, 13, 1]}, {"label": "Pos", "current": "Our future plans include further sophistication of features, such as the use of external gazetteers which is reported to improve the F-score by 1.0 and 2.7 points in (Zhou and Su, 2004) and (Friedrich et al., 2006), respectively.", "context": ["Our system showed clear improvement over many of the machine-learning-based systems reported to date, and also proved comparable to the existing state-ofthe-art systems that use rule-based post-processing.", "Our future plans include further sophistication of features, such as the use of external gazetteers which is reported to improve the F-score by 1.0 and 2.7 points in (Zhou and Su, 2004) and (Friedrich et al., 2006), respectively.", "We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase."], "vector_1": {"gazett": 1, "featur": 2, "show": 1, "appli": 1, "elabor": 1, "readili": 1, "date": 1, "exist": 1, "expect": 1, "respect": 1, "machinelearningbas": 1, "use": 2, "compar": 1, "prove": 1, "point": 1, "system": 3, "avail": 1, "also": 1, "includ": 1, "postprocess": 1, "fscore": 1, "futur": 1, "qualifi": 1, "string": 1, "rerank": 2, "sophist": 1, "candid": 1, "plan": 1, "improv": 2, "extern": 1, "algorithm": 1, "dictionarybas": 1, "clear": 1, "rulebas": 1, "stringmatch": 1, "accommod": 1, "phase": 1, "stateoftheart": 1, "report": 2, "mani": 1, "architectur": 1}, "marker": "(Friedrich et al., 2006)", "article": "W07-1033", "vector_2": [1, 0.9664055080721747, 2, 2, 0, 0]}, {"label": "Neut", "current": "Rather than estimating the complete density function, Tax and Duin (2000) approximate local density at the test object by comparing distances between nearest neighbors.", "context": ["One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.", "Rather than estimating the complete density function, Tax and Duin (2000) approximate local density at the test object by comparing distances between nearest neighbors.", "Given a test object x, the approach considers the training object t nearest to x and compares the distance dxt between x and t to the distance dtt, between t and its own nearest training data neighbor t'."], "vector_1": {"nearest": 3, "set": 2, "approxim": 1, "dxt": 1, "duin": 1, "one": 1, "distanc": 3, "probabl": 2, "given": 1, "detect": 1, "compar": 2, "rather": 1, "nonoutli": 1, "belong": 1, "classifi": 1, "estim": 2, "densiti": 3, "test": 3, "approach": 2, "complet": 1, "function": 1, "accord": 1, "outlier": 2, "tax": 1, "object": 4, "standard": 1, "train": 3, "consid": 1, "data": 1, "local": 1, "neighbor": 2, "x": 3, "dtt": 1}, "marker": "(2000)", "article": "N06-1017", "vector_2": [6, 0.4946626074222513, 1, 2, 1, 0]}, {"label": "Neut", "current": "where li is the next BIO tag, li1 is the previous BIO tag, S is the target sentence, and fj and lj are feature functions and parameters of a log-linear model (Berger et al., 1996).", "context": ["Table 2: (Recall/Precision/F-score) of forward and backward tagging.", "where li is the next BIO tag, li1 is the previous BIO tag, S is the target sentence, and fj and lj are feature functions and parameters of a log-linear model (Berger et al., 1996).", "As a first order MEMM, the probability of a label li is dependent on the previous label li1, and when we calculate the normalization constant in the right hand side (i.e."], "vector_1": {"featur": 1, "constant": 1, "tag": 3, "right": 1, "tabl": 1, "loglinear": 1, "next": 1, "ie": 1, "probabl": 1, "depend": 1, "lj": 1, "li": 4, "forward": 1, "paramet": 1, "recallprecisionfscor": 1, "function": 1, "bio": 2, "normal": 1, "memm": 1, "sentenc": 1, "previou": 2, "hand": 1, "fj": 1, "target": 1, "label": 2, "side": 1, "calcul": 1, "model": 1, "backward": 1, "order": 1, "first": 1}, "marker": "(Berger et al., 1996)", "article": "W07-1033", "vector_2": [11, 0.3608341247230136, 1, 1, 0, 0]}, {"label": "Weak", "current": "They are also larger than the 0.7- to 1.1-point gains reported by Pino et al (2010) when the full Giga-FrEn was added.", "context": ["Our BLEU score improvements of 1.2 to 1.9 points are statistically significant according to the paired bootstrap resampling method (Koehn, 2004) with n = 1000 and p < 0.01.", "They are also larger than the 0.7- to 1.1-point gains reported by Pino et al (2010) when the full Giga-FrEn was added.", "The 2011 system also shows a significant reduction in the out-of-vocabulary (OOV) rate on both test sets: 38% and 47% fewer OOV types, and 44% and 45% fewer OOV tokens, when compared to the 2010 system."], "vector_1": {"bleu": 1, "larger": 1, "ad": 1, "point": 2, "resampl": 1, "al": 1, "pair": 1, "signific": 2, "set": 1, "et": 1, "reduct": 1, "gigafren": 1, "pino": 1, "compar": 1, "show": 1, "system": 2, "also": 2, "score": 1, "test": 1, "type": 1, "method": 1, "accord": 1, "full": 1, "oov": 3, "gain": 1, "report": 1, "20": 1, "bootstrap": 1, "outofvocabulari": 1, "rate": 1, "fewer": 2, "n": 1, "p": 1, "token": 1, "statist": 1, "improv": 1}, "marker": "(2010)", "article": "W11-2143", "vector_2": [1, 0.7470009372071228, 2, 2, 0, 0]}, {"label": "Pos", "current": "For similarity-based method, the parameter tuning is important to improve performance but we use the simplified unweighted average equation as in (Lee and Pereira, 1999) 6 .", "context": ["Pd(Yilxi) frq(xi, yi) > 0 cx(xi) P(yi) frq(xi,yi) = 0", "For similarity-based method, the parameter tuning is important to improve performance but we use the simplified unweighted average equation as in (Lee and Pereira, 1999) 6 .", "Since this equation is the same as our estimation method in Section 2.3.3, we can say that the comparison is fair."], "vector_1": {"fair": 1, "cxxi": 1, "unweight": 1, "frqxi": 1, "say": 1, "paramet": 1, "use": 1, "perform": 1, "section": 1, "frqxiyi": 1, "estim": 1, "tune": 1, "import": 1, "method": 2, "yi": 1, "simplifi": 1, "pyi": 1, "sinc": 1, "averag": 1, "comparison": 1, "pdyilxi": 1, "similaritybas": 1, "equat": 2, "improv": 1}, "marker": "(Lee and Pereira, 1999)", "article": "P00-1072", "vector_2": [1, 0.7297929524188732, 1, 8, 0, 0]}, {"label": "Neut", "current": "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "context": ["Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).", "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations."], "vector_1": {"among": 1, "semant": 1, "wup": 1, "synonymi": 1, "past": 1, "packag": 1, "set": 1, "wordnet": 2, "use": 1, "instrument": 1, "perform": 1, "databas": 1, "lin": 1, "re": 1, "other": 1, "languag": 1, "test": 1, "import": 1, "jcn": 1, "method": 2, "variou": 1, "relat": 1, "path": 1, "becom": 1, "lch": 1, "measur": 2, "word": 2, "princeton": 1, "also": 1, "wordnetsimilar": 2, "implement": 1, "similar": 2, "propos": 1}, "marker": "(Rada et al., 1989)", "article": "W14-0118", "vector_2": [25, 0.04608496916189224, 8, 1, 0, 0]}, {"label": "Neut", "current": "9month old infants are believed to have learned some phonotactic constraints of their native language (Mattys and Jusczyk, 2001), and learning these constraints can be done with only minimal exposure (Onishi et al., 2002).", "context": ["While we performed syllabification as a preprocessing step outside of learning, a child learner would presumably learn the required phonotactics as a part of learning to segment words.", "9month old infants are believed to have learned some phonotactic constraints of their native language (Mattys and Jusczyk, 2001), and learning these constraints can be done with only minimal exposure (Onishi et al., 2002).", "Finally, spaces and punctuation between words were removed, but the boundaries between utterances-as indicated by line breaks in CHILDES-are retained."], "vector_1": {"infant": 1, "old": 1, "indic": 1, "month": 1, "done": 1, "syllabif": 1, "phonotact": 2, "languag": 1, "would": 1, "minim": 1, "perform": 1, "boundari": 1, "learner": 1, "utterancesa": 1, "preprocess": 1, "presum": 1, "final": 1, "childesar": 1, "break": 1, "nativ": 1, "step": 1, "part": 1, "believ": 1, "child": 1, "line": 1, "segment": 1, "requir": 1, "exposur": 1, "word": 2, "constraint": 2, "remov": 1, "space": 1, "punctuat": 1, "outsid": 1, "learn": 5, "retain": 1}, "marker": "(Mattys and Jusczyk, 2001)", "article": "W10-2912", "vector_2": [9, 0.6214257794882588, 2, 1, 2, 0]}, {"label": "Neut", "current": "Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b).", "context": ["An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al (2009).", "Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b).", "Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task."], "vector_1": {"reasonablywel": 1, "evalu": 1, "predict": 1, "challeng": 1, "al": 1, "msr": 2, "belz": 1, "result": 1, "year": 1, "et": 1, "subject": 1, "develop": 1, "compar": 1, "perform": 2, "neg": 1, "system": 3, "accuraci": 1, "score": 1, "compet": 1, "submiss": 1, "main": 1, "refer": 1, "string": 1, "especi": 1, "gener": 1, "previou": 1, "extens": 1, "although": 1, "along": 1, "regtyp": 1, "task": 3, "provid": 1, "descript": 1, "grec": 1, "indepth": 1, "disappointinglylow": 1}, "marker": "(Greenbacker and McCoy, 2009b)", "article": "W10-4231", "vector_2": [1, 0.16411181244364292, 3, 1, 1, 0]}, {"label": "Pos", "current": "In both models, Gaussian prior distributions are used to avoid overfitting (Chen and Rosenfeld, 1999), and the standard deviations of the Gaussian distributions are optimized to maximize the performance on the development set.", "context": ["The two log-linear models for the MEMM tagger and reranker are estimated using a limited-memory BFGS algorithm implemented in an open-source software Amis3.", "In both models, Gaussian prior distributions are used to avoid overfitting (Chen and Rosenfeld, 1999), and the standard deviations of the Gaussian distributions are optimized to maximize the performance on the development set.", "We also used a thresholding technique which discards features with low frequency."], "vector_1": {"softwar": 1, "set": 1, "gaussian": 2, "threshold": 1, "featur": 1, "tagger": 1, "opensourc": 1, "loglinear": 1, "use": 3, "bfg": 1, "techniqu": 1, "also": 1, "perform": 1, "avoid": 1, "two": 1, "optim": 1, "limitedmemori": 1, "deviat": 1, "low": 1, "ami": 1, "distribut": 2, "rerank": 1, "memm": 1, "standard": 1, "maxim": 1, "discard": 1, "develop": 1, "implement": 1, "algorithm": 1, "frequenc": 1, "prior": 1, "estim": 1, "overfit": 1, "model": 2}, "marker": "(Chen and Rosenfeld, 1999)", "article": "W07-1033", "vector_2": [8, 0.6960272238050016, 1, 1, 0, 0]}, {"label": "Neut", "current": "The algorithm starts with a preprocessing stage, where the text is tokenized and annotated with part-ofspeech tags; collocations are identified using a sliding window approach, where a collocation is defined as a sequence of words that forms a compound concept defined in WordNet (Miller, 1995); named entities are also identified at this stager.", "context": ["The output is a text with word meaning annotations for all open-class words.", "The algorithm starts with a preprocessing stage, where the text is tokenized and annotated with part-ofspeech tags; collocations are identified using a sliding window approach, where a collocation is defined as a sequence of words that forms a compound concept defined in WordNet (Miller, 1995); named entities are also identified at this stager.", "Next, a semantic model is learned for all predefined word categories, which are defined as groups of words that share some common syntactic or semantic properties."], "vector_1": {"semant": 2, "concept": 1, "identifi": 2, "text": 2, "share": 1, "tag": 1, "predefin": 1, "wordnet": 1, "entiti": 1, "colloc": 2, "group": 1, "also": 1, "slide": 1, "next": 1, "categori": 1, "start": 1, "window": 1, "preprocess": 1, "openclass": 1, "approach": 1, "syntact": 1, "form": 1, "sequenc": 1, "use": 1, "partofspeech": 1, "compound": 1, "model": 1, "stage": 1, "word": 5, "name": 1, "algorithm": 1, "properti": 1, "annot": 2, "defin": 3, "token": 1, "common": 1, "learn": 1, "output": 1, "stager": 1, "mean": 1}, "marker": "(Miller, 1995)", "article": "P05-3014", "vector_2": [10, 0.36014038231780165, 1, 1, 1, 0]}, {"label": "Neut", "current": "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "context": ["Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).", "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations."], "vector_1": {"among": 1, "semant": 1, "wup": 1, "synonymi": 1, "past": 1, "packag": 1, "set": 1, "wordnet": 2, "use": 1, "instrument": 1, "perform": 1, "databas": 1, "lin": 1, "re": 1, "other": 1, "languag": 1, "test": 1, "import": 1, "jcn": 1, "method": 2, "variou": 1, "relat": 1, "path": 1, "becom": 1, "lch": 1, "measur": 2, "word": 2, "princeton": 1, "also": 1, "wordnetsimilar": 2, "implement": 1, "similar": 2, "propos": 1}, "marker": "(Pedersen et al., 2004)", "article": "W14-0118", "vector_2": [10, 0.04608496916189224, 8, 1, 10, 0]}, {"label": "Neut", "current": "Over the years, various approaches have been proposed for relation classification (Zhang, 2004; Qian et al., 2009; Hendrickx et al., 2010; Rink and Harabagiu, 2010).", "context": ["5 Related Work", "Over the years, various approaches have been proposed for relation classification (Zhang, 2004; Qian et al., 2009; Hendrickx et al., 2010; Rink and Harabagiu, 2010).", "Most of them treat it as a multiclass classification problem and apply a variety of machine learning techniques to the task in order to achieve a high accuracy."], "vector_1": {"machin": 1, "classif": 2, "relat": 2, "techniqu": 1, "variou": 1, "appli": 1, "work": 1, "multiclass": 1, "high": 1, "varieti": 1, "achiev": 1, "task": 1, "learn": 1, "treat": 1, "year": 1, "problem": 1, "accuraci": 1, "approach": 1, "order": 1, "propos": 1}, "marker": "(Zhang, 2004", "article": "P15-1061", "vector_2": [11, 0.8300374131480491, 4, 2, 0, 0]}, {"label": "Neut", "current": "(Collins, 1996) focuses on bigram lexical dependencies (BLD).", "context": ["While this necessitates the involvement of a parsing supervisor for training, we are able to perform deterministic parsing and get already very good test results for only 256 training sentences.", "(Collins, 1996) focuses on bigram lexical dependencies (BLD).", "Trained on the same 40,000 sentences as Spatter, it relies on a much more limited type of context than our system and needs little background knowledge."], "vector_1": {"supervisor": 1, "knowledg": 1, "focus": 1, "system": 1, "alreadi": 1, "result": 1, "need": 1, "bigram": 1, "involv": 1, "depend": 1, "perform": 1, "abl": 1, "limit": 1, "littl": 1, "reli": 1, "much": 1, "test": 1, "bld": 1, "type": 1, "good": 1, "get": 1, "sentenc": 2, "lexic": 1, "train": 3, "pars": 2, "background": 1, "determinist": 1, "necessit": 1, "spatter": 1, "context": 1}, "marker": "(Collins, 1996)", "article": "P97-1062", "vector_2": [1, 0.9087181625490809, 1, 1, 0, 0]}, {"label": "Neut", "current": "The latter two use a transformation-based error-driven learning method [Brill, 1992].", "context": ["We also present the results of [Argamon et at., 1998], [Ramshaw and Marcus, 1995] and [Cardie and Pierce, 1998] in Table 4.", "The latter two use a transformation-based error-driven learning method [Brill, 1992].", "In [Ramshaw and Marcus, 1995], the method is used for NP chunking, and in [Cardie and Pierce, 1998] the approach is indirectly used to evaluate corpus-extracted NP chunking rules."], "vector_1": {"use": 3, "evalu": 1, "chunk": 2, "indirectli": 1, "argamon": 1, "learn": 1, "two": 1, "rule": 1, "errordriven": 1, "also": 1, "transformationbas": 1, "result": 1, "tabl": 1, "np": 2, "et": 1, "method": 2, "approach": 1, "latter": 1, "present": 1, "corpusextract": 1}, "marker": "Brill, 1992]", "article": "W99-0707", "vector_2": [7, 0.7362250879249707, 5, 1, 0, 0]}, {"label": "Neut", "current": "The approach we use for reconstructing a language family tree is to apply agglomerative hierarchical clustering (Han and Kamber, 2006) to English texts written by non-native speakers.", "context": ["If not, it suggests that some features other than mother tongue interference are more influential.", "The approach we use for reconstructing a language family tree is to apply agglomerative hierarchical clustering (Han and Kamber, 2006) to English texts written by non-native speakers.", "Researchers have already performed related work on reconstructing language family trees."], "vector_1": {"featur": 1, "influenti": 1, "appli": 1, "cluster": 1, "alreadi": 1, "famili": 2, "languag": 2, "hierarch": 1, "use": 1, "perform": 1, "suggest": 1, "research": 1, "written": 1, "interfer": 1, "speaker": 1, "text": 1, "approach": 1, "relat": 1, "mother": 1, "agglom": 1, "reconstruct": 2, "nonn": 1, "work": 1, "tree": 2, "tongu": 1, "english": 1}, "marker": "(Han and Kamber, 2006)", "article": "P13-1112", "vector_2": [7, 0.15430148750191688, 1, 1, 0, 0]}, {"label": "Neut", "current": "We then describe our port of the Collins (1999) parser to Chinese.", "context": ["We then introduce the features used by our SVM classifier, and show their performance on semantic parsing for both seen and unseen verbs, given hand-corrected (Chinese TreeBank) syntactic parses.", "We then describe our port of the Collins (1999) parser to Chinese.", "Finally, we apply our SVM semantic parser to a matching English corpus, and discuss the differences between English and Chinese that lead to significantly better performance on Chinese."], "vector_1": {"corpu": 1, "semant": 2, "featur": 1, "show": 1, "appli": 1, "treebank": 1, "parser": 2, "handcorrect": 1, "significantli": 1, "seen": 1, "differ": 1, "port": 1, "use": 1, "describ": 1, "lead": 1, "perform": 2, "collin": 1, "classifi": 1, "better": 1, "final": 1, "introduc": 1, "syntact": 1, "chines": 4, "given": 1, "verb": 1, "pars": 2, "unseen": 1, "discuss": 1, "svm": 2, "match": 1, "english": 2}, "marker": "(1999)", "article": "N04-1032", "vector_2": [5, 0.06948570674060871, 1, 4, 0, 0]}, {"label": "Neut", "current": "# LOOCV Train Train-Test LOOCV All Mean Baseline 1 .00 .00 .00 Sigott (1995) 7 .34 .38 .36 Full Model 87 .64 .32 .60 Selected Features 21 .68 .44 .57", "context": ["Starting from a sample size of about 70 instances, the learning curve proceeds as", "# LOOCV Train Train-Test LOOCV All Mean Baseline 1 .00 .00 .00 Sigott (1995) 7 .34 .38 .36 Full Model 87 .64 .32 .60 Selected Features 21 .68 .44 .57", "Table 8: Results on the train and the test set"], "vector_1": {"curv": 1, "featur": 1, "loocv": 2, "traintest": 1, "set": 1, "sampl": 1, "result": 1, "tabl": 1, "select": 1, "baselin": 1, "start": 1, "2": 1, "5": 1, "test": 1, "size": 1, "full": 1, "proce": 1, "sigott": 1, "train": 2, "instanc": 1, "learn": 1, "model": 1, "mean": 1}, "marker": "(1995)", "article": "Q14-1040", "vector_2": [19, 0.8985897254314793, 1, 7, 1, 0]}, {"label": "Neut", "current": "Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM).", "context": ["Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses.", "Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM).", "Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration."], "vector_1": {"mose": 1, "evalu": 2, "englishhindi": 1, "al": 2, "chinnakotla": 1, "englishkannada": 1, "meetei": 1, "et": 2, "charact": 1, "languag": 1, "csm": 1, "differ": 1, "script": 1, "similarlv": 1, "three": 1, "singh": 1, "method": 1, "machin": 1, "englishtamil": 1, "sequenc": 1, "tool": 1, "use": 2, "focus": 1, "bidirect": 1, "bengali": 1, "transliter": 2, "pair": 1, "kind": 1, "hindiurdu": 1, "finetun": 1, "rulebas": 1, "giza": 1, "statist": 2, "malik": 1, "model": 2, "mavek": 1}, "marker": "(2009)", "article": "W14-5502", "vector_2": [5, 0.23800577372496906, 3, 1, 0, 0]}, {"label": "Neut", "current": "For the graph-based approach, we adopt the iterative reinforcement approach from (Wan et al., 2007) in the hope of leveraging sentence information for keyword extraction.", "context": ["4.2 Graph-based Methods", "For the graph-based approach, we adopt the iterative reinforcement approach from (Wan et al., 2007) in the hope of leveraging sentence information for keyword extraction.", "This algorithm is based on the assumption that important sentences/words are connected to other important sentences/words."], "vector_1": {"reinforc": 1, "algorithm": 1, "keyword": 1, "assumpt": 1, "sentenc": 1, "adopt": 1, "graphbas": 2, "iter": 1, "inform": 1, "base": 1, "connect": 1, "sentencesword": 2, "import": 2, "extract": 1, "approach": 2, "method": 1, "hope": 1, "leverag": 1}, "marker": "(Wan et al., 2007)", "article": "N09-1070", "vector_2": [2, 0.5120079945843139, 1, 4, 0, 0]}, {"label": "Neut", "current": "The effect of the main parameters of the probability problems in Genpex (i.e., the type of question being asked) was already statistically analyzed by Holling et al (2009) and Zeuch (In preparation).", "context": ["This makes it relatively easy to test the influence of those factors on the difficulty of the exercise, for example by carrying out the kind of statistical and cognitive analysis advocated by Graf et al (2005).", "The effect of the main parameters of the probability problems in Genpex (i.e., the type of question being asked) was already statistically analyzed by Holling et al (2009) and Zeuch (In preparation).", "They used automatically generated items similar to the exercises generated by Genpex, except that their exercises did not have variations in wording apart from context-related ones."], "vector_1": {"carri": 1, "al": 2, "automat": 1, "one": 1, "alreadi": 1, "et": 2, "cognit": 1, "probabl": 1, "use": 1, "easi": 1, "influenc": 1, "ie": 1, "make": 1, "question": 1, "except": 1, "genpex": 2, "zeuch": 1, "advoc": 1, "analyz": 1, "exercis": 3, "factor": 1, "test": 1, "main": 1, "type": 1, "analysi": 1, "difficulti": 1, "rel": 1, "variat": 1, "gener": 2, "effect": 1, "holl": 1, "ask": 1, "apart": 1, "kind": 1, "word": 1, "paramet": 1, "prepar": 1, "item": 1, "exampl": 1, "contextrel": 1, "statist": 2, "problem": 1, "similar": 1, "graf": 1}, "marker": "(2009)", "article": "W11-1403", "vector_2": [2, 0.9204060141578424, 2, 5, 1, 0]}, {"label": "Neut", "current": "Character alignments were performed using Phonetisaurus (Novak et al., 2011).", "context": ["We found Thrax to be particularlv robust and flexible in generating various FSTs.", "Character alignments were performed using Phonetisaurus (Novak et al., 2011).", "N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also generates the models as FSTs."], "vector_1": {"librarv": 1, "use": 1, "creat": 1, "variou": 1, "perform": 1, "phonetisauru": 1, "gener": 2, "particularlv": 1, "charact": 1, "opengrm": 1, "also": 1, "ngram": 2, "fst": 2, "robust": 1, "found": 1, "model": 2, "flexibl": 1, "thrax": 1, "align": 1}, "marker": "(Novak et al., 2011)", "article": "W14-5502", "vector_2": [3, 0.36072034092471245, 2, 1, 0, 0]}, {"label": "Neut", "current": "Wu and Chang (2007), Kwok et al (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration.", "context": ["Their work was concentrated on terminologies, and assumed the English terms were given as input.", "Wu and Chang (2007), Kwok et al (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration.", "It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by."], "vector_1": {"lexicon": 1, "kwok": 1, "hard": 1, "al": 1, "terminolog": 1, "largescal": 1, "et": 1, "assum": 2, "concentr": 1, "given": 2, "engin": 1, "also": 1, "build": 1, "way": 1, "input": 2, "difficult": 1, "may": 1, "search": 1, "wu": 1, "translat": 1, "transliter": 1, "come": 1, "truli": 1, "term": 3, "name": 1, "work": 1, "focu": 1, "employ": 1, "english": 3, "chang": 1}, "marker": "(2005)", "article": "P08-1113", "vector_2": [3, 0.9475373586789828, 2, 3, 0, 0]}, {"label": "Neut", "current": "Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting.", "context": ["The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs).", "Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting.", "These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models."], "vector_1": {"em": 2, "suggest": 1, "set": 1, "somewhat": 1, "tagger": 1, "result": 1, "use": 2, "techniqu": 1, "semisupervis": 1, "perform": 1, "anoth": 1, "baumwelch": 2, "necessarili": 1, "merialdo": 1, "hidden": 1, "contrast": 1, "deploy": 1, "gener": 1, "hmm": 2, "elworthi": 1, "train": 2, "partofspeech": 1, "data": 1, "markov": 1, "demonstr": 1, "pereira": 1, "algorithm": 1, "fairli": 1, "unsupervis": 1, "requir": 1, "schabe": 1, "yield": 1, "instanc": 1, "determin": 1, "neg": 1, "ioa": 1, "improv": 1, "model": 2, "forwardbackward": 1}, "marker": "(1994)", "article": "W07-2203", "vector_2": [13, 0.15628610440079824, 3, 4, 0, 0]}, {"label": "Neut", "current": "Schwartz et al (2012) is a systematic study of how representation choices in dependency annotation schemes affect their learnability for parsing.", "context": ["2 Related work", "Schwartz et al (2012) is a systematic study of how representation choices in dependency annotation schemes affect their learnability for parsing.", "The choice points investigated, much like in the current paper, relate to the issue of headedness."], "vector_1": {"represent": 1, "schwartz": 1, "point": 1, "al": 1, "paper": 1, "et": 1, "systemat": 1, "depend": 1, "headed": 1, "current": 1, "learnabl": 1, "much": 1, "scheme": 1, "investig": 1, "relat": 2, "pars": 1, "affect": 1, "choic": 2, "like": 1, "work": 1, "annot": 1, "issu": 1, "studi": 1}, "marker": "(2012)", "article": "W15-2134", "vector_2": [3, 0.09014442152039359, 1, 6, 0, 0]}, {"label": "Neut", "current": "1We follow the transliteration scheme of the Hebrew Treebank (Sima'an et al., 2001).", "context": ["(1) barC beph dibrw hybrit ieral la tmid beph in-the-language hybrit the-Hebrew", "1We follow the transliteration scheme of the Hebrew Treebank (Sima'an et al., 2001).", "'They did not always speak in the Hebrew language in the land of Israel.'"], "vector_1": {"sima": 1, "ieral": 1, "land": 1, "la": 1, "we": 1, "treebank": 1, "beph": 2, "israel": 1, "dibrw": 1, "tmid": 1, "alway": 1, "thehebrew": 1, "barc": 1, "inthelanguag": 1, "transliter": 1, "hebrew": 2, "follow": 1, "scheme": 1, "hybrit": 2, "languag": 1, "speak": 1}, "marker": "an et al., 2001)", "article": "W12-2011", "vector_2": [11, 0.14871654334520873, 1, 2, 0, 0]}, {"label": "Neut", "current": "The range of possible values of T correlation coefficient is 1], where 1 mean (2011a).", "context": ["A higher value for T indicates more similarity to the human adequacy rankings by the evaluation metrics.", "The range of possible values of T correlation coefficient is 1], where 1 mean (2011a).", "state-of-the-art systems' Machacek Kendall's systems' Kendall's Kendall's [-1, s the"], "vector_1": {"rang": 1, "stateoftheart": 1, "kendal": 3, "evalu": 1, "possibl": 1, "metric": 1, "coeffici": 1, "system": 2, "indic": 1, "correl": 1, "rank": 1, "human": 1, "machacek": 1, "mean": 1, "higher": 1, "similar": 1, "valu": 2, "adequaci": 1}, "marker": "(2011a)", "article": "W14-4719", "vector_2": [3, 0.727936396819841, 1, 5, 0, 0]}, {"label": "Pos", "current": "In this paper, we adopt the second-order sibling factorization (Eisner, 1996; McDonald and", "context": ["Here y(x) denotes the set of possible dependency trees for sentence x.", "In this paper, we adopt the second-order sibling factorization (Eisner, 1996; McDonald and", "1322"], "vector_1": {"denot": 1, "set": 1, "depend": 1, "possibl": 1, "mcdonald": 1, "sentenc": 1, "adopt": 1, "tree": 1, "yx": 1, "paper": 1, "factor": 1, "x": 1, "secondord": 1, "sibl": 1}, "marker": "(Eisner, 1996", "article": "D15-1154", "vector_2": [19, 0.17442879682113013, 1, 1, 0, 0]}, {"label": "Neut", "current": "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "context": ["Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).", "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training."], "vector_1": {"corpu": 1, "satisfactori": 1, "result": 2, "year": 1, "largescal": 1, "intermedi": 1, "research": 1, "build": 1, "method": 1, "machin": 1, "train": 1, "link": 1, "translat": 1, "associ": 1, "requir": 1, "recent": 1, "measur": 1, "word": 1, "align": 2, "employ": 1, "achiev": 1, "statist": 2, "bilingu": 1, "mani": 1, "model": 1, "propos": 1, "order": 1, "first": 1}, "marker": "(Smadja et al., 1996", "article": "P05-1058", "vector_2": [9, 0.039255958147645806, 7, 1, 0, 0]}, {"label": "Neut", "current": "As noted by Leenheer and Moor (2005), 'No matter how expressive ontologies might be, they are all in fact lexical representations of concepts'.", "context": ["2 Related work", "As noted by Leenheer and Moor (2005), 'No matter how expressive ontologies might be, they are all in fact lexical representations of concepts'.", "The linguistic basis of formal ontology is such that a significant portion of domain ontology can be extracted automatically from the domain related texts using language processing techniques."], "vector_1": {"represent": 1, "domain": 2, "concept": 1, "process": 1, "ontolog": 3, "automat": 1, "signific": 1, "leenheer": 1, "extract": 1, "languag": 1, "use": 1, "techniqu": 1, "note": 1, "moor": 1, "text": 1, "basi": 1, "might": 1, "express": 1, "relat": 2, "lexic": 1, "formal": 1, "work": 1, "matter": 1, "portion": 1, "linguist": 1, "fact": 1}, "marker": "(2005)", "article": "W12-5209", "vector_2": [7, 0.2503308971629165, 1, 1, 0, 0]}, {"label": "Neut", "current": "More recently, Lignos (2010, 2011, 2012) further explored Yang's original algorithm, taking into account that function words should not be assumed to possess lexical stress cues.", "context": ["While the USC has been argued to be near-to-universal and follows from the \"culminative function of stress\" (Fromkin, 2001; Cutler, 2005), the high score Yang reported crucially depends on every word token carrying stress, including function words.", "More recently, Lignos (2010, 2011, 2012) further explored Yang's original algorithm, taking into account that function words should not be assumed to possess lexical stress cues.", "While his scores are in line with those reported by Yang, the importance of stress for this learner were more modest, providing a gain of around 2.5% (Lignos, 2011)."], "vector_1": {"origin": 1, "ligno": 1, "carri": 1, "learner": 1, "high": 1, "explor": 1, "follow": 1, "assum": 1, "line": 1, "usc": 1, "crucial": 1, "cue": 1, "score": 2, "includ": 1, "import": 1, "possess": 1, "take": 1, "function": 3, "everi": 1, "around": 1, "argu": 1, "lexic": 1, "modest": 1, "gain": 1, "report": 2, "depend": 1, "culmin": 1, "account": 1, "recent": 1, "stress": 4, "word": 3, "algorithm": 1, "provid": 1, "neartounivers": 1, "token": 1, "yang": 3}, "marker": "(2010, 2011, 2012)", "article": "Q14-1008", "vector_2": [4, 0.1531790383804302, 4, 3, 13, 0]}, {"label": "Neut", "current": "We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).", "context": ["4 Approximation error", "We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).", "We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models."], "vector_1": {"em": 3, "point": 1, "predict": 1, "approxim": 2, "increas": 1, "inform": 1, "decreas": 1, "follow": 1, "pcfg": 1, "incorrect": 1, "supervis": 1, "log": 1, "author": 1, "perform": 1, "diverg": 1, "px": 1, "show": 1, "question": 1, "accuraci": 2, "start": 1, "figur": 1, "estim": 1, "analyz": 1, "similarli": 1, "discrep": 1, "trend": 1, "experi": 1, "run": 1, "optim": 1, "act": 1, "initi": 2, "hmm": 1, "believ": 1, "mani": 1, "bgen": 1, "argmax": 1, "contain": 1, "likelihood": 3, "discuss": 1, "e": 1, "valuabl": 1, "specif": 1, "confront": 1, "iter": 1, "p": 3, "bias": 1, "behav": 1, "dmv": 1, "error": 2, "found": 1, "model": 3, "surrog": 1}, "marker": "(Merialdo, 1994", "article": "P08-1100", "vector_2": [14, 0.3141630320857423, 3, 1, 0, 0]}, {"label": "Weak", "current": "Furthermore the use of the infinitive-form in instructions in general as observed by (Kocourek, 1982) is declining, as some of the conventions already common in English technical writing are being adopted by French technical writers, e.g., (Timbal-Duclaux, 1990).", "context": ["These results, however, were obtained from a corpus of instructions mostly for domestic appliances as opposed to software manuals.", "Furthermore the use of the infinitive-form in instructions in general as observed by (Kocourek, 1982) is declining, as some of the conventions already common in English technical writing are being adopted by French technical writers, e.g., (Timbal-Duclaux, 1990).", "We also note that the patterns of realisations uncovered in our analysis follow the principle of good technical writing practice known as the minimalist approach, e.g., (Carroll, 1994; Hammond, 1994)."], "vector_1": {"corpu": 1, "softwar": 1, "declin": 1, "applianc": 1, "eg": 2, "obtain": 1, "uncov": 1, "instruct": 2, "alreadi": 1, "result": 1, "oppos": 1, "follow": 1, "infinitiveform": 1, "note": 1, "use": 1, "convent": 1, "pattern": 1, "writer": 1, "write": 2, "also": 1, "approach": 1, "domest": 1, "analysi": 1, "good": 1, "gener": 1, "french": 1, "mostli": 1, "known": 1, "realis": 1, "technic": 3, "minimalist": 1, "practic": 1, "furthermor": 1, "howev": 1, "adopt": 1, "manual": 1, "common": 1, "english": 1, "principl": 1, "observ": 1}, "marker": "(Kocourek, 1982)", "article": "P96-1026", "vector_2": [14, 0.9147678232349388, 4, 1, 0, 0]}, {"label": "Neut", "current": "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", "context": ["Once items have been identified as unknown, they are available for further processing: If possible one would like to assign some measure of sense information even to these items.", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", ""], "vector_1": {"even": 1, "measur": 1, "identifi": 1, "like": 1, "would": 1, "possibl": 2, "process": 1, "unknown": 1, "approxim": 1, "one": 1, "avail": 1, "item": 3, "inform": 1, "exist": 1, "includ": 1, "cluster": 1, "sens": 3, "similar": 1, "associ": 1, "assign": 1}, "marker": "(Widdows, 2003", "article": "N06-1017", "vector_2": [3, 0.9910593857181395, 3, 2, 0, 0]}, {"label": "Neut", "current": "ILP has already been used in SMT to find the optimal translation for word-based (Germann et al., 2001) and to study the complexity of learning phrase alignments (De Nero and Klein, 2008) models.", "context": ["To solve the combinatorial problem introduced in the previous section, we propose to cast it into an Integer Linear Programming (ILP) problem, for which many generic solvers exist.", "ILP has already been used in SMT to find the optimal translation for word-based (Germann et al., 2001) and to study the complexity of learning phrase alignments (De Nero and Klein, 2008) models.", "Following the latter reference, we introduce the following variables: fi,j (resp."], "vector_1": {"model": 1, "resp": 1, "combinatori": 1, "alreadi": 1, "exist": 1, "follow": 2, "phrase": 1, "find": 1, "variabl": 1, "use": 1, "section": 1, "smt": 1, "complex": 1, "program": 1, "fij": 1, "introduc": 2, "refer": 1, "wordbas": 1, "linear": 1, "optim": 1, "gener": 1, "previou": 1, "translat": 1, "solv": 1, "studi": 1, "solver": 1, "align": 1, "integ": 1, "cast": 1, "ilp": 2, "learn": 1, "mani": 1, "problem": 2, "latter": 1, "propos": 1}, "marker": "(Germann et al., 2001)", "article": "D10-1091", "vector_2": [9, 0.320983281382828, 2, 1, 3, 0]}, {"label": "Neut", "current": "However, our algorithms can be used with any weighted CFG, including discriminative ones, such as the ones in Petrov and Klein (2007a) and", "context": ["In our experiments we will use a probabilistic latent variable CFG (Petrov et al., 2006).", "However, our algorithms can be used with any weighted CFG, including discriminative ones, such as the ones in Petrov and Klein (2007a) and", "Figure 2: The chart that visualizes the bottom-up process of CKY parsing for the sentence \"I love you .\""], "vector_1": {"love": 1, "weight": 1, "process": 1, "cfg": 2, "one": 2, "variabl": 1, "use": 2, "latent": 1, "figur": 1, "includ": 1, "experi": 1, "klein": 1, "sentenc": 1, "chart": 1, "probabilist": 1, "visual": 1, "pars": 1, "algorithm": 1, "howev": 1, "cki": 1, "petrov": 1, "discrimin": 1, "bottomup": 1}, "marker": "(2007a)", "article": "W11-2921", "vector_2": [4, 0.13798790544255085, 2, 1, 7, 1]}, {"label": "Neut", "current": "FOL-based systems that have attained high precision (Bos and Markert, 2006) have done so at the cost of very poor recall.", "context": ["However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and so on.", "FOL-based systems that have attained high precision (Bos and Markert, 2006) have done so at the cost of very poor recall.", "In this work, we explore a different point on the spectrum, by developing a computational model of natural logic, that is, a logic whose vehicle of inference is natural language.'"], "vector_1": {"vehicl": 1, "comput": 1, "founder": 1, "point": 1, "natur": 3, "intension": 1, "high": 1, "attitud": 1, "cost": 1, "done": 1, "attain": 1, "explor": 1, "infer": 1, "causal": 1, "languag": 2, "precis": 1, "differ": 1, "accur": 1, "develop": 1, "system": 1, "tend": 1, "idiom": 1, "includ": 1, "certain": 1, "quantifi": 1, "proposit": 1, "approach": 1, "folbas": 1, "poor": 1, "difficulti": 1, "recal": 1, "whose": 1, "relat": 1, "spectrum": 1, "translat": 1, "foltricki": 1, "howev": 1, "work": 1, "tempor": 1, "issu": 1, "modal": 1, "logic": 2, "model": 1}, "marker": "(Bos and Markert, 2006)", "article": "W07-1431", "vector_2": [1, 0.08387052530517076, 1, 3, 0, 0]}, {"label": "Pos", "current": "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).", "context": ["The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.", "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).", "The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation."], "vector_1": {"xi": 1, "effici": 1, "evalu": 2, "obtain": 1, "indic": 2, "dataset": 1, "monolingu": 2, "decreas": 1, "significantli": 1, "tabl": 1, "leverag": 1, "use": 1, "accur": 1, "develop": 1, "primarili": 1, "semisupervis": 3, "perform": 1, "data": 1, "smt": 2, "highli": 1, "much": 3, "includ": 1, "xu": 1, "method": 5, "unlabel": 2, "rather": 1, "perplex": 1, "optim": 1, "step": 1, "zeng": 1, "slbd": 4, "preferenti": 1, "segment": 3, "demonstr": 2, "comparison": 1, "subsequ": 1, "provid": 1, "outperform": 1, "larger": 1, "stronger": 1, "focu": 1, "final": 1, "caus": 1, "inform": 1, "present": 1, "either": 1, "bilingu": 1, "produc": 1, "result": 2}, "marker": "(Zeng et al., 2014)", "article": "D15-1142", "vector_2": [1, 0.9519128677603863, 4, 2, 4, 0]}, {"label": "Neut", "current": "(Diaz-Negrillo and Fernandez-Dominguez, 2006; Boyd, 2010)).", "context": ["how errors are described in different taxonomies, e.g.", "(Diaz-Negrillo and Fernandez-Dominguez, 2006; Boyd, 2010)).", "Specific error types are unlikely to recur, making sparsity even more of a concern."], "vector_1": {"even": 1, "sparsiti": 1, "differ": 1, "recur": 1, "describ": 1, "specif": 1, "eg": 1, "unlik": 1, "error": 2, "taxonomi": 1, "type": 1, "make": 1, "concern": 1}, "marker": "Boyd, 2010)", "article": "W12-2011", "vector_2": [2, 0.05605797631010018, 2, 1, 0, 0]}, {"label": "Neut", "current": "The categorical distributional compositional model of meaning of Coecke et al (2010) combines the modularity of formal semantic models with the empirical nature of vector space models of lexical semantics.", "context": ["1 Background", "The categorical distributional compositional model of meaning of Coecke et al (2010) combines the modularity of formal semantic models with the empirical nature of vector space models of lexical semantics.", "The meaning of a sentence is defined to be the application of its grammatical structure- represented in a type-logical model-to the kronecker product of the meanings of its words, as computed in a distributional model."], "vector_1": {"semant": 2, "composit": 1, "natur": 1, "al": 1, "repres": 1, "empir": 1, "et": 1, "comput": 1, "space": 1, "modelto": 1, "typelog": 1, "coeck": 1, "applic": 1, "product": 1, "distribut": 2, "sentenc": 1, "kroneck": 1, "lexic": 1, "modular": 1, "background": 1, "categor": 1, "formal": 1, "grammat": 1, "word": 1, "structur": 1, "defin": 1, "vector": 1, "combin": 1, "model": 4, "mean": 3}, "marker": "(2010)", "article": "W11-2507", "vector_2": [1, 0.08285109386026818, 1, 4, 9, 1]}, {"label": "Pos", "current": "For task 1.3, we used a subset of the 80 most general BB features as in (Shah et al., 2013), for which we had all the necessary resources available for the extraction.", "context": ["For task 1.1, we performed this feature selection over all 160 features mentioned in Section 2.", "For task 1.3, we used a subset of the 80 most general BB features as in (Shah et al., 2013), for which we had all the necessary resources available for the extraction.", "We selected the top 25 features for both models, based on empirical results found by Shah et al (2013) for a number of datasets, and then retrained the GP using only the selected features."], "vector_1": {"subset": 1, "featur": 5, "gp": 1, "al": 1, "dataset": 1, "result": 1, "empir": 1, "et": 1, "extract": 1, "select": 3, "use": 2, "perform": 1, "section": 1, "avail": 1, "5": 1, "shah": 1, "resourc": 1, "bb": 1, "gener": 1, "number": 1, "retrain": 1, "mention": 1, "base": 1, "task": 2, "necessari": 1, "found": 1, "model": 1, "top": 1}, "marker": "(Shah et al., 2013)", "article": "W13-2241", "vector_2": [0, 0.49830319888734353, 2, 4, 7, 0]}, {"label": "Neut", "current": "A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain.", "context": ["Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.", "A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain.", "Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures."], "vector_1": {"domain": 2, "sacaleanu": 1, "explor": 1, "concern": 1, "discuss": 1, "vossen": 1, "mathemat": 1, "disambigu": 1, "baayen": 1, "sens": 1, "sublanguag": 1, "resourc": 1, "distribut": 1, "relat": 1, "sophist": 1, "mixtur": 2, "tailor": 1, "central": 1, "buitelaar": 1, "technic": 1, "present": 1, "like": 1, "word": 1, "practic": 1, "frequenc": 1, "potenti": 1, "model": 3, "generallanguag": 1}, "marker": "(2001)", "article": "J03-3001", "vector_2": [2, 0.7863959699263348, 3, 1, 0, 0]}, {"label": "CoCo", "current": "While this distinction has long been acknowledged in text classification literature (Lee, 2001; Dewdney et al., 2001; Lee and Myaeng, 2002), most work on domain adaptation in SMT uses in-domain and out-of-domain data that differs on both the topic and the genre level (e.g., Europarl political proceedings (Koehn, 2005) versus EMEA medical text (Tiedemann, 2009)), making it unclear whether the proposed solutions address topic or genre differences.", "context": ["Second, the commonly used notion of a domain neglects the fact that topic and genre are two distinct properties of text (Stein and Meyer Zu Eissen, 2006).", "While this distinction has long been acknowledged in text classification literature (Lee, 2001; Dewdney et al., 2001; Lee and Myaeng, 2002), most work on domain adaptation in SMT uses in-domain and out-of-domain data that differs on both the topic and the genre level (e.g., Europarl political proceedings (Koehn, 2005) versus EMEA medical text (Tiedemann, 2009)), making it unclear whether the proposed solutions address topic or genre differences.", "In this work, we follow text classification literature for definitions of the concepts topic and genre."], "vector_1": {"domain": 2, "work": 2, "text": 4, "eg": 1, "emea": 1, "classif": 2, "topic": 4, "second": 1, "unclear": 1, "follow": 1, "medic": 1, "concept": 1, "use": 2, "definit": 1, "proceed": 1, "distinct": 2, "make": 1, "smt": 1, "solut": 1, "two": 1, "long": 1, "outofdomain": 1, "adapt": 1, "acknowledg": 1, "polit": 1, "differ": 2, "address": 1, "data": 1, "level": 1, "whether": 1, "indomain": 1, "properti": 1, "genr": 4, "literatur": 2, "neglect": 1, "versu": 1, "commonli": 1, "europarl": 1, "notion": 1, "fact": 1, "propos": 1}, "marker": "(Lee, 2001", "article": "W15-2518", "vector_2": [14, 0.08085148580306085, 6, 1, 0, 0]}, {"label": "Neut", "current": "To determine if two policies may conflict, we plan to use a conflict detection mechanism similar to the one proposed by Sensoy et al (2010).", "context": ["Using such a strategy, the Policy Manager would be able to determine how to prioritise conflicting policies applying to a particular resource.", "To determine if two policies may conflict, we plan to use a conflict detection mechanism similar to the one proposed by Sensoy et al (2010).", "Moreover regarding usability, we need to implement a system that would allow users to easily create SPIN rules representing their policies, possibly using a NLG interface."], "vector_1": {"user": 1, "prioritis": 1, "creat": 1, "appli": 1, "al": 1, "one": 1, "nlg": 1, "need": 1, "et": 1, "use": 3, "detect": 1, "system": 1, "would": 2, "polici": 4, "moreov": 1, "abl": 1, "two": 1, "interfac": 1, "sensoy": 1, "conflict": 3, "strategi": 1, "resourc": 1, "allow": 1, "may": 1, "regard": 1, "usabl": 1, "mechan": 1, "plan": 1, "particular": 1, "repres": 1, "spin": 1, "possibl": 1, "rule": 1, "easili": 1, "determin": 2, "implement": 1, "similar": 1, "manag": 1, "propos": 1}, "marker": "(2010)", "article": "W11-2820", "vector_2": [1, 0.9529821966295102, 1, 1, 0, 0]}, {"label": "CoCo", "current": "F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF", "context": ["Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.", "F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF", "Table 7: Performance comparison on the test set."], "vector_1": {"gazett": 1, "set": 1, "evalu": 1, "al": 4, "paper": 1, "tagger": 1, "tabl": 2, "et": 4, "best": 1, "25": 1, "zhou": 1, "perform": 1, "friedrich": 1, "postprocess": 2, "test": 1, "okanohara": 1, "method": 1, "fscore": 2, "variou": 1, "rerank": 2, "memm": 1, "hmm": 1, "train": 1, "semicrf": 1, "11": 1, "comparison": 2, "svm": 1, "tsai": 1, "crf": 1, "crfgazett": 1}, "marker": "(2006)", "article": "W07-1033", "vector_2": [1, 0.9417141500474834, 4, 6, 0, 0]}, {"label": "Neut", "current": "Bresnihan and Ray (1992) show that students perform", "context": ["It would be interesting to repeat the test with the constraint that false answers have a negative influence on the overall score in order to find out whether the students are aware of the length violation.", "Bresnihan and Ray (1992) show that students perform", "522"], "vector_1": {"show": 1, "bresnihan": 1, "find": 1, "ray": 1, "awar": 1, "would": 1, "perform": 1, "neg": 1, "violat": 1, "score": 1, "interest": 1, "answer": 1, "repeat": 1, "fals": 1, "overal": 1, "test": 1, "student": 2, "constraint": 1, "whether": 1, "influenc": 1, "length": 1, "order": 1}, "marker": "(1992)", "article": "Q14-1040", "vector_2": [22, 0.5676709224450155, 1, 1, 0, 0]}, {"label": "CoCo", "current": "We compare our perceptron-like algorithms with the minimum error training used in (SMT Team, 2003) as shown in Table 2.", "context": ["This algorithm does not converge on the Large Set in 10000 iterations.", "We compare our perceptron-like algorithms with the minimum error training used in (SMT Team, 2003) as shown in Table 2.", "The splitting algorithm achieves slightly better results on the Baseline and the Best Feature set, while the minimum error training and the regression algorithm tie for first place on feature combinations."], "vector_1": {"set": 2, "perceptronlik": 1, "minimum": 2, "result": 1, "tabl": 1, "featur": 2, "best": 1, "baselin": 1, "use": 1, "compar": 1, "better": 1, "larg": 1, "split": 1, "tie": 1, "shown": 1, "train": 2, "algorithm": 4, "slightli": 1, "combin": 1, "iter": 1, "converg": 1, "achiev": 1, "place": 1, "error": 2, "regress": 1, "first": 1}, "marker": "(SMT Team, 2003)", "article": "N04-1023", "vector_2": [1, 0.9238396543076508, 1, 8, 0, 0]}, {"label": "Pos", "current": "We added some convenience methods for easy linking to some vocabularies or concept registries, among them, ISOcat (Windhouwer and Wright, 2012), XML Schema, Dublin Core, FOAF, and others.", "context": ["A category consists of (1) an identifier (which automatically is suffixed to the ontology URI to create an URI for the category), (2) a humanreadable label, (3) a human-readable definition (typically consisting of one or two sentences), (4) information about the class hierarchy, (S) information about possible domains and ranges, and (6) a number of relations, which express equivalence and similarity relations to other categories already existing outside the system (using appropriate vocabulary, such as rdfs:seeAlso or owl:sameAs).", "We added some convenience methods for easy linking to some vocabularies or concept registries, among them, ISOcat (Windhouwer and Wright, 2012), XML Schema, Dublin Core, FOAF, and others.", "At the moment, the ontology describing the FiESTA data model (cf."], "vector_1": {"among": 1, "domain": 1, "concept": 1, "identifi": 1, "suffix": 1, "creat": 1, "model": 1, "ontolog": 2, "rang": 1, "automat": 1, "one": 1, "rdfsseealso": 1, "alreadi": 1, "exist": 1, "hierarchi": 1, "ad": 1, "xml": 1, "use": 1, "system": 1, "equival": 1, "isocat": 1, "definit": 1, "two": 1, "label": 1, "categori": 3, "other": 1, "owlsamea": 1, "conveni": 1, "method": 1, "schema": 1, "foaf": 1, "core": 1, "express": 1, "registri": 1, "sentenc": 1, "relat": 2, "fiesta": 1, "number": 1, "moment": 1, "link": 1, "vocabulari": 2, "data": 1, "class": 1, "describ": 1, "appropri": 1, "easi": 1, "consist": 2, "possibl": 1, "humanread": 2, "dublin": 1, "uri": 2, "inform": 2, "cf": 1, "outsid": 1, "typic": 1, "similar": 1}, "marker": "(Windhouwer and Wright, 2012)", "article": "W13-5507", "vector_2": [1, 0.5865795334233872, 1, 2, 0, 0]}, {"label": "Neut", "current": "9month old infants are believed to have learned some phonotactic constraints of their native language (Mattys and Jusczyk, 2001), and learning these constraints can be done with only minimal exposure (Onishi et al., 2002).", "context": ["While we performed syllabification as a preprocessing step outside of learning, a child learner would presumably learn the required phonotactics as a part of learning to segment words.", "9month old infants are believed to have learned some phonotactic constraints of their native language (Mattys and Jusczyk, 2001), and learning these constraints can be done with only minimal exposure (Onishi et al., 2002).", "Finally, spaces and punctuation between words were removed, but the boundaries between utterances-as indicated by line breaks in CHILDES-are retained."], "vector_1": {"infant": 1, "old": 1, "indic": 1, "month": 1, "done": 1, "syllabif": 1, "phonotact": 2, "languag": 1, "would": 1, "minim": 1, "perform": 1, "boundari": 1, "learner": 1, "utterancesa": 1, "preprocess": 1, "presum": 1, "final": 1, "childesar": 1, "break": 1, "nativ": 1, "step": 1, "part": 1, "believ": 1, "child": 1, "line": 1, "segment": 1, "requir": 1, "exposur": 1, "word": 2, "constraint": 2, "remov": 1, "space": 1, "punctuat": 1, "outsid": 1, "learn": 5, "retain": 1}, "marker": "(Onishi et al., 2002)", "article": "W10-2912", "vector_2": [8, 0.6214257794882588, 2, 1, 0, 0]}, {"label": "CoCo", "current": "We show that the performance of our approach (using simple lexical features) is comparable to that of the state-of-art statistical MT system (Koehn et al., 2007).", "context": ["We used maximum entropy model in our experiments.", "We show that the performance of our approach (using simple lexical features) is comparable to that of the state-of-art statistical MT system (Koehn et al., 2007).", "When additional syntactic features (POS tags in this paper) are used, there is a boost in the performance which is likely to improve when richer syntactic features are incorporated in the model."], "vector_1": {"featur": 3, "entropi": 1, "show": 1, "richer": 1, "paper": 1, "syntact": 2, "use": 3, "compar": 1, "perform": 2, "system": 1, "approach": 1, "experi": 1, "simpl": 1, "po": 1, "stateofart": 1, "lexic": 1, "addit": 1, "like": 1, "boost": 1, "maximum": 1, "mt": 1, "tag": 1, "statist": 1, "incorpor": 1, "improv": 1, "model": 2}, "marker": "(Koehn et al., 2007)", "article": "W10-3805", "vector_2": [3, 0.04348341700596909, 1, 5, 1, 0]}, {"label": "Neut", "current": "N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also generates the models as FSTs.", "context": ["Character alignments were performed using Phonetisaurus (Novak et al., 2011).", "N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also generates the models as FSTs.", "Below, we describe the detailed architecture for each transliteration pair."], "vector_1": {"librarv": 1, "use": 1, "describ": 1, "creat": 1, "perform": 1, "phonetisauru": 1, "align": 1, "detail": 1, "charact": 1, "also": 1, "ngram": 2, "fst": 1, "transliter": 1, "opengrm": 1, "pair": 1, "model": 2, "gener": 1, "architectur": 1}, "marker": "(Roark et al., 2012)", "article": "W14-5502", "vector_2": [2, 0.36530266232873576, 2, 1, 3, 0]}, {"label": "Pos", "current": "For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).", "context": ["Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model.", "For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).", "Following the learning stage, each vector in the test data set is labeled with a predicted word and sense."], "vector_1": {"semant": 1, "set": 1, "predict": 2, "follow": 1, "label": 1, "previous": 1, "use": 2, "memori": 1, "next": 1, "process": 1, "vector": 1, "disambigu": 1, "timbl": 1, "test": 2, "sens": 3, "run": 1, "made": 1, "base": 1, "data": 1, "stage": 1, "task": 1, "word": 3, "algorithm": 1, "separ": 1, "exampl": 1, "learn": 4, "found": 1, "model": 1}, "marker": "(Hoste et al., 2002)", "article": "P05-3014", "vector_2": [3, 0.7244623655913979, 3, 1, 4, 0]}, {"label": "Neut", "current": "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "context": ["Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).", "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations."], "vector_1": {"among": 1, "semant": 1, "wup": 1, "synonymi": 1, "past": 1, "packag": 1, "set": 1, "wordnet": 2, "use": 1, "instrument": 1, "perform": 1, "databas": 1, "lin": 1, "re": 1, "other": 1, "languag": 1, "test": 1, "import": 1, "jcn": 1, "method": 2, "variou": 1, "relat": 1, "path": 1, "becom": 1, "lch": 1, "measur": 2, "word": 2, "princeton": 1, "also": 1, "wordnetsimilar": 2, "implement": 1, "similar": 2, "propos": 1}, "marker": "(Wu and Palmer, 1994)", "article": "W14-0118", "vector_2": [20, 0.04608496916189224, 8, 1, 0, 0]}, {"label": "Neut", "current": "In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.", "context": ["6, this paper reveals several crucial findings that contribute to improving native language identification.", "In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.", "The rest of this paper is structured as follows."], "vector_1": {"show": 1, "rest": 1, "one": 1, "paper": 3, "famili": 1, "follow": 1, "find": 2, "languag": 2, "crucial": 1, "identif": 1, "sever": 1, "contribut": 2, "nativ": 1, "reconstruct": 1, "addit": 1, "reveal": 1, "task": 1, "central": 1, "could": 1, "tree": 1, "histor": 1, "structur": 1, "improv": 1, "linguist": 1}, "marker": "Batagelj et al., 1992", "article": "P13-1112", "vector_2": [21, 0.12737310228492563, 5, 4, 0, 0]}, {"label": "Neut", "current": "Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).", "context": ["In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing.", "Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).", "The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models."], "vector_1": {"signific": 1, "result": 2, "year": 1, "perceptron": 1, "reduct": 1, "best": 1, "techniqu": 2, "support": 1, "especi": 1, "label": 1, "boost": 1, "machin": 2, "previou": 1, "recallprecis": 1, "variou": 1, "rerank": 4, "gener": 1, "error": 1, "pars": 3, "recent": 1, "algorithm": 1, "employ": 1, "discrimin": 1, "vector": 1, "learn": 1, "improv": 1, "model": 1}, "marker": "(Collins, 2000)", "article": "N04-1023", "vector_2": [4, 0.24773332304487056, 3, 2, 1, 0]}, {"label": "Neut", "current": "Determining what should be counted as a term is a difficult task and there are not yet well-accepted solutions (Sag et al, 2003).", "context": ["Since parenthetical translations are mostly translation of terms, it makes sense to further constrain the left boundary of the Chinese side to be a term boundary.", "Determining what should be counted as a term is a difficult task and there are not yet well-accepted solutions (Sag et al, 2003).", "We compiled an approximate term vocabulary by taking the top 5 million most frequent Chinese queries as according to a fully anonymized collection of search engine query logs."], "vector_1": {"compil": 1, "vocabulari": 1, "anonym": 1, "queri": 2, "approxim": 1, "top": 1, "yet": 1, "engin": 1, "log": 1, "make": 1, "solut": 1, "constrain": 1, "take": 1, "sens": 1, "difficult": 1, "accord": 1, "chines": 2, "million": 1, "search": 1, "translat": 2, "mostli": 1, "boundari": 2, "sinc": 1, "count": 1, "term": 4, "task": 1, "parenthet": 1, "collect": 1, "determin": 1, "fulli": 1, "wellaccept": 1, "side": 1, "frequent": 1, "left": 1}, "marker": "(Sag et al, 2003)", "article": "P08-1113", "vector_2": [5, 0.39024216270195083, 1, 4, 0, 0]}, {"label": "Neut", "current": "2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.", "context": ["En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.", "Les graphes sont essentiellement une forme de representation mathematique et visuelle des relations entre des objets/entites."], "vector_1": {"represent": 1, "en": 1, "modelis": 1, "al": 1, "accessibilitd": 1, "distanc": 1, "travaux": 1, "il": 1, "aspect": 2, "et": 3, "eu": 1, "phrase": 1, "diver": 1, "sen": 1, "le": 3, "graph": 2, "celui": 1, "visuel": 1, "capter": 1, "densitd": 1, "dictionnair": 1, "essentiel": 1, "relat": 1, "structur": 2, "form": 1, "effet": 1, "de": 7, "lexic": 1, "mot": 2, "sont": 1, "objetsentit": 1, "du": 2, "associ": 1, "mond": 1, "une": 1, "nombreux": 1, "montrant": 1, "pour": 2, "etc": 1, "gaum": 1, "entr": 2, "leur": 1, "lexical": 1, "dynamiqu": 1, "pertin": 1, "ou": 1, "moyenn": 1, "mathematiqu": 1}, "marker": "(Dion, 2012)", "article": "W14-6700", "vector_2": [2, 0.25580969807868253, 9, 1, 0, 0]}, {"label": "Pos", "current": "Like Cimiano et al (2005), we follow a hybrid approach and construct a concept hierarchy using distributional similarity, patterns and WordNet.", "context": ["Dominguez Garcia et al (2012) used wikipedia to extract ontology for different languages.", "Like Cimiano et al (2005), we follow a hybrid approach and construct a concept hierarchy using distributional similarity, patterns and WordNet.", "However, instead of performing top-down or bottom-up clustering, we pose ontology learning as a k-partite graph construction problem."], "vector_1": {"concept": 1, "ontolog": 2, "topdown": 1, "al": 2, "cluster": 1, "graph": 1, "dominguez": 1, "hierarchi": 1, "follow": 1, "extract": 1, "wordnet": 1, "use": 2, "cimiano": 1, "perform": 1, "pattern": 1, "hybrid": 1, "construct": 2, "languag": 1, "instead": 1, "approach": 1, "distribut": 1, "et": 2, "pose": 1, "differ": 1, "kpartit": 1, "wikipedia": 1, "problem": 1, "like": 1, "howev": 1, "garcia": 1, "learn": 1, "bottomup": 1, "similar": 1}, "marker": "(2005)", "article": "W12-5209", "vector_2": [7, 0.40789549404385106, 2, 2, 1, 0]}, {"label": "Neut", "current": "This process is known as wikification (Csomai and Mihalcea, 2008), and we used an implementation of Milne and Witten (2008).", "context": ["To compute document similarity, we first extract key representative Wikipedia concepts from a document to produce document concept vectors4.", "This process is known as wikification (Csomai and Mihalcea, 2008), and we used an implementation of Milne and Witten (2008).", "This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document."], "vector_1": {"concept": 4, "comput": 1, "weight": 1, "process": 1, "vi": 1, "repres": 1, "strongli": 1, "miln": 1, "identifi": 1, "extract": 1, "id": 2, "denot": 1, "use": 1, "wikipedia": 2, "articl": 1, "current": 1, "document": 5, "form": 1, "relat": 1, "wi": 1, "witten": 1, "key": 1, "known": 1, "wikif": 1, "vector": 2, "w": 2, "idi": 1, "implement": 1, "similar": 1, "produc": 2, "first": 1}, "marker": "(2008)", "article": "W10-3506", "vector_2": [2, 0.7859435779296802, 2, 1, 0, 0]}, {"label": "Neut", "current": "A child's competence grows gradually as she hears and produces more and more utterances, going through predictable changes to her working grammar (Marcus et al., 1992) that statistical optimization techniques typically do not go through and do not intend to replicate.", "context": ["Children are incremental learners (Brown, 1973), and learners relying on statistical optimization are generally not.", "A child's competence grows gradually as she hears and produces more and more utterances, going through predictable changes to her working grammar (Marcus et al., 1992) that statistical optimization techniques typically do not go through and do not intend to replicate.", "Statistical models provide excellent information about the features, distributional cues, and priors that can be used in learning, but provide little information about how a child learner can use this information and how her knowledge of language develops as the learning process evolves."], "vector_1": {"featur": 1, "predict": 1, "gradual": 1, "knowledg": 1, "cue": 1, "increment": 1, "go": 2, "utter": 1, "children": 1, "languag": 1, "use": 2, "develop": 1, "techniqu": 1, "prior": 1, "learner": 3, "excel": 1, "littl": 1, "process": 1, "reli": 1, "evolv": 1, "compet": 1, "model": 1, "intend": 1, "distribut": 1, "optim": 2, "gener": 1, "hear": 1, "child": 2, "grow": 1, "replic": 1, "grammar": 1, "provid": 2, "work": 1, "inform": 3, "statist": 3, "learn": 2, "chang": 1, "typic": 1, "produc": 1}, "marker": "(Marcus et al., 1992)", "article": "W10-2912", "vector_2": [18, 0.07829459695120118, 2, 1, 0, 0]}, {"label": "Neut", "current": "The improvement achieved by the joint model relative to the local model is about 2 points absolute in F-Measure, similar to the improvement when gold-standard syntactic parses are used (Toutanova et al., 2005).", "context": ["The percentage of perfectly labeled propositions for the three sets is 55.11% (development), 56.52% (test), and 37.06% (Brown test).", "The improvement achieved by the joint model relative to the local model is about 2 points absolute in F-Measure, similar to the improvement when gold-standard syntactic parses are used (Toutanova et al., 2005).", "The relative error reduction is much lower for automatic parses, possibly due to a lower upper bound on performance."], "vector_1": {"upper": 1, "set": 1, "point": 1, "bound": 1, "percentag": 1, "due": 1, "reduct": 1, "absolut": 1, "use": 1, "develop": 1, "perform": 1, "perfectli": 1, "three": 1, "label": 1, "much": 1, "rel": 2, "test": 2, "proposit": 1, "local": 1, "brown": 1, "syntact": 1, "goldstandard": 1, "fmeasur": 1, "pars": 2, "automat": 1, "lower": 2, "joint": 1, "possibl": 1, "achiev": 1, "error": 1, "improv": 2, "model": 2, "similar": 1}, "marker": "(Toutanova et al., 2005)", "article": "W05-0623", "vector_2": [0, 0.9632221444677708, 1, 5, 0, 0]}, {"label": "Neut", "current": "For example, in one of the earlier works a rule-based technique was developed for decomposing the complex sentence of a claim into a set of simple sentences while preserving the initial content (Sheremetyeva, 2003).", "context": ["Another group of studies focuses on segmenting, reformatting or highlighting certain parts of the patent claim without changing the content of the original.", "For example, in one of the earlier works a rule-based technique was developed for decomposing the complex sentence of a claim into a set of simple sentences while preserving the initial content (Sheremetyeva, 2003).", "Most recently (Shinmori et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while (Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and reformatting the original text so as to emphasis segments with the identified border marker."], "vector_1": {"origin": 2, "claim": 4, "rulebas": 1, "emphasi": 1, "certain": 1, "focus": 1, "decompos": 1, "one": 1, "set": 1, "marker": 1, "phrase": 1, "border": 2, "involv": 1, "group": 1, "techniqu": 1, "patent": 1, "anoth": 1, "section": 1, "complex": 1, "identifi": 1, "content": 2, "without": 1, "text": 2, "reformat": 2, "simpl": 1, "preserv": 1, "sentenc": 2, "earlier": 1, "initi": 1, "part": 1, "develop": 1, "segment": 3, "studi": 1, "recent": 1, "approach": 1, "explanatori": 1, "align": 1, "work": 1, "descript": 1, "exampl": 1, "suggest": 1, "highlight": 2, "chang": 1, "propos": 1}, "marker": "(Sheremetyeva, 2003)", "article": "W14-5605", "vector_2": [11, 0.2728980503655565, 3, 2, 3, 1]}, {"label": "Neut", "current": "We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-ofdomain training data.", "context": ["All but one category (reportage text) is drawn from different domains than the WSJ.", "We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-ofdomain training data.", "4 The Evaluation Scheme"], "vector_1": {"outofdomain": 1, "differ": 1, "b": 1, "gildea": 1, "evalu": 1, "text": 1, "train": 2, "wsj": 1, "domain": 1, "one": 1, "categori": 1, "also": 1, "other": 1, "consid": 1, "follow": 1, "drawn": 1, "scheme": 1, "therefor": 1, "data": 2, "reportag": 1, "baselin": 1}, "marker": "(2001)", "article": "W07-2203", "vector_2": [6, 0.4881840142842138, 1, 2, 0, 0]}, {"label": "Neut", "current": "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "context": ["Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.", "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc."], "vector_1": {"phrase": 1, "domain": 1, "en": 1, "aspect": 2, "modelis": 2, "celui": 1, "al": 1, "ce": 1, "accessibilitd": 1, "distanc": 1, "travaux": 1, "il": 1, "semblent": 1, "et": 2, "eu": 1, "sen": 1, "diver": 1, "le": 2, "la": 2, "associ": 1, "montrant": 1, "capter": 1, "densitd": 1, "dictionnair": 1, "dernier": 1, "preter": 1, "structur": 2, "effet": 1, "de": 5, "lexic": 1, "mot": 2, "compri": 1, "du": 2, "langu": 1, "mond": 1, "merveil": 1, "nombreux": 2, "pour": 2, "etc": 1, "gaum": 1, "entr": 1, "leur": 1, "lexical": 1, "dynamiqu": 1, "graph": 1, "pertin": 1, "ou": 1, "moyenn": 1, "se": 1}, "marker": "Widdows, 2004", "article": "W14-6700", "vector_2": [10, 0.2616651418115279, 11, 2, 0, 0]}, {"label": "Neut", "current": "First, mixture modeling approaches learn models from different subcorpora and interpolate these linearly (Foster and Kuhn, 2007) or log-linearly (Koehn and Schroeder, 2007).", "context": ["Most existing domain adaptation approaches can be grouped into two categories, depending on where in the SMT pipeline they adapt the system.", "First, mixture modeling approaches learn models from different subcorpora and interpolate these linearly (Foster and Kuhn, 2007) or log-linearly (Koehn and Schroeder, 2007).", "Sennrich (2012) enhances the approach by interpolating up to ten models, and Bertoldi and Federico (2009) use in-domain monolingual data to automatically generate in-domain bilingual data."], "vector_1": {"domain": 1, "ten": 1, "linearli": 1, "system": 1, "monolingu": 1, "exist": 1, "bilingu": 1, "categori": 1, "differ": 1, "group": 1, "smt": 1, "two": 1, "subcorpora": 1, "adapt": 2, "use": 1, "approach": 3, "pipelin": 1, "gener": 1, "learn": 1, "mixtur": 1, "enhanc": 1, "automat": 1, "depend": 1, "sennrich": 1, "federico": 1, "data": 2, "bertoldi": 1, "indomain": 2, "interpol": 2, "model": 3, "first": 1, "loglinearli": 1}, "marker": "(Foster and Kuhn, 2007)", "article": "W15-2518", "vector_2": [8, 0.18698972002776584, 4, 1, 7, 0]}, {"label": "Neut", "current": "2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just \"regular\" readers (Graesser et al., 2004).", "context": ["The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.", "2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just \"regular\" readers (Graesser et al., 2004).", "Text simplification is most often performed on the sentence level."], "vector_1": {"often": 1, "text": 3, "al": 1, "impair": 1, "mild": 1, "need": 1, "et": 1, "cognit": 1, "special": 1, "differ": 1, "develop": 1, "sentenc": 1, "perform": 1, "learner": 1, "elderli": 1, "reader": 3, "languag": 1, "type": 1, "gener": 1, "poor": 1, "mainstream": 1, "peopl": 2, "methodolog": 1, "tool": 1, "dell": 1, "literaci": 1, "regular": 1, "address": 1, "aluisio": 1, "simplif": 2, "level": 2}, "marker": "(Bott et al., 2012)", "article": "W14-5605", "vector_2": [2, 0.18475832656376928, 4, 2, 0, 0]}, {"label": "Neut", "current": "Finally, Blei and Moreno (2001) use an HMM augmented by an aspect model to automatically segment documents, similar in goal to the system of Hearst (1997), but using techniques more similar to the present work.", "context": ["Because the structure of the HMMs they learn is similar to ours it seems that their system could benefit from the techniques of this paper.", "Finally, Blei and Moreno (2001) use an HMM augmented by an aspect model to automatically segment documents, similar in goal to the system of Hearst (1997), but using techniques more similar to the present work.", "7 Conclusions"], "vector_1": {"automat": 1, "paper": 1, "aspect": 1, "seem": 1, "blei": 1, "use": 2, "techniqu": 2, "system": 2, "conclus": 1, "hearst": 1, "final": 1, "hmm": 2, "segment": 1, "present": 1, "goal": 1, "document": 1, "augment": 1, "could": 1, "work": 1, "structur": 1, "benefit": 1, "moreno": 1, "learn": 1, "model": 1, "similar": 3}, "marker": "(2001)", "article": "P05-1046", "vector_2": [4, 0.9447601885223177, 2, 1, 0, 0]}, {"label": "Neut", "current": "We next present two algorithms, MAXSIM and WIKISPREAD, for computing document similarity, and test these over the Lee (2005) document similarity dataset, a set of 50 documents between 51 and 126 words each, with the averaged gold standard similarity ratings produced by 83 test subjects (see (Lee et al., 2005)).", "context": ["This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document.", "We next present two algorithms, MAXSIM and WIKISPREAD, for computing document similarity, and test these over the Lee (2005) document similarity dataset, a set of 50 documents between 51 and 126 words each, with the averaged gold standard similarity ratings produced by 83 test subjects (see (Lee et al., 2005)).", "The first metric we propose is called MAXSIM (see Algorithm 2) and is based on the idea of measuring document similarity by pairing up each Wikipedia concept in one document's concept vector with its most similar concept in the other document."], "vector_1": {"concept": 5, "identifi": 1, "weight": 1, "vi": 1, "metric": 1, "idea": 1, "dataset": 1, "strongli": 1, "rate": 1, "set": 1, "comput": 1, "articl": 1, "id": 2, "subject": 1, "denot": 1, "gold": 1, "wikipedia": 2, "two": 1, "next": 1, "current": 1, "5": 1, "averag": 1, "6": 1, "test": 2, "call": 1, "document": 8, "lee": 1, "form": 1, "measur": 1, "relat": 1, "wi": 1, "standard": 1, "base": 1, "pair": 1, "one": 1, "present": 1, "maxsim": 2, "wikispread": 1, "word": 1, "algorithm": 2, "see": 2, "vector": 2, "w": 2, "idi": 1, "propos": 1, "similar": 5, "produc": 2, "first": 1}, "marker": "(Lee et al., 2005)", "article": "W10-3506", "vector_2": [5, 0.7981419237886627, 2, 4, 0, 0]}, {"label": "Neut", "current": "of India, 2001), mainlv in the Indian state of Goa.", "context": ["Konkani is an Indian language spoken bv approximatelv 2.5 million people (Gov.", "of India, 2001), mainlv in the Indian state of Goa.", "It also has a substantial amount of linguistic minoritv population living in neighboring states of Karnataka and Kerala."], "vector_1": {"mainlv": 1, "karnataka": 1, "spoken": 1, "substanti": 1, "languag": 1, "approximatelv": 1, "goa": 1, "state": 2, "kerala": 1, "also": 1, "live": 1, "indian": 2, "konkani": 1, "gov": 1, "peopl": 1, "minoritv": 1, "million": 1, "bv": 1, "popul": 1, "amount": 1, "neighbor": 1, "linguist": 1}, "marker": "of India, 2001)", "article": "W14-5502", "vector_2": [13, 0.03537552123905971, 1, 1, 0, 0]}, {"label": "Neut", "current": "We require that at least one of two these languages has a Wordnet type lexical ontology (Miller, 1995).", "context": ["In this section, we propose a series of algorithms, each one of which automatically creates a reverse dictionary, or ReverseDictionary, from a dictionary that translates a word in language L1 to a word or phrase in language L2.", "We require that at least one of two these languages has a Wordnet type lexical ontology (Miller, 1995).", "Our algorithms are used to create reverse dictionaries from them at various levels of accuracy and sophistication."], "vector_1": {"creat": 2, "ontolog": 1, "automat": 1, "one": 2, "phrase": 1, "wordnet": 1, "use": 1, "section": 1, "two": 1, "least": 1, "accuraci": 1, "languag": 3, "type": 1, "variou": 1, "sophist": 1, "lexic": 1, "translat": 1, "dictionari": 3, "requir": 1, "word": 2, "algorithm": 2, "level": 1, "revers": 2, "l": 2, "reversedictionari": 1, "seri": 1, "propos": 1}, "marker": "(Miller, 1995)", "article": "N13-1057", "vector_2": [18, 0.37067078666151665, 1, 1, 0, 0]}, {"label": "Neut", "current": "Another existing approach for creating bilingual dictionaries is using probabilistic inference (Mausam et al., 2010).", "context": ["They use four pivot languages, German, Spanish, Dutch and Italian, as intermediate languages.", "Another existing approach for creating bilingual dictionaries is using probabilistic inference (Mausam et al., 2010).", "They organize dictionaries in a graph topology and use random walks and probabilistic graph sampling."], "vector_1": {"creat": 1, "german": 1, "random": 1, "walk": 1, "four": 1, "exist": 1, "spanish": 1, "pivot": 1, "languag": 2, "use": 3, "anoth": 1, "intermedi": 1, "dutch": 1, "approach": 1, "topolog": 1, "bilingu": 1, "probabilist": 2, "dictionari": 2, "organ": 1, "infer": 1, "graph": 2, "sampl": 1, "italian": 1}, "marker": "(Mausam et al., 2010)", "article": "N13-1057", "vector_2": [3, 0.1202523496845629, 1, 1, 0, 0]}, {"label": "Neut", "current": "The nonnative data came from the TOEFL Practice Online system, a web-based practice program for prospective takers of the Test Of English as a Foreign Language (TOEFL) (Zechner et al., 2007).", "context": ["For this work, we are using a state-of-the-art gender-independent Hidden Markov Model speech recognizer whose acoustic model was trained on about 30 hours of non-native speech and whose language model was built on several hundred hours of both native and non-native speech.", "The nonnative data came from the TOEFL Practice Online system, a web-based practice program for prospective takers of the Test Of English as a Foreign Language (TOEFL) (Zechner et al., 2007).", "This data is somewhat different from the THT, as there are only high-entropy tasks in TOEFL Speaking and as the speakers are generally more proficient."], "vector_1": {"prospect": 1, "tht": 1, "highentropi": 1, "profici": 1, "onlin": 1, "sever": 1, "toefl": 3, "whose": 2, "use": 1, "taker": 1, "built": 1, "genderindepend": 1, "system": 1, "program": 1, "speech": 3, "languag": 2, "test": 1, "speaker": 1, "hidden": 1, "speak": 1, "gener": 1, "recogn": 1, "hundr": 1, "differ": 1, "task": 1, "nativ": 1, "train": 1, "somewhat": 1, "data": 2, "markov": 1, "acoust": 1, "practic": 2, "hour": 2, "nonn": 3, "work": 1, "foreign": 1, "webbas": 1, "stateoftheart": 1, "english": 1, "model": 3, "came": 1}, "marker": "(Zechner et al., 2007)", "article": "W08-0912", "vector_2": [1, 0.4182116799848394, 1, 3, 4, 1]}, {"label": "Neut", "current": "  2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are only of theoretical interest here [35,5].", "context": ["How well formalized is their definition and construction?", "  2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are only of theoretical interest here [35,5].", "3 There are several other published implementation of chart parsers [23,20,33], but they often do not give much detail on the output of the parsing process, or even side-step the problem altogether [33]."], "vector_1": {"often": 1, "give": 1, "process": 1, "parser": 1, "cf": 1, "construct": 1, "theoret": 1, "sever": 1, "even": 1, "much": 1, "definit": 1, "detail": 1, "publish": 1, "complex": 1, "interest": 1, "lowest": 1, "sidestep": 1, "recogn": 1, "asymptot": 1, "altogeth": 1, "chart": 1, "pars": 1, "consid": 1, "problem": 1, "formal": 1, "well": 1, "output": 1, "implement": 1}, "marker": "[5]", "article": "P89-1018", "vector_2": [18, 0.13727391260740196, 6, 1, 0, 0]}, {"label": "Neut", "current": "2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just \"regular\" readers (Graesser et al., 2004).", "context": ["The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.", "2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just \"regular\" readers (Graesser et al., 2004).", "Text simplification is most often performed on the sentence level."], "vector_1": {"often": 1, "text": 3, "al": 1, "impair": 1, "mild": 1, "need": 1, "et": 1, "cognit": 1, "special": 1, "differ": 1, "develop": 1, "sentenc": 1, "perform": 1, "learner": 1, "elderli": 1, "reader": 3, "languag": 1, "type": 1, "gener": 1, "poor": 1, "mainstream": 1, "peopl": 2, "methodolog": 1, "tool": 1, "dell": 1, "literaci": 1, "regular": 1, "address": 1, "aluisio": 1, "simplif": 2, "level": 2}, "marker": "(Crossley and McNamara, 2008)", "article": "W14-5605", "vector_2": [6, 0.18475832656376928, 4, 1, 0, 0]}, {"label": "Neut", "current": "The procedure of tuples extraction from awordto-word alignment according to certain constraints is explained in detail in et al (2006).", "context": ["It actually constitutes an Ngram-based LM of bilingual units (called tuples), which approximates the joint probability between the languages under consideration.", "The procedure of tuples extraction from awordto-word alignment according to certain constraints is explained in detail in et al (2006).", "The Ngram-based approach differs fr"], "vector_1": {"consider": 1, "ngrambas": 2, "certain": 1, "approxim": 1, "al": 1, "procedur": 1, "et": 1, "extract": 1, "languag": 1, "probabl": 1, "differ": 1, "constitut": 1, "explain": 1, "lm": 1, "detail": 1, "awordtoword": 1, "call": 1, "unit": 1, "approach": 1, "accord": 1, "tupl": 2, "fr": 1, "joint": 1, "actual": 1, "constraint": 1, "align": 1, "bilingu": 1}, "marker": "(2006)", "article": "W08-0315", "vector_2": [2, 0.22765502494654313, 1, 1, 0, 0]}, {"label": "Pos", "current": "We find the exact top N consistent' most likely local model labelings using a simple dynamic program described in (Toutanova et al., 2005).", "context": ["The model is trained to re-rank a set of N likely labelings according to the local model.", "We find the exact top N consistent' most likely local model labelings using a simple dynamic program described in (Toutanova et al., 2005).", "'A labeling is consistent if satisfies the constraint that argument phrases do not overlap."], "vector_1": {"set": 1, "argument": 1, "phrase": 1, "find": 1, "dynam": 1, "use": 1, "describ": 1, "top": 1, "overlap": 1, "label": 3, "program": 1, "simpl": 1, "accord": 1, "rerank": 1, "satisfi": 1, "train": 1, "exact": 1, "like": 2, "consist": 2, "constraint": 1, "local": 2, "n": 2, "model": 3}, "marker": "(Toutanova et al., 2005)", "article": "W05-0623", "vector_2": [0, 0.5309851488997746, 1, 5, 0, 0]}, {"label": "Pos", "current": "A different version of our own SENSELEARNER system (Mihalcea and Faruque, 2004), using three of the semantic models described in this paper, combined with semantic generalizations based on syntactic dependencies, achieved a performance of 64.6%.", "context": ["The statistical models are built based on SemCor and WordNet, for an overall disambiguation accuracy of 64.1%.", "A different version of our own SENSELEARNER system (Mihalcea and Faruque, 2004), using three of the semantic models described in this paper, combined with semantic generalizations based on syntactic dependencies, achieved a performance of 64.6%.", "3 SenseLearner"], "vector_1": {"semant": 2, "three": 1, "senselearn": 2, "paper": 1, "wordnet": 1, "differ": 1, "depend": 1, "built": 1, "perform": 1, "system": 1, "accuraci": 1, "semcor": 1, "version": 1, "disambigu": 1, "syntact": 1, "gener": 1, "overal": 1, "use": 1, "base": 2, "describ": 1, "achiev": 1, "combin": 1, "statist": 1, "model": 2}, "marker": "(Mihalcea and Faruque, 2004)", "article": "P05-3014", "vector_2": [1, 0.2857676224611708, 1, 2, 2, 1]}, {"label": "Neut", "current": "We selected the ICLE corpus v.2 (Granger et al., 2009) as the target language data.", "context": ["4 Experiments", "We selected the ICLE corpus v.2 (Granger et al., 2009) as the target language data.", "It consists of English essays written by a wide variety of nonnative speakers of English."], "vector_1": {"corpu": 1, "essay": 1, "target": 1, "consist": 1, "icl": 1, "nonn": 1, "english": 2, "varieti": 1, "written": 1, "speaker": 1, "languag": 1, "v": 1, "experi": 1, "data": 1, "select": 1, "wide": 1}, "marker": "(Granger et al., 2009)", "article": "P13-1112", "vector_2": [4, 0.3565404079128968, 1, 1, 2, 0]}, {"label": "Neut", "current": "The baseline systems for the translation directions German-English and English-German are both developed using Discriminative Word Alignment (Niehues and Vogel, 2008) and the Moses Toolkit (Koehn et al., 2007) for extracting phrase pairs and generating the phrase table from the discriminative word alignments.", "context": ["2 Baseline System", "The baseline systems for the translation directions German-English and English-German are both developed using Discriminative Word Alignment (Niehues and Vogel, 2008) and the Moses Toolkit (Koehn et al., 2007) for extracting phrase pairs and generating the phrase table from the discriminative word alignments.", "The difficult reordering between German and English was modeled using POS-based reordering rules."], "vector_1": {"german": 1, "direct": 1, "tabl": 1, "phrase": 2, "extract": 1, "posbas": 1, "baselin": 2, "use": 2, "develop": 1, "system": 2, "reorder": 2, "difficult": 1, "mose": 1, "germanenglish": 1, "gener": 1, "toolkit": 1, "englishgerman": 1, "translat": 1, "pair": 1, "word": 2, "align": 2, "rule": 1, "discrimin": 2, "english": 1, "model": 1}, "marker": "(Niehues and Vogel, 2008)", "article": "W10-1719", "vector_2": [2, 0.11559951191317193, 2, 3, 5, 0]}, {"label": "Neut", "current": "Recall Pullum's (2004) observation that the verb in the \"no\" interpretation involves explicitly not acting.", "context": ["We are particularly interested in further exploring the hypothesis that it is the semantics of the component verb that gives rise to the meaning of the target construction.", "Recall Pullum's (2004) observation that the verb in the \"no\" interpretation involves explicitly not acting.", "Using this intuition, we have informally observed that it is largely possible to (manually) predict the interpretation of the target construction knowing only the component verb."], "vector_1": {"semant": 1, "give": 1, "predict": 1, "particularli": 1, "explor": 1, "involv": 1, "use": 1, "explicitli": 1, "pullum": 1, "construct": 2, "compon": 2, "larg": 1, "interest": 1, "recal": 1, "hypothesi": 1, "rise": 1, "intuit": 1, "verb": 3, "know": 1, "interpret": 2, "target": 2, "possibl": 1, "manual": 1, "inform": 1, "act": 1, "observ": 2, "mean": 1}, "marker": "(2004)", "article": "W10-2109", "vector_2": [6, 0.9227553775145868, 1, 2, 0, 0]}, {"label": "Pos", "current": "We shall take as a baths a formalism developed by the second author in previous papers [15,16].", "context": ["  To discuss the above issues in a uniform way, we need a general framework that encompasses all forms of chart parsing and shared forest building in a unique formalism.", "We shall take as a baths a formalism developed by the second author in previous papers [15,16].", "The idea of this approach is to separate the dynamic programming constructs needed for efficient chart parsing from the chosen parsing schema."], "vector_1": {"effici": 1, "share": 1, "idea": 1, "bath": 1, "second": 1, "paper": 1, "need": 2, "encompass": 1, "dynam": 1, "separ": 1, "develop": 1, "author": 1, "chosen": 1, "construct": 1, "uniform": 1, "program": 1, "forest": 1, "way": 1, "approach": 1, "take": 1, "form": 1, "shall": 1, "gener": 1, "previou": 1, "chart": 2, "framework": 1, "pars": 3, "build": 1, "discuss": 1, "formal": 2, "uniqu": 1, "issu": 1, "schema": 1}, "marker": "[16]", "article": "P89-1018", "vector_2": [1, 0.22366353515429108, 2, 4, 7, 1]}, {"label": "Pos", "current": "On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004).", "context": ["A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high.", "On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004).", "Previous studies explored the trade-off between computational costs and parsing performance."], "vector_1": {"comput": 1, "often": 1, "proven": 1, "high": 1, "cost": 1, "explor": 1, "use": 1, "accur": 1, "tradeoff": 1, "perform": 1, "label": 1, "complex": 1, "import": 1, "method": 1, "previou": 1, "hand": 1, "joint": 2, "twostag": 1, "studi": 1, "edgelabel": 1, "provid": 1, "tree": 1, "structur": 1, "inform": 1, "benefit": 1, "unaccept": 1, "learn": 2, "pars": 1, "model": 2}, "marker": "(Nivre and Scholz, 2004)", "article": "D15-1154", "vector_2": [11, 0.08566586805352656, 2, 2, 6, 0]}, {"label": "Pos", "current": "As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).", "context": ["In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.", "As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).", "Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation."], "vector_1": {"nlp": 1, "domain": 1, "corpora": 1, "sacaleanu": 1, "directli": 1, "text": 1, "continu": 1, "one": 1, "explor": 1, "pass": 1, "et": 1, "gildea": 1, "kilgarriff": 1, "system": 1, "cavaglia": 1, "roland": 1, "sekin": 1, "quantifi": 1, "type": 1, "relat": 2, "al": 1, "mention": 1, "address": 1, "line": 1, "buitelaar": 1, "present": 1, "work": 1, "item": 1, "disambigu": 1, "sens": 1, "similar": 1, "first": 1}, "marker": "(1997)", "article": "J03-3001", "vector_2": [6, 0.7800167075918285, 6, 2, 0, 0]}, {"label": "Neut", "current": "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "context": ["Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.", "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates."], "vector_1": {"set": 1, "profici": 1, "process": 1, "natur": 1, "close": 1, "languag": 2, "multipl": 1, "student": 1, "cloze": 1, "distractor": 1, "field": 1, "exercis": 3, "test": 2, "perspect": 1, "correct": 1, "difficulti": 1, "format": 1, "gener": 2, "previou": 1, "answer": 1, "candid": 1, "vocabulari": 1, "educ": 1, "choic": 2, "grammar": 1, "provid": 1, "approach": 1, "work": 1, "focu": 1, "discrimin": 1, "determin": 1, "usual": 1}, "marker": "Mitkov et al., 2006)", "article": "Q14-1040", "vector_2": [8, 0.14527649070270926, 7, 1, 0, 0]}, {"label": "Neut", "current": "In order to compare our method with results reported by Gabrilovich and Markovitch (2007) and Witten and Milne (2008), we followed the same approach by randomly selecting Algorithm 1 Pseudo code to spread activation depth-first from node vi up to level Lp,max, using global decay d, and threshold T, given an adjacency list graph structure G and a weighting scheme W such that 0 < wig E W < 1.", "context": ["Experimental Method", "In order to compare our method with results reported by Gabrilovich and Markovitch (2007) and Witten and Milne (2008), we followed the same approach by randomly selecting Algorithm 1 Pseudo code to spread activation depth-first from node vi up to level Lp,max, using global decay d, and threshold T, given an adjacency list graph structure G and a weighting scheme W such that 0 < wig E W < 1.", "Require: G, Lp,-ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai < T then > Threshold return end if Add vi to P > To avoid cycles for vj E N(vi) do > Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ < Lp,-ax then aj =aj+ai*wij*d Replace (vj, aj) E A with (vj, aj ) SPREAD UNIDIR(vj, A, P) end if end for return end function"], "vector_1": {"randomli": 1, "wig": 1, "code": 1, "weight": 1, "ai": 2, "vi": 3, "vj": 5, "gabrilovich": 1, "threshold": 2, "list": 1, "jpj": 1, "result": 1, "miln": 1, "follow": 1, "select": 1, "spread": 3, "use": 1, "end": 5, "compar": 1, "decay": 1, "function": 2, "graph": 1, "avoid": 1, "lpmax": 1, "activ": 1, "add": 1, "depthfirst": 1, "adjac": 1, "experiment": 1, "nvi": 1, "replac": 1, "unidirvi": 1, "scheme": 1, "approach": 1, "method": 2, "node": 1, "process": 1, "global": 1, "return": 2, "lpax": 2, "aj": 5, "given": 1, "neighbour": 1, "cycl": 1, "witten": 1, "report": 1, "requir": 1, "unidirvj": 1, "e": 6, "g": 2, "algorithm": 1, "level": 1, "pseudo": 1, "structur": 1, "p": 4, "w": 2, "ajaiwijd": 1, "markovitch": 1, "order": 1}, "marker": "(2008)", "article": "W10-3506", "vector_2": [2, 0.5819706182257638, 2, 7, 1, 0]}, {"label": "Neut", "current": "Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.", "context": ["Since the first release of WordNet, researchers have tried to use it to simulate similarity.", "Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.", "Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011)."], "vector_1": {"synonymi": 1, "joubarn": 1, "releas": 1, "wordnet": 1, "lesk": 1, "use": 1, "describ": 1, "anoth": 1, "inkpen": 1, "except": 1, "research": 1, "languag": 1, "approach": 1, "across": 1, "differ": 1, "hyponymi": 1, "base": 1, "pair": 1, "sinc": 1, "tri": 1, "measur": 2, "algorithm": 1, "simul": 1, "vector": 2, "similar": 2, "first": 1}, "marker": "(Lesk, 1986)", "article": "W14-0118", "vector_2": [28, 0.16076516076516076, 4, 1, 0, 0]}, {"label": "Pos", "current": "Contextual information is extremely useful in making lexical selections of higher quality, as illustrated by the models for Global Lexical Selection (Bangalore et al., 2007; Venkatapathy and", "context": ["In contrast to a phrase-based model, a discriminative model has the power to integrate much richer contextual information into the training model.", "Contextual information is extremely useful in making lexical selections of higher quality, as illustrated by the models for Global Lexical Selection (Bangalore et al., 2007; Venkatapathy and", "34"], "vector_1": {"venkatapathi": 1, "use": 1, "power": 1, "inform": 2, "lexic": 2, "train": 1, "global": 1, "qualiti": 1, "richer": 1, "phrasebas": 1, "discrimin": 1, "select": 2, "much": 1, "integr": 1, "illustr": 1, "extrem": 1, "contextu": 2, "model": 4, "make": 1, "contrast": 1, "higher": 1}, "marker": "(Bangalore et al., 2007", "article": "W10-3805", "vector_2": [3, 0.10052575404198126, 1, 4, 3, 1]}, {"label": "Neut", "current": "Another argument is made vividly by Banko and Brill (2001).", "context": ["They find that probabilistic models of language based on very large quantities of data, even if those data are noisy, are better than ones based on estimates (using sophisticated smoothing techniques) from smaller, cleaner data sets.", "Another argument is made vividly by Banko and Brill (2001).", "They explore the performance of a number of machine learning algorithms (on a representative disambiguation task) as the size of the training corpus grows from a million to a billion words."], "vector_1": {"corpu": 1, "cleaner": 1, "smaller": 1, "argument": 1, "one": 1, "brill": 1, "set": 1, "explor": 1, "find": 1, "languag": 1, "size": 1, "even": 1, "task": 1, "use": 1, "number": 1, "techniqu": 1, "perform": 1, "anoth": 1, "better": 1, "estim": 1, "larg": 1, "disambigu": 1, "machin": 1, "vividli": 1, "million": 1, "sophist": 1, "probabilist": 1, "noisi": 1, "base": 2, "word": 1, "repres": 1, "data": 3, "grow": 1, "billion": 1, "made": 1, "banko": 1, "algorithm": 1, "smooth": 1, "quantiti": 1, "train": 1, "learn": 1, "model": 1}, "marker": "(2001)", "article": "J03-3001", "vector_2": [2, 0.29514214110320736, 1, 1, 0, 0]}, {"label": "Neut", "current": "As query log induced intent topic graph is of considerable large size, the pair-wise similarity is computationally prohibitive, hence we use Local Sensitive Hash (Indyk and Motwani, 1998) for each similarity metric so as to compute ISim just in candidate set.", "context": ["To put it in more details, we use jaccard similarity for name shinglings and cosine similarity for domain and topic vector.", "As query log induced intent topic graph is of considerable large size, the pair-wise similarity is computationally prohibitive, hence we use Local Sensitive Hash (Indyk and Motwani, 1998) for each similarity metric so as to compute ISim just in candidate set.", "We use random hyperplane based hash family proposed in (Charikar, 2002) and set the hash code dimension and hash table numbers empirically to ensure the number of nodes falling into each bucket is relatively stable."], "vector_1": {"dimens": 1, "prohibit": 1, "domain": 1, "set": 2, "comput": 2, "consider": 1, "queri": 1, "metric": 1, "random": 1, "henc": 1, "number": 2, "topic": 2, "code": 1, "famili": 1, "tabl": 1, "ensur": 1, "stabl": 1, "hyperplan": 1, "size": 1, "use": 3, "log": 1, "sensit": 1, "graph": 1, "detail": 1, "larg": 1, "rel": 1, "shingl": 1, "local": 1, "node": 1, "hash": 4, "jaccard": 1, "induc": 1, "candid": 1, "intent": 1, "base": 1, "fall": 1, "put": 1, "name": 1, "empir": 1, "bucket": 1, "vector": 1, "pairwis": 1, "cosin": 1, "isim": 1, "similar": 4, "propos": 1}, "marker": "(Indyk and Motwani, 1998)", "article": "D14-1114", "vector_2": [16, 0.38987487231869256, 2, 1, 0, 0]}, {"label": "Weak", "current": "In this paper, we use the randomization test for discarding unpromising models, since this statistical test was shown to be less likely to cause type-I errors6 than bootstrap methods (Riezler and Maxwell, 2005).", "context": ["In SMT, it is common to use either bootstrap resampling (Efron and Tibshirani, 1993; Och, 2003) or randomization tests (Noreen, 1989).", "In this paper, we use the randomization test for discarding unpromising models, since this statistical test was shown to be less likely to cause type-I errors6 than bootstrap methods (Riezler and Maxwell, 2005).", "Since both kinds of statistical tests involve a time-consuming sampling step, it 4Since Racing only discards suboptimal models, the current best model M* is one for which we have decoded the entire development set."], "vector_1": {"set": 1, "less": 1, "resampl": 1, "one": 1, "paper": 1, "sampl": 1, "suboptim": 1, "timeconsum": 1, "best": 1, "involv": 1, "use": 2, "develop": 1, "smt": 1, "current": 1, "decod": 1, "test": 4, "method": 1, "statist": 2, "random": 2, "typei": 1, "shown": 1, "step": 1, "entir": 1, "sinc": 3, "common": 1, "kind": 1, "like": 1, "bootstrap": 2, "caus": 1, "race": 1, "either": 1, "error": 1, "discard": 2, "model": 3, "unpromis": 1}, "marker": "(Riezler and Maxwell, 2005)", "article": "W12-3159", "vector_2": [7, 0.5405622087488099, 4, 2, 0, 0]}, {"label": "Pos", "current": "Many of these features required creating an automatic parse; in doing so we showed that the Collins (1999) parser when ported to Chinese achieved the best reported performance on Chinese syntactic parsing.", "context": ["Second, the features that we extracted for English semantic parsing worked well when applied to Chinese.", "Many of these features required creating an automatic parse; in doing so we showed that the Collins (1999) parser when ported to Chinese achieved the best reported performance on Chinese syntactic parsing.", "Finally, we showed that semantic parsing is significantly easier in Chinese than in English."], "vector_1": {"semant": 2, "featur": 2, "creat": 1, "show": 2, "appli": 1, "parser": 1, "automat": 1, "second": 1, "significantli": 1, "extract": 1, "port": 1, "best": 1, "perform": 1, "collin": 1, "final": 1, "syntact": 1, "chines": 4, "pars": 4, "report": 1, "requir": 1, "work": 1, "well": 1, "easier": 1, "achiev": 1, "english": 2, "mani": 1}, "marker": "(1999)", "article": "N04-1032", "vector_2": [5, 0.9567571136198587, 1, 4, 0, 0]}, {"label": "Neut", "current": "It has been claimed (by Gale et al, 1992) on the basis of corpus analysis that to a very large extent a word keeps the same meaning throughout a text.", "context": ["Text-based disambiguation.", "It has been claimed (by Gale et al, 1992) on the basis of corpus analysis that to a very large extent a word keeps the same meaning throughout a text.", "6."], "vector_1": {"corpu": 1, "claim": 1, "throughout": 1, "word": 1, "text": 1, "keep": 1, "analysi": 1, "disambigu": 1, "extent": 1, "larg": 1, "mean": 1, "textbas": 1, "basi": 1}, "marker": "(by Gale et al, 1992)", "article": "W03-1807", "vector_2": [11, 0.46364680781224354, 1, 1, 0, 0]}, {"label": "Neut", "current": "Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "context": ["2.2 Bilingual Semi-supervised CWS Methods", "Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004)."], "vector_1": {"dictionari": 1, "individu": 1, "focus": 1, "one": 1, "cw": 1, "leverag": 2, "semisupervis": 1, "perform": 1, "segment": 2, "smt": 1, "construct": 1, "label": 2, "better": 1, "consecut": 1, "approach": 1, "method": 1, "unlabel": 1, "either": 1, "machin": 1, "map": 1, "form": 1, "sequenc": 1, "chines": 2, "previou": 1, "train": 1, "translat": 1, "although": 1, "dataset": 1, "data": 1, "model": 1, "maximummatch": 1, "word": 2, "english": 1, "work": 1, "charact": 1, "achiev": 1, "statist": 1, "bilingu": 2, "studi": 1}, "marker": "(Xi et al., 2012)", "article": "D15-1142", "vector_2": [3, 0.2968798164194952, 7, 3, 0, 0]}, {"label": "Neut", "current": "The initial tagset was loosely based on Tom McArthur's Longman Lexicon of Contemporary English (McArthur, 1981) as this appeared to offer the most appropriate thesaurus type classification of word senses for this kind of analysis.", "context": ["The groups include not only synonyms and antonyms but also hypernyms and hyponyms.", "The initial tagset was loosely based on Tom McArthur's Longman Lexicon of Contemporary English (McArthur, 1981) as this appeared to offer the most appropriate thesaurus type classification of word senses for this kind of analysis.", "The tagset has since been considerably revised in the light of practical tagging problems met in the course of the research."], "vector_1": {"hypernym": 1, "lexicon": 1, "consider": 1, "classif": 1, "tag": 1, "revis": 1, "research": 1, "contemporari": 1, "longman": 1, "cours": 1, "group": 1, "appear": 1, "sinc": 1, "mcarthur": 1, "also": 1, "includ": 1, "tagset": 2, "tom": 1, "sens": 1, "type": 1, "hyponym": 1, "analysi": 1, "offer": 1, "initi": 1, "met": 1, "base": 1, "word": 1, "antonym": 1, "kind": 1, "appropri": 1, "synonym": 1, "practic": 1, "light": 1, "thesauru": 1, "english": 1, "problem": 1, "loos": 1}, "marker": "(McArthur, 1981)", "article": "W03-1807", "vector_2": [22, 0.2506975217462662, 1, 1, 0, 0]}, {"label": "Pos", "current": "We used the SRILM toolkit (Stolcke, 2002) for automatic word clustering over the entire document collection.", "context": ["Thus we want to assign higher weights to the words in this cluster.", "We used the SRILM toolkit (Stolcke, 2002) for automatic word clustering over the entire document collection.", "It minimizes the perplexity of the induced class-based n-gram language model compared to the original word-based model."], "vector_1": {"origin": 1, "weight": 1, "toolkit": 1, "automat": 1, "cluster": 2, "srilm": 1, "want": 1, "languag": 1, "use": 1, "classbas": 1, "minim": 1, "document": 1, "higher": 1, "wordbas": 1, "perplex": 1, "induc": 1, "ngram": 1, "entir": 1, "word": 2, "compar": 1, "thu": 1, "collect": 1, "model": 2, "assign": 1}, "marker": "(Stolcke, 2002)", "article": "N09-1070", "vector_2": [7, 0.4654588826923697, 1, 1, 2, 0]}, {"label": "Neut", "current": "Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.", "context": ["3 Constraining the Learning Space", "Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.", "If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand."], "vector_1": {"effici": 1, "modern": 1, "fail": 1, "equip": 1, "children": 2, "space": 2, "suggest": 1, "research": 1, "machin": 1, "knowledg": 1, "hand": 1, "succeed": 1, "model": 1, "like": 1, "task": 2, "essenti": 1, "algorithm": 1, "specif": 2, "constraint": 2, "realist": 1, "learn": 6, "domainneutr": 1, "constrain": 1}, "marker": "Vapnik, 2000)", "article": "W10-2912", "vector_2": [10, 0.2615833308277459, 3, 1, 0, 0]}, {"label": "Neut", "current": "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", "context": ["To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", "Plan of the paper."], "vector_1": {"detect": 2, "word": 1, "closest": 1, "unknown": 3, "howev": 1, "knowledg": 1, "next": 1, "known": 1, "step": 1, "paper": 1, "exist": 1, "plan": 1, "determin": 1, "complementari": 1, "date": 1, "sens": 3, "problem": 2, "approach": 2, "logic": 1, "view": 1}, "marker": "Curran, 2005", "article": "N06-1017", "vector_2": [1, 0.15268411114108443, 3, 2, 0, 0]}, {"label": "Neut", "current": "Though CRFs (Lafferty et al., 2001) can be regarded as improved version of MEMMs, we have chosen MEMMs because MEMMs are usually much faster to train compared to CRFs, which enables extensive feature selection.", "context": ["As our n-best tagger, we use a first order MEMM model (McCallum et al., 2000).", "Though CRFs (Lafferty et al., 2001) can be regarded as improved version of MEMMs, we have chosen MEMMs because MEMMs are usually much faster to train compared to CRFs, which enables extensive feature selection.", "Training a CRF tagger with features selected using an MEMM may result in yet another performance boost, but in this paper we concentrate on the MEMM as our n-best tagger, and consider CRFs as one of our future extensions."], "vector_1": {"featur": 2, "version": 1, "one": 1, "paper": 1, "tagger": 3, "result": 1, "yet": 1, "select": 2, "concentr": 1, "use": 2, "compar": 1, "chosen": 1, "anoth": 1, "may": 1, "much": 1, "futur": 1, "boost": 1, "though": 1, "memm": 6, "regard": 1, "extens": 2, "train": 2, "consid": 1, "nbest": 2, "faster": 1, "enabl": 1, "perform": 1, "crf": 4, "improv": 1, "model": 1, "first": 1, "order": 1, "usual": 1}, "marker": "(Lafferty et al., 2001)", "article": "W07-1033", "vector_2": [6, 0.31240107628996516, 2, 2, 4, 0]}, {"label": "Neut", "current": "Table 3: Comparison of the results by Pedersen (2010) and the replication of these results using Wordnet-LMF and the WordnetToolkit", "context": ["SM McPed McWT diff RgPed RgWT diff path 0.68 0.72 -0.04 0.69 0.78 -0.09 lch 0.71 0.72 -0.01 0.70 0.78 -0.08 wup 0.74 0.74 0.00 0.69 0.78 -0.09 res 0.74 0.75 -0.01 0.69 0.76 -0.07 jcn 0.72 0.65 0.07 0.51 0.56 -0.05 lin 0.73 0.67 0.06 0.58 0.60 -0.02", "Table 3: Comparison of the results by Pedersen (2010) and the replication of these results using Wordnet-LMF and the WordnetToolkit", "7The depth parameter is set to 19, For more information, we refer to section 6."], "vector_1": {"wup": 1, "rgped": 1, "set": 1, "result": 2, "tabl": 1, "diff": 2, "rgwt": 1, "paramet": 1, "mcwt": 1, "use": 1, "wordnettoolkit": 1, "wordnetlmf": 1, "lin": 1, "pedersen": 1, "mcped": 1, "re": 1, "jcn": 1, "refer": 1, "replic": 1, "path": 1, "lch": 1, "comparison": 1, "inform": 1, "depth": 1, "sm": 1, "the": 1, "section": 1}, "marker": "(2010)", "article": "W14-0118", "vector_2": [4, 0.7612159919852227, 1, 5, 6, 0]}, {"label": "Neut", "current": "Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.", "context": ["The corpus we use to evaluate it is the same corpus used by Yang (2004).", "Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.", "We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word."], "vector_1": {"corpu": 3, "evalu": 1, "obtain": 1, "phonet": 1, "utter": 1, "extract": 1, "children": 1, "sarah": 1, "use": 3, "carnegi": 1, "pronounc": 1, "data": 2, "three": 1, "version": 1, "cmudict": 1, "brown": 1, "mellon": 1, "adult": 1, "dictionari": 1, "child": 1, "pronunci": 1, "transcript": 1, "word": 2, "consist": 1, "eve": 1, "yang": 1, "adam": 1, "first": 1}, "marker": "(1973)", "article": "W10-2912", "vector_2": [37, 0.5840825039838841, 4, 2, 0, 0]}, {"label": "Pos", "current": "Gama and Brazdil (2000) showed that a cascade can outperform other ensemble methods like stacking or boosting.", "context": ["Cascade generalization is the process of sequentially using a set of small classifiers to perform an overall classification task.", "Gama and Brazdil (2000) showed that a cascade can outperform other ensemble methods like stacking or boosting.", "Kaynak and Alpaydin (2000) proposed a method to sequentially cascade classifiers and showed that this improves the accuracy without increasing the computational complexity and cost."], "vector_1": {"classif": 1, "set": 1, "alpaydin": 1, "show": 2, "process": 1, "cost": 1, "comput": 1, "use": 1, "perform": 1, "sequenti": 2, "kaynak": 1, "classifi": 2, "accuraci": 1, "complex": 1, "ensembl": 1, "boost": 1, "method": 2, "gener": 1, "overal": 1, "gama": 1, "increas": 1, "stack": 1, "task": 1, "like": 1, "outperform": 1, "brazdil": 1, "without": 1, "cascad": 3, "small": 1, "improv": 1, "propos": 1}, "marker": "(2000)", "article": "W13-1708", "vector_2": [13, 0.6493798310264246, 2, 1, 0, 0]}, {"label": "CoCo", "current": "This is comparable to the better implementations presented in Dunlop et al (2011).", "context": ["The exhaustive sequential CKY parser was written in C and is reasonably optimized, taking 5.5 seconds per sentence (or 5,505 seconds for the 1000 benchmark sentences).", "This is comparable to the better implementations presented in Dunlop et al (2011).", "As can be seen in Figure 11, the fastest configuration on the GTX285 is Block+PR+SS+tex:scores, which shows a 17.4x speedup against the sequential parser."], "vector_1": {"show": 1, "parser": 2, "al": 1, "second": 2, "et": 1, "seen": 1, "compar": 1, "05": 1, "sequenti": 2, "configur": 1, "figur": 1, "per": 1, "better": 1, "written": 1, "take": 1, "blockprsstexscor": 1, "gtx": 1, "exhaust": 1, "dunlop": 1, "optim": 1, "sentenc": 2, "benchmark": 1, "speedup": 1, "reason": 1, "fastest": 1, "present": 1, "c": 1, "cki": 1, "x": 1, "implement": 1}, "marker": "(2011)", "article": "W11-2921", "vector_2": [0, 0.7613026519018823, 1, 2, 0, 0]}, {"label": "Neut", "current": "Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.", "context": ["Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).", "Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.", "Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006)."], "vector_1": {"function": 1, "develop": 1, "syntact": 1, "also": 1, "perform": 1, "object": 1, "smith": 2, "procedur": 1, "learnabl": 1, "eisner": 2, "train": 1, "introduc": 1, "empir": 1, "improv": 1, "bayesian": 1, "new": 2, "model": 1, "approach": 1, "pcfg": 1, "propos": 1}, "marker": "(2006)", "article": "P08-1100", "vector_2": [2, 0.04740169726476654, 7, 1, 2, 0]}, {"label": "Neut", "current": "2006; Radack, 1995) on how to read patent claims and conducted extensive interviews with patent experts of several companies in the US and Europe handling intellectual property1.", "context": ["In preparing for this research we have investigated professional instructions (Pressman.", "2006; Radack, 1995) on how to read patent claims and conducted extensive interviews with patent experts of several companies in the US and Europe handling intellectual property1.", "The recommendations are as follows."], "vector_1": {"claim": 1, "profession": 1, "instruct": 1, "intellectu": 1, "follow": 1, "sever": 1, "europ": 1, "expert": 1, "patent": 2, "research": 1, "read": 1, "recommend": 1, "interview": 1, "investig": 1, "handl": 1, "pressman": 1, "extens": 1, "compani": 1, "prepar": 1, "properti": 1, "us": 1, "conduct": 1}, "marker": "Radack, 1995)", "article": "W14-5605", "vector_2": [19, 0.3319201868399675, 1, 5, 0, 0]}, {"label": "Pos", "current": "While a variety of kernel functions are available, here we followed previous work on QE using GP (Cohn and Specia, 2013; Shah et al., 2013) and employed a squared exponential (SE) kernel with automatic relevance determination (ARD): where F is the number of features, 2f is the covariance magnitude and li > 0 are the feature length scales.", "context": ["The kernel function encodes the covariance (similarity) between each input pair.", "While a variety of kernel functions are available, here we followed previous work on QE using GP (Cohn and Specia, 2013; Shah et al., 2013) and employed a squared exponential (SE) kernel with automatic relevance determination (ARD): where F is the number of features, 2f is the covariance magnitude and li > 0 are the feature length scales.", "The resulting model hyperparameters (SE variance 2f, noise variance 2n and SE length scales li) were learned from data by maximising the model likelihood."], "vector_1": {"kernel": 3, "featur": 2, "gp": 1, "encod": 1, "number": 1, "automat": 1, "ard": 1, "result": 1, "follow": 1, "maximis": 1, "use": 1, "scale": 2, "squar": 1, "exponenti": 1, "likelihood": 1, "li": 2, "avail": 1, "varieti": 1, "input": 1, "function": 2, "previou": 1, "hyperparamet": 1, "covari": 2, "pair": 1, "varianc": 2, "data": 1, "relev": 1, "nois": 1, "f": 3, "work": 1, "n": 1, "employ": 1, "length": 2, "qe": 1, "determin": 1, "learn": 1, "model": 2, "similar": 1, "magnitud": 1, "se": 3}, "marker": "(Cohn and Specia, 2013", "article": "W13-2241", "vector_2": [0, 0.42536856745479834, 2, 3, 8, 0]}, {"label": "Neut", "current": "In addition, Laplace estimation can be used to ensure that all actions in the 2The parse forest is an instance of a feature forest as defined by Miyao and Tsujii (2002).", "context": ["Therefore, normalization is performed over all lookaheads for a state or over each lookahead for the state depending on whether the state is a member of Ss or Sr, respectively (hereafter the I function).", "In addition, Laplace estimation can be used to ensure that all actions in the 2The parse forest is an instance of a feature forest as defined by Miyao and Tsujii (2002).", "We will use the term 'node' herein to refer to an element in a derivation tree or in the parse forest that corresponds to a (sub-)analysis whose label is the mother's label in the corresponding CF 'backbone' rule."], "vector_1": {"node": 1, "featur": 1, "herein": 1, "ensur": 1, "respect": 1, "whose": 1, "use": 2, "depend": 1, "defin": 1, "perform": 1, "subanalysi": 1, "label": 2, "member": 1, "state": 3, "estim": 1, "forest": 3, "therefor": 1, "refer": 1, "function": 1, "miyao": 1, "deriv": 1, "normal": 1, "mother": 1, "tsujii": 1, "cf": 1, "pars": 2, "term": 1, "addit": 1, "hereaft": 1, "backbon": 1, "instanc": 1, "ss": 1, "lookahead": 2, "tree": 1, "correspond": 2, "rule": 1, "element": 1, "whether": 1, "laplac": 1, "sr": 1, "action": 1, "the": 1}, "marker": "(2002)", "article": "W07-2203", "vector_2": [5, 0.3090711759969191, 1, 1, 1, 0]}, {"label": "Neut", "current": "Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).", "context": ["Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).", "Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).", "In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features."], "vector_1": {"classif": 1, "set": 1, "pair": 1, "featur": 1, "need": 1, "extract": 1, "pmi": 2, "select": 1, "supervis": 1, "use": 3, "neg": 1, "top": 1, "classifi": 1, "research": 1, "also": 2, "score": 2, "treat": 1, "document": 1, "approach": 2, "final": 1, "machin": 1, "candid": 2, "solv": 1, "highest": 1, "averag": 1, "task": 1, "word": 2, "algorithm": 1, "keyword": 3, "k": 1, "exampl": 1, "learn": 3, "posit": 1}, "marker": "Turney, 2003)", "article": "N09-1070", "vector_2": [6, 0.15750620547371136, 6, 1, 4, 0]}, {"label": "Pos", "current": "We use the data set used in (SMT Team, 2003).", "context": ["We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.", "We use the data set used in (SMT Team, 2003).", "The training data consists of about 170M English words, on which the baseline translation system is trained."], "vector_1": {"use": 2, "set": 1, "word": 1, "consist": 1, "evalu": 1, "provid": 1, "train": 2, "m": 1, "system": 1, "track": 1, "experiment": 1, "larg": 1, "result": 1, "translat": 1, "english": 1, "baselin": 1, "data": 3, "nist": 1, "chineseenglish": 1}, "marker": "(SMT Team, 2003)", "article": "N04-1023", "vector_2": [1, 0.7473282148231027, 1, 8, 0, 0]}, {"label": "Neut", "current": "Average word and sentence length are the underlying basis of traditional readability measures such as Flesch-Kincaid and Fry which correlate with cloze test difficulty according to Brown (1989).", "context": ["We calculate the following readability features for the whole paragraph and for the sentence containing the gap.", "Average word and sentence length are the underlying basis of traditional readability measures such as Flesch-Kincaid and Fry which correlate with cloze test difficulty according to Brown (1989).", "We calculate both, but do not find much variety as the paragraphs in our data are all of comparable length (64-99 words, 3-7 sentences, 4.85 characters per word)."], "vector_1": {"underli": 1, "featur": 1, "brown": 1, "correl": 1, "per": 1, "follow": 1, "find": 1, "compar": 1, "fri": 1, "readabl": 2, "calcul": 2, "cloze": 1, "much": 1, "varieti": 1, "test": 1, "difficulti": 1, "accord": 1, "fleschkincaid": 1, "sentenc": 3, "gap": 1, "data": 1, "averag": 1, "basi": 1, "tradit": 1, "measur": 1, "word": 3, "charact": 1, "length": 2, "paragraph": 2, "contain": 1, "whole": 1}, "marker": "(1989)", "article": "Q14-1040", "vector_2": [25, 0.6572850697043886, 1, 5, 0, 0]}, {"label": "Neut", "current": "See Boer Rookhuiszen (2011) for more details on how probability problems are constructed.", "context": ["A warning is also issued if the edited problem contains properties for which no lexical information is available.", "See Boer Rookhuiszen (2011) for more details on how probability problems are constructed.", "3 Language Generation"], "vector_1": {"issu": 1, "inform": 1, "edit": 1, "rookhuiszen": 1, "properti": 1, "boer": 1, "detail": 1, "lexic": 1, "warn": 1, "also": 1, "see": 1, "construct": 1, "contain": 1, "problem": 2, "probabl": 1, "gener": 1, "languag": 1, "avail": 1}, "marker": "(2011)", "article": "W11-1403", "vector_2": [0, 0.36444743462236, 1, 1, 0, 0]}, {"label": "Neut", "current": "In studies of speech, Erman (2007) notes that a pause can be caused by the cognitive demands of lexical retrieval, and Pawley (1985) notes that pauses are much less acceptable within a lexicalized phrase than within a free expression.", "context": ["We augment the previous findings, though, by investigating how varying cognitive demands affect MWE production.", "In studies of speech, Erman (2007) notes that a pause can be caused by the cognitive demands of lexical retrieval, and Pawley (1985) notes that pauses are much less acceptable within a lexicalized phrase than within a free expression.", "This led Dahlmann and Adolphs (2007) to study pausing within spoken MWEs."], "vector_1": {"paus": 3, "erman": 1, "less": 1, "spoken": 1, "within": 3, "accept": 1, "pawley": 1, "phrase": 1, "cognit": 2, "find": 1, "vari": 1, "dahlmann": 1, "note": 2, "much": 1, "speech": 1, "mwe": 2, "product": 1, "investig": 1, "led": 1, "though": 1, "previou": 1, "adolph": 1, "lexic": 2, "free": 1, "demand": 2, "affect": 1, "retriev": 1, "augment": 1, "caus": 1, "express": 1, "studi": 2}, "marker": "(1985)", "article": "W15-0914", "vector_2": [30, 0.24206235989402894, 3, 2, 0, 0]}, {"label": "Pos", "current": "Our analysis was carried out within the framework of Systemic-Functional Linguistics (sFL) (Halliday, 1978; Halliday, 1985) which views language as a resource for the creation of meaning.", "context": ["3 Linguistic Framework: Systemic Functional Linguistics", "Our analysis was carried out within the framework of Systemic-Functional Linguistics (sFL) (Halliday, 1978; Halliday, 1985) which views language as a resource for the creation of meaning.", "SFL stratifies meaning into context and language."], "vector_1": {"function": 1, "analysi": 1, "creation": 1, "resourc": 1, "carri": 1, "within": 1, "stratifi": 1, "system": 1, "framework": 2, "systemicfunct": 1, "context": 1, "sfl": 2, "mean": 2, "linguist": 3, "languag": 2, "view": 1}, "marker": "(Halliday, 1978", "article": "P96-1026", "vector_2": [18, 0.14336267909546005, 2, 1, 5, 0]}, {"label": "Neut", "current": "Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.", "context": ["The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.", "Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.", "In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues."], "vector_1": {"interfer": 2, "pronoun": 1, "definit": 1, "strongli": 1, "davidsennielsen": 1, "spanish": 1, "research": 1, "languag": 2, "smith": 1, "would": 1, "transfer": 2, "tapper": 1, "swan": 1, "aart": 1, "sometim": 1, "littl": 1, "articl": 1, "write": 1, "overusedunderus": 1, "altenberg": 1, "speech": 1, "answer": 1, "probabl": 1, "po": 1, "contrast": 1, "relat": 1, "tongu": 3, "mother": 3, "usag": 1, "french": 1, "part": 1, "granger": 1, "known": 1, "neg": 1, "modifi": 1, "possess": 1, "reveal": 1, "grammat": 1, "word": 1, "work": 1, "harder": 1, "across": 1, "item": 1, "anoth": 1, "allow": 1, "english": 1}, "marker": "(2001)", "article": "P13-1112", "vector_2": [12, 0.04195675509891121, 4, 2, 0, 0]}, {"label": "Neut", "current": "Most recent work in word segmentation of childdirected speech has operated within statistical optimization frameworks, particularly Bayesian approaches (Goldwater et al., 2009; Johnson and Goldwater, 2009).", "context": ["2 Related Work", "Most recent work in word segmentation of childdirected speech has operated within statistical optimization frameworks, particularly Bayesian approaches (Goldwater et al., 2009; Johnson and Goldwater, 2009).", "These models have established the state-of-the-art for the task of selecting appropriate word boundaries from a stream of unstructured phonemes."], "vector_1": {"oper": 1, "stream": 1, "within": 1, "particularli": 1, "phonem": 1, "establish": 1, "select": 1, "boundari": 1, "speech": 1, "approach": 1, "statist": 1, "optim": 1, "relat": 1, "framework": 1, "task": 1, "bayesian": 1, "segment": 1, "recent": 1, "appropri": 1, "word": 2, "unstructur": 1, "work": 2, "childdirect": 1, "stateoftheart": 1, "model": 1}, "marker": "(Goldwater et al., 2009", "article": "W10-2912", "vector_2": [1, 0.05914188640668691, 2, 1, 3, 0]}, {"label": "Neut", "current": "There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).", "context": ["3.6 Large Margin Classifiers", "There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).", "The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization."], "vector_1": {"quit": 1, "svm": 2, "linear": 2, "superior": 1, "perform": 1, "boost": 1, "winnow": 1, "maxim": 1, "classifi": 3, "separ": 1, "larg": 2, "perceptron": 1, "sampl": 1, "abil": 1, "margin": 3}, "marker": "(Zhang, 2000)", "article": "N04-1023", "vector_2": [4, 0.5118253019020795, 4, 1, 0, 0]}, {"label": "Pos", "current": "Therefore, we build lattices that encode the different reorderings for every training sentence, as described in Niehues et al (2009).", "context": ["139", "Therefore, we build lattices that encode the different reorderings for every training sentence, as described in Niehues et al (2009).", "Then we can not only extract phrase pairs from the monotone source path, but also from the reordered paths."], "vector_1": {"phrase": 1, "differ": 1, "sourc": 1, "everi": 1, "describ": 1, "sentenc": 1, "monoton": 1, "encod": 1, "al": 1, "also": 1, "train": 1, "niehu": 1, "build": 1, "lattic": 1, "pair": 1, "et": 1, "path": 2, "therefor": 1, "extract": 1, "reorder": 2}, "marker": "(2009)", "article": "W10-1719", "vector_2": [1, 0.47877464517372037, 1, 2, 4, 0]}, {"label": "Neut", "current": "Inverse Link-Frequency (ILF) is inspired by the term-frequency inverse document-frequency (tf-idf) heuristic (Salton and McGill, 1983) in which a term's weight is reduced as it is contained in more documents in the corpus.", "context": ["For instance, we consider a path connecting two nodes via a general article such as USA (connected to 322,000 articles) not nearly as indicative of a semantic relationship, as a path connecting them via a very specific concept, such as Hair Pin (only connected to 20 articles).", "Inverse Link-Frequency (ILF) is inspired by the term-frequency inverse document-frequency (tf-idf) heuristic (Salton and McGill, 1983) in which a term's weight is reduced as it is contained in more documents in the corpus.", "It is based on the idea that the more a term appears in documents across the corpus, the less it can discriminate any one of those documents."], "vector_1": {"corpu": 2, "semant": 1, "via": 2, "pin": 1, "less": 1, "inspir": 1, "idea": 1, "indic": 1, "two": 1, "hair": 1, "concept": 1, "connect": 4, "heurist": 1, "weight": 1, "usa": 1, "invers": 2, "articl": 3, "linkfrequ": 1, "document": 3, "across": 1, "node": 1, "reduc": 1, "nearli": 1, "relationship": 1, "gener": 1, "documentfrequ": 1, "base": 1, "consid": 1, "path": 2, "one": 1, "appear": 1, "ilf": 1, "term": 2, "specif": 1, "termfrequ": 1, "tfidf": 1, "discrimin": 1, "instanc": 1, "contain": 1}, "marker": "(Salton and McGill, 1983)", "article": "W10-3506", "vector_2": [27, 0.3683296196986291, 1, 2, 0, 0]}, {"label": "Pos", "current": "Another top ranked system is the one developed by (Yuret, 2004), which combines two Naive Bayes statistical models, one based on surrounding collocations and another one based on a bag of words around the target word.", "context": ["The performance of this system on the SENSEVAL-3 English all words data set was evaluated at 65.2%.", "Another top ranked system is the one developed by (Yuret, 2004), which combines two Naive Bayes statistical models, one based on surrounding collocations and another one based on a bag of words around the target word.", "The statistical models are built based on SemCor and WordNet, for an overall disambiguation accuracy of 64.1%."], "vector_1": {"set": 1, "evalu": 1, "naiv": 1, "rank": 1, "one": 3, "wordnet": 1, "colloc": 1, "develop": 1, "built": 1, "perform": 1, "anoth": 2, "top": 1, "system": 2, "accuraci": 1, "semcor": 1, "disambigu": 1, "around": 1, "sensev": 1, "overal": 1, "base": 3, "two": 1, "data": 1, "word": 3, "target": 1, "surround": 1, "bay": 1, "bag": 1, "combin": 1, "statist": 2, "english": 1, "model": 2}, "marker": "(Yuret, 2004)", "article": "P05-3014", "vector_2": [1, 0.26329151732377537, 1, 1, 0, 0]}, {"label": "Neut", "current": "More recently, Lignos (2010, 2011, 2012) further explored Yang's original algorithm, taking into account that function words should not be assumed to possess lexical stress cues.", "context": ["While the USC has been argued to be near-to-universal and follows from the \"culminative function of stress\" (Fromkin, 2001; Cutler, 2005), the high score Yang reported crucially depends on every word token carrying stress, including function words.", "More recently, Lignos (2010, 2011, 2012) further explored Yang's original algorithm, taking into account that function words should not be assumed to possess lexical stress cues.", "While his scores are in line with those reported by Yang, the importance of stress for this learner were more modest, providing a gain of around 2.5% (Lignos, 2011)."], "vector_1": {"origin": 1, "ligno": 1, "carri": 1, "learner": 1, "high": 1, "explor": 1, "follow": 1, "assum": 1, "line": 1, "usc": 1, "crucial": 1, "cue": 1, "score": 2, "includ": 1, "import": 1, "possess": 1, "take": 1, "function": 3, "everi": 1, "around": 1, "argu": 1, "lexic": 1, "modest": 1, "gain": 1, "report": 2, "depend": 1, "culmin": 1, "account": 1, "recent": 1, "stress": 4, "word": 3, "algorithm": 1, "provid": 1, "neartounivers": 1, "token": 1, "yang": 3}, "marker": "(2010, 2011, 2012)", "article": "Q14-1008", "vector_2": [4, 0.1531790383804302, 4, 3, 13, 0]}, {"label": "Neut", "current": "It is known that many simple parsing schemata can be expressed with stack based machines [32].", "context": ["This is indeed the case.", "It is known that many simple parsing schemata can be expressed with stack based machines [32].", "This is certainly the case for all left-to-right CF chart parsing schemata."], "vector_1": {"case": 2, "machin": 1, "chart": 1, "express": 1, "lefttoright": 1, "cf": 1, "inde": 1, "certainli": 1, "base": 1, "pars": 2, "schemata": 2, "known": 1, "mani": 1, "simpl": 1, "stack": 1}, "marker": "[32]", "article": "P89-1018", "vector_2": [1, 0.9274848717862993, 1, 2, 0, 0]}, {"label": "CoCo", "current": "For comparison, the FOL-based system reported in (Bos and Markert, 2006) attained a similarly high precision of 76% on RTE2 problems, but was able to make a positive prediction in only about 4% of cases.", "context": ["Relative to the Stanford RTE system, NatLog achieves high precision on its yes predictions-about 76% on the development set, and 68% on the test set-suggesting that hybridizing may be effective.", "For comparison, the FOL-based system reported in (Bos and Markert, 2006) attained a similarly high precision of 76% on RTE2 problems, but was able to make a positive prediction in only about 4% of cases.", "NatLog makes positive predictions far more often-at a rate of 18% on the development set, and 24% on the test set."], "vector_1": {"rte": 2, "set": 3, "predict": 2, "predictionsabout": 1, "high": 2, "oftenat": 1, "attain": 1, "develop": 2, "make": 2, "hybrid": 1, "system": 2, "far": 1, "abl": 1, "rel": 1, "similarli": 1, "test": 2, "folbas": 1, "natlog": 2, "may": 1, "ye": 1, "effect": 1, "report": 1, "setsuggest": 1, "case": 1, "comparison": 1, "stanford": 1, "rate": 1, "precis": 2, "achiev": 1, "posit": 2, "problem": 1}, "marker": "(Bos and Markert, 2006)", "article": "W07-1431", "vector_2": [1, 0.8610272939240159, 1, 3, 0, 0]}, {"label": "Neut", "current": "Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving.", "context": ["In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001).", "Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving.", "Therefore, we developed a model for C-test difficulty that incorporates features from both processing levels (see Figure 4)."], "vector_1": {"featur": 1, "process": 2, "share": 1, "terminolog": 1, "see": 1, "establish": 1, "develop": 1, "learner": 1, "detail": 1, "figur": 1, "ctest": 3, "therefor": 1, "analys": 1, "strategi": 3, "difficulti": 1, "solv": 2, "categor": 1, "discuss": 1, "requir": 1, "success": 1, "level": 1, "microlevel": 1, "macrolevel": 1, "incorpor": 1, "psycholinguist": 1, "model": 1, "order": 1}, "marker": "(Sigott, 2006", "article": "Q14-1040", "vector_2": [8, 0.3302510976670781, 3, 1, 7, 0]}, {"label": "Neut", "current": "This could also be an artifact of the difficulty of assigning labels using Blooms Taxonomy, as has been demonstrated even among a group of subject-matter experts (van Hoeij et al., 2004) These results seem to demonstrate competing cognitive demands, operating in parallel.", "context": ["This is to be expected, as there are many dimensions to each of Blooms tasks, and each dimension could have greater or lesser effects on pauses within typing.", "This could also be an artifact of the difficulty of assigning labels using Blooms Taxonomy, as has been demonstrated even among a group of subject-matter experts (van Hoeij et al., 2004) These results seem to demonstrate competing cognitive demands, operating in parallel.", "The canonical theory of MWE production holds that MWEs are retrieved as a single unit."], "vector_1": {"dimens": 2, "among": 1, "paus": 1, "within": 1, "canon": 1, "expect": 1, "taxonomi": 1, "seem": 1, "cognit": 1, "unit": 1, "even": 1, "use": 1, "group": 1, "subjectmatt": 1, "expert": 1, "label": 1, "also": 1, "compet": 1, "mwe": 2, "type": 1, "singl": 1, "difficulti": 1, "product": 1, "greater": 1, "oper": 1, "effect": 1, "artifact": 1, "theori": 1, "demand": 1, "hold": 1, "bloom": 2, "parallel": 1, "demonstr": 2, "task": 1, "retriev": 1, "could": 2, "lesser": 1, "mani": 1, "assign": 1, "result": 1}, "marker": "(van Hoeij et al., 2004)", "article": "W15-0914", "vector_2": [11, 0.7982474016710821, 1, 1, 0, 0]}, {"label": "Neut", "current": "This also leads to connections with the work on partial evaluation [8].", "context": ["More importantly, our work on the parsing of incomplete sentences [16] has exhibited the fundamental character of our grammatical view of shared forests: when parsing the completely unknown sentence, the shared forest obtained is precisely the complete grammar of the analyzed language.", "This also leads to connections with the work on partial evaluation [8].", "3 Implementation and Experimental Results"], "vector_1": {"exhibit": 1, "partial": 1, "evalu": 1, "share": 2, "obtain": 1, "incomplet": 1, "connect": 1, "analyz": 1, "languag": 1, "result": 1, "lead": 1, "unknown": 1, "also": 1, "experiment": 1, "forest": 2, "complet": 2, "importantli": 1, "sentenc": 2, "pars": 2, "grammat": 1, "grammar": 1, "work": 2, "charact": 1, "precis": 1, "fundament": 1, "implement": 1, "view": 1}, "marker": "[8]", "article": "P89-1018", "vector_2": [24, 0.46628330714452876, 2, 1, 0, 0]}, {"label": "Neut", "current": "As Sag et al (2001b) suggest, it is important to find the right balance between symbolic and statistical approaches.", "context": ["They are language dependent and not flexible enough to cope with complex structures of MWEs.", "As Sag et al (2001b) suggest, it is important to find the right balance between symbolic and statistical approaches.", "In this paper, we propose a new approach to MWEs extraction using semantic field information."], "vector_1": {"semant": 1, "right": 1, "sag": 1, "al": 1, "paper": 1, "et": 1, "flexibl": 1, "extract": 1, "find": 1, "languag": 1, "use": 1, "depend": 1, "suggest": 1, "field": 1, "enough": 1, "complex": 1, "mwe": 2, "new": 1, "approach": 2, "statist": 1, "cope": 1, "symbol": 1, "import": 1, "structur": 1, "inform": 1, "balanc": 1, "propos": 1}, "marker": "(2001b)", "article": "W03-1807", "vector_2": [2, 0.20400459543738717, 1, 3, 0, 0]}, {"label": "Neut", "current": "Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).", "context": ["2 Related Work and Overview", "Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).", "Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage."], "vector_1": {"wikipedia": 3, "semant": 3, "larger": 1, "comput": 2, "creat": 1, "overview": 1, "ir": 1, "cost": 1, "cognit": 1, "network": 1, "memori": 1, "activ": 1, "due": 1, "coverag": 2, "linkbas": 1, "spread": 1, "knowledg": 1, "esa": 1, "appli": 1, "hurdl": 1, "method": 2, "analysi": 1, "regard": 1, "variou": 1, "relat": 1, "wordnetbas": 1, "foremost": 1, "knowledgebas": 1, "biggest": 1, "base": 2, "theori": 1, "although": 1, "arguabl": 1, "associ": 1, "model": 1, "earlier": 1, "recent": 1, "measur": 1, "success": 1, "level": 1, "outperform": 1, "work": 1, "explicit": 1, "conceptu": 2, "adequ": 1, "text": 1, "wlm": 1, "found": 1, "sa": 1, "similar": 1}, "marker": "(Crestani, 1997)", "article": "W10-3506", "vector_2": [13, 0.11850900713773178, 6, 3, 0, 0]}, {"label": "Neut", "current": "For these experiments, we therefore hand-labeled a small corpus following the Penn Chinese Propbank labeling guidelines (Xue, 2002).", "context": ["Although a project to produce a Chinese PropBank is underway (Xue and Palmer 2003), this data is not expected to be available for another year.", "For these experiments, we therefore hand-labeled a small corpus following the Penn Chinese Propbank labeling guidelines (Xue, 2002).", "In this section, we first describe the semantic roles we used in the annotation and then introduce the data for our experiments."], "vector_1": {"corpu": 1, "semant": 1, "guidelin": 1, "expect": 1, "year": 1, "follow": 1, "use": 1, "describ": 1, "anoth": 1, "section": 1, "label": 1, "avail": 1, "role": 1, "penn": 1, "therefor": 1, "introduc": 1, "propbank": 2, "chines": 2, "palmer": 1, "although": 1, "data": 2, "experi": 2, "annot": 1, "project": 1, "xue": 1, "underway": 1, "small": 1, "handlabel": 1, "produc": 1, "first": 1}, "marker": "(Xue, 2002)", "article": "N04-1032", "vector_2": [2, 0.09160229748465043, 1, 1, 0, 0]}, {"label": "CoCo", "current": "Though existing studies suggest that changing the tag set of the original corpus, such as splitting of O tags, can contribute to the performances of named entity recognizers (Peshkin and Pfefer, 2003), our system uses the original tagset of the training data, except that the 'BOS' label is added to represent the state before the beginning of sentences.", "context": ["Table 1 shows the state transition table of our MEMM model.", "Though existing studies suggest that changing the tag set of the original corpus, such as splitting of O tags, can contribute to the performances of named entity recognizers (Peshkin and Pfefer, 2003), our system uses the original tagset of the training data, except that the 'BOS' label is added to represent the state before the beginning of sentences.", "Probability of state transition to the i-th label of a sentence is calculated by the following formula:"], "vector_1": {"origin": 2, "corpu": 1, "set": 1, "ad": 1, "ith": 1, "show": 1, "transit": 2, "except": 1, "tag": 2, "exist": 1, "tabl": 2, "follow": 1, "entiti": 1, "use": 1, "perform": 1, "suggest": 1, "system": 1, "label": 2, "state": 3, "split": 1, "tagset": 1, "formula": 1, "probabl": 1, "begin": 1, "recogn": 1, "contribut": 1, "though": 1, "memm": 1, "sentenc": 2, "bo": 1, "train": 1, "studi": 1, "repres": 1, "data": 1, "model": 1, "name": 1, "calcul": 1, "chang": 1}, "marker": "(Peshkin and Pfefer, 2003)", "article": "W07-1033", "vector_2": [4, 0.3330167774612219, 1, 1, 0, 0]}, {"label": "CoCo", "current": "  While keystroke dynamics is concerned with a number of timing metrics, such as key holds (h in   We also did not remove any outliers, although this is common in keystroke dynamics (Epp et al., 2011; Zhong et al., 2012).", "context": ["4 Experiments", "  While keystroke dynamics is concerned with a number of timing metrics, such as key holds (h in   We also did not remove any outliers, although this is common in keystroke dynamics (Epp et al., 2011; Zhong et al., 2012).", "We feel it is difficult-toimpossible to discriminate between a true pause that is indicative of a subjects increased cognitive effort and any other type of pause, such as those caused by distraction or physical fatigue."], "vector_1": {"key": 1, "paus": 2, "feel": 1, "metric": 1, "concern": 1, "number": 1, "indic": 1, "increas": 1, "fatigu": 1, "cognit": 1, "dynam": 2, "subject": 1, "difficulttoimposs": 1, "effort": 1, "also": 1, "experi": 1, "type": 1, "outlier": 1, "although": 1, "keystrok": 2, "hold": 1, "true": 1, "physic": 1, "h": 1, "remov": 1, "distract": 1, "caus": 1, "discrimin": 1, "common": 1, "time": 1}, "marker": "(Epp et al., 2011", "article": "W15-0914", "vector_2": [4, 0.49773792541267575, 2, 1, 0, 0]}, {"label": "CoCo", "current": "For example, van Lohuizen (1999) reports a 1.8x speedup, while Manousopoulou et al (1997) claims a 7-8x speedup.", "context": ["The parallel parsers in past work are implemented on multicore systems, where the limited parallelization possibilities provided by the systems restrict the speedups that can be achieved.", "For example, van Lohuizen (1999) reports a 1.8x speedup, while Manousopoulou et al (1997) claims a 7-8x speedup.", "In contrast, our parallel parser is implemented on a manycore system with an abundant number of threads and pro"], "vector_1": {"claim": 1, "abund": 1, "pro": 1, "parser": 2, "speedup": 3, "restrict": 1, "et": 1, "van": 1, "system": 3, "multicor": 1, "contrast": 1, "manousopoul": 1, "al": 1, "lohuizen": 1, "report": 1, "number": 1, "parallel": 3, "past": 1, "thread": 1, "possibl": 1, "provid": 1, "work": 1, "achiev": 1, "exampl": 1, "limit": 1, "x": 2, "implement": 2, "manycor": 1}, "marker": "(1997)", "article": "W11-2921", "vector_2": [14, 0.880153930731171, 2, 3, 0, 0]}, {"label": "Pos", "current": "Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).", "context": ["A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle.", "Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).", "Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years."], "vector_1": {"represent": 1, "rang": 1, "explor": 1, "approxim": 1, "monoton": 1, "semant": 2, "confound": 1, "particularli": 1, "confirm": 1, "impoverish": 1, "year": 1, "rabi": 1, "extract": 1, "deepbutbrittl": 1, "involv": 1, "use": 1, "acquir": 1, "p": 1, "broad": 1, "reli": 1, "overlap": 1, "patternbas": 1, "imprecis": 1, "approach": 2, "method": 1, "match": 1, "polar": 1, "infect": 1, "relat": 1, "effect": 1, "lexic": 1, "spectrum": 1, "fairli": 1, "broadli": 1, "robust": 1, "case": 1, "measur": 1, "shallowbutrobust": 1, "success": 1, "indigen": 1, "ubiquitu": 1, "past": 1, "predicateargu": 1, "structur": 1, "infer": 1, "easili": 1, "context": 1, "neg": 1}, "marker": "(Hickl et al., 2006)", "article": "W07-1431", "vector_2": [1, 0.04954738718968591, 3, 1, 0, 0]}, {"label": "Neut", "current": "There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).", "context": ["They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).", "There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).", "(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data."], "vector_1": {"semant": 2, "show": 2, "process": 1, "eg": 1, "signific": 1, "extract": 1, "leverag": 1, "use": 2, "ratio": 1, "compar": 1, "perform": 2, "also": 1, "speech": 2, "rel": 1, "approach": 1, "resourc": 1, "broadcast": 1, "base": 1, "news": 1, "data": 1, "task": 1, "retriev": 1, "keyword": 2, "frequenc": 1, "work": 1, "yield": 1, "combin": 1, "verif": 1, "idf": 1, "improv": 2, "similar": 1}, "marker": "(Munteanu et al., 2007", "article": "N09-1070", "vector_2": [2, 0.2224622030237581, 6, 1, 0, 0]}, {"label": "Neut", "current": "Some of the researchers admit avoiding qualitative evaluation due to the lack of resources that would have made it possible (Mille and Wanner, 2008).", "context": ["Given that no reliable evaluation metrics exist so far for text simplification we performed a preliminary qualitative evaluation of our methodology based on human judgment (as in all cited works on claim simplification).", "Some of the researchers admit avoiding qualitative evaluation due to the lack of resources that would have made it possible (Mille and Wanner, 2008).", "The number of patents the authors use to evaluate their methodologies might seem quite limited, e.g., (Mille and Wanner, 2008) report evaluation results based on 30 patents; in (Bouayad-Agha et al.)"], "vector_1": {"claim": 1, "evalu": 5, "text": 1, "metric": 1, "lack": 1, "number": 1, "possibl": 1, "exist": 1, "result": 1, "human": 1, "et": 1, "seem": 1, "quit": 1, "given": 1, "would": 1, "author": 1, "perform": 1, "avoid": 1, "due": 1, "research": 1, "might": 1, "cite": 1, "resourc": 1, "preliminari": 1, "far": 1, "eg": 1, "methodolog": 2, "use": 1, "al": 1, "base": 2, "report": 1, "judgment": 1, "made": 1, "simplif": 2, "reliabl": 1, "work": 1, "qualit": 2, "patent": 2, "admit": 1, "limit": 1, "bouayadagha": 1}, "marker": "(Mille and Wanner, 2008)", "article": "W14-5605", "vector_2": [6, 0.8527112103980503, 2, 3, 0, 0]}, {"label": "Neut", "current": "(2001b) suggest that MWEs can roughly be defined as \"idiosyncratic interpretations that cross word boundaries (or spaces)\".", "context": ["Sag et el.", "(2001b) suggest that MWEs can roughly be defined as \"idiosyncratic interpretations that cross word boundaries (or spaces)\".", "Biber et al (2003) describe MWEs as lexical bundles, which they go on to define as combinations of words that can be repeated frequently and tend to be used frequently by many different speakers/writers within a register."], "vector_1": {"el": 1, "within": 1, "sag": 1, "al": 1, "speakerswrit": 1, "go": 1, "et": 2, "tend": 1, "use": 1, "describ": 1, "space": 1, "suggest": 1, "boundari": 1, "regist": 1, "cross": 1, "biber": 1, "mwe": 2, "idiosyncrat": 1, "repeat": 1, "bundl": 1, "differ": 1, "lexic": 1, "interpret": 1, "word": 2, "defin": 2, "combin": 1, "mani": 1, "roughli": 1, "frequent": 2}, "marker": "(2001b)", "article": "W03-1807", "vector_2": [2, 0.6012227145905137, 2, 3, 0, 0]}, {"label": "CoCo", "current": "Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners.", "context": ["The word class has been studied as a difficulty indicator by several researchers but with mixed results.", "Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners.", "Sigott (1995) could not confirm any effect of the word class on C-test difficulty."], "vector_1": {"preposit": 1, "claim": 1, "often": 1, "brown": 1, "kleinbraley": 1, "indic": 1, "result": 1, "find": 1, "sever": 1, "confirm": 1, "learner": 1, "research": 1, "mix": 1, "easier": 1, "function": 1, "difficulti": 2, "sigott": 1, "effect": 1, "solv": 1, "class": 2, "ctest": 1, "word": 3, "could": 1, "harder": 1, "studi": 1}, "marker": "(1996)", "article": "Q14-1040", "vector_2": [18, 0.407522813265079, 3, 2, 9, 0]}, {"label": "Neut", "current": "Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.", "context": ["The corpus we use to evaluate it is the same corpus used by Yang (2004).", "Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.", "We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word."], "vector_1": {"corpu": 3, "evalu": 1, "obtain": 1, "phonet": 1, "utter": 1, "extract": 1, "children": 1, "sarah": 1, "use": 3, "carnegi": 1, "pronounc": 1, "data": 2, "three": 1, "version": 1, "cmudict": 1, "brown": 1, "mellon": 1, "adult": 1, "dictionari": 1, "child": 1, "pronunci": 1, "transcript": 1, "word": 2, "consist": 1, "eve": 1, "yang": 1, "adam": 1, "first": 1}, "marker": "(MacWhinney, 2000)", "article": "W10-2912", "vector_2": [10, 0.5840825039838841, 4, 1, 0, 0]}, {"label": "Neut", "current": "As expounded by Salthouse (1986), a typist must simultaneously employ multiple cognitive and motor schemata, often with a formidable amount of noise between signals.", "context": ["Beginning in the 1980s (Rumelhart and Norman, 1982), investigators used typing data to construct cognitive and motor models of language production.", "As expounded by Salthouse (1986), a typist must simultaneously employ multiple cognitive and motor schemata, often with a formidable amount of noise between signals.", "Translating from lexical retrieval into physical action is a non-trivial task, which involves multiple pipelines that can be occluded, and also result in mixed up signals."], "vector_1": {"often": 1, "product": 1, "salthous": 1, "result": 1, "motor": 2, "occlud": 1, "simultan": 1, "cognit": 2, "languag": 1, "involv": 1, "use": 1, "multipl": 2, "nois": 1, "mix": 1, "amount": 1, "construct": 1, "formid": 1, "also": 1, "nontrivi": 1, "type": 1, "begin": 1, "investig": 1, "lexic": 1, "pipelin": 1, "translat": 1, "data": 1, "physic": 1, "must": 1, "expound": 1, "task": 1, "retriev": 1, "signal": 2, "employ": 1, "s": 1, "schemata": 1, "action": 1, "model": 1, "typist": 1}, "marker": "(1986)", "article": "W15-0914", "vector_2": [29, 0.2879559812512737, 2, 1, 0, 0]}, {"label": "Neut", "current": "These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).", "context": ["Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).", "(Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT."], "vector_1": {"dictionari": 1, "individu": 1, "focus": 1, "one": 1, "bilingu": 1, "leverag": 2, "supervis": 1, "use": 1, "perform": 1, "data": 1, "smt": 2, "solut": 1, "construct": 1, "label": 2, "better": 2, "also": 1, "consecut": 1, "approach": 1, "unlabel": 1, "either": 1, "machin": 1, "map": 2, "form": 1, "sequenc": 1, "chines": 2, "previou": 1, "bia": 1, "train": 1, "translat": 1, "although": 1, "dataset": 1, "segment": 3, "model": 2, "maximummatch": 1, "word": 2, "work": 1, "charact": 1, "achiev": 1, "statist": 1, "english": 1, "studi": 1, "toward": 1}, "marker": "(Peng et al., 2004)", "article": "D15-1142", "vector_2": [11, 0.29941432338938934, 8, 3, 2, 0]}, {"label": "Neut", "current": "Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces (Baroni and Lenci, 2010).", "context": ["Abstract", "Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces (Baroni and Lenci, 2010).", "However, they are also sparse, with resulting reliability and coverage problems."], "vector_1": {"wide": 1, "use": 1, "syntaxbas": 1, "space": 2, "spars": 1, "abstract": 1, "semant": 1, "lexic": 1, "also": 1, "vector": 1, "versatil": 1, "reliabl": 1, "problem": 1, "coverag": 1, "howev": 1, "wordbas": 1, "result": 1}, "marker": "(Baroni and Lenci, 2010)", "article": "P13-2128", "vector_2": [3, 0.02339986235375086, 1, 3, 0, 0]}, {"label": "Neut", "current": "The objective of the Recognizing Textual Entailment Challenge is determining whether the meaning of the Hypothesis (H) can be inferred from a text (T) (Ido Dagan et al., 2006).", "context": ["1 Introduction", "The objective of the Recognizing Textual Entailment Challenge is determining whether the meaning of the Hypothesis (H) can be inferred from a text (T) (Ido Dagan et al., 2006).", "This challenge has been organized by NIST in recent years."], "vector_1": {"recogn": 1, "entail": 1, "hypothesi": 1, "whether": 1, "h": 1, "challeng": 2, "object": 1, "year": 1, "textual": 1, "recent": 1, "determin": 1, "text": 1, "organ": 1, "mean": 1, "infer": 1, "nist": 1, "introduct": 1}, "marker": "(Ido Dagan et al., 2006)", "article": "W10-1609", "vector_2": [4, 0.05126655310731359, 1, 1, 0, 0]}, {"label": "Neut", "current": "These findings may not hold if the level of bracketing available does not adequately constrain the parses considered - see Hwa (1999) for a related investigation with EM.", "context": ["The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents.", "These findings may not hold if the level of bracketing available does not adequately constrain the parses considered - see Hwa (1999) for a related investigation with EM.", "In future work we intend to further investigate the problem of tuning to a new domain, given that minimal manual effort is a major priority."], "vector_1": {"em": 1, "domain": 1, "constitu": 1, "major": 1, "high": 1, "see": 1, "adequ": 1, "proport": 1, "find": 1, "compat": 1, "given": 1, "nois": 1, "minim": 1, "avail": 1, "confidencebas": 1, "futur": 1, "new": 1, "correct": 1, "introduc": 1, "prioriti": 1, "investig": 2, "intend": 1, "deriv": 2, "may": 1, "relat": 1, "hwa": 1, "pars": 1, "consid": 1, "hold": 1, "effort": 1, "problem": 1, "tune": 1, "outweigh": 1, "success": 1, "level": 2, "work": 1, "manual": 1, "method": 1, "benefit": 1, "bracket": 2, "incorpor": 1, "contain": 1, "constrain": 1}, "marker": "(1999)", "article": "W07-2203", "vector_2": [8, 0.9520358505759199, 1, 1, 0, 0]}, {"label": "Neut", "current": "A LexicalUnit is a word or a phrase being defined, also called definiendum (Landau, 1984).", "context": ["A dictionary entry, called LexicalEntry, is a 2-tuple <LexicalUnit, Definition>.", "A LexicalUnit is a word or a phrase being defined, also called definiendum (Landau, 1984).", "A list of entries sorted by the LexicalUnit is called a lexicon or a dictionary."], "vector_1": {"phrase": 1, "sort": 1, "lexicon": 1, "lexicalentri": 1, "lexicalunit": 3, "also": 1, "entri": 2, "definit": 1, "definiendum": 1, "defin": 1, "list": 1, "call": 3, "dictionari": 2, "tupl": 1, "word": 1}, "marker": "(Landau, 1984)", "article": "N13-1057", "vector_2": [29, 0.314728981588773, 1, 1, 0, 0]}, {"label": "Pos", "current": "Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.", "context": ["Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).", "Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.", "Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006)."], "vector_1": {"function": 1, "develop": 1, "syntact": 1, "also": 1, "perform": 1, "object": 1, "smith": 2, "procedur": 1, "learnabl": 1, "eisner": 2, "train": 1, "introduc": 1, "empir": 1, "improv": 1, "bayesian": 1, "new": 2, "model": 1, "approach": 1, "pcfg": 1, "propos": 1}, "marker": "(2005)", "article": "P08-1100", "vector_2": [3, 0.04740169726476654, 7, 2, 1, 0]}, {"label": "Neut", "current": "Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.", "context": ["4 Conclusion", "Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.", "Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html)."], "vector_1": {"domain": 1, "lexical": 1, "nouvel": 1, "ce": 2, "vu": 3, "et": 3, "nombr": 1, "facil": 1, "ont": 1, "causer": 1, "le": 6, "jour": 1, "autr": 1, "semantiqu": 1, "la": 2, "avec": 1, "gnralis": 1, "cogalex": 1, "vectoriel": 1, "silenc": 1, "pa": 1, "soumiss": 1, "sur": 1, "francophon": 1, "distributionel": 1, "ressourc": 1, "normment": 1, "memoir": 1, "contrast": 1, "pu": 1, "travail": 1, "consacr": 1, "nest": 1, "de": 4, "succ": 1, "comm": 1, "voisin": 1, "savoir": 1, "vivier": 1, "qui": 1, "nou": 1, "vnement": 1, "du": 3, "problem": 1, "conclus": 1, "faibl": 1, "distributionnel": 1, "mond": 1, "surpri": 1, "dun": 1, "httppagepersolifunivmrsfrmichaelzockcogalexivcogalexwebpageindexhtml": 1, "car": 1, "od": 1, "tre": 2, "thori": 1, "method": 1, "etc": 1, "dynamism": 1, "sont": 1, "tion": 1, "se": 1, "il": 2}, "marker": "(Widdows, 2004, ", "article": "W14-6700", "vector_2": [10, 0.9209515096065873, 4, 2, 0, 0]}, {"label": "CoCo", "current": "Specifically, we want to employ our feature set in a multi-task kernel setting, similar to the one proposed by Cohn and Specia (2013).", "context": ["In the future, we plan to further investigate these models by devising more advanced kernels and feature selection methods.", "Specifically, we want to employ our feature set in a multi-task kernel setting, similar to the one proposed by Cohn and Specia (2013).", "These kernels have the power to model inter-annotator variance and noise, which can lead to better results in the prediction of post-editing time."], "vector_1": {"kernel": 3, "set": 2, "cohn": 1, "predict": 1, "one": 1, "featur": 2, "result": 1, "want": 1, "postedit": 1, "multitask": 1, "select": 1, "nois": 1, "lead": 1, "better": 1, "futur": 1, "devis": 1, "method": 1, "investig": 1, "power": 1, "advanc": 1, "plan": 1, "varianc": 1, "specia": 1, "specif": 1, "interannot": 1, "employ": 1, "time": 1, "model": 2, "similar": 1, "propos": 1}, "marker": "(2013)", "article": "W13-2241", "vector_2": [0, 0.9508205841446453, 1, 3, 8, 0]}, {"label": "Neut", "current": "Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).", "context": ["There is also a rich body of theoretical work on learning latent-variable models.", "Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).", "But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV."], "vector_1": {"em": 1, "certain": 1, "except": 1, "littl": 1, "famili": 1, "complex": 1, "special": 1, "provabl": 1, "capac": 2, "theoret": 2, "also": 1, "discret": 1, "pcfg": 1, "rich": 1, "dmv": 1, "dasgupta": 1, "gener": 2, "weak": 1, "bodi": 1, "hmm": 1, "understand": 1, "strong": 1, "constrain": 1, "schulman": 1, "latentvari": 1, "term": 2, "algorithm": 1, "work": 1, "alon": 1, "hiddenvari": 1, "learn": 2, "let": 1, "model": 3, "other": 1}, "marker": "Adriaans, 1999)", "article": "P08-1100", "vector_2": [9, 0.9644994421340907, 6, 1, 0, 0]}, {"label": "CoCo", "current": "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "context": ["The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\"", "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level."], "vector_1": {"featur": 1, "profici": 1, "focus": 1, "connect": 1, "touch": 1, "skill": 1, "languag": 3, "involv": 1, "rather": 1, "question": 1, "construct": 1, "valid": 1, "advoc": 1, "factor": 1, "test": 2, "instead": 1, "analys": 1, "tightli": 1, "difficulti": 2, "gener": 1, "argu": 1, "gap": 1, "vocabulari": 1, "earliest": 1, "ctest": 4, "reduc": 1, "measur": 2, "search": 1, "grammar": 1, "level": 3, "aim": 1, "paragraph": 1, "combin": 1, "determin": 1, "model": 1, "other": 1}, "marker": "(Chapelle, 1994", "article": "Q14-1040", "vector_2": [20, 0.1619488901928252, 6, 1, 0, 0]}, {"label": "Neut", "current": "The final result of p = 0.72 is equal to that reported for ESA (Gabrilovich and Markovitch, 2007), while requiring less than 10% of the Wikipedia database required for ESA.", "context": ["Finally, we show that using our best Wikipedia-based method to augment the cosine VSM method using tf-idf, leads to the best results.", "The final result of p = 0.72 is equal to that reported for ESA (Gabrilovich and Markovitch, 2007), while requiring less than 10% of the Wikipedia database required for ESA.", "Table 4 summarises the document-similarity results."], "vector_1": {"show": 1, "summaris": 1, "result": 3, "tabl": 1, "vsm": 1, "best": 2, "use": 2, "lead": 1, "databas": 1, "less": 1, "wikipedia": 1, "wikipediabas": 1, "esa": 2, "final": 2, "documentsimilar": 1, "report": 1, "requir": 2, "augment": 1, "equal": 1, "method": 2, "tfidf": 1, "p": 1, "cosin": 1}, "marker": "(Gabrilovich and Markovitch, 2007)", "article": "W10-3506", "vector_2": [3, 0.9856867706484383, 1, 6, 1, 0]}, {"label": "Pos", "current": "We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001); allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space.", "context": ["We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999), a memory-based learner (MBL), for both phases.", "We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001); allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space.", "We mostly use the default settings of TiMBL-the IB1 learning algorithm and overlap comparison metric between instances-and experiment with different values of k. For prediction of phenomenon level (phase 1) and learner level (phase 2), the system is trained on data from placement exams previously collected in a Hebrew language program, as described in sec."], "vector_1": {"mbl": 2, "set": 2, "phenomenon": 1, "predict": 1, "well": 1, "metric": 1, "featur": 1, "differ": 1, "hebrew": 1, "suffer": 1, "languag": 1, "previous": 1, "use": 4, "ib": 1, "describ": 1, "space": 1, "learner": 2, "system": 1, "overlap": 1, "fragment": 1, "program": 1, "timbl": 2, "experi": 1, "timblth": 1, "exam": 1, "numer": 1, "shown": 1, "train": 1, "mostli": 1, "instancesand": 1, "phase": 3, "data": 2, "class": 1, "valu": 1, "comparison": 1, "placement": 1, "algorithm": 1, "level": 2, "default": 1, "k": 1, "work": 1, "memorybas": 1, "collect": 1, "allow": 1, "learn": 1, "small": 1, "sec": 1, "textbas": 1}, "marker": "(Banko and Brill, 2001)", "article": "W12-2011", "vector_2": [11, 0.7265308608142261, 3, 1, 0, 0]}, {"label": "Weak", "current": "This creates an opportunity for losses that did not exist in the experiments of Schwartz et al (2012).", "context": ["Here, since the main concern is the design of a parsing representation that is meant simply as an intermediary step, all output has to be evaluated against the same gold standard.", "This creates an opportunity for losses that did not exist in the experiments of Schwartz et al (2012).", "7 Conclusion"], "vector_1": {"represent": 1, "simpli": 1, "gold": 1, "schwartz": 1, "al": 1, "design": 1, "opportun": 1, "et": 1, "exist": 1, "concern": 1, "evalu": 1, "intermediari": 1, "creat": 1, "experi": 1, "meant": 1, "standard": 1, "step": 1, "pars": 1, "sinc": 1, "conclus": 1, "loss": 1, "main": 1, "output": 1}, "marker": "(2012)", "article": "W15-2134", "vector_2": [3, 0.9586097444850024, 1, 6, 0, 0]}, {"label": "Neut", "current": "learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.", "context": ["The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95-104, Montreal, Canada, June 3-8, 2012. c2012 Association for Computational Linguistics", "learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.", "For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses."], "vector_1": {"nlp": 2, "comput": 1, "deal": 1, "obtain": 1, "exist": 1, "canada": 1, "use": 1, "illform": 1, "gold": 1, "innov": 1, "perform": 1, "associ": 1, "learner": 2, "workshop": 1, "build": 1, "th": 1, "answer": 1, "analys": 1, "adapt": 1, "analysi": 1, "applic": 1, "product": 1, "resourc": 1, "optim": 1, "tool": 1, "june": 1, "standard": 1, "reason": 1, "educ": 1, "data": 1, "c": 1, "potenti": 1, "linguist": 3, "page": 1, "montreal": 1}, "marker": "Itai and Wintner, 2008)", "article": "W12-2011", "vector_2": [4, 0.09415060442739259, 3, 1, 2, 0]}, {"label": "Pos", "current": "The models we examine are derived from the collocational model of Johnson and Goldwater (2009) by varying three parameters, resulting in 6 models: two baselines that do not take advantage of stress cues and either do or do not use phonotactics, as described in Section 3.2; and four stress models that differ with respect to the use of phonotactics, and as to whether they embody the Unique Stress Constraint introduced by Yang (2004).", "context": ["We give an intuitive description of the mathematical background of Adaptor Grammars in 3.1, referring the reader to Johnson et al (2007) for technical details.", "The models we examine are derived from the collocational model of Johnson and Goldwater (2009) by varying three parameters, resulting in 6 models: two baselines that do not take advantage of stress cues and either do or do not use phonotactics, as described in Section 3.2; and four stress models that differ with respect to the use of phonotactics, and as to whether they embody the Unique Stress Constraint introduced by Yang (2004).", "We describe these models in section 3.3."], "vector_1": {"background": 1, "advantag": 1, "give": 1, "al": 1, "three": 1, "four": 1, "differ": 1, "result": 1, "respect": 1, "et": 1, "phonotact": 2, "vari": 1, "paramet": 1, "colloc": 1, "goldwat": 1, "section": 2, "detail": 1, "cue": 1, "mathemat": 1, "take": 1, "reader": 1, "baselin": 1, "introduc": 1, "refer": 1, "deriv": 1, "adaptor": 1, "use": 2, "intuit": 1, "two": 1, "examin": 1, "technic": 1, "describ": 2, "stress": 3, "grammar": 1, "johnson": 2, "constraint": 1, "whether": 1, "descript": 1, "uniqu": 1, "embodi": 1, "yang": 1, "either": 1, "model": 5}, "marker": "(2009)", "article": "Q14-1008", "vector_2": [5, 0.21697068747363982, 3, 11, 20, 0]}, {"label": "Neut", "current": "(These issues, and related themes of cut-and-paste authorship, ownership, and plagiarism, are explored in Wilks [2003].)", "context": ["In the text domain, organizations such as Reuters produce news feeds that are typically adapted to the style of a particular newspaper and then republished: Is each republication a new writing event?", "(These issues, and related themes of cut-and-paste authorship, ownership, and plagiarism, are explored in Wilks [2003].)", "4.2 Technology"], "vector_1": {"feed": 1, "domain": 1, "text": 1, "newspap": 1, "explor": 1, "news": 1, "cutandpast": 1, "event": 1, "style": 1, "reuter": 1, "write": 1, "theme": 1, "adapt": 1, "new": 1, "relat": 1, "ownership": 1, "particular": 1, "authorship": 1, "republish": 1, "organ": 1, "technolog": 1, "issu": 1, "republ": 1, "plagiar": 1, "wilk": 1, "typic": 1, "produc": 1}, "marker": "[2003]", "article": "J03-3001", "vector_2": [0, 0.6479001594815583, 1, 1, 4, 1]}, {"label": "Neut", "current": "According to Wason and Reich (1979) (as explained in more detail below), sentences such as (2) are actually nonsensical, but people coerce them into a sensible reading by reversing the interpretation.", "context": ["First, the contradictory nature of the possible meanings has been explained in terms of pragmatic factors concerning the relevant presuppositions of the sentences.", "According to Wason and Reich (1979) (as explained in more detail below), sentences such as (2) are actually nonsensical, but people coerce them into a sensible reading by reversing the interpretation.", "One of our goals in this work is to explore whether computational linguistic techniques-specifically automatic corpus analysis drawing on lexical resources-can help to elucidate the factors influencing interpretation of such sentences across a collection of actual usages."], "vector_1": {"corpu": 1, "pragmat": 1, "comput": 1, "help": 1, "natur": 1, "draw": 1, "automat": 1, "one": 1, "presupposit": 1, "explor": 1, "work": 1, "concern": 1, "coerc": 1, "goal": 1, "nonsens": 1, "influenc": 1, "explain": 2, "detail": 1, "relev": 1, "interpret": 2, "factor": 2, "across": 1, "analysi": 1, "accord": 1, "sensibl": 1, "contradictori": 1, "techniquesspecif": 1, "read": 1, "sentenc": 3, "usag": 1, "lexic": 1, "wason": 1, "peopl": 1, "term": 1, "linguist": 1, "actual": 2, "elucid": 1, "possibl": 1, "whether": 1, "revers": 1, "collect": 1, "reich": 1, "resourcescan": 1, "mean": 1, "first": 1}, "marker": "(1979)", "article": "W10-2109", "vector_2": [31, 0.05666347354059624, 1, 7, 0, 0]}, {"label": "Neut", "current": "Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].", "context": [" Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].", "Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].", "Extracting word correspondences [Gale and Church, 1991a]."], "vector_1": {"word": 1, "sentenc": 1, "align": 1, "correspond": 1, "automat": 1, "disambigu": 1, "wordsens": 1, "extract": 1}, "marker": "Brown et al., 1991b, ", "article": "P93-1003", "vector_2": [2, 0.05604523532346582, 7, 10, 0, 0]}, {"label": "Neut", "current": "All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).", "context": ["We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings.", "All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).", "The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus."], "vector_1": {"corpu": 2, "telephon": 1, "natur": 1, "obtain": 1, "system": 1, "topic": 1, "rate": 1, "extract": 1, "occur": 1, "use": 1, "icsi": 1, "transcrib": 1, "speech": 1, "asr": 1, "error": 1, "da": 1, "entir": 1, "meet": 4, "data": 1, "word": 1, "convers": 1, "sri": 1, "annot": 1, "record": 1, "stateoftheart": 1, "dialog": 1, "act": 1, "output": 1, "summari": 1}, "marker": "(Shriberg et al., 2004)", "article": "N09-1070", "vector_2": [5, 0.25337674478579025, 4, 1, 0, 0]}, {"label": "Neut", "current": "Examples with translations in Function of the initalic parenthesis text a ARTi3l9t1.4~3.0ZN to provide citation (MacArthur, 1967) The range of its values is within", "context": ["The example in row f is ruled out because '/' is not found in the pre-parenthesis text.", "Examples with translations in Function of the initalic parenthesis text a ARTi3l9t1.4~3.0ZN to provide citation (MacArthur, 1967) The range of its values is within", "1.4~3.0 (MacArthur, 1967) b cJILS/ (VN901 flight information 15:20-22:30) Vietnam Airlines Beijing/Ho Chi Minh (VN901 15:20-22:30) c ' Af*#(255-8FT) product Id."], "vector_1": {"minh": 1, "rang": 1, "citat": 1, "cjil": 1, "text": 2, "within": 1, "airlin": 1, "id": 1, "row": 1, "chi": 1, "beijingho": 1, "vietnam": 1, "vn01": 2, "function": 1, "product": 1, "afft": 1, "flight": 1, "140": 1, "1520220": 2, "artilt140zn": 1, "parenthesi": 1, "b": 1, "translat": 1, "valu": 1, "c": 1, "inital": 1, "f": 1, "provid": 1, "rule": 1, "inform": 1, "exampl": 2, "preparenthesi": 1, "found": 1}, "marker": "(MacArthur, 1967)", "article": "P08-1113", "vector_2": [41, 0.3492278611933379, 2, 4, 0, 0]}, {"label": "Neut", "current": "In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011).", "context": ["Second, instance weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013).", "In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011).", "In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations."], "vector_1": {"domain": 1, "concept": 1, "weight": 3, "particular": 1, "proven": 1, "second": 1, "phrase": 1, "select": 1, "priorit": 1, "binari": 1, "hardlabel": 1, "test": 1, "method": 1, "topicgenr": 1, "sentenc": 1, "previou": 1, "train": 2, "extrem": 1, "pair": 2, "data": 1, "relev": 1, "case": 1, "work": 1, "correspond": 1, "instanc": 3, "combin": 1, "either": 1, "discard": 1, "typic": 1, "assign": 1}, "marker": "(Moore and Lewis, 2010", "article": "W15-2518", "vector_2": [5, 0.20087264072984498, 5, 1, 0, 0]}, {"label": "Neut", "current": "Although more general than models that are built individually for each word in a test corpus (Decadt et al., 2004), the applicability of the semantic models built as part of SENSELEARNER is still limited to those words previously seen in the training corpus, and therefore their overall coverage is not 100%.", "context": ["Different semantic models can be defined and trained for the disambiguation of different word categories.", "Although more general than models that are built individually for each word in a test corpus (Decadt et al., 2004), the applicability of the semantic models built as part of SENSELEARNER is still limited to those words previously seen in the training corpus, and therefore their overall coverage is not 100%.", "Starting with an annotated corpus consisting of all annotated files in SemCor, a separate training data set is built for each model."], "vector_1": {"corpu": 3, "semant": 2, "set": 1, "individu": 1, "senselearn": 1, "file": 1, "seen": 1, "still": 1, "previous": 1, "differ": 2, "built": 3, "semcor": 1, "coverag": 1, "categori": 1, "start": 1, "disambigu": 1, "test": 1, "therefor": 1, "applic": 1, "gener": 1, "overal": 1, "train": 3, "although": 1, "data": 1, "word": 3, "consist": 1, "annot": 2, "separ": 1, "defin": 1, "part": 1, "limit": 1, "model": 4}, "marker": "(Decadt et al., 2004)", "article": "P05-3014", "vector_2": [1, 0.5342741935483871, 1, 3, 2, 0]}, {"label": "Neut", "current": "We had to wait until Tsai et al (2006), who combine pattern-based postprocessing with CRFs, for CRF-based systems to achieve the same level of performance as Zhou et al.", "context": ["Another reason may be that the computational complexity of the models prevented the developers to invent effective features for the task.", "We had to wait until Tsai et al (2006), who combine pattern-based postprocessing with CRFs, for CRF-based systems to achieve the same level of performance as Zhou et al.", "As such, a key to further improvement of the performance of bio-entity recognition has been to employ global features, which are effective to capture the features of long names appearing in the bio domain."], "vector_1": {"prevent": 1, "domain": 1, "featur": 3, "comput": 1, "captur": 1, "global": 1, "al": 2, "invent": 1, "et": 2, "develop": 1, "appear": 1, "perform": 2, "anoth": 1, "system": 1, "long": 1, "crfbase": 1, "recognit": 1, "patternbas": 1, "complex": 1, "postprocess": 1, "combin": 1, "bio": 1, "zhou": 1, "may": 1, "effect": 2, "reason": 1, "key": 1, "bioentiti": 1, "wait": 1, "task": 1, "name": 1, "level": 1, "tsai": 1, "employ": 1, "achiev": 1, "crf": 1, "improv": 1, "model": 1}, "marker": "(2006)", "article": "W07-1033", "vector_2": [1, 0.10137701804368471, 1, 6, 0, 0]}, {"label": "Neut", "current": "in Tatu and Moldovan (2005).", "context": ["This is problematic for the use of FrameNet analyses as a basis for inferences over text, as e.g.", "in Tatu and Moldovan (2005).", "For example, the verb prepare from Figure 1 is associated with the frames"], "vector_1": {"associ": 1, "use": 1, "prepar": 1, "text": 1, "eg": 1, "tatu": 1, "figur": 1, "exampl": 1, "frame": 1, "verb": 1, "framenet": 1, "problemat": 1, "analys": 1, "infer": 1, "moldovan": 1, "basi": 1}, "marker": "(2005)", "article": "N06-1017", "vector_2": [1, 0.22825542410112143, 1, 1, 0, 0]}, {"label": "Weak", "current": "Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26].", "context": ["The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithms.", "Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26].", "The worst size complexity of such a chart is only a square function of the size of the input2."], "vector_1": {"process": 1, "repres": 1, "parsetre": 1, "still": 1, "extract": 1, "size": 2, "describ": 1, "associ": 1, "publish": 1, "categori": 1, "complex": 1, "analyz": 2, "nontrivi": 1, "input": 1, "function": 1, "sentenc": 2, "chart": 2, "pars": 1, "kay": 1, "characterist": 1, "segment": 1, "requir": 1, "squar": 1, "kind": 1, "essenti": 1, "algorithm": 2, "thu": 1, "structur": 1, "nontermin": 1, "worst": 1, "produc": 2}, "marker": "[3]", "article": "P89-1018", "vector_2": [14, 0.09869278860619839, 7, 2, 0, 0]}, {"label": "Neut", "current": "In [28], Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that \"split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing\".", "context": ["  Several natural language parser start with a pure ContextFree (CF) backbone that makes a first sketch of the structure of the analyzed sentence, before it is handed to a more elaborate analyzer (possibly a coroutine), that takes into account the finer grammatical structure to filter out undesirable parses (see for example [24,28]).", "In [28], Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that \"split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing\".", "The basic motivation for this approach is to benefit from the CF parsing technology whose development over 30 years has lead to powerful and efficient parsers [1,7]."], "vector_1": {"sketch": 1, "set": 1, "effici": 1, "give": 1, "natur": 1, "parser": 2, "cf": 2, "restrict": 1, "see": 1, "exist": 1, "domain": 1, "year": 1, "analyz": 2, "sever": 1, "infinit": 1, "whose": 1, "use": 1, "develop": 1, "equival": 1, "shieber": 1, "make": 1, "lead": 1, "start": 1, "tunabl": 1, "take": 1, "pure": 1, "basic": 1, "languag": 1, "approach": 3, "finer": 1, "power": 1, "sentenc": 1, "variant": 1, "grammat": 1, "hand": 1, "base": 1, "pars": 3, "finit": 1, "elabor": 1, "class": 1, "undesir": 1, "account": 1, "backbon": 1, "possibl": 1, "contextfre": 1, "split": 1, "technolog": 1, "structur": 2, "nontermin": 1, "filter": 1, "coroutin": 1, "benefit": 1, "exampl": 1, "survey": 1, "motiv": 1, "first": 1}, "marker": "[28]", "article": "P89-1018", "vector_2": [3, 0.045367924843702986, 5, 2, 2, 0]}, {"label": "Neut", "current": "Popescu et al (2003) identify a class of \"semantically tractable\" natural language questions that can be mapped to an SQL query to return the question's unique correct answer.", "context": ["Linguistic complexity is measured by utterance length, vocabulary size and perplexity.", "Popescu et al (2003) identify a class of \"semantically tractable\" natural language questions that can be mapped to an SQL query to return the question's unique correct answer.", "Ambiguous questions with multiple correct answers are not considered semantically tractable."], "vector_1": {"semant": 2, "identifi": 1, "consid": 1, "popescu": 1, "natur": 1, "al": 1, "tractabl": 2, "utter": 1, "et": 1, "languag": 1, "size": 1, "multipl": 1, "question": 3, "complex": 1, "queri": 1, "answer": 2, "correct": 2, "map": 1, "return": 1, "perplex": 1, "vocabulari": 1, "sql": 1, "class": 1, "measur": 1, "uniqu": 1, "ambigu": 1, "length": 1, "linguist": 1}, "marker": "(2003)", "article": "W12-1635", "vector_2": [9, 0.32159155733029093, 1, 1, 0, 0]}, {"label": "Pos", "current": "As a parser, any available tool may be used (the TreeTagger (Schmid, 1994) is used in the present implementation for English).", "context": ["The phrases are assumed to be flat and linguistically valid.", "As a parser, any available tool may be used (the TreeTagger (Schmid, 1994) is used in the present implementation for English).", "PAM processes a bilingual corpus of SL - TL sentence pairs, taking into account the parsing information in one language (in the current implementation the TL side) and making use of a bilingual lexicon and information on potential phrase heads; the output being the bilingual corpus aligned at word, phrase and clause level."], "vector_1": {"corpu": 2, "lexicon": 1, "process": 1, "parser": 1, "one": 1, "treetagg": 1, "phrase": 3, "bilingu": 3, "assum": 1, "languag": 1, "current": 1, "use": 3, "make": 1, "avail": 1, "tl": 2, "valid": 1, "take": 1, "pam": 1, "flat": 1, "head": 1, "claus": 1, "may": 1, "sentenc": 1, "tool": 1, "english": 1, "pars": 1, "pair": 1, "present": 1, "account": 1, "word": 1, "level": 1, "align": 1, "inform": 2, "potenti": 1, "sl": 1, "output": 1, "implement": 2, "linguist": 1, "side": 1}, "marker": "(Schmid, 1994)", "article": "W12-0108", "vector_2": [18, 0.35364989369241673, 1, 1, 0, 0]}, {"label": "Neut", "current": "Georgescul et al (2009) note that while WindowDiff technically penalizes false positives and false negatives equally, false positives are in fact more likely; a false positive error occurs anywhere were there are more computed boundaries than boundaries in the reference, while a false negative error can only occur when a boundary is missed.", "context": ["Tasks requiring a uniform theme in a segment might tolerate false positives, while tasks requiring complete ideas or complete themes might accept false negatives.", "Georgescul et al (2009) note that while WindowDiff technically penalizes false positives and false negatives equally, false positives are in fact more likely; a false positive error occurs anywhere were there are more computed boundaries than boundaries in the reference, while a false negative error can only occur when a boundary is missed.", "Consider figure 1, only 3 of the 8 windows contain a boundary; only those 3 windows may have false negatives (a missed boundary), while all other windows may contain false positives (too many boundaries)."], "vector_1": {"comput": 1, "georgescul": 1, "al": 1, "accept": 1, "et": 1, "miss": 2, "occur": 2, "neg": 4, "boundari": 6, "figur": 1, "uniform": 1, "note": 1, "may": 2, "theme": 2, "window": 3, "might": 2, "complet": 2, "windowdiff": 1, "fals": 9, "contain": 2, "idea": 1, "consid": 1, "toler": 1, "segment": 1, "technic": 1, "requir": 2, "task": 2, "like": 1, "anywher": 1, "equal": 1, "error": 2, "posit": 5, "mani": 1, "refer": 1, "fact": 1, "penal": 1}, "marker": "(2009)", "article": "N12-1038", "vector_2": [3, 0.2723895038315659, 1, 1, 0, 0]}, {"label": "Neut", "current": "Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).", "context": ["In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.", "Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).", "For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option."], "vector_1": {"advantag": 1, "almost": 1, "kleinbraley": 1, "stabl": 1, "indic": 1, "restrict": 1, "prefix": 1, "correl": 1, "thorough": 1, "empir": 1, "raatz": 1, "follow": 2, "guess": 1, "languag": 1, "automat": 2, "score": 1, "space": 1, "solut": 2, "cloze": 2, "valid": 1, "overcom": 1, "test": 4, "analys": 1, "singl": 1, "altern": 1, "option": 1, "benefici": 1, "regard": 1, "weak": 1, "given": 1, "theori": 1, "ctest": 3, "case": 1, "enabl": 1, "provid": 1, "approach": 1, "reliabl": 1, "properti": 1, "without": 1, "principl": 1, "order": 1, "propos": 1}, "marker": "Klein-Braley, 1997", "article": "Q14-1040", "vector_2": [17, 0.12160330210630678, 4, 1, 10, 0]}, {"label": "Neut", "current": "A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).", "context": ["iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).", "A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).", "However, this system focuses on semantic factors influencing the expression of events with different participants (e.g., different types of vehicles) rather than on generating linguistic variations."], "vector_1": {"vehicl": 1, "semant": 1, "text": 1, "eg": 1, "focus": 1, "differ": 2, "event": 1, "use": 1, "influenc": 1, "particip": 1, "system": 2, "genpex": 1, "factor": 1, "approach": 1, "variat": 2, "gener": 1, "express": 1, "sophist": 1, "linguist": 2, "type": 1, "thu": 1, "modelcr": 1, "rather": 1, "allow": 1, "principl": 1, "similar": 1, "howev": 1}, "marker": "Higgins et al., 2005)", "article": "W11-1403", "vector_2": [6, 0.1472240098285848, 5, 1, 4, 0]}, {"label": "Pos", "current": "However, Balahur and Turchi (2014) conclude that MT systems can be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English.", "context": ["In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012).", "However, Balahur and Turchi (2014) conclude that MT systems can be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English.", "All this work was performed at sentence or document level."], "vector_1": {"obtain": 2, "one": 1, "languag": 1, "balahur": 1, "use": 1, "compar": 1, "perform": 2, "transfer": 1, "system": 3, "avail": 1, "build": 1, "sentenc": 1, "document": 1, "method": 1, "analysi": 1, "may": 1, "conclud": 1, "pair": 1, "highqual": 1, "appropri": 1, "sentiment": 1, "level": 1, "howev": 1, "work": 1, "turchi": 1, "mt": 3, "english": 1}, "marker": "(2014)", "article": "P15-2128", "vector_2": [1, 0.4658590308370044, 3, 3, 0, 0]}, {"label": "Pos", "current": "In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences.", "context": ["Another line of research for keyword extraction has adopted graph-based methods similar to Google's PageRank algorithm (Brin and Page, 1998).", "In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences.", "We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study."], "vector_1": {"corpu": 1, "find": 1, "assumpt": 2, "seen": 1, "simultan": 1, "extract": 2, "summar": 1, "use": 3, "googl": 1, "anoth": 1, "research": 1, "also": 2, "import": 2, "approach": 1, "method": 1, "graphbas": 1, "sentenc": 2, "reinforc": 1, "obtain": 1, "pagerank": 1, "particular": 1, "line": 1, "hold": 1, "attempt": 1, "algorithm": 1, "keyword": 4, "adopt": 1, "statist": 1, "contain": 1, "meet": 1, "studi": 1, "similar": 1, "usual": 2}, "marker": "(Wan et al., 2007)", "article": "N09-1070", "vector_2": [2, 0.1988008123529222, 2, 4, 0, 0]}, {"label": "Neut", "current": "These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).", "context": ["Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).", "(Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT."], "vector_1": {"dictionari": 1, "individu": 1, "focus": 1, "one": 1, "bilingu": 1, "leverag": 2, "supervis": 1, "use": 1, "perform": 1, "data": 1, "smt": 2, "solut": 1, "construct": 1, "label": 2, "better": 2, "also": 1, "consecut": 1, "approach": 1, "unlabel": 1, "either": 1, "machin": 1, "map": 2, "form": 1, "sequenc": 1, "chines": 2, "previou": 1, "bia": 1, "train": 1, "translat": 1, "although": 1, "dataset": 1, "segment": 3, "model": 2, "maximummatch": 1, "word": 2, "work": 1, "charact": 1, "achiev": 1, "statist": 1, "english": 1, "studi": 1, "toward": 1}, "marker": "(Xu et al., 2004)", "article": "D15-1142", "vector_2": [11, 0.29941432338938934, 8, 1, 7, 0]}, {"label": "Neut", "current": "(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.", "context": ["There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).", "(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.", "In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures."], "vector_1": {"semant": 1, "show": 1, "process": 1, "queri": 1, "eg": 1, "lectur": 2, "result": 1, "extract": 2, "languag": 1, "web": 1, "use": 3, "recognit": 1, "perform": 2, "better": 1, "also": 1, "speech": 3, "document": 1, "broadcast": 1, "news": 1, "data": 1, "relev": 1, "task": 1, "retriev": 2, "keyword": 3, "work": 1, "slide": 1, "combin": 1, "verif": 1, "improv": 2, "model": 1}, "marker": "(Wu et al., 2007)", "article": "N09-1070", "vector_2": [2, 0.22894168466522677, 7, 2, 0, 0]}, {"label": "Neut", "current": "A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).", "context": ["In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner.", "A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).", "The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly."], "vector_1": {"corpu": 1, "infant": 1, "constitu": 1, "evalu": 1, "intermedi": 1, "transit": 2, "number": 1, "procedur": 1, "daysold": 1, "result": 1, "human": 1, "onlin": 1, "artifici": 1, "surprisingli": 1, "languag": 1, "probabl": 2, "even": 1, "syllabifi": 1, "colloc": 1, "would": 1, "support": 1, "perfectli": 1, "learner": 2, "young": 1, "tp": 2, "research": 1, "identif": 1, "reli": 1, "experiment": 2, "unit": 1, "basic": 1, "input": 1, "approach": 1, "realiti": 1, "poor": 1, "assumpt": 1, "syntact": 1, "use": 4, "syllab": 1, "base": 1, "segment": 2, "studi": 1, "addit": 1, "linguist": 1, "word": 2, "level": 2, "local": 1, "clear": 1, "work": 1, "psychologicallymotiv": 1, "syllabl": 2, "inform": 1, "alreadi": 1, "learn": 2, "model": 1, "psycholog": 2}, "marker": "(Saffran et al., 1996a", "article": "W10-2912", "vector_2": [14, 0.16909708650290148, 5, 1, 1, 0]}, {"label": "Neut", "current": "A desired feature of computer-assisted translation (CAT) systems is the integration of the human speech into the system, as skilled human translators are faster at dictating than typing the translations (Brown et al., 1994).", "context": ["1 Introduction", "A desired feature of computer-assisted translation (CAT) systems is the integration of the human speech into the system, as skilled human translators are faster at dictating than typing the translations (Brown et al., 1994).", "Additionally, incorporation of a statistical prediction engine, i.e."], "vector_1": {"dictat": 1, "featur": 1, "desir": 1, "faster": 1, "ie": 1, "computerassist": 1, "incorpor": 1, "system": 2, "cat": 1, "predict": 1, "engin": 1, "integr": 1, "speech": 1, "translat": 3, "human": 2, "statist": 1, "skill": 1, "addit": 1, "type": 1, "introduct": 1}, "marker": "(Brown et al., 1994)", "article": "P06-2061", "vector_2": [12, 0.041766917293233084, 1, 3, 3, 0]}, {"label": "Neut", "current": "This knowledge is structured following the methodology described in (Sheremetyeva, 1999; Sheremetyeva, 2003) and is mostly coded in the system lexicon as well as in analysis and generation rules.", "context": ["Micro-level simplification at each of its stages is done by means of a specific combination of rulebased and statistical techniques and relies on linguistic knowledge of different depth.", "This knowledge is structured following the methodology described in (Sheremetyeva, 1999; Sheremetyeva, 2003) and is mostly coded in the system lexicon as well as in analysis and generation rules.", "Different modules of the micro-level simplification component use specific parts and types of linguistic knowledge included in the lexicon and their own specific sets of rules."], "vector_1": {"code": 1, "set": 1, "modul": 1, "knowledg": 3, "lexicon": 2, "done": 1, "rule": 2, "follow": 1, "differ": 2, "techniqu": 1, "system": 1, "compon": 1, "reli": 1, "includ": 1, "stage": 1, "microlevel": 2, "gener": 1, "analysi": 1, "methodolog": 1, "use": 1, "part": 1, "mostli": 1, "describ": 1, "specif": 3, "simplif": 2, "type": 1, "rulebas": 1, "well": 1, "structur": 1, "depth": 1, "combin": 1, "statist": 1, "linguist": 2, "mean": 1}, "marker": "(Sheremetyeva, 1999", "article": "W14-5605", "vector_2": [15, 0.5623730706742486, 2, 1, 4, 1]}, {"label": "Pos", "current": "In our experiments, we used the free solver SCIP (Achterberg, 2007).", "context": ["Even though ILP problems are NP-hard in general, there exist several off-theshelf ILP solvers able to efficiently find an optimal solution or decide that the problem is infeasible.", "In our experiments, we used the free solver SCIP (Achterberg, 2007).", "An optimal solution was found for all problems we considered."], "vector_1": {"effici": 1, "abl": 1, "exist": 1, "find": 1, "sever": 1, "even": 1, "scip": 1, "decid": 1, "nphard": 1, "solut": 2, "infeas": 1, "experi": 1, "though": 1, "optim": 2, "gener": 1, "use": 1, "free": 1, "consid": 1, "solver": 2, "offtheshelf": 1, "ilp": 2, "found": 1, "problem": 3}, "marker": "(Achterberg, 2007)", "article": "D10-1091", "vector_2": [3, 0.5580664966468311, 1, 1, 0, 0]}, {"label": "Neut", "current": "Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995).", "context": ["However, 'explicit' feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier.", "Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995).", "The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008)."], "vector_1": {"function": 1, "kernel": 1, "classif": 1, "featur": 4, "effici": 1, "weight": 1, "nlp": 1, "known": 1, "significantli": 1, "use": 1, "slow": 2, "space": 1, "support": 1, "classifi": 2, "also": 1, "spaceeffici": 1, "test": 1, "method": 1, "machin": 1, "conjunct": 1, "task": 1, "polynomi": 1, "train": 1, "consid": 1, "increas": 1, "svm": 1, "howev": 2, "explicit": 2, "vector": 1, "combin": 2, "kernelbas": 2, "sum": 1}, "marker": "(Cortes and Vapnik, 1995)", "article": "D09-1160", "vector_2": [14, 0.056238908267547884, 4, 2, 0, 0]}, {"label": "CoCo", "current": "The approach described in (Auli et al., 2009) is very similar to ours: in this study, the authors propose to find and analyze the limits of machine translation systems by studying the reference reachability.", "context": ["To the best of our knowledge, there are only a few works that try to study the expressive power of phrase-based machine translation systems or to provide tools for analyzing potential causes of failure.", "The approach described in (Auli et al., 2009) is very similar to ours: in this study, the authors propose to find and analyze the limits of machine translation systems by studying the reference reachability.", "A reference is reachable for a given system if it can be exactly generated by this system."], "vector_1": {"reachabl": 2, "knowledg": 1, "find": 1, "best": 1, "given": 1, "describ": 1, "author": 1, "system": 4, "analyz": 2, "approach": 1, "gener": 1, "refer": 2, "machin": 2, "power": 1, "tool": 1, "express": 1, "translat": 2, "failur": 1, "exactli": 1, "tri": 1, "provid": 1, "work": 1, "phrasebas": 1, "caus": 1, "limit": 1, "potenti": 1, "studi": 3, "similar": 1, "propos": 1}, "marker": "(Auli et al., 2009)", "article": "D10-1091", "vector_2": [1, 0.8712335883630868, 1, 10, 6, 0]}, {"label": "Neut", "current": "Previous studies applied this hypothesis to silent reading (Fodor, 2002).", "context": ["The Implicit Prosody Hypothesis, for example, posits that a silent prosodic contour is projected onto a stimulus, and may help a reader resolve syntactic ambiguity (Fodor, 2002).", "Previous studies applied this hypothesis to silent reading (Fodor, 2002).", "Principles related to prosody need not be limited to spoken language production."], "vector_1": {"help": 1, "spoken": 1, "appli": 1, "prosodi": 2, "need": 1, "contour": 1, "languag": 1, "silent": 2, "read": 1, "reader": 1, "product": 1, "relat": 1, "syntact": 1, "hypothesi": 2, "may": 1, "previou": 1, "implicit": 1, "prosod": 1, "resolv": 1, "ambigu": 1, "project": 1, "exampl": 1, "limit": 1, "stimulu": 1, "posit": 1, "studi": 1, "principl": 1, "onto": 1}, "marker": "(Fodor, 2002)", "article": "W15-0914", "vector_2": [13, 0.08669248013042592, 2, 2, 0, 0]}, {"label": "Neut", "current": "(Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT.", "context": ["These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).", "(Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT.", "However, because most of these approaches focus on SMT performance, they emphasize decreasing the perplexity of the bilingual data and word alignment rather than improving the CWS accuracy."], "vector_1": {"individu": 1, "dataset": 1, "decreas": 1, "accuraci": 1, "cw": 1, "leverag": 1, "supervis": 1, "use": 1, "perform": 1, "data": 1, "smt": 2, "solut": 1, "construct": 1, "label": 2, "better": 1, "also": 1, "consecut": 1, "approach": 2, "map": 2, "perplex": 1, "form": 1, "emphas": 1, "sequenc": 1, "chines": 2, "bilingu": 1, "bia": 1, "train": 1, "dictionari": 1, "one": 1, "segment": 2, "maximummatch": 1, "word": 3, "howev": 1, "focu": 1, "charact": 1, "rather": 1, "either": 1, "english": 1, "improv": 1, "model": 2, "toward": 1, "align": 1}, "marker": "(Zeng et al., 2014)", "article": "D15-1142", "vector_2": [1, 0.3031133335616673, 3, 2, 4, 0]}, {"label": "Neut", "current": "It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 - following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models - then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5.", "context": ["ing (Pietra et al., 1995) between the background exponential model B - assumed fixed - and an exponential model A built using the Fbackground U Fadapt feature set.", "It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 - following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models - then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5.", "However, we wish to point out that the equivalence holds only if the feature set for the new model A is Fbackground U Fadapt."], "vector_1": {"set": 2, "fadapt": 2, "weight": 1, "point": 1, "gaussian": 1, "procedur": 1, "maximum": 1, "featur": 3, "follow": 1, "ing": 1, "assum": 1, "use": 1, "built": 1, "equival": 1, "exponenti": 2, "fix": 1, "estim": 1, "adapt": 2, "new": 1, "approach": 1, "updat": 1, "map": 1, "ident": 1, "shown": 1, "mindiv": 1, "background": 1, "hold": 1, "data": 1, "b": 1, "center": 1, "wish": 1, "howev": 1, "smooth": 2, "fbackground": 2, "prior": 1, "u": 2, "equat": 1, "model": 5, "entropi": 1, "propos": 1}, "marker": "(Chen and Rosenfeld, 2000)", "article": "W04-3237", "vector_2": [4, 0.6549891492094424, 2, 5, 0, 0]}, {"label": "Neut", "current": "A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) but previously discussed by Halle and Vergnaud (1987), is as follows: Unique Stress Constraint (USC): A word can bear at most one primary stress.", "context": ["It is important to identify such constraints to see to what extent they complement, or even replace, domain neutral learning mechanisms.", "A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) but previously discussed by Halle and Vergnaud (1987), is as follows: Unique Stress Constraint (USC): A word can bear at most one primary stress.", "A simple example of how adult learners might use the USC is upon hearing novel names or words."], "vector_1": {"domain": 1, "identifi": 1, "vergnaud": 1, "replac": 1, "one": 1, "see": 1, "particularli": 1, "follow": 1, "primari": 1, "previous": 1, "even": 1, "use": 2, "usc": 2, "learner": 1, "discuss": 1, "exampl": 1, "import": 1, "simpl": 1, "introduc": 1, "complement": 1, "upon": 1, "bear": 1, "neutral": 1, "mechan": 1, "adult": 1, "extent": 1, "segment": 2, "hall": 1, "stress": 2, "novel": 1, "word": 4, "name": 1, "constraint": 3, "might": 1, "uniqu": 1, "yang": 1, "learn": 1, "hear": 1, "problem": 1}, "marker": "(1987)", "article": "W10-2912", "vector_2": [23, 0.2741212904777654, 2, 1, 0, 0]}, {"label": "Neut", "current": "There is only little work investigating this correspondence formally (see (Hawkins and Filipovic, 2010; Alexopoulou et al., 2010) for discussion) and only on error-annotated English learner corpora.", "context": ["To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels.", "There is only little work investigating this correspondence formally (see (Hawkins and Filipovic, 2010; Alexopoulou et al., 2010) for discussion) and only on error-annotated English learner corpora.", "For this reason, we follow a data-driven approach to learn the correspondence between errors and levels, based on exercises from written placement exams."], "vector_1": {"corpora": 1, "datadriven": 1, "process": 1, "automat": 1, "see": 1, "learn": 1, "need": 1, "follow": 1, "learner": 2, "littl": 1, "written": 1, "approach": 1, "decis": 1, "exercis": 1, "type": 1, "investig": 1, "exam": 1, "error": 2, "reason": 1, "base": 1, "understand": 1, "discuss": 1, "formal": 1, "placement": 1, "level": 2, "frequenc": 1, "work": 1, "well": 1, "correspond": 3, "english": 1, "errorannot": 1, "model": 1}, "marker": "Alexopoulou et al., 2010)", "article": "W12-2011", "vector_2": [2, 0.060747236685850005, 2, 1, 3, 0]}, {"label": "Pos", "current": "We use the refined method from Och and Ney (2003) which starts from the intersection of the two models' predictions and 'grows' the predicted alignments to neighbouring alignments which only appear in the output of one of the models.", "context": ["In order to produce many-to-many alignments we combine the outputs of two models, one for each translation direction.", "We use the refined method from Och and Ney (2003) which starts from the intersection of the two models' predictions and 'grows' the predicted alignments to neighbouring alignments which only appear in the output of one of the models.", "4 Experiments"], "vector_1": {"predict": 2, "direct": 1, "one": 2, "och": 1, "use": 1, "appear": 1, "two": 2, "start": 1, "intersect": 1, "ney": 1, "experi": 1, "method": 1, "neighbour": 1, "translat": 1, "refin": 1, "grow": 1, "align": 3, "order": 1, "combin": 1, "manytomani": 1, "output": 2, "model": 3, "produc": 1}, "marker": "(2003)", "article": "P06-1009", "vector_2": [3, 0.5816795452224529, 1, 6, 2, 0]}, {"label": "Neut", "current": "In particular, adaptive hypertext (O'Donnell et al., 2001) adapts the content and form of natural language text.", "context": ["Adaptive interfaces change the style and content of interaction according to the context of use.", "In particular, adaptive hypertext (O'Donnell et al., 2001) adapts the content and form of natural language text.", "Systems like this introduce the need for a good model of the context and how it influences language."], "vector_1": {"natur": 1, "hypertext": 1, "need": 1, "languag": 2, "style": 1, "influenc": 1, "interact": 1, "system": 1, "content": 2, "interfac": 1, "adapt": 3, "text": 1, "introduc": 1, "accord": 1, "good": 1, "form": 1, "use": 1, "particular": 1, "chang": 1, "like": 1, "context": 2, "model": 1}, "marker": "Donnell et al., 2001)", "article": "W11-2820", "vector_2": [10, 0.03312377832954726, 1, 3, 0, 0]}, {"label": "Neut", "current": "Gorin et al (2000) distinguish between semantic and linguistic complexity of calls to a spoken DS.", "context": ["Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space.", "Gorin et al (2000) distinguish between semantic and linguistic complexity of calls to a spoken DS.", "Semantic complexity is measured by inheritance relations between call types, the number of type labels per call, and how often calls are routed to human agents."], "vector_1": {"semant": 3, "often": 1, "spoken": 1, "number": 2, "agent": 1, "human": 1, "et": 1, "bierman": 1, "describ": 1, "space": 1, "rout": 1, "per": 1, "label": 1, "complex": 2, "call": 4, "gorin": 1, "attribut": 1, "relat": 1, "everi": 1, "relationship": 1, "object": 1, "al": 1, "consid": 1, "distinguish": 2, "bit": 1, "ds": 1, "requir": 1, "measur": 2, "type": 2, "inherit": 1, "pollard": 1, "linguist": 1, "similar": 1}, "marker": "(2000)", "article": "W12-1635", "vector_2": [12, 0.28936109526525955, 2, 1, 0, 0]}, {"label": "Neut", "current": "One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).", "context": ["Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.", "One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).", "Our phrasal semantic relatedness approach is inspired from those methods."], "vector_1": {"semant": 6, "featur": 1, "entail": 1, "knowledg": 1, "direct": 1, "related": 4, "one": 1, "set": 1, "path": 1, "wordnet": 1, "use": 2, "group": 1, "network": 2, "reli": 1, "call": 1, "includ": 2, "synset": 2, "approach": 3, "method": 1, "inspir": 1, "knowledgebas": 1, "relat": 1, "hyponymi": 1, "base": 1, "link": 1, "synonym": 1, "measur": 2, "word": 1, "princeton": 1, "type": 1, "meronymi": 1, "structur": 1, "depth": 1, "hypernymi": 1, "phrasal": 1, "frequent": 1}, "marker": "(Hirst and St-Onge, 1998)", "article": "S13-2019", "vector_2": [15, 0.14110836398721582, 5, 1, 0, 0]}, {"label": "Pos", "current": "These rules were compiled as finite-state transducers (Narasimhan et al., 2004) We have produced a list of possible suffixes and prefixes from the collected corpus.", "context": ["The rules below are listed with a corresponding sample case.", "These rules were compiled as finite-state transducers (Narasimhan et al., 2004) We have produced a list of possible suffixes and prefixes from the collected corpus.", "Let Pre denote the set of prefixes and 8uf the set of prefixes."], "vector_1": {"case": 1, "compil": 1, "pre": 1, "denot": 1, "suffix": 1, "transduc": 1, "possibl": 1, "list": 2, "correspond": 1, "rule": 2, "finitest": 1, "collect": 1, "prefix": 3, "set": 2, "sampl": 1, "corpu": 1, "let": 1, "uf": 1, "produc": 1}, "marker": "(Narasimhan et al., 2004)", "article": "W14-5502", "vector_2": [10, 0.4553911011318334, 1, 1, 2, 0]}, {"label": "Neut", "current": "The Meter Corpus chosen as the test data is a collection of court reports from the British Press Association (PA) and some leading British newspapers (Gaizauskas 2001; Clough et al., 2002).", "context": ["Finally, we manually checked the results.", "The Meter Corpus chosen as the test data is a collection of court reports from the British Press Association (PA) and some leading British newspapers (Gaizauskas 2001; Clough et al., 2002).", "In our experiment, we used the newspaper part of the corpus containing 774 articles with more than 250,000 words."], "vector_1": {"corpu": 2, "newspap": 2, "meter": 1, "report": 1, "result": 1, "check": 1, "use": 1, "court": 1, "lead": 1, "chosen": 1, "data": 1, "articl": 1, "pa": 1, "test": 1, "experi": 1, "final": 1, "part": 1, "press": 1, "associ": 1, "word": 1, "gaizauska": 1, "manual": 1, "british": 2, "collect": 1, "contain": 1}, "marker": "Clough et al., 2002)", "article": "W03-1807", "vector_2": [1, 0.5434104710323322, 1, 3, 2, 0]}, {"label": "Neut", "current": "The underlying principle of most language proficiency tests is the concept of reduced redundancy testing (Spolsky, 1969).", "context": ["This results in a subjective difficulty estimation that often lacks the consistency required for comparing learners over different tests.", "The underlying principle of most language proficiency tests is the concept of reduced redundancy testing (Spolsky, 1969).", "It is based on the idea that \"natural language is redundant\" and that more advanced learners can be distinguished from beginners by their ability to deal with reduced redundancy."], "vector_1": {"underli": 1, "concept": 1, "often": 1, "deal": 1, "natur": 1, "lack": 1, "idea": 1, "result": 1, "abil": 1, "languag": 2, "profici": 1, "differ": 1, "compar": 1, "learner": 2, "estim": 1, "test": 3, "subject": 1, "difficulti": 1, "advanc": 1, "base": 1, "distinguish": 1, "redund": 3, "requir": 1, "reduc": 2, "consist": 1, "principl": 1, "beginn": 1}, "marker": "(Spolsky, 1969)", "article": "Q14-1040", "vector_2": [45, 0.03694636100600935, 1, 1, 0, 0]}, {"label": "Neut", "current": "Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.", "context": ["Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques.", "Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.", "Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text."], "vector_1": {"set": 1, "primarili": 1, "ontolog": 3, "lexicosyntact": 1, "via": 1, "heurist": 2, "find": 1, "languag": 1, "use": 1, "techniqu": 1, "divid": 1, "pattern": 2, "hybrid": 1, "three": 1, "categori": 1, "reli": 1, "varieti": 1, "hearst": 1, "approach": 2, "express": 1, "relat": 2, "base": 1, "outlin": 1, "text": 1, "statist": 1, "learn": 1, "typic": 1, "linguist": 1, "fact": 1}, "marker": "Girju et al., 2003)", "article": "W12-5209", "vector_2": [9, 0.27904701617080047, 4, 1, 0, 0]}, {"label": "Pos", "current": "This alignment lattice was then used to create a joint sequence n-gram model (Galescu and Allen, 2002) Nknrm.", "context": ["Figure 5: 5 best paths of Lp for Kannada input 26Af aikv", "This alignment lattice was then used to create a joint sequence n-gram model (Galescu and Allen, 2002) Nknrm.", "This is then composed with the input word I, whose output projection we use."], "vector_1": {"creat": 1, "af": 1, "kannada": 1, "best": 1, "whose": 1, "use": 2, "compos": 1, "figur": 1, "lp": 1, "lattic": 1, "input": 2, "sequenc": 1, "joint": 1, "ngram": 1, "nknrm": 1, "path": 1, "word": 1, "align": 1, "aikv": 1, "project": 1, "output": 1, "model": 1}, "marker": "(Galescu and Allen, 2002)", "article": "W14-5502", "vector_2": [12, 0.7557622691655592, 1, 1, 0, 0]}, {"label": "Neut", "current": "ESA score from Gabrilovich and Markovitch (2007).", "context": ["Table 4: Summary of final document similarity correlations over the Lee & Pincombe document similarity dataset.", "ESA score from Gabrilovich and Markovitch (2007).", "Pearson p Cosine VSM (with tf-idf) only 0.56 MaxSim method 0.68 WikiSpread method 0.62 ESA 0.72 Combined (Cosine + MaxSim) 0.72"], "vector_1": {"gabrilovich": 1, "dataset": 1, "correl": 1, "tabl": 1, "vsm": 1, "pincomb": 1, "score": 1, "esa": 2, "document": 2, "method": 2, "lee": 1, "pearson": 1, "maxsim": 2, "wikispread": 1, "similar": 2, "final": 1, "tfidf": 1, "p": 1, "combin": 1, "cosin": 2, "markovitch": 1, "summari": 1}, "marker": "(2007)", "article": "W10-3506", "vector_2": [3, 0.8948223120208467, 1, 6, 1, 0]}, {"label": "Neut", "current": "Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "context": ["In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.", "Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose."], "vector_1": {"corpu": 1, "domain": 1, "task": 1, "comput": 1, "techniqu": 1, "appli": 1, "use": 2, "tree": 2, "histor": 1, "research": 1, "other": 1, "cluster": 1, "corpusbas": 1, "statist": 1, "famili": 2, "reconstruct": 2, "linguist": 1, "method": 1, "languag": 2, "purpos": 1}, "marker": "Batagelj et al., 1992", "article": "P13-1112", "vector_2": [21, 0.9278944947094004, 7, 4, 0, 0]}, {"label": "Pos", "current": "As a baseline, we use the best performing feature set from Beigman Klebanov et al (2014), who investigated supervised word-level identification of metaphors.", "context": ["Performance will be evaluated using Precision, Recall, and F-1 score, for the positive (\"metaphor\") class.", "As a baseline, we use the best performing feature set from Beigman Klebanov et al (2014), who investigated supervised word-level identification of metaphors.", "We investigate the effect of reweighting of examples, as well as the effectiveness of features related to the notion of concreteness."], "vector_1": {"concret": 1, "klebanov": 1, "featur": 2, "evalu": 1, "al": 1, "set": 1, "et": 1, "best": 1, "baselin": 1, "supervis": 1, "use": 2, "metaphor": 2, "perform": 2, "identif": 1, "beigman": 1, "score": 1, "investig": 2, "recal": 1, "wordlevel": 1, "effect": 2, "relat": 1, "class": 1, "f": 1, "reweight": 1, "well": 1, "precis": 1, "exampl": 1, "posit": 1, "notion": 1}, "marker": "(2014)", "article": "W15-1402", "vector_2": [1, 0.16000397772474145, 1, 8, 5, 0]}, {"label": "Pos", "current": "Finally, we also computed consensus translation from some of the submissions to the TC-STAR 2005 evaluation campaign (TC-STAR, 2005).", "context": ["Here, the involved MT systems had used about 60K sentence pairs (420K running words) for training.", "Finally, we also computed consensus translation from some of the submissions to the TC-STAR 2005 evaluation campaign (TC-STAR, 2005).", "The TC-STAR participants had submitted translations of manually transcribed speeches from the European Parliament Plenary Sessions (EPPS)."], "vector_1": {"comput": 1, "evalu": 1, "transcrib": 1, "session": 1, "plenari": 1, "involv": 1, "use": 1, "campaign": 1, "consensu": 1, "particip": 1, "system": 1, "submit": 1, "also": 1, "speech": 1, "submiss": 1, "final": 1, "european": 1, "parliament": 1, "run": 1, "tcstar": 2, "sentenc": 1, "train": 1, "translat": 2, "pair": 1, "word": 1, "k": 2, "manual": 1, "epp": 1, "mt": 1}, "marker": "(TC-STAR, 2005)", "article": "E06-1005", "vector_2": [1, 0.5204234744914972, 1, 1, 0, 1]}, {"label": "Neut", "current": "In our experiments, we will use 4 different kinds of feature combinations: Baseline: The 6 baseline features used in (Och, 2003), such as cost of word penalty, cost of aligned template penalty.", "context": ["By combining the four features, Model 1, matched parentheses, matched quotation marks and POS language model, the system achieved a BLEU score of 32.6%.", "In our experiments, we will use 4 different kinds of feature combinations: Baseline: The 6 baseline features used in (Och, 2003), such as cost of word penalty, cost of aligned template penalty.", "Best Feature: Baseline + IBM Model 1 + matched parentheses + matched quotation marks + POS language model."], "vector_1": {"bleu": 1, "featur": 4, "templat": 1, "four": 1, "cost": 2, "languag": 2, "baselin": 3, "penalti": 2, "use": 2, "quotat": 2, "system": 1, "mark": 2, "score": 1, "best": 1, "experi": 1, "po": 2, "match": 4, "differ": 1, "kind": 1, "word": 1, "ibm": 1, "align": 1, "parenthes": 2, "achiev": 1, "combin": 2, "model": 4}, "marker": "(Och, 2003)", "article": "N04-1023", "vector_2": [1, 0.8244145221652069, 1, 6, 5, 1]}, {"label": "Neut", "current": "Fairon and Williams (2002).", "context": ["Given that understanding the question is crucial for solving the exercise, and that varying the way the questions are asked might cause confusion, we chose to adhere to a fixed format for the questions, cf.", "Fairon and Williams (2002).", "Aggregation."], "vector_1": {"adher": 1, "given": 1, "william": 1, "way": 1, "format": 1, "crucial": 1, "fix": 1, "question": 3, "cf": 1, "chose": 1, "caus": 1, "ask": 1, "fairon": 1, "understand": 1, "aggreg": 1, "exercis": 1, "solv": 1, "confus": 1, "vari": 1, "might": 1}, "marker": "(2002)", "article": "W11-1403", "vector_2": [9, 0.5226408471304043, 1, 3, 0, 0]}, {"label": "Neut", "current": "For example, Hockenmeier (2003) trains a statistical parser based on Combinatory Categorial Grammar (CCG) on the WSJ PTB, but first maps the treebank to CCG derivations semi-automatically.", "context": ["Secondly, the richer the annotation required, the harder it is to adapt the treebank to train parsers which make different assumptions about the structure of syntactic analyses.", "For example, Hockenmeier (2003) trains a statistical parser based on Combinatory Categorial Grammar (CCG) on the WSJ PTB, but first maps the treebank to CCG derivations semi-automatically.", "Thirdly, many (lexical) parameter estimates do not generalize well between domains."], "vector_1": {"domain": 1, "well": 1, "treebank": 2, "parser": 2, "ptb": 1, "annot": 1, "combinatori": 1, "secondli": 1, "hockenmei": 1, "deriv": 1, "thirdli": 1, "differ": 1, "semiautomat": 1, "make": 1, "categori": 1, "exampl": 1, "adapt": 1, "estim": 1, "paramet": 1, "analys": 1, "map": 1, "assumpt": 1, "syntact": 1, "gener": 1, "lexic": 1, "train": 2, "requir": 1, "grammar": 1, "wsj": 1, "harder": 1, "richer": 1, "structur": 1, "base": 1, "statist": 1, "mani": 1, "ccg": 2, "first": 1}, "marker": "(2003)", "article": "W07-2203", "vector_2": [4, 0.05461611175296713, 1, 1, 0, 0]}, {"label": "Pos", "current": "To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).", "context": ["The extracted translations may serve as training data for statistical machine translation systems.", "To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).", "We then added the extracted translation pairs as additional parallel training corpus."], "vector_1": {"corpu": 1, "ad": 1, "evalu": 1, "text": 1, "fbi": 1, "extract": 2, "baselin": 1, "smt": 1, "system": 2, "machin": 1, "may": 1, "effect": 1, "train": 3, "translat": 3, "pair": 1, "data": 1, "parallel": 2, "addit": 1, "serv": 1, "phrasebas": 1, "statist": 1, "purpos": 1, "chineseenglish": 1}, "marker": "(Koehn et al, 2003", "article": "P08-1113", "vector_2": [5, 0.9291700845251257, 3, 1, 1, 0]}, {"label": "Pos", "current": "As the number of samples (sentences) is large we use the normal approximation for z. Siegel and Castellan (1988) describe and motivate this test.", "context": ["For example, to compare the accuracy of two parsers over the same data set.", "As the number of samples (sentences) is large we use the normal approximation for z. Siegel and Castellan (1988) describe and motivate this test.", "We use a 0.05 level of significance, and provide z-value probabilities for significant results reported below."], "vector_1": {"set": 1, "approxim": 1, "parser": 1, "number": 1, "signific": 2, "motiv": 1, "result": 1, "probabl": 1, "use": 2, "describ": 1, "compar": 1, "two": 1, "accuraci": 1, "larg": 1, "zvalu": 1, "test": 1, "castellan": 1, "normal": 1, "sentenc": 1, "siegel": 1, "report": 1, "data": 1, "level": 1, "provid": 1, "exampl": 1, "sampl": 1, "z": 1}, "marker": "(1988)", "article": "W07-2203", "vector_2": [19, 0.5323320379511957, 1, 1, 0, 0]}, {"label": "Neut", "current": "In this regard, we will be examining extension of existing methods for automatically building lexicons of positive/negative words (Turney, 2002; Esuli and Sebastiani, 2005) to the more complex task of estimating also attitude type and force.", "context": ["sions, such as where an attitude is expressed via a noun or a verb.", "In this regard, we will be examining extension of existing methods for automatically building lexicons of positive/negative words (Turney, 2002; Esuli and Sebastiani, 2005) to the more complex task of estimating also attitude type and force.", ""], "vector_1": {"lexicon": 1, "automat": 1, "attitud": 2, "via": 1, "exist": 1, "sion": 1, "positiveneg": 1, "complex": 1, "also": 1, "estim": 1, "build": 1, "type": 1, "method": 1, "express": 1, "forc": 1, "regard": 1, "extens": 1, "verb": 1, "examin": 1, "word": 1, "task": 1, "noun": 1}, "marker": "(Turney, 2002", "article": "N07-1039", "vector_2": [5, 0.9870953788479562, 2, 3, 0, 0]}, {"label": "Neut", "current": "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "context": ["Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).", "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations."], "vector_1": {"among": 1, "semant": 1, "wup": 1, "synonymi": 1, "past": 1, "packag": 1, "set": 1, "wordnet": 2, "use": 1, "instrument": 1, "perform": 1, "databas": 1, "lin": 1, "re": 1, "other": 1, "languag": 1, "test": 1, "import": 1, "jcn": 1, "method": 2, "variou": 1, "relat": 1, "path": 1, "becom": 1, "lch": 1, "measur": 2, "word": 2, "princeton": 1, "also": 1, "wordnetsimilar": 2, "implement": 1, "similar": 2, "propos": 1}, "marker": "(Jiang and Conrath, 1997)", "article": "W14-0118", "vector_2": [17, 0.04608496916189224, 8, 1, 0, 0]}, {"label": "Neut", "current": " AltaVista covers only a fraction of the indexable Web pages available (the fraction was estimated at just 15% by Lawrence and Giles [1999]).", "context": ["The numbers presented in Table 3 are lower bounds, for a number of reasons:", " AltaVista covers only a fraction of the indexable Web pages available (the fraction was estimated at just 15% by Lawrence and Giles [1999]).", "AltaVista may be biased toward North American (mainly English-language) pages by the strategy it uses to crawl the Web."], "vector_1": {"lawrenc": 1, "bound": 1, "bias": 1, "tabl": 1, "mainli": 1, "index": 1, "use": 1, "englishlanguag": 1, "avail": 1, "estim": 1, "fraction": 2, "strategi": 1, "north": 1, "web": 2, "may": 1, "gile": 1, "number": 2, "reason": 1, "altavista": 2, "crawl": 1, "present": 1, "lower": 1, "cover": 1, "american": 1, "toward": 1, "page": 2}, "marker": "[1999]", "article": "J03-3001", "vector_2": [4, 0.5362377540946257, 1, 1, 4, 1]}, {"label": "CoCo", "current": "In design and construction, the system is similar to our submission from last year's workshop (Hanneman et al., 2010), with changes in the methods we employed for training data selection and SCFG filtering.", "context": ["This is our fourth yearly submission to the WMT shared translation task.", "In design and construction, the system is similar to our submission from last year's workshop (Hanneman et al., 2010), with changes in the methods we employed for training data selection and SCFG filtering.", "Continuing WMT's general trend, we worked with more data than in previous years, basing our 2011 system on 13.9 million sentences of parallel French-English training data and an English language model of 1.8 billion words."], "vector_1": {"trend": 1, "share": 1, "continu": 1, "yearli": 1, "design": 1, "year": 2, "select": 1, "filter": 1, "system": 2, "construct": 1, "wmt": 2, "workshop": 1, "languag": 1, "fourth": 1, "submiss": 2, "method": 1, "model": 1, "million": 1, "sentenc": 1, "gener": 1, "previou": 1, "train": 2, "translat": 1, "word": 1, "data": 3, "parallel": 1, "billion": 1, "task": 1, "last": 1, "frenchenglish": 1, "work": 1, "employ": 1, "base": 1, "english": 1, "chang": 1, "scfg": 1, "similar": 1}, "marker": "(Hanneman et al., 2010)", "article": "W11-2143", "vector_2": [1, 0.05440487347703842, 1, 1, 3, 0]}, {"label": "Neut", "current": "  Many studies have concluded that multiword expressions are stored and retrieved as single lexical units (Wray, 2005; Dahlmann and Adolphs, 2007, and references therein).", "context": ["2.2 Typing Behavior and Cognition", "  Many studies have concluded that multiword expressions are stored and retrieved as single lexical units (Wray, 2005; Dahlmann and Adolphs, 2007, and references therein).", "As such, MWEs exhibit unique phonological and prosodic characteristics."], "vector_1": {"therein": 1, "exhibit": 1, "characterist": 1, "retriev": 1, "multiword": 1, "prosod": 1, "phonolog": 1, "type": 1, "express": 1, "uniqu": 1, "lexic": 1, "unit": 1, "singl": 1, "behavior": 1, "conclud": 1, "mani": 1, "studi": 1, "mwe": 1, "cognit": 1, "store": 1, "refer": 1}, "marker": "(Wray, 2005", "article": "W15-0914", "vector_2": [10, 0.20538006928877114, 2, 1, 0, 0]}, {"label": "Neut", "current": "Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).", "context": ["In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing.", "Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).", "The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models."], "vector_1": {"signific": 1, "result": 2, "year": 1, "perceptron": 1, "reduct": 1, "best": 1, "techniqu": 2, "support": 1, "especi": 1, "label": 1, "boost": 1, "machin": 2, "previou": 1, "recallprecis": 1, "variou": 1, "rerank": 4, "gener": 1, "error": 1, "pars": 3, "recent": 1, "algorithm": 1, "employ": 1, "discrimin": 1, "vector": 1, "learn": 1, "improv": 1, "model": 1}, "marker": "(Shen and Joshi, 2003)", "article": "N04-1023", "vector_2": [1, 0.24773332304487056, 3, 2, 4, 1]}, {"label": "Neut", "current": "For example, Krauthammer et al (2004) report the inter-annotater agreement rate of 77.6% for the three way bio-entity classification task.)", "context": ["As naturally expected, our system outperformed the systems that cannot accommodate truly global features (Note that one point of F-score improvement is valuable in this task, because inter-annotator agreement rate of human experts in bio-entity recognition is likely to be about 80%.", "For example, Krauthammer et al (2004) report the inter-annotater agreement rate of 77.6% for the three way bio-entity classification task.)", "and the performance can be said to be at the same level as the best systems."], "vector_1": {"classif": 1, "featur": 1, "krauthamm": 1, "point": 1, "natur": 1, "global": 1, "al": 1, "one": 1, "rate": 2, "expect": 1, "human": 1, "et": 1, "best": 1, "said": 1, "expert": 1, "interannotat": 1, "perform": 1, "system": 3, "note": 1, "recognit": 1, "way": 1, "fscore": 1, "agreement": 2, "cannot": 1, "report": 1, "bioentiti": 2, "truli": 1, "task": 2, "like": 1, "valuabl": 1, "level": 1, "outperform": 1, "accommod": 1, "exampl": 1, "three": 1, "improv": 1, "interannot": 1}, "marker": "(2004)", "article": "W07-1033", "vector_2": [3, 0.8541073124406457, 1, 1, 0, 0]}, {"label": "Pos", "current": "To illustrate the process of converting pre-existing LE systems into GATE-compatible CREOLE sets we use as an example the creation of VIE (Vanilla Information Extraction system) from LaSIE (LargeScale Information Extraction system) (Gaizauskas et al., 1995), Sheffield's entry in the MUC-6 system evaluations.", "context": ["(Modules running as external executables might also be recompiled between runs.)", "To illustrate the process of converting pre-existing LE systems into GATE-compatible CREOLE sets we use as an example the creation of VIE (Vanilla Information Extraction system) from LaSIE (LargeScale Information Extraction system) (Gaizauskas et al., 1995), Sheffield's entry in the MUC-6 system evaluations.", "LaSIE module interfaces were not standardised when originally produced and its CREOLEization gives a good indication of the ease of integrating other LE tools into GATE."], "vector_1": {"origin": 1, "set": 1, "execut": 1, "vie": 1, "process": 1, "modul": 2, "creation": 1, "standardis": 1, "indic": 1, "inform": 2, "largescal": 1, "extract": 2, "use": 1, "le": 2, "evalu": 1, "also": 1, "give": 1, "system": 4, "creol": 1, "illustr": 1, "interfac": 1, "muc": 1, "gate": 1, "might": 1, "good": 1, "run": 2, "sheffield": 1, "tool": 1, "preexist": 1, "extern": 1, "eas": 1, "convert": 1, "gatecompat": 1, "entri": 1, "vanilla": 1, "creoleiz": 1, "recompil": 1, "exampl": 1, "integr": 1, "lasi": 2, "produc": 1}, "marker": "(Gaizauskas et al., 1995)", "article": "A97-2017", "vector_2": [2, 0.9429899681918278, 1, 1, 1, 1]}, {"label": "Pos", "current": "To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).", "context": ["The extracted translations may serve as training data for statistical machine translation systems.", "To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).", "We then added the extracted translation pairs as additional parallel training corpus."], "vector_1": {"corpu": 1, "ad": 1, "evalu": 1, "text": 1, "fbi": 1, "extract": 2, "baselin": 1, "smt": 1, "system": 2, "machin": 1, "may": 1, "effect": 1, "train": 3, "translat": 3, "pair": 1, "data": 1, "parallel": 2, "addit": 1, "serv": 1, "phrasebas": 1, "statist": 1, "purpos": 1, "chineseenglish": 1}, "marker": "Brants et al, 2007)", "article": "P08-1113", "vector_2": [1, 0.9291700845251257, 3, 1, 1, 0]}, {"label": "Neut", "current": "The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).", "context": ["Linguistic analysis.", "The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).", "Frame semantic analysis is especially interesting for the task of recognising textual entailment as it offers a robust yet relatively precise measure for semantic overlap."], "vector_1": {"lfg": 1, "entail": 1, "frame": 4, "semant": 3, "shalmanes": 1, "overlap": 1, "primari": 1, "rel": 1, "develop": 1, "offer": 1, "system": 3, "textual": 1, "yet": 1, "compon": 1, "role": 1, "interest": 1, "analysi": 3, "parc": 1, "recognis": 1, "especi": 1, "detour": 1, "probabilist": 2, "robust": 1, "measur": 1, "task": 1, "grammar": 1, "rulebas": 1, "annot": 2, "precis": 1, "combin": 1, "english": 1, "linguist": 2, "assign": 1}, "marker": "(Burchardt et al., 2005)", "article": "W07-1402", "vector_2": [2, 0.18257587671017456, 3, 1, 3, 0]}, {"label": "Neut", "current": "In the case of the language pair English-toGerman, these results are contrasted to the ones obtained when translating the same test set with Moses (Koehn et al., 2007).It is observed that for the English-to-German language pair, PRESEMT achieved approximately 50% of the MOSES BLEU score and 80% of the MOSES with respect to the Meteor and TER scores.", "context": ["Table 1 illustrates an indicative set of results obtained by running automatic evaluation metrics on test data translated by the 1st PRESEMT prototype for a selection of language pairs, due to space restrictions.", "In the case of the language pair English-toGerman, these results are contrasted to the ones obtained when translating the same test set with Moses (Koehn et al., 2007).It is observed that for the English-to-German language pair, PRESEMT achieved approximately 50% of the MOSES BLEU score and 80% of the MOSES with respect to the Meteor and TER scores.", "These are reasonably competitive results compared to an established system such as Moses."], "vector_1": {"bleu": 1, "set": 2, "evalu": 1, "illustr": 1, "metric": 1, "tabl": 1, "obtain": 2, "automat": 1, "one": 1, "restrict": 1, "englishtogerman": 2, "result": 3, "presemt": 2, "respect": 1, "ter": 1, "establish": 1, "languag": 3, "system": 1, "compar": 1, "space": 1, "due": 1, "score": 2, "select": 1, "translat": 2, "test": 2, "approxim": 1, "contrast": 1, "mose": 4, "competit": 1, "run": 1, "meteor": 1, "reason": 1, "indic": 1, "pair": 3, "data": 1, "case": 1, "st": 1, "achiev": 1, "observ": 1, "prototyp": 1}, "marker": "(Koehn et al., 2007)", "article": "W12-0108", "vector_2": [5, 0.8856602882116702, 1, 1, 0, 0]}, {"label": "Neut", "current": "There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).", "context": ["3.6 Large Margin Classifiers", "There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).", "The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization."], "vector_1": {"quit": 1, "svm": 2, "linear": 2, "superior": 1, "perform": 1, "boost": 1, "winnow": 1, "maxim": 1, "classifi": 3, "separ": 1, "larg": 2, "perceptron": 1, "sampl": 1, "abil": 1, "margin": 3}, "marker": "(Schapire et al., 1997)", "article": "N04-1023", "vector_2": [7, 0.5118253019020795, 4, 1, 0, 0]}, {"label": "Pos", "current": "It is shown that the SET-10 test scores can predict different levels on the Oral Interaction Scale of the Council of Europe's Framework (North, 2000) for describing oral proficiency of second/foreign language speakers with reasonable accuracy.", "context": ["In Bernstein et al (2000), an experiment is performed to investigate the performance of the SET-10 test in predicting speakers' oral proficiency.", "It is shown that the SET-10 test scores can predict different levels on the Oral Interaction Scale of the Council of Europe's Framework (North, 2000) for describing oral proficiency of second/foreign language speakers with reasonable accuracy.", "This paper further reports on studies done to correlate the SET-10 automated scores with the human scores from two other tests of oral English communication skills."], "vector_1": {"set": 3, "profici": 2, "predict": 2, "al": 1, "secondforeign": 1, "paper": 1, "done": 1, "council": 1, "et": 1, "skill": 1, "languag": 1, "shown": 1, "scale": 1, "europ": 1, "describ": 1, "commun": 1, "perform": 2, "interact": 1, "two": 1, "accuraci": 1, "score": 3, "speaker": 2, "test": 3, "experi": 1, "investig": 1, "human": 1, "differ": 1, "autom": 1, "framework": 1, "reason": 1, "report": 1, "bernstein": 1, "level": 1, "correl": 1, "english": 1, "studi": 1, "oral": 4}, "marker": "(North, 2000)", "article": "W08-0912", "vector_2": [8, 0.17200972805659961, 2, 1, 0, 0]}, {"label": "Neut", "current": "AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.", "context": ["2 System Architecture", "AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.", "In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions."], "vector_1": {"classif": 1, "stump": 1, "predict": 1, "obtain": 1, "rate": 1, "design": 1, "domain": 1, "highli": 1, "use": 1, "accur": 2, "system": 2, "classifi": 1, "version": 1, "decis": 1, "simpl": 1, "method": 1, "architectur": 1, "may": 1, "gener": 2, "weak": 2, "moder": 1, "algorithm": 1, "hypothes": 1, "work": 1, "adaboostmh": 1, "rule": 1, "combin": 1, "adaboost": 2, "partit": 1, "mani": 1, "confid": 1}, "marker": "(Freund and Schapire, 1997)", "article": "W01-0726", "vector_2": [4, 0.08214741387771125, 2, 1, 0, 0]}, {"label": "Neut", "current": "Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.", "context": ["Since the first release of WordNet, researchers have tried to use it to simulate similarity.", "Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.", "Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011)."], "vector_1": {"synonymi": 1, "joubarn": 1, "releas": 1, "wordnet": 1, "lesk": 1, "use": 1, "describ": 1, "anoth": 1, "inkpen": 1, "except": 1, "research": 1, "languag": 1, "approach": 1, "across": 1, "differ": 1, "hyponymi": 1, "base": 1, "pair": 1, "sinc": 1, "tri": 1, "measur": 2, "algorithm": 1, "simul": 1, "vector": 2, "similar": 2, "first": 1}, "marker": "(Patwardhan and Pedersen, 2006)", "article": "W14-0118", "vector_2": [8, 0.16076516076516076, 4, 2, 9, 0]}, {"label": "Neut", "current": "Unknown sense detection is related to word sense disambiguation (WSD) and to word sense discrimination (Schutze, 1998), but differs from both.", "context": ["against which new occurrences are compared will consist of sense-annotated text.", "Unknown sense detection is related to word sense disambiguation (WSD) and to word sense discrimination (Schutze, 1998), but differs from both.", "In WSD all senses are assumed known, and the task is to select one of them, while in unknown sense detection the task is to decide whether a given occurrence matches any of the known senses or none of them, and all training instances, regardless of the sense to which they belong, are modeled as one group of known data."], "vector_1": {"regardless": 1, "text": 1, "one": 2, "senseannot": 1, "assum": 1, "select": 1, "differ": 1, "detect": 2, "group": 1, "compar": 1, "unknown": 2, "belong": 1, "disambigu": 1, "sens": 7, "occurr": 2, "match": 1, "relat": 1, "decid": 1, "train": 1, "new": 1, "known": 3, "given": 1, "data": 1, "none": 1, "task": 2, "word": 2, "wsd": 2, "consist": 1, "whether": 1, "discrimin": 1, "instanc": 1, "model": 1}, "marker": "(Schutze, 1998)", "article": "N06-1017", "vector_2": [8, 0.05838375274577055, 1, 1, 0, 0]}, {"label": "Neut", "current": "The monotonicity calculus developed in (Sanchez Valencia, 1991) explains these polarity effects by (1) defining an entailment relation over multifarious expressions of natural language, (2) defining monotonicity properties of semantic functions, and finally (3) specifying how monotonicities combine during Fregean composition of semantic functions.", "context": ["Other elements can only be contracted (not expanded) salva veritate, and thus have negative polarity: meal can be narrowed to dinner.", "The monotonicity calculus developed in (Sanchez Valencia, 1991) explains these polarity effects by (1) defining an entailment relation over multifarious expressions of natural language, (2) defining monotonicity properties of semantic functions, and finally (3) specifying how monotonicities combine during Fregean composition of semantic functions.", "The entailment relation."], "vector_1": {"semant": 2, "composit": 1, "entail": 2, "natur": 1, "salva": 1, "monoton": 3, "languag": 1, "develop": 1, "neg": 1, "explain": 1, "fregean": 1, "calculu": 1, "final": 1, "function": 2, "polar": 2, "express": 1, "relat": 2, "effect": 1, "multifari": 1, "specifi": 1, "expand": 1, "verit": 1, "thu": 1, "properti": 1, "contract": 1, "element": 1, "defin": 2, "dinner": 1, "combin": 1, "narrow": 1, "meal": 1}, "marker": "(Sanchez Valencia, 1991)", "article": "W07-1431", "vector_2": [16, 0.16643807433822522, 1, 2, 0, 0]}, {"label": "Neut", "current": "Nearest neighbors (by Euclidean distance) were computed using the ANN tool (Mount and Arya, 2005).", "context": ["We model unknown sense detection as an outlier detection task, using Tax and Duin's outlier detection approach that we have outlined in the previous section.", "Nearest neighbors (by Euclidean distance) were computed using the ANN tool (Mount and Arya, 2005).", "We compute one outlier detection model per lemma."], "vector_1": {"nearest": 1, "distanc": 1, "comput": 2, "ann": 1, "duin": 1, "one": 1, "use": 2, "detect": 4, "unknown": 1, "section": 1, "per": 1, "lemma": 1, "sens": 1, "approach": 1, "outlier": 3, "tax": 1, "tool": 1, "previou": 1, "task": 1, "outlin": 1, "euclidean": 1, "neighbor": 1, "model": 2}, "marker": "(Mount and Arya, 2005)", "article": "N06-1017", "vector_2": [1, 0.5489999614628694, 1, 1, 0, 0]}, {"label": "Pos", "current": "To perform feature selection, we followed the approach used in Shah et al (2013) and ranked the features according to their learned length scales (from the lowest to the highest).", "context": ["To avoid poor hyperparameter values due to this, we performed a two-step procedure where we first optimise a model with all the SE length scales tied to the same value (which is equivalent to an isotropic model) and we used the resulting values as starting point for the ARD optimisation.", "To perform feature selection, we followed the approach used in Shah et al (2013) and ranked the features according to their learned length scales (from the lowest to the highest).", "The length scales of a feature can be interpreted as the relevance of such feature for the model."], "vector_1": {"lowest": 1, "featur": 4, "point": 1, "al": 1, "procedur": 1, "interpret": 1, "ard": 1, "result": 1, "et": 1, "follow": 1, "optimis": 2, "use": 2, "scale": 3, "equival": 1, "perform": 2, "avoid": 1, "due": 1, "relev": 1, "start": 1, "isotrop": 1, "select": 1, "tie": 1, "shah": 1, "approach": 1, "poor": 1, "accord": 1, "rank": 1, "highest": 1, "model": 3, "valu": 3, "twostep": 1, "length": 3, "learn": 1, "hyperparamet": 1, "se": 1, "first": 1}, "marker": "(2013)", "article": "W13-2241", "vector_2": [0, 0.4564673157162726, 1, 4, 7, 0]}, {"label": "Neut", "current": "We will list these only briefly here, since they are described in more detail elsewhere in a sentence, the following heuristics are ap(Garside and Rayson, 1997).", "context": ["As in the case of grammatical tagging, the task of semantic tagging subdivides broadly into two phases: Phase I (Tag assignment): attaching a set of potential semantic tags to each lexical unit and Phase II (Tag disambiguation): selecting the contextually appropriate semantic tag from the set provided by Phase I. USAS makes use of seven major techniques or sources of information in phase II.", "We will list these only briefly here, since they are described in more detail elsewhere in a sentence, the following heuristics are ap(Garside and Rayson, 1997).", "plied in sequence:"], "vector_1": {"semant": 3, "set": 2, "seven": 1, "detail": 1, "ii": 2, "ap": 1, "tag": 6, "heurist": 1, "follow": 1, "select": 1, "use": 1, "techniqu": 1, "usa": 1, "make": 1, "elsewher": 1, "two": 1, "attach": 1, "disambigu": 1, "unit": 1, "sequenc": 1, "sourc": 1, "subdivid": 1, "sentenc": 1, "lexic": 1, "briefli": 1, "appropri": 1, "broadli": 1, "phase": 5, "major": 1, "sinc": 1, "pli": 1, "describ": 1, "case": 1, "grammat": 1, "task": 1, "provid": 1, "list": 1, "contextu": 1, "inform": 1, "potenti": 1, "assign": 1}, "marker": "(Garside and Rayson, 1997)", "article": "W03-1807", "vector_2": [6, 0.404849827671098, 1, 1, 1, 0]}, {"label": "Weak", "current": "3 There are several other published implementation of chart parsers [23,20,33], but they often do not give much detail on the output of the parsing process, or even side-step the problem altogether [33].", "context": ["  2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are only of theoretical interest here [35,5].", "3 There are several other published implementation of chart parsers [23,20,33], but they often do not give much detail on the output of the parsing process, or even side-step the problem altogether [33].", "We do not consider here the well formed stastring tables of Shell [26] which falls somewhere in between in our classification."], "vector_1": {"classif": 1, "often": 1, "give": 1, "process": 1, "parser": 1, "cf": 1, "shell": 1, "publish": 1, "tabl": 1, "complex": 1, "sever": 1, "even": 1, "detail": 1, "theoret": 1, "much": 1, "interest": 1, "lowest": 1, "sidestep": 1, "recogn": 1, "form": 1, "asymptot": 1, "altogeth": 1, "chart": 1, "stastr": 1, "pars": 1, "consid": 2, "fall": 1, "problem": 1, "somewher": 1, "well": 1, "output": 1, "implement": 1}, "marker": "[23]", "article": "P89-1018", "vector_2": [2, 0.1401825415399017, 7, 1, 0, 0]}, {"label": "Neut", "current": "Table 1: BLEU scores reported in (SMT Team, 2003).", "context": ["do 4: compute and for all ; 5: for ( ) do 6: if and and then", "Table 1: BLEU scores reported in (SMT Team, 2003).", "Every single feature was combined with the 6 baseline features for the training and test."], "vector_1": {"bleu": 1, "featur": 2, "comput": 1, "train": 1, "test": 1, "score": 1, "combin": 1, "tabl": 1, "report": 1, "everi": 1, "singl": 1, "baselin": 1}, "marker": "(SMT Team, 2003)", "article": "N04-1023", "vector_2": [1, 0.7746440834908754, 1, 8, 0, 0]}, {"label": "CoCo", "current": "In order to do this, we compare the correlations that Pedersen (2010) reports when calculating the correlations between the original gold standards and the scores from the six similarity measures using WordNet::Similarity to the same procedure but using the WordNetTools to compute the similarity scores.", "context": ["can we reproduce the results of WordNet::Similarity with the original WordNet database with WordnetTools with the WordnetLMF version of the English WordNet.", "In order to do this, we compare the correlations that Pedersen (2010) reports when calculating the correlations between the original gold standards and the scores from the six similarity measures using WordNet::Similarity to the same procedure but using the WordNetTools to compute the similarity scores.", "We used the following settings for WordNetTools:7 -lmf-file Path to WordNet in LMF format -pos no pos-filter was used -relations has hypernym, has hyperonym, -input path to English gold standards -pairs \"words\" -method all."], "vector_1": {"origin": 2, "hypernym": 1, "set": 1, "comput": 1, "gold": 2, "procedur": 1, "pair": 1, "posfilt": 1, "correl": 2, "result": 1, "path": 2, "follow": 1, "wordnet": 3, "use": 4, "wordnetlmf": 1, "score": 2, "compar": 1, "databas": 1, "six": 1, "pedersen": 1, "hyperonym": 1, "version": 1, "input": 1, "reproduc": 1, "po": 1, "format": 1, "relat": 1, "standard": 2, "report": 1, "lmffile": 1, "lmf": 1, "measur": 1, "word": 1, "wordnettool": 3, "method": 1, "calcul": 1, "wordnetsimilar": 2, "english": 2, "similar": 2, "order": 1}, "marker": "(2010)", "article": "W14-0118", "vector_2": [4, 0.731724116339501, 1, 5, 6, 0]}, {"label": "Neut", "current": "In (SMT Team, 2003), aggressive search was used to combine features.", "context": ["Table 1 shows some of the best performing features.", "In (SMT Team, 2003), aggressive search was used to combine features.", "After combining about a dozen features, the BLEU score did not improve any more, and the score was 32.9%."], "vector_1": {"bleu": 1, "aggress": 1, "search": 1, "featur": 3, "show": 1, "perform": 1, "use": 1, "score": 2, "combin": 2, "tabl": 1, "improv": 1, "best": 1, "dozen": 1}, "marker": "(SMT Team, 2003)", "article": "N04-1023", "vector_2": [1, 0.8113353138624175, 1, 8, 0, 0]}, {"label": "Pos", "current": "An intuitive approach relies mainly on the developers' intuition and experience (Allen, 2009) that leads to using less lexical diversity, less sophisticated words, less syntactic complexity, and greater cohesion.", "context": ["Simplifying texts to provide more comprehensible input to a targeted audience the developers generally work within two approaches: an intuitive approach and a structural approach.", "An intuitive approach relies mainly on the developers' intuition and experience (Allen, 2009) that leads to using less lexical diversity, less sophisticated words, less syntactic complexity, and greater cohesion.", "A structural approach depends on the use of structure and word lists that are predefined by the intelligence level, as typically found in targeted readers."], "vector_1": {"less": 3, "intellig": 1, "text": 1, "within": 1, "cohes": 1, "mainli": 1, "predefin": 1, "greater": 1, "use": 2, "develop": 2, "lead": 1, "two": 1, "reli": 1, "complex": 1, "reader": 1, "comprehens": 1, "input": 1, "experi": 1, "approach": 5, "divers": 1, "simplifi": 1, "syntact": 1, "gener": 1, "intuit": 3, "lexic": 1, "sophist": 1, "depend": 1, "word": 2, "target": 2, "level": 1, "provid": 1, "work": 1, "list": 1, "structur": 3, "audienc": 1, "found": 1, "typic": 1}, "marker": "(Allen, 2009)", "article": "W14-5605", "vector_2": [5, 0.19950751421608448, 1, 5, 0, 0]}, {"label": "Neut", "current": "By representing the English and Chinese sentences as eI1 = e1e2...eI and cJ1 = c1c2...cJ, respectively, where ei and cj represent single elements of the sentences, we define their alignment as aK1 , of which each element is a span ak =< s, t > and represents the alignment of the English word es with the Chinese character ct. Then, the corpus of unlabeled bilingual data can be represented as the set of sentence tuples <eI1, cJ1 , aK1 > To obtain the character-based alignment, we employ an open-source toolkit Pialign1 ((Neubig et al., 2011); (Neubig et al., 2012)) which uses Bayesian learning and inversion transduction grammars.", "context": ["Then, they are associated with English words using a statistical word aligner.", "By representing the English and Chinese sentences as eI1 = e1e2...eI and cJ1 = c1c2...cJ, respectively, where ei and cj represent single elements of the sentences, we define their alignment as aK1 , of which each element is a span ak =< s, t > and represents the alignment of the English word es with the Chinese character ct. Then, the corpus of unlabeled bilingual data can be represented as the set of sentence tuples <eI1, cJ1 , aK1 > To obtain the character-based alignment, we employ an open-source toolkit Pialign1 ((Neubig et al., 2011); (Neubig et al., 2012)) which uses Bayesian learning and inversion transduction grammars.", "3.2.2 Features Obtained from the"], "vector_1": {"corpu": 1, "cj": 3, "set": 1, "ei": 3, "ak": 3, "obtain": 2, "repres": 4, "featur": 1, "learn": 1, "opensourc": 1, "respect": 1, "element": 2, "data": 1, "es": 1, "ct": 1, "use": 2, "span": 1, "pialign": 1, "associ": 1, "characterbas": 1, "3": 1, "invers": 1, "tupl": 1, "toolkit": 1, "singl": 1, "sentenc": 3, "chines": 2, "bilingu": 1, "unlabel": 1, "grammar": 1, "bayesian": 1, "cccj": 1, "eeei": 1, "word": 3, "align": 4, "charact": 1, "employ": 1, "defin": 1, "statist": 1, "english": 3, "transduct": 1}, "marker": "(Neubig et al., 2011)", "article": "D15-1142", "vector_2": [4, 0.4062061170668219, 2, 1, 1, 0]}, {"label": "Neut", "current": "SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.", "context": ["This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).", "SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.", "More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output."], "vector_1": {"set": 1, "number": 1, "templat": 1, "minimum": 1, "featur": 3, "och": 1, "use": 4, "smt": 1, "also": 1, "larg": 1, "approach": 2, "wellformed": 1, "function": 2, "syntact": 1, "team": 1, "differ": 1, "train": 1, "output": 1, "align": 1, "mt": 1, "error": 1, "improv": 1, "order": 1}, "marker": "(2003)", "article": "N04-1023", "vector_2": [1, 0.22354257494502103, 3, 8, 0, 0]}, {"label": "Neut", "current": "Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU.", "context": ["While the total number of feature functions was small, each feature function was a complex statistical model by itself, as for example, the alignment template feature functions used in this approach.", "Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU.", "The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation."], "vector_1": {"bleu": 1, "featur": 3, "directli": 1, "show": 1, "metric": 1, "number": 1, "templat": 1, "rate": 1, "minimum": 1, "significantli": 1, "total": 1, "paramet": 1, "och": 1, "use": 3, "describ": 1, "inform": 1, "better": 1, "complex": 1, "criterion": 1, "evalu": 1, "estim": 1, "experi": 1, "approach": 2, "function": 3, "optim": 1, "mutual": 1, "obtain": 1, "train": 1, "automat": 1, "align": 1, "maximum": 1, "mt": 1, "exampl": 1, "statist": 1, "error": 2, "small": 1, "model": 1, "result": 1}, "marker": "(2003)", "article": "N04-1023", "vector_2": [1, 0.20564064971642423, 1, 6, 5, 1]}, {"label": "Neut", "current": "The parser's output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1.", "context": ["4 The Evaluation Scheme", "The parser's output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1.", "Relations are organized into a hierarchy with the root node specifying an unlabeled dependency."], "vector_1": {"node": 1, "measur": 1, "use": 1, "relat": 2, "depend": 2, "organ": 1, "evalu": 3, "f": 1, "parser": 1, "specifi": 1, "standard": 1, "precis": 1, "hierarchi": 1, "output": 1, "scheme": 2, "root": 1, "recal": 1, "unlabel": 1}, "marker": "(Carroll, et al., 1998", "article": "W07-2203", "vector_2": [9, 0.4953261212057557, 2, 1, 3, 1]}, {"label": "CoCo", "current": "In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004), our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004).", "context": ["It also achieves a relative error rate reduction of 6.56% as compared with the previous work in (Wu and Wang, 2004).", "In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004), our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004).", "We also use in-domain corpora and out-of-domain corpora of different sizes to perform adaptation experiments."], "vector_1": {"corpu": 1, "corpora": 2, "rate": 2, "reduct": 2, "size": 1, "use": 1, "describ": 1, "compar": 2, "perform": 1, "outofdomain": 1, "also": 2, "adapt": 1, "rel": 1, "experi": 1, "method": 2, "previou": 1, "differ": 1, "train": 1, "smallerscal": 1, "addit": 1, "indomain": 2, "work": 1, "achiev": 2, "error": 2, "model": 1}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.9774462313505135, 3, 13, 0, 1]}, {"label": "Neut", "current": "For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003).", "context": ["However, SVMs are extremely slow in training since they need to solve a quadratic programming search.", "For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003).", "Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data."], "vector_1": {"treebank": 1, "perceptronlik": 1, "need": 1, "quadrat": 1, "perceptron": 1, "even": 1, "use": 2, "slow": 1, "fast": 1, "program": 1, "take": 1, "penn": 1, "experi": 1, "allow": 1, "rerank": 1, "account": 1, "realworld": 1, "train": 3, "pars": 1, "solv": 1, "data": 1, "sinc": 2, "search": 1, "svm": 2, "algorithm": 2, "howev": 1, "us": 1, "exampl": 1, "cannot": 1, "extrem": 1, "whole": 1}, "marker": "(Shen and Joshi, 2003)", "article": "N04-1023", "vector_2": [1, 0.5215093174891007, 1, 2, 4, 1]}, {"label": "Neut", "current": "Intuitively, the reason stress helps infants in segmenting English is that a stressed syllable is a reliable indicator of the beginning of a word (Jusczyk et al., 1993).", "context": ["In order for stress cues to be helpful, the model must have some way of associating the position of stress with word-boundaries.", "Intuitively, the reason stress helps infants in segmenting English is that a stressed syllable is a reliable indicator of the beginning of a word (Jusczyk et al., 1993).", "More generally, if there is a (reasonably) reliable relationship between the position of stressed syllables and beginnings (or"], "vector_1": {"infant": 1, "help": 2, "wordboundari": 1, "associ": 1, "cue": 1, "way": 1, "begin": 2, "relationship": 1, "gener": 1, "intuit": 1, "reason": 2, "indic": 1, "segment": 1, "must": 1, "stress": 5, "word": 1, "reliabl": 2, "syllabl": 2, "english": 1, "posit": 2, "model": 1, "order": 1}, "marker": "(Jusczyk et al., 1993)", "article": "Q14-1008", "vector_2": [21, 0.3353542808941375, 1, 4, 11, 0]}, {"label": "Neut", "current": "The analysis is based on FrameNet (Baker et al., 1998), a resource that lists senses and semantic roles for English expressions.", "context": ["Figure 1 shows an example, a sentence from the Hound of the Baskervilles, analyzed by the SHALMANESER (Erk and Pado, 2006) shallow semantic parser.", "The analysis is based on FrameNet (Baker et al., 1998), a resource that lists senses and semantic roles for English expressions.", "FrameNet is lacking a sense of \"expectation\" or \"being mentally prepared\" for the verb prepare, so prepared has been assigned the sense COOKING CREATION, a possible but improbable analysis2."], "vector_1": {"semant": 2, "creation": 1, "mental": 1, "show": 1, "parser": 1, "shalmanes": 1, "expect": 1, "hound": 1, "figur": 1, "role": 1, "analyz": 1, "sens": 3, "baskervil": 1, "analysi": 2, "verb": 1, "lack": 1, "resourc": 1, "sentenc": 1, "shallow": 1, "base": 1, "cook": 1, "possibl": 1, "prepar": 3, "list": 1, "improb": 1, "express": 1, "exampl": 1, "english": 1, "framenet": 2, "assign": 1}, "marker": "(Baker et al., 1998)", "article": "N06-1017", "vector_2": [8, 0.1035878068519018, 2, 2, 0, 0]}, {"label": "Neut", "current": "However, efficient extraction of MWEs still remains an unsolved issue, to the extent that Sag et al (2001b) call it \"a pain in the neck of NLP\".", "context": ["A number of approaches have been suggested and tested to address this problem.", "However, efficient extraction of MWEs still remains an unsolved issue, to the extent that Sag et al (2001b) call it \"a pain in the neck of NLP\".", "In this paper, we present our work in which we approach the issue of MWE extraction by using a semantic field annotator."], "vector_1": {"nlp": 1, "semant": 1, "effici": 1, "sag": 1, "number": 1, "paper": 1, "et": 1, "still": 1, "extract": 2, "use": 1, "suggest": 1, "field": 1, "call": 1, "mwe": 2, "test": 1, "approach": 2, "pain": 1, "al": 1, "extent": 1, "address": 1, "present": 1, "neck": 1, "howev": 1, "work": 1, "annot": 1, "issu": 2, "remain": 1, "unsolv": 1, "problem": 1}, "marker": "(2001b)", "article": "W03-1807", "vector_2": [2, 0.07574265550631873, 1, 3, 0, 0]}, {"label": "Neut", "current": "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "context": ["Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).", "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations."], "vector_1": {"among": 1, "semant": 1, "wup": 1, "synonymi": 1, "past": 1, "packag": 1, "set": 1, "wordnet": 2, "use": 1, "instrument": 1, "perform": 1, "databas": 1, "lin": 1, "re": 1, "other": 1, "languag": 1, "test": 1, "import": 1, "jcn": 1, "method": 2, "variou": 1, "relat": 1, "path": 1, "becom": 1, "lch": 1, "measur": 2, "word": 2, "princeton": 1, "also": 1, "wordnetsimilar": 2, "implement": 1, "similar": 2, "propos": 1}, "marker": "(Leacock and Chodorow, 1998)", "article": "W14-0118", "vector_2": [16, 0.04608496916189224, 8, 1, 1, 0]}, {"label": "Pos", "current": "get construction-we use two corpora: the British National Corpus (Burnard, 2000), an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008), approximately one billion words of non-newswire text from the New York Times from the years 1987-2006.", "context": ["62", "get construction-we use two corpora: the British National Corpus (Burnard, 2000), an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008), approximately one billion words of non-newswire text from the New York Times from the years 1987-2006.", "We extract all sentences in these corpora containing the sequence of strings no, is too, and to separated by one or more words."], "vector_1": {"corpu": 3, "text": 1, "approxim": 2, "year": 1, "latetwentieth": 1, "sequenc": 1, "centuri": 1, "extract": 1, "use": 1, "constructionw": 1, "two": 1, "contain": 1, "new": 2, "string": 1, "get": 1, "sentenc": 1, "hundr": 1, "million": 1, "nation": 1, "york": 2, "one": 3, "nonnewswir": 1, "billion": 1, "word": 3, "english": 1, "corpora": 2, "annot": 1, "british": 2, "separ": 1, "time": 2}, "marker": "(Burnard, 2000)", "article": "W10-2109", "vector_2": [10, 0.245115968533194, 2, 1, 0, 0]}, {"label": "Neut", "current": "We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999), a memory-based learner (MBL), for both phases.", "context": ["6.1 Details of the experiments", "We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999), a memory-based learner (MBL), for both phases.", "We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001); allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space."], "vector_1": {"mbl": 2, "set": 1, "well": 1, "featur": 1, "suffer": 1, "use": 3, "space": 1, "learner": 1, "detail": 1, "fragment": 1, "timbl": 2, "experi": 1, "numer": 1, "shown": 1, "phase": 1, "data": 1, "class": 1, "work": 1, "memorybas": 1, "allow": 1, "small": 1, "textbas": 1}, "marker": "Daelemans et al., 1999)", "article": "W12-2011", "vector_2": [13, 0.7237294844858561, 3, 1, 1, 0]}, {"label": "Neut", "current": "As a post-processing, in the En2Es direction we used a POS target LM as a feature (instead of the target language model based on classes) that allowed to recover the segmentations (de Gispert, 2006).", "context": ["In particular, the pronouns attached to the verb were separated and contractions as del or al were splitted into de el or a el.", "As a post-processing, in the En2Es direction we used a POS target LM as a feature (instead of the target language model based on classes) that allowed to recover the segmentations (de Gispert, 2006).", "4.3 Experiments and Results"], "vector_1": {"el": 2, "featur": 1, "pronoun": 1, "al": 1, "result": 1, "languag": 1, "recov": 1, "use": 1, "ene": 1, "lm": 1, "attach": 1, "split": 1, "postprocess": 1, "instead": 1, "experi": 1, "po": 1, "de": 1, "direct": 1, "verb": 1, "particular": 1, "segment": 1, "class": 1, "target": 2, "contract": 1, "separ": 1, "base": 1, "del": 1, "allow": 1, "model": 1}, "marker": "(de Gispert, 2006)", "article": "W08-0315", "vector_2": [2, 0.7856735566642908, 1, 4, 0, 1]}, {"label": "Neut", "current": "(Diaz-Negrillo and Fernandez-Dominguez, 2006; Boyd, 2010)).", "context": ["how errors are described in different taxonomies, e.g.", "(Diaz-Negrillo and Fernandez-Dominguez, 2006; Boyd, 2010)).", "Specific error types are unlikely to recur, making sparsity even more of a concern."], "vector_1": {"even": 1, "sparsiti": 1, "differ": 1, "recur": 1, "describ": 1, "specif": 1, "eg": 1, "unlik": 1, "error": 2, "taxonomi": 1, "type": 1, "make": 1, "concern": 1}, "marker": "(Diaz-Negrillo and Fernandez-Dominguez, 2006", "article": "W12-2011", "vector_2": [6, 0.05605797631010018, 2, 2, 0, 0]}, {"label": "Neut", "current": "However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB.", "context": ["Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001).", "However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB.", "They constrain the training data (parses) considered within the IOA to those consistent with the constituent boundaries defined by the bracketing."], "vector_1": {"em": 1, "constitu": 1, "appli": 1, "parser": 1, "maxim": 1, "expect": 1, "extract": 1, "pcfg": 1, "unsuccess": 1, "semisupervis": 1, "boundari": 1, "insideoutsid": 1, "adapt": 1, "rel": 1, "method": 1, "unlabel": 1, "includ": 1, "within": 1, "ptb": 1, "schabe": 1, "train": 2, "pars": 1, "consid": 1, "date": 1, "data": 2, "ioa": 3, "consist": 1, "algorithm": 1, "unsupervis": 1, "defin": 1, "bracket": 2, "constrain": 1, "pereira": 1, "howev": 1}, "marker": "(1992)", "article": "W07-2203", "vector_2": [15, 0.09197213177887477, 3, 3, 0, 0]}, {"label": "Neut", "current": "Stress is better organized into hierarchical patterns constructed on top of syllables that vary in relative prominence based on the domain of each level of the hierarchy, and generally languages avoid adjacent strong syllables (Liberman and Prince, 1977).", "context": ["It should be noted that the classification of every syllable as \"weak\" or \"strong\" is a significant simplification.", "Stress is better organized into hierarchical patterns constructed on top of syllables that vary in relative prominence based on the domain of each level of the hierarchy, and generally languages avoid adjacent strong syllables (Liberman and Prince, 1977).", "We later discuss a manipulation of the corpus used by Yang (2004) to address this concern."], "vector_1": {"corpu": 1, "classif": 1, "later": 1, "domain": 1, "signific": 1, "hierarchi": 1, "vari": 1, "languag": 1, "concern": 1, "hierarch": 1, "better": 1, "use": 1, "pattern": 1, "top": 1, "construct": 1, "note": 1, "adjac": 1, "rel": 1, "everi": 1, "promin": 1, "gener": 1, "manipul": 1, "weak": 1, "base": 1, "address": 1, "strong": 2, "discuss": 1, "stress": 1, "organ": 1, "simplif": 1, "level": 1, "syllabl": 3, "yang": 1, "avoid": 1}, "marker": "(Liberman and Prince, 1977)", "article": "W10-2912", "vector_2": [33, 0.3531976307164978, 2, 1, 0, 0]}, {"label": "Neut", "current": "Ryu and Choi (2006) also used term frequency as a measure of domain specificity, but instead of partitioning they combined term frequency and distributional similarity to construct hierarchy.", "context": ["We use term frequency to determine the position of a concept in the hierarchy.", "Ryu and Choi (2006) also used term frequency as a measure of domain specificity, but instead of partitioning they combined term frequency and distributional similarity to construct hierarchy.", "Other method similar to our work is proposed in Fountain and Lapata (2012)."], "vector_1": {"domain": 1, "concept": 1, "hierarchi": 2, "distribut": 1, "use": 2, "lapata": 1, "construct": 1, "also": 1, "fountain": 1, "instead": 1, "ryu": 1, "method": 1, "determin": 1, "measur": 1, "term": 3, "specif": 1, "frequenc": 3, "work": 1, "combin": 1, "partit": 1, "choi": 1, "posit": 1, "similar": 2, "propos": 1}, "marker": "(2006)", "article": "W12-5209", "vector_2": [6, 0.4289002704724636, 2, 1, 0, 0]}, {"label": "Neut", "current": "These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).", "context": ["To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.", "These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).", "In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed."], "vector_1": {"scarciti": 1, "predict": 1, "number": 1, "monolingu": 1, "year": 1, "largescal": 1, "extract": 1, "sever": 1, "baselin": 1, "mutual": 1, "techniqu": 1, "semisupervis": 1, "cw": 1, "characterbas": 1, "label": 1, "also": 1, "cotrain": 1, "addit": 1, "approach": 3, "unlabel": 1, "updat": 1, "wordbas": 1, "investig": 1, "distribut": 1, "use": 1, "intens": 1, "address": 1, "segment": 2, "recent": 1, "attempt": 1, "corpora": 2, "manual": 2, "data": 1, "employ": 1, "inform": 1, "either": 1, "learn": 1, "model": 2}, "marker": "(Sun and Xu, 2011)", "article": "D15-1142", "vector_2": [4, 0.09189300270575744, 5, 3, 4, 0]}, {"label": "Neut", "current": "Finally, we improved the machine learning back-end which feeds our extracted features into the Weka toolkit (Witten and Frank, 2005).", "context": ["Weka Interface.", "Finally, we improved the machine learning back-end which feeds our extracted features into the Weka toolkit (Witten and Frank, 2005).", "This allows to train features in arbitrary combinations, with different machine learners.'"], "vector_1": {"machin": 2, "feed": 1, "differ": 1, "featur": 2, "weka": 2, "arbitrari": 1, "learner": 1, "toolkit": 1, "combin": 1, "train": 1, "interfac": 1, "allow": 1, "learn": 1, "improv": 1, "extract": 1, "final": 1, "backend": 1}, "marker": "(Witten and Frank, 2005)", "article": "W07-1402", "vector_2": [2, 0.4513288252869948, 1, 1, 0, 0]}, {"label": "Neut", "current": "According to Vygotsky's zone of proximal development (Vygotsky, 1978), the range of suitable material is very small.", "context": ["The test difficulty needs to match the intended target group as the test should be challenging for the learner but not lead to frustration.", "According to Vygotsky's zone of proximal development (Vygotsky, 1978), the range of suitable material is very small.", "Thus, creating a test that fits this narrow target zone is a tedious and timeconsuming task."], "vector_1": {"rang": 1, "creat": 1, "challeng": 1, "fit": 1, "need": 1, "tediou": 1, "timeconsum": 1, "develop": 1, "lead": 1, "frustrat": 1, "proxim": 1, "learner": 1, "suitabl": 1, "test": 3, "zone": 2, "match": 1, "difficulti": 1, "accord": 1, "intend": 1, "group": 1, "task": 1, "target": 2, "thu": 1, "small": 1, "narrow": 1, "materi": 1, "vygotski": 1}, "marker": "(Vygotsky, 1978)", "article": "Q14-1040", "vector_2": [36, 0.02636424336847217, 1, 1, 0, 0]}, {"label": "Neut", "current": "7We train the regressor using OWLQN (Andrew and Gao, 2007), modified and distributed by Mark Johnson as part of the Charniak-Johnson parse reranker (Charniak and Johnson, 2005).", "context": ["The standard entity grid estimates that such an entity will be the subject of the next sentence with a probability of about", "7We train the regressor using OWLQN (Andrew and Gao, 2007), modified and distributed by Mark Johnson as part of the Charniak-Johnson parse reranker (Charniak and Johnson, 2005).", "127"], "vector_1": {"regressor": 1, "mark": 1, "probabl": 1, "use": 1, "next": 1, "estim": 1, "subject": 1, "we": 1, "distribut": 1, "rerank": 1, "sentenc": 1, "owlqn": 1, "standard": 1, "train": 1, "grid": 1, "modifi": 1, "charniakjohnson": 1, "12": 1, "johnson": 1, "part": 1, "entiti": 2, "pars": 1}, "marker": "(Andrew and Gao, 2007)", "article": "P11-2022", "vector_2": [4, 0.7228531579996015, 2, 1, 0, 0]}, {"label": "Pos", "current": "A detailed description can be found in Crego and Marino (2006).", "context": ["In our system we use an extended monotone reordering model based on automatically learned reordering rules.", "A detailed description can be found in Crego and Marino (2006).", "1http://www.speech.sri.com/projects/srilm/ 2http://gps-tsc.upc.es/veu/soft/soft/marie/"], "vector_1": {"use": 1, "httpwwwspeechsricomprojectssrilm": 1, "extend": 1, "monoton": 1, "descript": 1, "system": 1, "automat": 1, "detail": 1, "httpgpstscupcesveusoftsoftmari": 1, "base": 1, "rule": 1, "marino": 1, "learn": 1, "found": 1, "model": 1, "crego": 1, "reorder": 2}, "marker": "(2006)", "article": "W08-0315", "vector_2": [2, 0.40926585887384176, 1, 4, 0, 1]}, {"label": "Neut", "current": "Word alignment is a well-studied topic in Machine Translation with many algorithms having been proposed (Brown et al, 1993; Och and Ney 2003).", "context": ["4 Word Alignment", "Word alignment is a well-studied topic in Machine Translation with many algorithms having been proposed (Brown et al, 1993; Och and Ney 2003).", "We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000)."], "vector_1": {"machin": 1, "och": 1, "use": 1, "word": 3, "algorithm": 2, "competit": 1, "align": 3, "simplest": 1, "wellstudi": 1, "one": 1, "topic": 1, "version": 1, "call": 1, "translat": 1, "ney": 1, "link": 1, "mani": 1, "modifi": 1, "propos": 1}, "marker": "(Brown et al, 1993", "article": "P08-1113", "vector_2": [15, 0.4498733906344734, 2, 1, 0, 0]}, {"label": "CoCo", "current": "The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P(s|t) are smoothed using the Good-Turing technique (Foster et al., 2006).", "context": ["Portage's model for P(t1s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature.", "The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P(s|t) are smoothed using the Good-Turing technique (Foster et al., 2006).", "The distortion model is also very similar to Koehn's, with the exception of a final cost to account for sentence endings."], "vector_1": {"featur": 1, "one": 2, "four": 1, "cost": 1, "loglinear": 1, "phrase": 2, "probabl": 1, "sentencelength": 1, "use": 1, "end": 1, "techniqu": 1, "pt": 1, "except": 2, "compon": 1, "also": 1, "estim": 1, "goodtur": 1, "main": 1, "final": 1, "koehn": 2, "sentenc": 1, "distort": 2, "ngram": 1, "translat": 2, "pst": 1, "portag": 1, "account": 1, "smooth": 1, "phrasebas": 1, "targetlanguag": 1, "combin": 1, "model": 6, "similar": 2, "wordreord": 1}, "marker": "(Foster et al., 2006)", "article": "N07-1064", "vector_2": [1, 0.4447607779338904, 1, 1, 0, 0]}, {"label": "Neut", "current": "Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).", "context": ["Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).", "Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).", "In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features."], "vector_1": {"classif": 1, "set": 1, "pair": 1, "featur": 1, "need": 1, "extract": 1, "pmi": 2, "select": 1, "supervis": 1, "use": 3, "neg": 1, "top": 1, "classifi": 1, "research": 1, "also": 2, "score": 2, "treat": 1, "document": 1, "approach": 2, "final": 1, "machin": 1, "candid": 2, "solv": 1, "highest": 1, "averag": 1, "task": 1, "word": 2, "algorithm": 1, "keyword": 3, "k": 1, "exampl": 1, "learn": 3, "posit": 1}, "marker": "(Frank et al., 1999", "article": "N09-1070", "vector_2": [10, 0.15750620547371136, 6, 2, 0, 0]}, {"label": "Neut", "current": "For instance, Batagelj et al (1992) use basic vocabularies such as belly in English and ventre in French to measure similarity between languages.", "context": ["This significant difference prevents us from directly applying techniques in the literature to our task.", "For instance, Batagelj et al (1992) use basic vocabularies such as belly in English and ventre in French to measure similarity between languages.", "Obviously, this does not work on our task; belly is belly in English writing whoever writes it."], "vector_1": {"prevent": 1, "directli": 1, "whoever": 1, "appli": 1, "al": 1, "signific": 1, "batagelj": 1, "et": 1, "languag": 1, "differ": 1, "techniqu": 1, "write": 2, "basic": 1, "ventr": 1, "use": 1, "french": 1, "vocabulari": 1, "belli": 3, "measur": 1, "task": 2, "work": 1, "literatur": 1, "us": 1, "instanc": 1, "obvious": 1, "english": 2, "similar": 1}, "marker": "(1992)", "article": "P13-1112", "vector_2": [21, 0.20331237540254563, 1, 4, 0, 0]}, {"label": "Neut", "current": "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "context": ["Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).", "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training."], "vector_1": {"corpu": 1, "satisfactori": 1, "result": 2, "year": 1, "largescal": 1, "intermedi": 1, "research": 1, "build": 1, "method": 1, "machin": 1, "train": 1, "link": 1, "translat": 1, "associ": 1, "requir": 1, "recent": 1, "measur": 1, "word": 1, "align": 2, "employ": 1, "achiev": 1, "statist": 2, "bilingu": 1, "mani": 1, "model": 1, "propos": 1, "order": 1, "first": 1}, "marker": "Cherry and Lin, 2003)", "article": "P05-1058", "vector_2": [2, 0.039255958147645806, 7, 1, 0, 0]}, {"label": "Neut", "current": "Our natural logic system, dubbed the NatLog system, has a three-stage architecture similar to those in (Marsi and Krahmer, 2005; MacCartney et al., 2006), comprising (1) linguistic pre-preprocessing, (2) alignment, and (3) entailment classification.", "context": ["3 The NatLog System", "Our natural logic system, dubbed the NatLog system, has a three-stage architecture similar to those in (Marsi and Krahmer, 2005; MacCartney et al., 2006), comprising (1) linguistic pre-preprocessing, (2) alignment, and (3) entailment classification.", "3.1 Linguistic pre-processing"], "vector_1": {"prepreprocess": 1, "classif": 1, "natlog": 2, "entail": 1, "natur": 1, "dub": 1, "system": 3, "threestag": 1, "compris": 1, "logic": 1, "preprocess": 1, "linguist": 2, "similar": 1, "align": 1, "architectur": 1}, "marker": "(Marsi and Krahmer, 2005", "article": "W07-1431", "vector_2": [2, 0.3256412014812783, 2, 1, 0, 0]}, {"label": "Neut", "current": "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "context": ["Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.", "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc."], "vector_1": {"phrase": 1, "domain": 1, "en": 1, "aspect": 2, "modelis": 2, "celui": 1, "al": 1, "ce": 1, "accessibilitd": 1, "distanc": 1, "travaux": 1, "il": 1, "semblent": 1, "et": 2, "eu": 1, "sen": 1, "diver": 1, "le": 2, "la": 2, "associ": 1, "montrant": 1, "capter": 1, "densitd": 1, "dictionnair": 1, "dernier": 1, "preter": 1, "structur": 2, "effet": 1, "de": 5, "lexic": 1, "mot": 2, "compri": 1, "du": 2, "langu": 1, "mond": 1, "merveil": 1, "nombreux": 2, "pour": 2, "etc": 1, "gaum": 1, "entr": 1, "leur": 1, "lexical": 1, "dynamiqu": 1, "graph": 1, "pertin": 1, "ou": 1, "moyenn": 1, "se": 1}, "marker": "Sowa, 1991)", "article": "W14-6700", "vector_2": [23, 0.2616651418115279, 11, 1, 0, 0]}, {"label": "Neut", "current": "King et al (2003) describe the development of the PARC 700 Dependency Bank, a goldstandard set of relational dependencies for 700 sentences (from the PTB) drawn at random from section 23 of the WSJ (the de facto standard test set for statistical parsing).", "context": ["3.4 The DepBank Test Data", "King et al (2003) describe the development of the PARC 700 Dependency Bank, a goldstandard set of relational dependencies for 700 sentences (from the PTB) drawn at random from section 23 of the WSJ (the de facto standard test set for statistical parsing).", "In all the evaluations reported in this paper we test our parser over a gold-standard set of relational dependencies compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth)."], "vector_1": {"set": 3, "facto": 1, "evalu": 1, "random": 1, "parser": 2, "al": 1, "depbank": 2, "paper": 1, "et": 1, "compat": 1, "develop": 1, "describ": 1, "section": 1, "henceforth": 1, "test": 3, "drawn": 1, "de": 1, "parc": 2, "deriv": 1, "sentenc": 1, "relat": 2, "ptb": 1, "goldstandard": 2, "pars": 1, "report": 1, "depend": 4, "standard": 1, "data": 1, "bank": 2, "king": 1, "wsj": 1, "statist": 1, "output": 1}, "marker": "(2003)", "article": "W07-2203", "vector_2": [4, 0.46931344746700276, 2, 1, 1, 0]}, {"label": "Neut", "current": "Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact decoder (Chang and Collins, 2011).", "context": ["First, the Nbest error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum1 of the N-best error surface is not guaranteed to be any close to an optimum of the true error surface.", "Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact decoder (Chang and Collins, 2011).", "MERT calculates an envelope from candidate translations and assumes all translations on the envelope are reachable by the decoder, but these translations may become unreachable due to search errors."], "vector_1": {"unreach": 1, "mert": 3, "one": 1, "second": 1, "explor": 1, "guarante": 1, "close": 1, "optimum": 2, "yet": 1, "surfac": 5, "exact": 1, "make": 1, "smt": 1, "due": 1, "decod": 4, "reachabl": 1, "may": 1, "gener": 1, "envelop": 2, "differ": 1, "candid": 1, "translat": 3, "becom": 1, "true": 2, "nbest": 2, "search": 2, "errorpron": 1, "rate": 1, "calcul": 1, "ignor": 1, "assum": 1, "error": 8, "first": 1, "fact": 1, "mean": 1}, "marker": "(Chang and Collins, 2011)", "article": "W12-3159", "vector_2": [1, 0.053013980057122816, 1, 1, 0, 0]}, {"label": "Pos", "current": "There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).", "context": ["They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).", "There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).", "(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data."], "vector_1": {"semant": 2, "show": 2, "process": 1, "eg": 1, "signific": 1, "extract": 1, "leverag": 1, "use": 2, "ratio": 1, "compar": 1, "perform": 2, "also": 1, "speech": 2, "rel": 1, "approach": 1, "resourc": 1, "broadcast": 1, "base": 1, "news": 1, "data": 1, "task": 1, "retriev": 1, "keyword": 2, "frequenc": 1, "work": 1, "yield": 1, "combin": 1, "verif": 1, "idf": 1, "improv": 2, "similar": 1}, "marker": "Desilets et al., 2002", "article": "N09-1070", "vector_2": [7, 0.2224622030237581, 6, 1, 1, 0]}, {"label": "Neut", "current": "(Fulcher, 1997)).", "context": ["Placing learners into levels is generally done by a human, based on a written exam (e.g.", "(Fulcher, 1997)).", "To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels."], "vector_1": {"exam": 1, "level": 2, "process": 1, "well": 1, "gener": 1, "frequenc": 1, "learner": 2, "correspond": 1, "automat": 1, "written": 1, "base": 1, "place": 1, "decis": 1, "human": 1, "error": 1, "need": 1, "done": 1, "model": 1, "type": 1, "eg": 1, "understand": 1}, "marker": "(Fulcher, 1997)", "article": "W12-2011", "vector_2": [15, 0.05605797631010018, 1, 3, 0, 0]}, {"label": "Neut", "current": "The PSET project (Carroll et al., 1998) dealt with simplification of news articles in English for aphasic readers.", "context": ["The earliest simplification systems employed a rule-based approach and focused on syntactic structure of the text (Chandrasekar et al., 1996).", "The PSET project (Carroll et al., 1998) dealt with simplification of news articles in English for aphasic readers.", "Together with syntactic analysis and transformations similar to those of Chandrasekar et al (1996), they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency from the Oxford Psycholinguistic Database (Quinlan, 1992)."], "vector_1": {"analysi": 1, "text": 1, "focus": 1, "transform": 1, "pset": 1, "et": 1, "extract": 1, "wordnet": 1, "psycholinguist": 1, "employ": 2, "kucerafr": 1, "databas": 1, "system": 1, "articl": 1, "reader": 1, "approach": 1, "aphas": 1, "syntact": 2, "dealt": 1, "lexic": 1, "al": 1, "chandrasekar": 1, "togeth": 1, "oxford": 1, "news": 1, "earliest": 1, "synonym": 1, "look": 1, "simplif": 3, "frequenc": 1, "rulebas": 1, "structur": 1, "project": 1, "base": 1, "english": 1, "similar": 1}, "marker": "(Carroll et al., 1998)", "article": "W12-2202", "vector_2": [14, 0.1663156487146057, 4, 2, 1, 0]}, {"label": "Pos", "current": "In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions.", "context": ["AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.", "In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions.", "This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP-attachment disambiguation, text categorization, and word sense disambiguation."], "vector_1": {"nlp": 1, "classif": 1, "featur": 1, "stump": 1, "predict": 1, "appli": 1, "obtain": 1, "high": 1, "rate": 1, "tag": 1, "design": 1, "domain": 1, "effici": 1, "highli": 1, "use": 1, "accur": 2, "space": 1, "abl": 1, "system": 1, "classifi": 1, "version": 1, "disambigu": 3, "decis": 1, "text": 1, "sens": 1, "simpl": 1, "method": 1, "confid": 1, "may": 1, "gener": 2, "weak": 2, "number": 1, "particular": 1, "word": 1, "categor": 1, "ppattach": 1, "task": 1, "moder": 1, "success": 1, "algorithm": 2, "boost": 1, "hypothes": 1, "work": 2, "adaboostmh": 1, "signific": 1, "rule": 1, "po": 1, "combin": 1, "adaboost": 2, "partit": 1, "mani": 1, "dimension": 1}, "marker": "(Schapire and Singer, 1999)", "article": "W01-0726", "vector_2": [2, 0.0866620865639415, 2, 2, 0, 0]}, {"label": "Pos", "current": "The minimum error training (Och, 2003) was used on the development data for parameter estimation.", "context": ["Every single feature was combined with the 6 baseline features for the training and test.", "The minimum error training (Och, 2003) was used on the development data for parameter estimation.", "Feature BLEU% Baseline 31.6 POS Language Model 31.7 Supertag Language Model 31.7 Wrong NN Position 31.7 Word Popularity 31.8 Aligned Template Models 31.9 Count of Missing Word 31.9 Template Right Continuity 32.0 IBM Model 1 32.5"], "vector_1": {"bleu": 1, "featur": 3, "continu": 1, "templat": 2, "minimum": 1, "right": 1, "miss": 1, "languag": 2, "paramet": 1, "use": 1, "develop": 1, "nn": 1, "3": 1, "estim": 1, "test": 1, "supertag": 1, "po": 1, "singl": 1, "everi": 1, "popular": 1, "wrong": 1, "train": 2, "data": 1, "count": 1, "word": 2, "ibm": 1, "align": 1, "combin": 1, "error": 1, "posit": 1, "model": 4, "baselin": 2}, "marker": "(Och, 2003)", "article": "N04-1023", "vector_2": [1, 0.7789652378563988, 1, 6, 5, 1]}, {"label": "Pos", "current": "Using the switchboard corpus, (Stolcke et al., 2000) achieved good dialogue act labeling accuracy (71% on manual transcriptions) for a set of 42 dialogue act types, and constructed probabilistic models of dialogue act sequencing in order to test the hypothesis that dialogue act sequence information could boost speech recognition performance.", "context": ["Their maximum entropy ranking approach achieved 90% accuracy on the 4-way classification into agreement, disagreement, backchannel and other.", "Using the switchboard corpus, (Stolcke et al., 2000) achieved good dialogue act labeling accuracy (71% on manual transcriptions) for a set of 42 dialogue act types, and constructed probabilistic models of dialogue act sequencing in order to test the hypothesis that dialogue act sequence information could boost speech recognition performance.", "There has been far less work on developing manual and automatic dialogue act annotation schemes for email."], "vector_1": {"corpu": 1, "classif": 1, "set": 1, "entropi": 1, "less": 1, "rank": 1, "annot": 1, "type": 1, "label": 1, "backchannel": 1, "dialogu": 5, "develop": 1, "perform": 1, "construct": 1, "accuraci": 2, "recognit": 1, "2": 1, "speech": 1, "way": 1, "test": 1, "scheme": 1, "disagr": 1, "email": 1, "good": 1, "hypothesi": 1, "far": 1, "sequenc": 2, "use": 1, "probabilist": 1, "agreement": 1, "boost": 1, "automat": 1, "switchboard": 1, "transcript": 1, "approach": 1, "could": 1, "work": 1, "manual": 2, "maximum": 1, "inform": 1, "achiev": 2, "act": 5, "model": 1, "order": 1}, "marker": "(Stolcke et al., 2000)", "article": "W09-3953", "vector_2": [9, 0.15833743795703406, 1, 2, 2, 0]}, {"label": "Neut", "current": "More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.", "context": ["For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.", "More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.", "Among them, the 'Recently, native language identification has drawn the attention of NLP researchers."], "vector_1": {"kita": 1, "nlp": 1, "among": 1, "metric": 1, "al": 1, "cluster": 1, "chrietien": 1, "famili": 1, "batagelj": 1, "et": 1, "languag": 3, "use": 1, "ellegard": 1, "research": 1, "identif": 1, "drawn": 1, "method": 2, "kroeber": 1, "nativ": 1, "attent": 1, "reconstruct": 1, "recent": 2, "measur": 1, "tree": 1, "instanc": 1, "statist": 1, "similar": 1, "propos": 2}, "marker": "(1992)", "article": "P13-1112", "vector_2": [21, 0.1641159331390891, 4, 4, 0, 0]}, {"label": "Pos", "current": "However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available.", "context": ["When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997).", "However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available.", "In this paper, we address the problem of word alignment in a specific domain, in which only a small-scale corpus is available."], "vector_1": {"corpu": 3, "domain": 1, "directli": 1, "paper": 1, "exist": 1, "largescal": 2, "use": 1, "research": 1, "avail": 3, "domainspecif": 3, "neither": 1, "translat": 1, "dictionari": 2, "address": 2, "studi": 1, "smallscal": 1, "word": 3, "specif": 1, "align": 3, "bilingu": 2, "improv": 1, "problem": 2, "howev": 1}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.05363301685719822, 2, 13, 0, 1]}, {"label": "Neut", "current": "In the dependency model with valence (DMV) (Klein and Manning, 2004), the input x = (x1, ... , xm) is a sequence of POS tags and the output y specifies the directed links of a projective dependency tree.", "context": ["We represent y as a multiset of binary rewrites of the form (y -* y1 y2), where y is a nonterminal and y1, y2 can be either nonterminals or terminals.", "In the dependency model with valence (DMV) (Klein and Manning, 2004), the input x = (x1, ... , xm) is a sequence of POS tags and the output y specifies the directed links of a projective dependency tree.", "The generative model is as follows: for each head xi, we generate an independent sequence of arguments to the left and to the right from a direction-dependent distribution over tags."], "vector_1": {"xi": 1, "xm": 1, "independ": 1, "direct": 1, "repres": 1, "tag": 2, "right": 1, "follow": 1, "directiondepend": 1, "depend": 2, "binari": 1, "rewrit": 1, "input": 1, "po": 1, "valenc": 1, "head": 1, "distribut": 1, "form": 1, "termin": 1, "sequenc": 2, "gener": 2, "specifi": 1, "argument": 1, "link": 1, "output": 1, "multiset": 1, "tree": 1, "nontermin": 2, "project": 1, "either": 1, "dmv": 1, "y": 4, "x": 2, "model": 2, "left": 1}, "marker": "(Klein and Manning, 2004)", "article": "P08-1100", "vector_2": [4, 0.15721675626331272, 1, 5, 2, 1]}, {"label": "Neut", "current": "However, his distinction between raw (=non-symbolic) and processed (=symbolic) data (Lehmann, 2005, pp.", "context": ["We use these terms in Lehmann's reading: \"Primary linguistic data are [...] representations of [...] speech events with their spatio-temporal coordinates\" (Lehmann, 2005, p. 187).", "However, his distinction between raw (=non-symbolic) and processed (=symbolic) data (Lehmann, 2005, pp.", "205ff.)"], "vector_1": {"raw": 1, "represent": 1, "use": 1, "nonsymbol": 1, "spatiotempor": 1, "read": 1, "lehmann": 1, "pp": 1, "term": 1, "p": 1, "event": 1, "symbol": 1, "coordin": 1, "speech": 1, "ff": 1, "howev": 1, "process": 1, "linguist": 1, "data": 2, "primari": 1, "distinct": 1}, "marker": "(Lehmann, 2005, ", "article": "W13-5507", "vector_2": [8, 0.2543171114599686, 2, 2, 0, 0]}, {"label": "Neut", "current": "Figure 1: Parameter sweep over A showing contributions from cosine (A) and Wikipedia-based MAXSIM method (1  A) to the final performance over the Lee (2005) dataset.", "context": ["This suggests that selective knowledge-based augmentation of simple VSM methods can lead to more accurate document similarity performance.", "Figure 1: Parameter sweep over A showing contributions from cosine (A) and Wikipedia-based MAXSIM method (1  A) to the final performance over the Lee (2005) dataset.", "9 Conclusion"], "vector_1": {"show": 1, "sweep": 1, "dataset": 1, "vsm": 1, "select": 1, "contribut": 1, "accur": 1, "lead": 1, "perform": 2, "suggest": 1, "wikipediabas": 1, "figur": 1, "paramet": 1, "document": 1, "simpl": 1, "final": 1, "lee": 1, "knowledgebas": 1, "conclus": 1, "maxsim": 1, "augment": 1, "method": 2, "cosin": 1, "similar": 1}, "marker": "(2005)", "article": "W10-3506", "vector_2": [5, 0.9496204539446353, 1, 4, 0, 0]}, {"label": "Pos", "current": "For synonym choice, we follow the method established by Mohammad et al (2007), measuring accuracy over covered items, with partial credit for ties.", "context": ["We measure quality of the semantic similarity task as the Pearson correlation between the model predictions and the human judgments for covered items (Zesch et al., 2007).", "For synonym choice, we follow the method established by Mohammad et al (2007), measuring accuracy over covered items, with partial credit for ties.", "Results for Semantic Similarity."], "vector_1": {"semant": 2, "partial": 1, "predict": 1, "qualiti": 1, "al": 1, "correl": 1, "result": 1, "human": 1, "follow": 1, "et": 1, "establish": 1, "credit": 1, "accuraci": 1, "tie": 1, "method": 1, "choic": 1, "task": 1, "judgment": 1, "mohammad": 1, "measur": 2, "pearson": 1, "synonym": 1, "cover": 2, "item": 2, "model": 1, "similar": 2}, "marker": "(2007)", "article": "P13-2128", "vector_2": [6, 0.7270850278420822, 2, 4, 0, 0]}, {"label": "Neut", "current": "We use the training data and computed the three features (Frequent Collocation (FC), Semantic Relatedness word Before (SRB), and Semantic Relatedness word After (SRA), and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999) to learn a set of rule that differentiate between a figurative and literal phrase use.", "context": ["The data set contains a total of 1114 training instances, and 518 test instances.", "We use the training data and computed the three features (Frequent Collocation (FC), Semantic Relatedness word Before (SRB), and Semantic Relatedness word After (SRA), and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999) to learn a set of rule that differentiate between a figurative and literal phrase use.", "This method resulted in a set of rules that can be summarized as follows: if FC is equal to 0 and SRB < 75% then it is used literally in this context, else if FC is equal to 0 and SRA < 75% then it is is also used literally, otherwise it is used figuratively."], "vector_1": {"semant": 2, "set": 3, "comput": 1, "cohen": 1, "related": 2, "jrip": 1, "result": 1, "follow": 1, "phrase": 1, "total": 1, "featur": 1, "summar": 1, "use": 6, "weka": 1, "three": 1, "also": 1, "figur": 2, "instanc": 2, "test": 1, "els": 1, "method": 1, "liter": 3, "learn": 2, "colloc": 1, "differenti": 1, "train": 2, "ripper": 1, "data": 2, "srb": 2, "sra": 2, "word": 2, "algorithm": 1, "equal": 2, "rule": 3, "fc": 3, "context": 1, "contain": 1, "otherwis": 1, "implement": 1, "frequent": 1}, "marker": "(Cohen and Singer, 1999)", "article": "S13-2019", "vector_2": [14, 0.9035156485557498, 2, 2, 0, 0]}, {"label": "Neut", "current": "(SMT Team, 2003) show that to get a reasonable improvement in the BLEU score at least 1000 candidates need to be considered in the -best list.", "context": ["In parse reranking, each training sentence has a ranked list of 27 candidates on average (Collins, 2000), but for machine translation, the number of candidate translations in the -best list is much higher.", "(SMT Team, 2003) show that to get a reasonable improvement in the BLEU score at least 1000 candidates need to be considered in the -best list.", "In addition, the parallel hyperplanes separating and actually are unable to distinguish good translations from bad translations, since they are not trained to distinguish any translations in ."], "vector_1": {"bleu": 1, "show": 1, "number": 1, "rank": 1, "need": 1, "hyperplan": 1, "best": 2, "much": 1, "get": 1, "least": 1, "sinc": 1, "score": 1, "higher": 1, "machin": 1, "good": 1, "rerank": 1, "sentenc": 1, "reason": 1, "candid": 3, "pars": 1, "translat": 5, "consid": 1, "distinguish": 2, "parallel": 1, "averag": 1, "addit": 1, "actual": 1, "list": 3, "separ": 1, "bad": 1, "train": 2, "unabl": 1, "improv": 1}, "marker": "(SMT Team, 2003)", "article": "N04-1023", "vector_2": [1, 0.4089278135730545, 2, 8, 0, 0]}, {"label": "Neut", "current": "Given the relevant literature that has put forward concreteness and difference in concreteness as important predictors of metaphoricity (Dunn, 2014; Tsvetkov et al., 2014; Gandy et al., 2013; Assaf et al., 2013; Turney et al., 2011), it is instructive to evaluate the overall contribution of the concreteness features over the UPT baseline (no concreteness features), across the different weighting regimes.", "context": ["The small improvement is perhaps not surprising, since the baseline model itself already contains a version of the concreteness features.", "Given the relevant literature that has put forward concreteness and difference in concreteness as important predictors of metaphoricity (Dunn, 2014; Tsvetkov et al., 2014; Gandy et al., 2013; Assaf et al., 2013; Turney et al., 2011), it is instructive to evaluate the overall contribution of the concreteness features over the UPT baseline (no concreteness features), across the different weighting regimes.", "Table 9 provides this information."], "vector_1": {"featur": 3, "version": 1, "evalu": 1, "instruct": 1, "alreadi": 1, "tabl": 1, "surpris": 1, "baselin": 2, "given": 1, "metaphor": 1, "weight": 1, "sinc": 1, "concret": 5, "forward": 1, "import": 1, "across": 1, "contribut": 1, "overal": 1, "differ": 2, "regim": 1, "upt": 1, "put": 1, "relev": 1, "provid": 1, "literatur": 1, "inform": 1, "perhap": 1, "contain": 1, "small": 1, "improv": 1, "model": 1, "predictor": 1}, "marker": "(Dunn, 2014", "article": "W15-1402", "vector_2": [1, 0.6956046141607001, 5, 1, 1, 0]}, {"label": "Neut", "current": "To improve raw output from decoding, Portage relies on a rescoring strategy: given a list of n-best translations from the decoder, the system reorders this list, this time using a more elaborate loglinear model, incorporating more feature functions, in addition to those of the decoding model: these typically include IBM-1 and IBM-2 model probabilities (Brown et al., 1993) and an IBM-1-based feature function designed to detect whether any word in one language appears to have been left without satisfactory translation in the other language; all of these feature functions can be used in both language directions, i.e.", "context": ["no new translations are produced at the decoding step.", "To improve raw output from decoding, Portage relies on a rescoring strategy: given a list of n-best translations from the decoder, the system reorders this list, this time using a more elaborate loglinear model, incorporating more feature functions, in addition to those of the decoding model: these typically include IBM-1 and IBM-2 model probabilities (Brown et al., 1993) and an IBM-1-based feature function designed to detect whether any word in one language appears to have been left without satisfactory translation in the other language; all of these feature functions can be used in both language directions, i.e.", "source-to-target and target-to-source."], "vector_1": {"featur": 3, "ibmbas": 1, "direct": 1, "one": 1, "raw": 1, "satisfactori": 1, "design": 1, "loglinear": 1, "typic": 1, "improv": 1, "rescor": 1, "languag": 3, "probabl": 1, "given": 1, "detect": 1, "appear": 1, "ie": 1, "system": 1, "reli": 1, "targettosourc": 1, "decod": 4, "includ": 1, "new": 1, "addit": 1, "reorder": 1, "strategi": 1, "function": 3, "use": 2, "sourcetotarget": 1, "step": 1, "translat": 3, "elabor": 1, "nbest": 1, "portag": 1, "word": 1, "ibm": 2, "whether": 1, "list": 2, "without": 1, "incorpor": 1, "time": 1, "output": 1, "model": 3, "produc": 1, "left": 1}, "marker": "(Brown et al., 1993)", "article": "N07-1064", "vector_2": [14, 0.48898173482215485, 1, 1, 0, 0]}, {"label": "CoCo", "current": "F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF", "context": ["Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.", "F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF", "Table 7: Performance comparison on the test set."], "vector_1": {"gazett": 1, "set": 1, "evalu": 1, "al": 4, "paper": 1, "tagger": 1, "tabl": 2, "et": 4, "best": 1, "25": 1, "zhou": 1, "perform": 1, "friedrich": 1, "postprocess": 2, "test": 1, "okanohara": 1, "method": 1, "fscore": 2, "variou": 1, "rerank": 2, "memm": 1, "hmm": 1, "train": 1, "semicrf": 1, "11": 1, "comparison": 2, "svm": 1, "tsai": 1, "crf": 1, "crfgazett": 1}, "marker": "(2004)", "article": "W07-1033", "vector_2": [3, 0.9417141500474834, 4, 5, 0, 0]}, {"label": "Neut", "current": "Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).", "context": ["It should be noted that there are a also number of orthogonal approaches for accelerating natural language parsers.", "Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).", "These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest."], "vector_1": {"acceler": 1, "often": 1, "orthogon": 1, "natur": 1, "approxim": 1, "parser": 1, "number": 1, "languag": 1, "use": 1, "space": 1, "note": 1, "also": 1, "reli": 1, "interest": 2, "appli": 1, "approach": 2, "final": 1, "pars": 1, "constrain": 1, "search": 1, "grammar": 1, "prune": 1, "possibl": 1, "tree": 1, "coars": 2, "model": 2}, "marker": "(Goodman, 1997", "article": "W11-2921", "vector_2": [14, 0.9485065054059006, 3, 1, 0, 0]}, {"label": "Neut", "current": "Prompts were designed to test all aspects of Blooms Taxonomy of learning (Krathwohl, 2002), from simple to more complex tasks.", "context": ["Each subject responded to 1012 prompts, with the average response comprising 448 characters and 87 words.", "Prompts were designed to test all aspects of Blooms Taxonomy of learning (Krathwohl, 2002), from simple to more complex tasks.", "Blooms Taxonomy includes six different types of tasks: remember, understand, apply, analyze, evaluate and create."], "vector_1": {"respond": 1, "prompt": 2, "evalu": 1, "appli": 1, "design": 1, "aspect": 1, "respons": 1, "taxonomi": 2, "subject": 1, "differ": 1, "creat": 1, "six": 1, "complex": 1, "includ": 1, "rememb": 1, "test": 1, "simpl": 1, "analyz": 1, "understand": 1, "bloom": 2, "averag": 1, "task": 2, "word": 1, "type": 1, "charact": 1, "compris": 1, "learn": 1}, "marker": "(Krathwohl, 2002)", "article": "W15-0914", "vector_2": [13, 0.4175259832891787, 1, 1, 0, 0]}, {"label": "Neut", "current": "(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.", "context": ["Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form.", "(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.", "Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment."], "vector_1": {"yamada": 1, "tree": 2, "treebank": 2, "parser": 1, "syntax": 1, "restrict": 1, "chomskynorm": 1, "languag": 1, "use": 4, "selforgan": 1, "introduc": 1, "sourc": 1, "string": 1, "form": 1, "probabilist": 1, "wu": 1, "train": 1, "pars": 1, "implicit": 1, "grammar": 1, "constraint": 1, "contextfre": 1, "knight": 1, "align": 2, "synchron": 1, "statist": 1, "model": 2, "produc": 1, "propos": 1}, "marker": "(Wu, 1997)", "article": "N04-1023", "vector_2": [7, 0.1669431691037463, 3, 2, 0, 0]}, {"label": "CoCo", "current": "These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008) (p = 0.69) and ESA (Gabrilovich and Markovitch, 2007) (p = 0.75).", "context": ["Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (pmax,wlm = 0.60 vs pmax,cos = 0.70).", "These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008) (p = 0.69) and ESA (Gabrilovich and Markovitch, 2007) (p = 0.75).", "Maximum path length Lp,max is related to how far one node can spread its activation in the network."], "vector_1": {"set": 1, "aaco": 1, "one": 1, "secondli": 1, "vs": 1, "sampl": 1, "result": 2, "network": 1, "compar": 1, "lpmax": 1, "activ": 1, "favour": 1, "spread": 2, "esa": 1, "length": 1, "strategi": 2, "node": 1, "far": 1, "relat": 1, "pmaxco": 1, "aawlm": 1, "report": 1, "path": 1, "pmaxwlm": 1, "outperform": 1, "maximum": 1, "p": 2, "interconcept": 1, "wlm": 1, "similar": 1, "significantli": 1}, "marker": "(Witten and Milne, 2008)", "article": "W10-3506", "vector_2": [2, 0.6805392952906076, 2, 7, 1, 0]}, {"label": "Neut", "current": "Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "context": ["2.2 Bilingual Semi-supervised CWS Methods", "Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004)."], "vector_1": {"dictionari": 1, "individu": 1, "focus": 1, "one": 1, "cw": 1, "leverag": 2, "semisupervis": 1, "perform": 1, "segment": 2, "smt": 1, "construct": 1, "label": 2, "better": 1, "consecut": 1, "approach": 1, "method": 1, "unlabel": 1, "either": 1, "machin": 1, "map": 1, "form": 1, "sequenc": 1, "chines": 2, "previou": 1, "train": 1, "translat": 1, "although": 1, "dataset": 1, "data": 1, "model": 1, "maximummatch": 1, "word": 2, "english": 1, "work": 1, "charact": 1, "achiev": 1, "statist": 1, "bilingu": 2, "studi": 1}, "marker": "(Ma and Way, 2009)", "article": "D15-1142", "vector_2": [6, 0.2968798164194952, 7, 3, 0, 0]}, {"label": "Neut", "current": "Polifroni and Walker (2008) address how to present informative options to users who are exploring a database, for example, to choose a restaurant.", "context": ["Ambiguous questions with multiple correct answers are not considered semantically tractable.", "Polifroni and Walker (2008) address how to present informative options to users who are exploring a database, for example, to choose a restaurant.", "When a query returns many options, their system summarizes the return using attribute value pairs shared by many of the members."], "vector_1": {"semant": 1, "queri": 1, "polifroni": 1, "share": 1, "system": 1, "tractabl": 1, "explor": 1, "summar": 1, "use": 1, "multipl": 1, "databas": 1, "question": 1, "restaur": 1, "member": 1, "valu": 1, "answer": 1, "correct": 1, "attribut": 1, "return": 2, "option": 2, "user": 1, "consid": 1, "address": 1, "pair": 1, "present": 1, "ambigu": 1, "inform": 1, "exampl": 1, "choos": 1, "walker": 1, "mani": 2}, "marker": "(2008)", "article": "W12-1635", "vector_2": [4, 0.33834854535082715, 1, 2, 0, 0]}, {"label": "Weak", "current": "For instance, the annotation tool EXMARaLDA provides a mechanism for creating time forks (Schmidt and Worner, 2005), but this is useful only for shorter stretches of simultaneous events surrounded by synchronised time points (e. g., for shorter segments of simultaneous speech), and not for timelines that might be completely independent from each other in the beginning and need to be merged and aligned later.", "context": ["With most of the given models, such an undertaking is either impossible, or it involves the alienation of model components (e.g., creation of phantom annotations being used as fake time points), which both inflates the resulting data structure and makes it less comprehensible.", "For instance, the annotation tool EXMARaLDA provides a mechanism for creating time forks (Schmidt and Worner, 2005), but this is useful only for shorter stretches of simultaneous events surrounded by synchronised time points (e. g., for shorter segments of simultaneous speech), and not for timelines that might be completely independent from each other in the beginning and need to be merged and aligned later.", "Also, there are various potential reasons in a scientific workflow that call for the use of an annotation tool different from EXMARaLDA."], "vector_1": {"creat": 1, "point": 2, "eg": 1, "creation": 1, "alien": 1, "result": 1, "fake": 1, "need": 1, "simultan": 2, "event": 1, "merg": 1, "involv": 1, "shorter": 2, "given": 1, "stretch": 1, "segment": 1, "make": 1, "less": 1, "compon": 1, "also": 1, "speech": 1, "comprehens": 1, "call": 1, "might": 1, "complet": 1, "fork": 1, "phantom": 1, "begin": 1, "timelin": 1, "synchronis": 1, "variou": 1, "workflow": 1, "tool": 2, "use": 3, "inflat": 1, "reason": 1, "mechan": 1, "potenti": 1, "data": 1, "surround": 1, "e": 1, "g": 1, "scientif": 1, "provid": 1, "align": 1, "later": 1, "differ": 1, "annot": 3, "structur": 1, "independ": 1, "instanc": 1, "either": 1, "undertak": 1, "time": 3, "model": 2, "imposs": 1, "exmaralda": 2}, "marker": "(Schmidt and Worner, 2005)", "article": "W13-5507", "vector_2": [8, 0.45774159395421854, 1, 1, 0, 0]}, {"label": "Neut", "current": "Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses.", "context": ["Jia et al (2009) also developed a noisv channel model for the English-Chinese language pair using Moses, an SMT tool.", "Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses.", "Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM)."], "vector_1": {"machin": 1, "evalu": 1, "englishhindi": 1, "al": 3, "chinnakotla": 1, "englishkannada": 1, "et": 3, "charact": 1, "languag": 2, "csm": 1, "use": 3, "develop": 1, "jia": 1, "similarlv": 1, "smt": 1, "three": 1, "also": 1, "channel": 1, "mose": 2, "englishtamil": 1, "sequenc": 1, "tool": 2, "differ": 1, "focus": 1, "noisv": 1, "englishchines": 1, "transliter": 1, "pair": 2, "kind": 1, "hindiurdu": 1, "finetun": 1, "giza": 1, "statist": 1, "malik": 1, "model": 3}, "marker": "(2013)", "article": "W14-5502", "vector_2": [1, 0.23186546304357789, 3, 1, 2, 0]}, {"label": "Neut", "current": "Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "context": ["We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.", "Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed."], "vector_1": {"often": 1, "process": 1, "natur": 1, "parser": 2, "focus": 1, "finegrain": 1, "exploit": 1, "result": 1, "massiv": 1, "languag": 1, "differ": 1, "anoth": 1, "priorit": 1, "intermedi": 1, "sentenc": 1, "previou": 1, "speedup": 1, "pars": 1, "inher": 1, "parallel": 2, "agendabas": 2, "refin": 1, "work": 1, "iter": 1, "order": 1, "queue": 1, "achiev": 1, "maintain": 1, "combin": 1, "whole": 1, "magnitud": 1}, "marker": "Giachin and Rullent, 1989", "article": "W11-2921", "vector_2": [22, 0.8912013403492238, 4, 2, 0, 0]}, {"label": "Pos", "current": "Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).", "context": ["Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.", "Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).", "Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches."], "vector_1": {"earli": 1, "difficulti": 1, "procedur": 1, "immun": 1, "still": 1, "involv": 1, "perform": 1, "associ": 1, "smith": 2, "accuraci": 1, "also": 1, "new": 2, "approach": 2, "method": 1, "introduc": 1, "complet": 1, "function": 2, "core": 1, "relat": 1, "though": 1, "optim": 1, "object": 2, "train": 1, "nonconvex": 1, "bayesian": 1, "likelihood": 1, "induct": 1, "thu": 1, "eisner": 2, "improv": 2, "model": 1, "propos": 1}, "marker": "Johnson, 2007", "article": "P08-1100", "vector_2": [1, 0.04905838996517564, 5, 1, 0, 0]}, {"label": "Pos", "current": "At the other extreme, textual inference can be approached as deduction, building on work in formal computational semantics to translate sentences into first-order logic (FOL), and then applying a theorem prover or a model builder (Akhmatova, 2005; Fowler et al., 2005).", "context": ["Because it drops important qualifiers in a negative context, the hypothesis does not follow; yet both the lexical content and the predicate-argument structure of the hypothesis closely match the premise.", "At the other extreme, textual inference can be approached as deduction, building on work in formal computational semantics to translate sentences into first-order logic (FOL), and then applying a theorem prover or a model builder (Akhmatova, 2005; Fowler et al., 2005).", "However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and so on."], "vector_1": {"semant": 1, "work": 1, "comput": 1, "founder": 1, "appli": 1, "intension": 1, "certain": 1, "prover": 1, "deduct": 1, "close": 1, "follow": 1, "yet": 1, "languag": 1, "accur": 1, "neg": 1, "textual": 1, "tend": 1, "content": 1, "idiom": 1, "causal": 1, "build": 1, "theorem": 1, "fol": 1, "import": 1, "premis": 1, "approach": 2, "infer": 1, "match": 1, "firstord": 1, "difficulti": 1, "qualifi": 1, "modal": 1, "hypothesi": 2, "sentenc": 1, "relat": 1, "lexic": 1, "translat": 2, "quantifi": 1, "natur": 1, "proposit": 1, "formal": 1, "foltricki": 1, "drop": 1, "builder": 1, "predicateargu": 1, "attitud": 1, "structur": 1, "tempor": 1, "issu": 1, "includ": 1, "context": 1, "logic": 1, "extrem": 1, "model": 1, "howev": 1}, "marker": "Fowler et al., 2005)", "article": "W07-1431", "vector_2": [2, 0.07152653956933205, 2, 1, 0, 0]}, {"label": "Neut", "current": "A closer look at the knowledge acquired by our models shows that the Unique Stress Constraint of Yang (2004) can be acquired jointly with segmenting the input instead", "context": ["Relatedly and in line with the finding that stress cues are used by infants before phonotactic cues (Jusczyk et al., 1999a), we observe that phonotactic cues require more input than stress cues to be used efficiently.", "A closer look at the knowledge acquired by our models shows that the Unique Stress Constraint of Yang (2004) can be acquired jointly with segmenting the input instead", "93"], "vector_1": {"infant": 1, "effici": 1, "relatedli": 1, "show": 1, "knowledg": 1, "phonotact": 2, "find": 1, "closer": 1, "use": 2, "acquir": 2, "cue": 4, "requir": 1, "input": 2, "instead": 1, "line": 1, "segment": 1, "jointli": 1, "stress": 3, "look": 1, "constraint": 1, "uniqu": 1, "yang": 1, "model": 1, "observ": 1}, "marker": "(2004)", "article": "Q14-1008", "vector_2": [10, 0.07583825390130747, 2, 9, 3, 0]}, {"label": "Neut", "current": "Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).", "context": ["The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection.", "Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).", "However, we may not have a big background collection that matches the test domain for a reliable IDF estimate."], "vector_1": {"domain": 1, "identifi": 1, "eg": 1, "idea": 1, "extract": 1, "occur": 1, "shown": 1, "appear": 1, "much": 1, "test": 1, "document": 2, "match": 1, "may": 1, "big": 1, "journal": 1, "effect": 1, "entir": 1, "background": 1, "word": 1, "keyword": 1, "scientif": 1, "howev": 1, "work": 1, "tfidf": 1, "collect": 2, "estim": 1, "idf": 1, "reliabl": 1, "frequent": 2}, "marker": "Kerner et al., 2005)", "article": "N09-1070", "vector_2": [4, 0.12530221462879984, 3, 2, 0, 0]}, {"label": "Neut", "current": "Furthermore, these extensions can also express, within the PDA model, non-left-to-right behavior such as is used in island parsing [38] or in Shell's approach [26].", "context": ["Hence we are reusing and developing our theoretical [18] and experimental [36] approach in this much more general setting which is more likely to be effectively usable for natural language parsing.", "Furthermore, these extensions can also express, within the PDA model, non-left-to-right behavior such as is used in island parsing [38] or in Shell's approach [26].", "More generally they allow the formal analysis of agenda strategies, which we have not considered here."], "vector_1": {"set": 1, "natur": 1, "within": 1, "henc": 1, "nonlefttoright": 1, "languag": 1, "use": 1, "develop": 1, "much": 1, "theoret": 1, "also": 1, "experiment": 1, "approach": 2, "strategi": 1, "analysi": 1, "shell": 1, "allow": 1, "gener": 2, "express": 1, "effect": 1, "usabl": 1, "extens": 1, "pars": 2, "consid": 1, "formal": 1, "like": 1, "furthermor": 1, "island": 1, "reus": 1, "behavior": 1, "model": 1, "agenda": 1, "pda": 1}, "marker": "[26]", "article": "P89-1018", "vector_2": [4, 0.9417271238006085, 4, 4, 1, 0]}, {"label": "Neut", "current": "Most of these models are \"synthetic\" in the sense of Brent (1999): the raw material for segmentation is a stream of segments, which are then successively grouped into larger units and eventually, conjectured words.", "context": ["On the other hand, previous computational models often underestimate the human learner's knowledge of linguistic representations.", "Most of these models are \"synthetic\" in the sense of Brent (1999): the raw material for segmentation is a stream of segments, which are then successively grouped into larger units and eventually, conjectured words.", "This assumption may make the child learner's job unnecessarily hard; since syllables are hierarchical structures consisting of segments, treating the linguistic data as unstructured segment sequences makes the problem harder than it actually is."], "vector_1": {"represent": 1, "underestim": 1, "comput": 1, "often": 1, "stream": 1, "knowledg": 1, "raw": 1, "human": 1, "unit": 1, "hierarch": 1, "group": 1, "data": 1, "make": 2, "learner": 2, "brent": 1, "sens": 1, "eventu": 1, "treat": 1, "assumpt": 1, "hard": 1, "conjectur": 1, "may": 1, "sequenc": 1, "previou": 1, "problem": 1, "unnecessarili": 1, "hand": 1, "job": 1, "child": 1, "actual": 1, "segment": 4, "sinc": 1, "unstructur": 1, "word": 1, "consist": 1, "success": 1, "synthet": 1, "larger": 1, "harder": 1, "structur": 1, "syllabl": 1, "model": 2, "linguist": 2, "materi": 1}, "marker": "(1999)", "article": "W10-2912", "vector_2": [11, 0.13256562133557834, 1, 1, 1, 0]}, {"label": "Neut", "current": "In 2009, we trained a series of decision trees to predict REG08Type based on our psycholinguistically-inspired feature set (described in (Greenbacker and McCoy, 2009c)), and then simply chose the first option in the list of REs matching the predicted type.", "context": ["The first improvement we made to our existing methods related to the manner by which we selected the specific RE to employ.", "In 2009, we trained a series of decision trees to predict REG08Type based on our psycholinguistically-inspired feature set (described in (Greenbacker and McCoy, 2009c)), and then simply chose the first option in the list of REs matching the predicted type.", "For 2010, we incorporated the case of each RE into our target attribute so that the decision tree classifier would predict both the type and case for the given reference."], "vector_1": {"set": 1, "simpli": 1, "predict": 3, "list": 1, "regtyp": 1, "featur": 1, "exist": 1, "manner": 1, "select": 1, "option": 1, "given": 1, "describ": 1, "would": 1, "classifi": 1, "re": 1, "decis": 2, "type": 2, "method": 1, "match": 1, "attribut": 1, "chose": 1, "relat": 1, "psycholinguisticallyinspir": 1, "train": 1, "refer": 1, "case": 2, "made": 1, "target": 1, "specif": 1, "tree": 2, "employ": 1, "base": 1, "incorpor": 1, "improv": 1, "first": 2, "seri": 1}, "marker": "(Greenbacker and McCoy, 2009c)", "article": "W10-4231", "vector_2": [1, 0.2878268710550045, 1, 1, 0, 0]}, {"label": "Pos", "current": "WordNet To tap into general lexical semantic properties of the words in the construction, we use features that draw on the semantic classes of words in WordNet (Fellbaum, 1998).", "context": ["To test our hypothesis that the interaction of the semantics of the noun, adjective, and verb in the target construction contributes to its pragmatic interpretation, we represent each instance in our dataset as a vector of features that capture aspects of the semantics of its component words.", "WordNet To tap into general lexical semantic properties of the words in the construction, we use features that draw on the semantic classes of words in WordNet (Fellbaum, 1998).", "These binary features each represent a synset in WordNet, and are turned on or off for the component words (the noun, adjective, and verb) in each instance of the target construction."], "vector_1": {"semant": 4, "pragmat": 1, "tap": 1, "captur": 1, "repres": 2, "featur": 3, "aspect": 1, "wordnet": 3, "use": 1, "interact": 1, "construct": 3, "compon": 2, "binari": 1, "instanc": 2, "test": 1, "draw": 1, "contribut": 1, "hypothesi": 1, "word": 4, "gener": 1, "lexic": 1, "synset": 1, "verb": 2, "adject": 2, "dataset": 1, "class": 1, "interpret": 1, "noun": 2, "target": 2, "properti": 1, "turn": 1, "vector": 1}, "marker": "(Fellbaum, 1998)", "article": "W10-2109", "vector_2": [12, 0.5635867514296496, 1, 2, 0, 0]}, {"label": "Neut", "current": "Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.", "context": ["The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.", "Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.", "In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues."], "vector_1": {"interfer": 2, "pronoun": 1, "definit": 1, "strongli": 1, "davidsennielsen": 1, "spanish": 1, "research": 1, "languag": 2, "smith": 1, "would": 1, "transfer": 2, "tapper": 1, "swan": 1, "aart": 1, "sometim": 1, "littl": 1, "articl": 1, "write": 1, "overusedunderus": 1, "altenberg": 1, "speech": 1, "answer": 1, "probabl": 1, "po": 1, "contrast": 1, "relat": 1, "tongu": 3, "mother": 3, "usag": 1, "french": 1, "part": 1, "granger": 1, "known": 1, "neg": 1, "modifi": 1, "possess": 1, "reveal": 1, "grammat": 1, "word": 1, "work": 1, "harder": 1, "across": 1, "item": 1, "anoth": 1, "allow": 1, "english": 1}, "marker": "(1998)", "article": "P13-1112", "vector_2": [15, 0.04195675509891121, 4, 2, 1, 0]}, {"label": "Pos", "current": "We built a 5-gram language model from it with the SRI language modeling toolkit (Stolcke, 2002).", "context": ["The final prepared corpus was made up of approximately 1.8 billion words of running text.", "We built a 5-gram language model from it with the SRI language modeling toolkit (Stolcke, 2002).", "To match the treatment given to the training data, the language model was also built in mixed case."], "vector_1": {"corpu": 1, "text": 1, "approxim": 1, "toolkit": 1, "languag": 3, "given": 1, "built": 2, "also": 1, "mix": 1, "treatment": 1, "final": 1, "match": 1, "case": 1, "run": 1, "train": 1, "data": 1, "billion": 1, "made": 1, "word": 1, "prepar": 1, "sri": 1, "gram": 1, "model": 3}, "marker": "(Stolcke, 2002)", "article": "W11-2143", "vector_2": [9, 0.4753045923149016, 1, 1, 0, 0]}, {"label": "Neut", "current": "Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006).", "context": ["This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.", "Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006).", "This opens up new opportunities for increasing the speed of parsers."], "vector_1": {"comput": 2, "process": 1, "converg": 1, "number": 1, "high": 1, "somewher": 1, "second": 1, "ghz": 1, "year": 1, "opportun": 1, "n": 1, "open": 1, "clock": 1, "speed": 1, "doubl": 1, "system": 1, "accuraci": 1, "new": 1, "symbol": 1, "core": 1, "parser": 1, "everi": 1, "around": 1, "sentenc": 1, "thousand": 1, "million": 1, "increas": 1, "word": 1, "averag": 1, "grammar": 1, "contextfre": 1, "frequenc": 1, "rule": 1, "nontermin": 1, "meanwhil": 1, "era": 1, "enter": 1, "manycor": 1}, "marker": "(Asanovic et al., 2006)", "article": "W11-2921", "vector_2": [5, 0.05078666980811016, 4, 1, 0, 1]}, {"label": "Neut", "current": "Beginning in the 1980s (Rumelhart and Norman, 1982), investigators used typing data to construct cognitive and motor models of language production.", "context": ["The act of typing involves juggling both the high-level text creation process, and low-level motor execution.", "Beginning in the 1980s (Rumelhart and Norman, 1982), investigators used typing data to construct cognitive and motor models of language production.", "As expounded by Salthouse (1986), a typist must simultaneously employ multiple cognitive and motor schemata, often with a formidable amount of noise between signals."], "vector_1": {"often": 1, "execut": 1, "process": 1, "text": 1, "creation": 1, "product": 1, "salthous": 1, "motor": 3, "simultan": 1, "cognit": 2, "languag": 1, "involv": 1, "use": 1, "multipl": 1, "nois": 1, "amount": 1, "construct": 1, "formid": 1, "juggl": 1, "lowlevel": 1, "must": 1, "type": 2, "begin": 1, "investig": 1, "data": 1, "highlevel": 1, "expound": 1, "signal": 1, "employ": 1, "s": 1, "schemata": 1, "act": 1, "model": 1, "typist": 1}, "marker": "(Rumelhart and Norman, 1982)", "article": "W15-0914", "vector_2": [33, 0.2828612186672101, 2, 1, 0, 0]}, {"label": "Pos", "current": "To overcome this problem, we adopt a discriminative re-ranking approach reminiscent of (Collins, 2000).", "context": ["Since the space of possible joint labelings is exponential in the number of parse tree nodes, a model cannot exhaustively consider these labelings unless it makes strong independence assumptions.", "To overcome this problem, we adopt a discriminative re-ranking approach reminiscent of (Collins, 2000).", "We use a local model, which labels arguments independently, to generate a smaller number of likely joint labelings."], "vector_1": {"smaller": 1, "independ": 2, "number": 2, "use": 1, "exponenti": 1, "make": 1, "label": 4, "sinc": 1, "overcom": 1, "approach": 1, "node": 1, "exhaust": 1, "assumpt": 1, "unless": 1, "rerank": 1, "gener": 1, "argument": 1, "joint": 2, "cannot": 1, "pars": 1, "consid": 1, "strong": 1, "model": 2, "like": 1, "possibl": 1, "local": 1, "adopt": 1, "tree": 1, "space": 1, "discrimin": 1, "problem": 1, "reminisc": 1}, "marker": "(Collins, 2000)", "article": "W05-0623", "vector_2": [5, 0.1002254879091828, 1, 1, 0, 0]}, {"label": "Neut", "current": "The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).", "context": ["Linguistic analysis.", "The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).", "Frame semantic analysis is especially interesting for the task of recognising textual entailment as it offers a robust yet relatively precise measure for semantic overlap."], "vector_1": {"lfg": 1, "entail": 1, "frame": 4, "semant": 3, "shalmanes": 1, "overlap": 1, "primari": 1, "rel": 1, "develop": 1, "offer": 1, "system": 3, "textual": 1, "yet": 1, "compon": 1, "role": 1, "interest": 1, "analysi": 3, "parc": 1, "recognis": 1, "especi": 1, "detour": 1, "probabilist": 2, "robust": 1, "measur": 1, "task": 1, "grammar": 1, "rulebas": 1, "annot": 2, "precis": 1, "combin": 1, "english": 1, "linguist": 2, "assign": 1}, "marker": "(Riezler et al., 2002)", "article": "W07-1402", "vector_2": [5, 0.18257587671017456, 3, 1, 1, 0]}, {"label": "Neut", "current": "21me Traitement Automatique des Langues Naturelles, Marseille, 2014", "context": ["", "21me Traitement Automatique des Langues Naturelles, Marseille, 2014", "Actes de l'atelier  Reseaux Lexicaux et Traitement des Langues Naturelles."], "vector_1": {"traitement": 1, "naturel": 1, "de": 2, "lexicaux": 1, "reseaux": 1, "act": 1, "et": 1, "langu": 1, "lateli": 1}, "marker": "me Traitement Automatique des Langues Naturelles, Marseille, 2014", "article": "W14-6700", "vector_2": [0, 0.9587374199451052, 1, 4, 0, 1]}, {"label": "Pos", "current": "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).", "context": ["3.1 Internal representation", "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).", "There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures."], "vector_1": {"represent": 1, "tei": 1, "among": 1, "intern": 1, "facil": 1, "shortcom": 1, "develop": 1, "spatiotempor": 1, "graph": 1, "xce": 1, "data": 1, "complex": 1, "speech": 1, "take": 1, "nite": 1, "approach": 3, "difficult": 1, "express": 1, "variou": 1, "format": 1, "acronym": 1, "object": 1, "fiesta": 1, "standard": 1, "extens": 1, "multimod": 1, "transcript": 1, "account": 1, "made": 1, "specif": 1, "annot": 2, "structur": 1, "p": 1, "model": 1}, "marker": "(TEI Consortium, 2008)", "article": "W13-5507", "vector_2": [5, 0.3696104559891935, 4, 1, 0, 0]}, {"label": "Neut", "current": "These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).", "context": ["To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.", "These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).", "In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed."], "vector_1": {"scarciti": 1, "predict": 1, "number": 1, "monolingu": 1, "year": 1, "largescal": 1, "extract": 1, "sever": 1, "baselin": 1, "mutual": 1, "techniqu": 1, "semisupervis": 1, "cw": 1, "characterbas": 1, "label": 1, "also": 1, "cotrain": 1, "addit": 1, "approach": 3, "unlabel": 1, "updat": 1, "wordbas": 1, "investig": 1, "distribut": 1, "use": 1, "intens": 1, "address": 1, "segment": 2, "recent": 1, "attempt": 1, "corpora": 2, "manual": 2, "data": 1, "employ": 1, "inform": 1, "either": 1, "learn": 1, "model": 2}, "marker": "(Zeng et al., 2013a)", "article": "D15-1142", "vector_2": [2, 0.09189300270575744, 5, 4, 2, 0]}, {"label": "Neut", "current": "In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures.", "context": ["(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.", "In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures.", "There are many differences between written text and speech - meetings in particular."], "vector_1": {"semant": 1, "show": 1, "queri": 1, "particular": 1, "lectur": 2, "result": 1, "extract": 2, "languag": 1, "web": 1, "use": 2, "perform": 2, "better": 1, "recognit": 1, "written": 1, "speech": 3, "text": 1, "document": 1, "differ": 1, "broadcast": 1, "mani": 1, "news": 1, "data": 1, "relev": 1, "retriev": 2, "keyword": 2, "slide": 1, "combin": 1, "verif": 1, "improv": 2, "model": 1, "meet": 1}, "marker": "(Rogina, 2002)", "article": "N09-1070", "vector_2": [7, 0.23245543341607297, 2, 2, 0, 0]}, {"label": "Neut", "current": "One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).", "context": ["Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).", "One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).", "Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011)."], "vector_1": {"rang": 2, "help": 1, "intellig": 1, "challeng": 1, "one": 1, "topic": 1, "see": 1, "abil": 1, "morphologicallycomplex": 1, "increasingli": 1, "sever": 1, "detect": 2, "hoo": 1, "learner": 1, "research": 1, "write": 1, "languag": 3, "type": 1, "assist": 1, "learn": 1, "autom": 1, "ical": 1, "becom": 1, "strand": 1, "recent": 1, "task": 1, "rare": 1, "computerassist": 1, "work": 1, "focu": 1, "exampl": 1, "determin": 1, "error": 3, "popular": 1}, "marker": "Tetreault and Chodorow, 2008)", "article": "W12-2011", "vector_2": [4, 0.03127188575256539, 6, 1, 0, 0]}, {"label": "Neut", "current": "Wu and Wang (2004) adapted the alignment results obtained with the out-of-domain corpus to the results obtained with the in-domain corpus.", "context": ["Although the adaptation technology is widely used for other tasks such as language modeling (Iyer et al., 1997), only a few studies, to the best of our knowledge, directly address word alignment adaptation.", "Wu and Wang (2004) adapted the alignment results obtained with the out-of-domain corpus to the results obtained with the in-domain corpus.", "This method first trained two models and two translation dictionaries with the in-domain corpus and the out-of-domain corpus, respectively."], "vector_1": {"corpu": 4, "wang": 1, "directli": 1, "knowledg": 1, "obtain": 2, "result": 2, "respect": 1, "best": 1, "use": 1, "two": 2, "outofdomain": 2, "adapt": 3, "languag": 1, "method": 1, "wu": 1, "train": 1, "translat": 1, "although": 1, "address": 1, "studi": 1, "wide": 1, "task": 1, "word": 1, "indomain": 2, "align": 2, "technolog": 1, "dictionari": 1, "model": 2, "first": 1}, "marker": "(2004)", "article": "P05-1058", "vector_2": [1, 0.08537105212168185, 2, 13, 0, 1]}, {"label": "Neut", "current": "In (Mille and Wanner, 2008) the claim sentence (by means of lexical and punctuation clues) is segmented into clausal units, that are then compressed into a summary.", "context": ["For example, in (Shinmori et al., 2003) the discourse structure of the patent claim is built by means of a rule-based technique; each discourse segment is then paraphrased.", "In (Mille and Wanner, 2008) the claim sentence (by means of lexical and punctuation clues) is segmented into clausal units, that are then compressed into a summary.", "The simplification methods proposed by this group of researches to some extent change the original content of the claim that might not always be desirable, especially for patent experts."], "vector_1": {"origin": 1, "claim": 3, "clausal": 1, "paraphras": 1, "unit": 1, "group": 1, "built": 1, "expert": 1, "patent": 2, "research": 1, "content": 1, "alway": 1, "mean": 2, "might": 1, "method": 1, "discours": 2, "desir": 1, "especi": 1, "sentenc": 1, "compress": 1, "lexic": 1, "clue": 1, "punctuat": 1, "extent": 1, "segment": 2, "techniqu": 1, "simplif": 1, "rulebas": 1, "structur": 1, "exampl": 1, "chang": 1, "summari": 1, "propos": 1}, "marker": "(Mille and Wanner, 2008)", "article": "W14-5605", "vector_2": [6, 0.2603828188464663, 2, 3, 0, 0]}, {"label": "Neut", "current": "Therefore, the final document similarity metric we evaluate (COMBINED) is a linear combination of the best-performing Wikipedia-based methods described above, and the well-known Vector Space Model (VSM) with cosine similarity and tf-idf (Salton and McGill, 1983).", "context": ["We therefore hypothesise that combining the two approaches will lead to more robust document similarity performance.", "Therefore, the final document similarity metric we evaluate (COMBINED) is a linear combination of the best-performing Wikipedia-based methods described above, and the well-known Vector Space Model (VSM) with cosine similarity and tf-idf (Salton and McGill, 1983).", "Results"], "vector_1": {"evalu": 1, "metric": 1, "bestperform": 1, "wikipediabas": 1, "result": 1, "vsm": 1, "describ": 1, "lead": 1, "space": 1, "perform": 1, "two": 1, "document": 2, "wellknown": 1, "method": 1, "linear": 1, "hypothesis": 1, "robust": 1, "therefor": 2, "approach": 1, "final": 1, "tfidf": 1, "vector": 1, "combin": 3, "cosin": 1, "model": 1, "similar": 3}, "marker": "(Salton and McGill, 1983)", "article": "W10-3506", "vector_2": [27, 0.9135919030174856, 1, 2, 0, 0]}, {"label": "Neut", "current": "The SVM classifier outputs class labels and the class labels are converted into confidence scores using the techniques given in (Lin et al., 2007).", "context": ["System-independent features for each translation output are fed as input to the SVM classifier (Cortes and Vapnik, 1995).", "The SVM classifier outputs class labels and the class labels are converted into confidence scores using the techniques given in (Lin et al., 2007).", "Relying on system independent black-box features has allowed us to build"], "vector_1": {"featur": 2, "independ": 1, "fed": 1, "techniqu": 1, "system": 1, "classifi": 2, "label": 2, "reli": 1, "score": 1, "build": 1, "input": 1, "confid": 1, "blackbox": 1, "use": 1, "translat": 1, "given": 1, "class": 2, "convert": 1, "svm": 2, "systemindepend": 1, "us": 1, "allow": 1, "output": 2}, "marker": "(Lin et al., 2007)", "article": "N13-3003", "vector_2": [6, 0.4185733512786003, 2, 1, 0, 0]}, {"label": "Neut", "current": "Traditional readability formulas normally take into account the number of words per sentence or/and the number of \"hard\", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).", "context": ["Anybody who has seen patent claims at least once will find it unnecessary to calculate claim readability indices to get persuaded that the claim text is extremely low readable.", "Traditional readability formulas normally take into account the number of words per sentence or/and the number of \"hard\", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).", "Both the first and the second ratio will be equal to the number of words in a claim sentence where practically all words are \"hard\" terms, some of them used for the first time."], "vector_1": {"claim": 4, "text": 1, "hard": 2, "number": 3, "indic": 1, "orand": 1, "second": 1, "per": 2, "seen": 1, "find": 1, "use": 1, "long": 1, "ratio": 1, "patent": 1, "normal": 1, "readabl": 3, "persuad": 1, "least": 1, "low": 2, "formula": 1, "take": 1, "get": 1, "sentenc": 3, "term": 1, "tradit": 1, "account": 1, "word": 4, "practic": 1, "frequenc": 1, "equal": 1, "unnecessari": 1, "calcul": 1, "time": 1, "extrem": 1, "anybodi": 1, "first": 2}, "marker": "(Kincaid, Fishburne, Rogers, & Chissom, 1975", "article": "W14-5605", "vector_2": [39, 0.11372867587327376, 3, 1, 0, 0]}, {"label": "Neut", "current": "The training data and the testing data described in (Wu and Wang, 2004) are from a single manual.", "context": ["The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004).", "The training data and the testing data described in (Wu and Wang, 2004) are from a single manual.", "The data in our corpus are from several manuals describing how to use the diagnostic ultrasound systems."], "vector_1": {"corpu": 3, "differ": 1, "describ": 2, "ultrasound": 1, "indomain": 1, "manual": 2, "use": 1, "system": 1, "paper": 1, "diagnost": 1, "reason": 1, "train": 2, "sever": 1, "test": 2, "main": 1, "data": 3, "singl": 1}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.799341212943228, 2, 13, 0, 1]}, {"label": "CoCo", "current": "(Mohammad and Pedersen, 2004) show results to the same conclusions for SENSEVAL-1 and SENSEVAL-2 data that have similar low training data per task, while, the line, hard, serve and interest data which have much larger training data per task are shown to benefit from a larger context.", "context": ["We believe this is due to the low training data per task ratio, which usually means that the weak indicators (P and P ) are likely to be overwhelmed by idiosyncrasies of the data.", "(Mohammad and Pedersen, 2004) show results to the same conclusions for SENSEVAL-1 and SENSEVAL-2 data that have similar low training data per task, while, the line, hard, serve and interest data which have much larger training data per task are shown to benefit from a larger context.", "Duluth-ELSS (a sister system of SyntaLex) achieves an accuracy of 61.7%."], "vector_1": {"show": 1, "overwhelm": 1, "hard": 1, "indic": 1, "system": 1, "result": 1, "67": 1, "shown": 1, "ratio": 1, "syntalex": 1, "due": 1, "per": 3, "accuraci": 1, "much": 1, "low": 2, "interest": 1, "duluthelss": 1, "sensev": 2, "weak": 1, "train": 3, "believ": 1, "line": 1, "data": 6, "conclus": 1, "serv": 1, "sister": 1, "task": 3, "like": 1, "larger": 2, "p": 2, "benefit": 1, "achiev": 1, "context": 1, "idiosyncrasi": 1, "usual": 1, "similar": 1, "mean": 1}, "marker": "(Mohammad and Pedersen, 2004)", "article": "W04-0839", "vector_2": [0, 0.7715150641487675, 1, 3, 3, 1]}, {"label": "Pos", "current": "The alignment links in this intersection set are extended by iteratively adding word alignment links into it as described in (Och and Ney, 2000).", "context": ["By taking the intersection of the two word alignment results, we build a new alignment set.", "The alignment links in this intersection set are extended by iteratively adding word alignment links into it as described in (Och and Ney, 2000).", "Based on the extended alignment links, we build a translation dictionary."], "vector_1": {"set": 2, "word": 2, "ad": 1, "extend": 2, "align": 5, "two": 1, "iter": 1, "dictionari": 1, "describ": 1, "base": 1, "link": 3, "result": 1, "intersect": 2, "new": 1, "build": 2, "translat": 1, "take": 1}, "marker": "(Och and Ney, 2000)", "article": "P05-1058", "vector_2": [5, 0.46742879286959893, 1, 3, 1, 0]}, {"label": "Pos", "current": "Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11.", "context": ["Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.", "Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11.", "The experimental results show that n-grams containing articles are predictive for identifying native languages."], "vector_1": {"identifi": 1, "show": 3, "predict": 2, "text": 1, "expect": 1, "articl": 1, "differ": 1, "languag": 4, "result": 1, "identif": 1, "use": 2, "perform": 3, "research": 1, "better": 1, "also": 1, "experiment": 1, "speaker": 2, "method": 1, "machin": 1, "sourc": 1, "emphas": 1, "relat": 1, "given": 1, "nativ": 3, "ngram": 1, "task": 1, "translat": 2, "although": 1, "contain": 1, "grammat": 1, "learningbas": 1, "contrari": 1, "target": 1, "nonn": 1, "rather": 1, "inform": 1, "achiev": 1, "wong": 1, "determin": 1, "error": 2, "dra": 1, "improv": 1, "typic": 1, "propos": 1}, "marker": "(Koppel and Ordan, 2011", "article": "P13-1112", "vector_2": [2, 0.8352399938659715, 3, 1, 1, 0]}, {"label": "Neut", "current": "Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).", "context": ["national project on item generation for testing student competencies in solving probability problems.", "Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).", "The goal of our item generation project is to develop a model to support optimal problem and test construction."], "vector_1": {"control": 1, "set": 1, "automat": 1, "predefin": 1, "paramet": 1, "develop": 1, "goal": 1, "support": 1, "construct": 2, "compet": 1, "way": 1, "test": 2, "probabl": 1, "difficulti": 1, "optim": 1, "gener": 3, "effect": 1, "nation": 1, "base": 1, "student": 1, "solv": 1, "model": 1, "task": 1, "project": 2, "item": 4, "mani": 1, "problem": 2}, "marker": "Holling et al., 2009)", "article": "W11-1403", "vector_2": [2, 0.05554905516878254, 4, 5, 1, 0]}, {"label": "Neut", "current": "There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.", "context": ["Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements.", "There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.", "We suspect that most of the ideas therein are orthogonal to our approach, and therefore leave their integration into our GPU-based parser for future work."], "vector_1": {"therein": 1, "orthogon": 1, "parser": 1, "idea": 1, "speed": 2, "optim": 2, "techniqu": 1, "refactor": 1, "also": 1, "suspect": 1, "futur": 1, "leav": 1, "gpubas": 1, "therefor": 1, "approach": 4, "infer": 1, "asearch": 1, "integr": 1, "preserv": 2, "multipass": 1, "base": 1, "addit": 1, "grammar": 1, "could": 1, "work": 1, "cki": 1, "yield": 1, "aim": 1, "combin": 1, "contrast": 1, "improv": 1, "principl": 1}, "marker": "(Klein and Manning, 2003", "article": "W11-2921", "vector_2": [8, 0.9600251315479463, 3, 1, 7, 0]}, {"label": "Neut", "current": "Yang (2004) evaluates the effectiveness of the USC in conjunction with a simple approach to using transitional probabilities.", "context": ["Moreover, it also knows that in the window between S1 and S2 there must be one or more word boundaries.", "Yang (2004) evaluates the effectiveness of the USC in conjunction with a simple approach to using transitional probabilities.", "The performance of the approach presented there improves dramatically if the learner is equipped with the assumption that each word can have only one primary stress."], "vector_1": {"evalu": 1, "transit": 1, "one": 2, "equip": 1, "primari": 1, "probabl": 1, "conjunct": 1, "usc": 1, "perform": 1, "moreov": 1, "learner": 1, "also": 1, "window": 1, "approach": 2, "assumpt": 1, "use": 1, "effect": 1, "know": 1, "present": 1, "must": 1, "stress": 1, "word": 2, "simpl": 1, "dramat": 1, "s": 2, "yang": 1, "improv": 1, "boundari": 1}, "marker": "(2004)", "article": "W10-2912", "vector_2": [6, 0.3159144893111639, 1, 9, 0, 0]}, {"label": "Pos", "current": "Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.", "context": ["Such a large variety of vocabulary naturally leads to long names with productive use of general words, making the task difficult to be solved by systems with naive Markov assumption of label sequences, because such systems must perform their prediction without seeing the entire string of the entities.", "Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.", "After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000)."], "vector_1": {"entropi": 1, "predict": 1, "natur": 1, "naiv": 1, "global": 1, "share": 2, "assumpt": 1, "indic": 1, "see": 1, "attain": 1, "seem": 1, "research": 1, "improv": 1, "best": 1, "entiti": 2, "use": 3, "lead": 1, "perform": 3, "make": 2, "enabl": 1, "particip": 1, "system": 4, "long": 2, "label": 2, "field": 1, "score": 1, "larg": 1, "treatment": 1, "postprocess": 1, "varieti": 1, "import": 1, "tackl": 1, "might": 1, "difficult": 1, "product": 1, "random": 1, "string": 1, "optim": 1, "sequenc": 1, "gener": 1, "jnlpba": 1, "like": 1, "memm": 1, "extens": 1, "entir": 1, "promis": 1, "vocabulari": 1, "condit": 1, "solv": 1, "implicitli": 1, "model": 2, "must": 1, "comparison": 1, "task": 4, "word": 1, "name": 2, "local": 1, "maximum": 1, "inform": 1, "without": 1, "crf": 1, "mani": 1, "markov": 2}, "marker": "(Kim et al., 2004)", "article": "W07-1033", "vector_2": [3, 0.0606204495093384, 4, 3, 3, 0]}, {"label": "Neut", "current": "In [28], Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that \"split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing\".", "context": ["  Several natural language parser start with a pure ContextFree (CF) backbone that makes a first sketch of the structure of the analyzed sentence, before it is handed to a more elaborate analyzer (possibly a coroutine), that takes into account the finer grammatical structure to filter out undesirable parses (see for example [24,28]).", "In [28], Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that \"split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing\".", "In this paper we shall call shared forests such data struc"], "vector_1": {"sketch": 1, "set": 1, "forest": 1, "give": 1, "natur": 1, "parser": 1, "share": 1, "cf": 1, "restrict": 1, "see": 1, "paper": 1, "exist": 1, "domain": 1, "analyz": 2, "sever": 1, "infinit": 1, "use": 1, "equival": 1, "shieber": 1, "make": 1, "start": 1, "tunabl": 1, "call": 1, "take": 1, "pure": 1, "languag": 1, "approach": 2, "finer": 1, "shall": 1, "sentenc": 1, "variant": 1, "grammat": 1, "hand": 1, "base": 1, "pars": 2, "finit": 1, "elabor": 1, "struc": 1, "data": 1, "class": 1, "undesir": 1, "account": 1, "backbon": 1, "possibl": 1, "contextfre": 1, "split": 1, "structur": 2, "nontermin": 1, "filter": 1, "coroutin": 1, "exampl": 1, "survey": 1, "first": 1}, "marker": "[28]", "article": "P89-1018", "vector_2": [3, 0.045367924843702986, 3, 2, 2, 0]}, {"label": "Neut", "current": "d j a ( | j t (fj   |e j a (3) 1 A cept is defined as the set of target words connected to a source word (Brown et al., 1993). )", "context": ["p(a,  |e) =  Pr( ,  |) f ( , )   e   l m , , 1 j = j : 0 a j =  n(i  |ei ) i i =1 i=1 m m e) m 0 l l |  ,  e)Pr(=  p(a, | f ( , )     m 0 = 0   p   20 0 p1  ! )", "d j a ( | j t (fj   |e j a (3) 1 A cept is defined as the set of target words connected to a source word (Brown et al., 1993). )", ") ( |   t f e j a j i 1, 0 aj= = l m  )  n(i  |e = 1 = i j 1 m j j= 1, 0 a j = m 0 ( ([ ( )] ( j h a d j c =   )) j 1 aj m ([ ( )] ( ( )))) j h a d j p j =   j > 1   + 0 =   p   m 0  2  0 0 p1 } i = max{i ': i' >"], "vector_1": {"set": 1, "ei": 1, "aj": 2, "cept": 1, "connect": 1, "maxi": 1, "pr": 1, "ni": 2, "p": 5, "pa": 2, "sourc": 1, "word": 2, "fj": 1, "c": 1, "e": 6, "target": 1, "f": 3, "i": 1, "h": 2, "j": 19, "epr": 1, "l": 4, "defin": 1}, "marker": "(Brown et al., 1993)", "article": "P05-1058", "vector_2": [12, 0.22635148227087773, 1, 4, 0, 0]}, {"label": "Neut", "current": "Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level.", "context": ["Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators - average sentence length and type-token ratio - obtaining good results for her target group.", "Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level.", "He created a tailored C-test that only contains selected gaps in order to better discriminate between the students."], "vector_1": {"calibr": 1, "creat": 1, "classic": 1, "pool": 1, "kleinbraley": 1, "obtain": 1, "indic": 1, "result": 1, "select": 1, "use": 1, "ratio": 1, "compar": 1, "discrimin": 1, "perform": 2, "two": 1, "better": 1, "averag": 1, "test": 1, "build": 1, "analysi": 2, "difficulti": 2, "good": 1, "intend": 1, "linear": 1, "sentenc": 1, "eck": 1, "differ": 1, "rasch": 1, "gap": 2, "tailor": 1, "student": 1, "group": 1, "ctest": 3, "target": 1, "typetoken": 1, "level": 1, "kamimoto": 1, "item": 1, "length": 1, "contain": 1, "regress": 1, "model": 1, "order": 2, "first": 1}, "marker": "(2011)", "article": "Q14-1040", "vector_2": [3, 0.17166096756570828, 3, 1, 2, 0]}, {"label": "Neut", "current": "In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation.", "context": ["Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders.", "In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation.", "However, phrase level alignment cannot handle long distance reordering effectively."], "vector_1": {"distanc": 1, "directli": 1, "within": 1, "templat": 2, "phrase": 6, "languag": 1, "differ": 1, "long": 1, "accuraci": 1, "reorder": 3, "higher": 1, "handl": 3, "shallow": 1, "use": 2, "effect": 1, "util": 1, "cannot": 1, "translat": 3, "sinc": 1, "twolevel": 1, "word": 2, "howev": 1, "align": 4, "level": 1, "structur": 1, "employ": 1, "achiev": 1, "model": 2, "order": 1}, "marker": "Och et al., 1999)", "article": "N04-1023", "vector_2": [5, 0.1502758594081562, 2, 1, 10, 1]}, {"label": "Pos", "current": "This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.", "context": ["If a system has seen only positive examples, how does it recognize a negative example?", "This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.", "Outlier detection approaches typically derive some model of \"normal\" objects from the training set and use a distance measure and a threshold to detect abnormal items."], "vector_1": {"set": 1, "distanc": 1, "threshold": 1, "seen": 2, "differ": 1, "detect": 5, "unknown": 1, "normal": 1, "system": 1, "also": 1, "call": 1, "approach": 1, "model": 1, "recogn": 1, "deriv": 1, "outlier": 2, "object": 1, "use": 1, "train": 2, "address": 1, "data": 1, "typic": 1, "measur": 1, "novel": 1, "novelti": 1, "item": 2, "exampl": 2, "abnorm": 1, "neg": 1, "posit": 1, "problem": 1}, "marker": "Marsland, 2003)", "article": "N06-1017", "vector_2": [3, 0.026821842845581718, 3, 3, 0, 0]}, {"label": "Pos", "current": "This has motivated the development of semantic re-rendering algorithms, designed to enrich the ontological specificity of a domain-specific semantic tag set, based on inductive techniques described in (Pustejovsky et al., 2002c).", "context": ["'protein', rather than 'Amino Acid, Peptid, or Protein).", "This has motivated the development of semantic re-rendering algorithms, designed to enrich the ontological specificity of a domain-specific semantic tag set, based on inductive techniques described in (Pustejovsky et al., 2002c).", "3.4 Rerendering Semantic Ontologies"], "vector_1": {"rerend": 2, "semant": 3, "set": 1, "peptid": 1, "specif": 1, "algorithm": 1, "design": 1, "rather": 1, "enrich": 1, "induct": 1, "describ": 1, "techniqu": 1, "tag": 1, "motiv": 1, "base": 1, "ontolog": 2, "domainspecif": 1, "develop": 1, "protein": 2, "acid": 1, "amino": 1}, "marker": "(Pustejovsky et al., 2002c)", "article": "W02-0312", "vector_2": [0, 0.21071335249542722, 1, 1, 4, 1]}, {"label": "CoCo", "current": "In terms of LAS, MaltParser (Nivre et al., 2007) performs best of the 3 parsers with SD, and MSTParser (McDonald et al., 2006) performs best with CoNLL.", "context": ["For all parsers and in most experiments (which explore several pipelines with different POS-tagging strategies), SD is easier to label (i.e., label accuracy scores are higher) and CoNLL is easier to structure (i.e., unlabeled attachment scores are higher).", "In terms of LAS, MaltParser (Nivre et al., 2007) performs best of the 3 parsers with SD, and MSTParser (McDonald et al., 2006) performs best with CoNLL.", "In Nilsson et al (2006), the authors investigate the effects of two types of input transformation on the performance of MaltParser."], "vector_1": {"parser": 2, "al": 1, "transform": 1, "explor": 1, "et": 1, "label": 2, "ie": 2, "sever": 1, "conll": 2, "differ": 1, "maltpars": 2, "la": 1, "perform": 3, "two": 1, "accuraci": 1, "attach": 1, "score": 2, "best": 2, "input": 1, "experi": 1, "type": 1, "easier": 2, "unlabel": 1, "higher": 2, "pipelin": 1, "investig": 1, "nilsson": 1, "effect": 1, "postag": 1, "mstparser": 1, "term": 1, "author": 1, "structur": 1, "strategi": 1, "sd": 2}, "marker": "(Nivre et al., 2007)", "article": "W15-2134", "vector_2": [8, 0.14242183780352324, 3, 3, 10, 0]}, {"label": "Neut", "current": "Links into pages are used, since this leads to better results (Witten and Milne, 2008).", "context": ["47", "Links into pages are used, since this leads to better results (Witten and Milne, 2008).", "The Wikipedia graph structure is represented in an adjacency list structure, i.e."], "vector_1": {"use": 1, "lead": 1, "graph": 1, "wikipedia": 1, "structur": 2, "sinc": 1, "better": 1, "list": 1, "adjac": 1, "link": 1, "result": 1, "repres": 1, "ie": 1, "page": 1}, "marker": "(Witten and Milne, 2008)", "article": "W10-3506", "vector_2": [2, 0.260319498470486, 1, 7, 1, 0]}, {"label": "CoCo", "current": "(Mohammad and Pedersen, 2004) show results to the same conclusions for SENSEVAL-1 and SENSEVAL-2 data that have similar low training data per task, while, the line, hard, serve and interest data which have much larger training data per task are shown to benefit from a larger context.", "context": ["We believe this is due to the low training data per task ratio, which usually means that the weak indicators (P and P ) are likely to be overwhelmed by idiosyncrasies of the data.", "(Mohammad and Pedersen, 2004) show results to the same conclusions for SENSEVAL-1 and SENSEVAL-2 data that have similar low training data per task, while, the line, hard, serve and interest data which have much larger training data per task are shown to benefit from a larger context.", "Duluth-ELSS (a sister system of SyntaLex) achieves an accuracy of 61.7%."], "vector_1": {"show": 1, "overwhelm": 1, "hard": 1, "indic": 1, "system": 1, "result": 1, "67": 1, "shown": 1, "ratio": 1, "syntalex": 1, "due": 1, "per": 3, "accuraci": 1, "much": 1, "low": 2, "interest": 1, "duluthelss": 1, "sensev": 2, "weak": 1, "train": 3, "believ": 1, "line": 1, "data": 6, "conclus": 1, "serv": 1, "sister": 1, "task": 3, "like": 1, "larger": 2, "p": 2, "benefit": 1, "achiev": 1, "context": 1, "idiosyncrasi": 1, "usual": 1, "similar": 1, "mean": 1}, "marker": "(Mohammad and Pedersen, 2004)", "article": "W04-0839", "vector_2": [0, 0.7715150641487675, 1, 3, 3, 1]}, {"label": "Neut", "current": "Providing a non-error driven model, Crossley et al (2011) studied the impact of various lexical indices in predicting the learner proficiency level.", "context": ["Although error-rate is a strong indicator of a learner's proficiency in a language, considering other factors like lexical indices or syntactic and morphological complexity would help in providing multiple views about the same data.", "Providing a non-error driven model, Crossley et al (2011) studied the impact of various lexical indices in predicting the learner proficiency level.", "Using a corpus of 100 writing samples by L2 learners of English classified in to three levels (beginner, intermediate, advanced), they built a classification system that analyses language proficiency using the Coh-metrix5 lexical indices."], "vector_1": {"corpu": 1, "classif": 1, "profici": 3, "predict": 1, "intermedi": 1, "al": 1, "indic": 4, "system": 1, "sampl": 1, "et": 1, "cohmetrix": 1, "languag": 2, "help": 1, "impact": 1, "write": 1, "errorr": 1, "multipl": 1, "built": 1, "would": 1, "learner": 3, "three": 1, "classifi": 1, "morpholog": 1, "complex": 1, "factor": 1, "crossley": 1, "analys": 1, "syntact": 1, "variou": 1, "advanc": 1, "use": 2, "lexic": 3, "driven": 1, "nonerror": 1, "although": 1, "consid": 1, "strong": 1, "data": 1, "studi": 1, "like": 1, "level": 2, "provid": 2, "l": 1, "english": 1, "model": 1, "beginn": 1, "view": 1}, "marker": "(2011)", "article": "W13-1708", "vector_2": [2, 0.1882677212535203, 1, 1, 0, 0]}, {"label": "Neut", "current": "A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "context": ["6 Related Work", "A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization."], "vector_1": {"finegrain": 1, "none": 1, "relat": 2, "last": 1, "compar": 1, "directli": 1, "possibl": 1, "provid": 1, "natur": 1, "howev": 1, "work": 3, "parser": 1, "two": 1, "bodi": 1, "accumul": 1, "much": 1, "substanti": 1, "gpu": 1, "decad": 1, "parallel": 2, "languag": 1}, "marker": "(van Lohuizen, 1999", "article": "W11-2921", "vector_2": [12, 0.8736092567868269, 4, 3, 0, 0]}, {"label": "Neut", "current": "For samples, the total number of pairwise samples in (Herbrich et al., 2000) is roughly .", "context": ["However, the size of generated training samples will be very large.", "For samples, the total number of pairwise samples in (Herbrich et al., 2000) is roughly .", "In the next section, we will introduce two perceptron-like algorithms that utilize pairwise samples while keeping the complexity of data space unchanged."], "vector_1": {"number": 1, "perceptronlik": 1, "sampl": 4, "total": 1, "size": 1, "space": 1, "section": 1, "two": 1, "next": 1, "complex": 1, "larg": 1, "introduc": 1, "gener": 1, "util": 1, "train": 1, "data": 1, "algorithm": 1, "howev": 1, "keep": 1, "pairwis": 2, "roughli": 1, "unchang": 1}, "marker": "(Herbrich et al., 2000)", "article": "N04-1023", "vector_2": [4, 0.5747521123500136, 1, 2, 1, 0]}, {"label": "Neut", "current": "Chen[3] points that some verbs belong to more than one category, and gives a method to distinguish these cases.", "context": ["(2) Separate classification of the verb situation from classification of the sentence situation.", "Chen[3] points that some verbs belong to more than one category, and gives a method to distinguish these cases.", "To make the ideal more clear, we use two steps to complete the sentence situation recognition."], "vector_1": {"classif": 2, "belong": 1, "give": 1, "one": 1, "chen": 1, "use": 1, "make": 1, "point": 1, "two": 1, "categori": 1, "recognit": 1, "ideal": 1, "method": 1, "complet": 1, "sentenc": 2, "situat": 3, "step": 1, "verb": 2, "distinguish": 1, "case": 1, "clear": 1, "separ": 1}, "marker": "[3]", "article": "W00-1220", "vector_2": [5, 0.5478599221789884, 1, 4, 0, 0]}, {"label": "Neut", "current": "The effects of predictability are well documented, in that more likely sequences are produced and comprehended at a faster rate (GoldmanEisler, 1958; Hale, 2006; Nottbusch et al., 2007; Levy, 2008; Smith and Levy, 2013, and references therein).", "context": ["A final confound to be investigated was sequence likelihood.", "The effects of predictability are well documented, in that more likely sequences are produced and comprehended at a faster rate (GoldmanEisler, 1958; Hale, 2006; Nottbusch et al., 2007; Levy, 2008; Smith and Levy, 2013, and references therein).", "Since MWEs are frequently made up of collocations, i.e."], "vector_1": {"therein": 1, "predict": 1, "confound": 1, "rate": 1, "ie": 1, "colloc": 1, "comprehend": 1, "mwe": 1, "document": 1, "final": 1, "refer": 1, "investig": 1, "sequenc": 2, "effect": 1, "likelihood": 1, "sinc": 1, "faster": 1, "made": 1, "like": 1, "well": 1, "produc": 1, "frequent": 1}, "marker": "(GoldmanEisler, 1958", "article": "W15-0914", "vector_2": [57, 0.6084369268392092, 5, 1, 0, 0]}, {"label": "Neut", "current": "In the VRE, the link between the social network and digital artifacts is established formally, by the integration of the FOAF social networking vocabulary (Brickley and Miller, 2010) with our provenance ontologies.", "context": ["The user's social context.", "In the VRE, the link between the social network and digital artifacts is established formally, by the integration of the FOAF social networking vocabulary (Brickley and Miller, 2010) with our provenance ontologies.", "FOAF characterises an individual and their social network by defining a vocabulary describing people, the links between them and the things they create and do."], "vector_1": {"vocabulari": 2, "creat": 1, "ontolog": 1, "individu": 1, "proven": 1, "establish": 1, "network": 3, "defin": 1, "characteris": 1, "formal": 1, "foaf": 2, "digit": 1, "peopl": 1, "artifact": 1, "link": 2, "user": 1, "vre": 1, "describ": 1, "thing": 1, "integr": 1, "context": 1, "social": 4}, "marker": "(Brickley and Miller, 2010)", "article": "W11-2820", "vector_2": [1, 0.338052723334566, 1, 1, 0, 0]}, {"label": "Neut", "current": "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "context": ["Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).", "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training."], "vector_1": {"corpu": 1, "satisfactori": 1, "result": 2, "year": 1, "largescal": 1, "intermedi": 1, "research": 1, "build": 1, "method": 1, "machin": 1, "train": 1, "link": 1, "translat": 1, "associ": 1, "requir": 1, "recent": 1, "measur": 1, "word": 1, "align": 2, "employ": 1, "achiev": 1, "statist": 2, "bilingu": 1, "mani": 1, "model": 1, "propos": 1, "order": 1, "first": 1}, "marker": "Ahrenberg et al., 1998", "article": "P05-1058", "vector_2": [7, 0.039255958147645806, 7, 1, 0, 0]}, {"label": "Neut", "current": "A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).", "context": ["In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner.", "A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).", "The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly."], "vector_1": {"corpu": 1, "infant": 1, "constitu": 1, "evalu": 1, "intermedi": 1, "transit": 2, "number": 1, "procedur": 1, "daysold": 1, "result": 1, "human": 1, "onlin": 1, "artifici": 1, "surprisingli": 1, "languag": 1, "probabl": 2, "even": 1, "syllabifi": 1, "colloc": 1, "would": 1, "support": 1, "perfectli": 1, "learner": 2, "young": 1, "tp": 2, "research": 1, "identif": 1, "reli": 1, "experiment": 2, "unit": 1, "basic": 1, "input": 1, "approach": 1, "realiti": 1, "poor": 1, "assumpt": 1, "syntact": 1, "use": 4, "syllab": 1, "base": 1, "segment": 2, "studi": 1, "addit": 1, "linguist": 1, "word": 2, "level": 2, "local": 1, "clear": 1, "work": 1, "psychologicallymotiv": 1, "syllabl": 2, "inform": 1, "alreadi": 1, "learn": 2, "model": 1, "psycholog": 2}, "marker": "(Swingley, 2005)", "article": "W10-2912", "vector_2": [5, 0.16909708650290148, 5, 1, 0, 0]}, {"label": "Pos", "current": "This service generates text containing a description of the resource using a deep model of the syntactic structure of sentences and their combinations, inspired by the work of Hielkema (2010).", "context": ["In order to generate the text, we have implemented a RESTful service that invokes a Text Generator service based on the RDF ID of the resource being described, passed as a parameter by the Web interface.", "This service generates text containing a description of the resource using a deep model of the syntactic structure of sentences and their combinations, inspired by the work of Hielkema (2010).", "VRE User Interface User Core Services Social Networking Metadata Repository Policy Policy Manager Text Interface"], "vector_1": {"repositori": 1, "text": 4, "inspir": 1, "rest": 1, "hielkema": 1, "pass": 1, "id": 1, "paramet": 1, "web": 1, "use": 1, "describ": 1, "polici": 2, "interfac": 3, "metadata": 1, "core": 1, "resourc": 2, "syntact": 1, "sentenc": 1, "gener": 3, "social": 1, "base": 1, "deep": 1, "vre": 1, "model": 1, "network": 1, "servic": 4, "work": 1, "descript": 1, "structur": 1, "manag": 1, "invok": 1, "rdf": 1, "combin": 1, "contain": 1, "implement": 1, "order": 1, "user": 2}, "marker": "(2010)", "article": "W11-2820", "vector_2": [1, 0.47313645728775955, 1, 3, 0, 0]}, {"label": "Neut", "current": "Relatedly, recent work such as Borschinger et al (2013) has found that artificially created data often masks the complexity exhibited by real speech.", "context": ["For example, languages such as French lack lexical stress; it would be interesting to know whether in such a case, phonotactic (or other) cues are more important.", "Relatedly, recent work such as Borschinger et al (2013) has found that artificially created data often masks the complexity exhibited by real speech.", "This suggests that future work should use data directly derived from the acoustic signal to account for contextual effects, rather than using dictionary look-up or other heuristics."], "vector_1": {"exhibit": 1, "relatedli": 1, "creat": 1, "lack": 1, "al": 1, "heurist": 1, "et": 1, "artifici": 1, "languag": 1, "often": 1, "use": 2, "phonotact": 1, "directli": 1, "would": 1, "rather": 1, "suggest": 1, "cue": 1, "complex": 1, "speech": 1, "futur": 1, "interest": 1, "import": 1, "real": 1, "deriv": 1, "account": 1, "lexic": 1, "effect": 1, "lookup": 1, "french": 1, "dictionari": 1, "data": 2, "recent": 1, "case": 1, "stress": 1, "acoust": 1, "whether": 1, "signal": 1, "work": 2, "mask": 1, "contextu": 1, "exampl": 1, "borsching": 1, "found": 1, "know": 1}, "marker": "(2013)", "article": "Q14-1008", "vector_2": [1, 0.9658108393083087, 1, 2, 25, 0]}, {"label": "Pos", "current": "Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).", "context": ["Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.", "Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).", "Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches."], "vector_1": {"earli": 1, "difficulti": 1, "procedur": 1, "immun": 1, "still": 1, "involv": 1, "perform": 1, "associ": 1, "smith": 2, "accuraci": 1, "also": 1, "new": 2, "approach": 2, "method": 1, "introduc": 1, "complet": 1, "function": 2, "core": 1, "relat": 1, "though": 1, "optim": 1, "object": 2, "train": 1, "nonconvex": 1, "bayesian": 1, "likelihood": 1, "induct": 1, "thu": 1, "eisner": 2, "improv": 2, "model": 1, "propos": 1}, "marker": "Kurihara and Sato, 2006)", "article": "P08-1100", "vector_2": [2, 0.04905838996517564, 5, 1, 0, 0]}, {"label": "Neut", "current": "The usage of concreteness features was previously discussed in the literature; to our knowledge, these features have not yet been evaluated for their impact in a comprehensive system for word-level metaphor detection, apart from the concreteness features as used in Beigman Klebanov et al (2014), which we use as a baseline.", "context": ["We start with a baseline set of features and training regime from Beigman Klebanov et al (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations.", "The usage of concreteness features was previously discussed in the literature; to our knowledge, these features have not yet been evaluated for their impact in a comprehensive system for word-level metaphor detection, apart from the concreteness features as used in Beigman Klebanov et al (2014), which we use as a baseline.", "2 Data"], "vector_1": {"klebanov": 2, "set": 1, "evalu": 1, "certain": 1, "within": 1, "al": 2, "concept": 1, "et": 2, "featur": 5, "yet": 1, "baselin": 2, "impact": 2, "differ": 1, "detect": 1, "depend": 1, "system": 1, "start": 1, "beigman": 2, "concret": 4, "knowledg": 1, "suit": 1, "comprehens": 1, "previous": 1, "type": 1, "wordlevel": 1, "investig": 1, "relat": 2, "usag": 1, "use": 2, "regim": 1, "train": 2, "metaphor": 1, "data": 1, "discuss": 1, "apart": 1, "target": 1, "reweight": 1, "well": 1, "literatur": 1, "exampl": 1}, "marker": "(2014)", "article": "W15-1402", "vector_2": [1, 0.06778705913550782, 2, 8, 5, 0]}, {"label": "Neut", "current": "Cao et al (2007), like us, used a 300GB collection of web documents as input.", "context": ["It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by.", "Cao et al (2007), like us, used a 300GB collection of web documents as input.", "They used supervised learning to build models that deal with phonetic transliterations and semantic translations separately."], "vector_1": {"semant": 1, "lexicon": 1, "deal": 1, "hard": 1, "al": 1, "phonet": 1, "gb": 1, "largescal": 1, "et": 1, "web": 1, "use": 2, "build": 2, "way": 1, "input": 1, "document": 1, "difficult": 1, "supervis": 1, "may": 1, "learn": 1, "translat": 2, "transliter": 1, "come": 1, "truli": 1, "term": 1, "like": 1, "us": 1, "separ": 1, "collect": 1, "cao": 1, "english": 1, "model": 1}, "marker": "(2007)", "article": "P08-1113", "vector_2": [1, 0.9546346160704733, 1, 6, 0, 0]}, {"label": "Neut", "current": "The use of name-gender associations are problematic when non-English content is considered because databases of anglophone name-gender associations are no longer useful (Mislove et al., 2011).", "context": ["(Rao et al., 2010; Pennacchiotti and Popescu, 2011; Zamal et al., 2012), the dominant way of obtaining datasets consisting of Twitter users with highconfidence gender-labels is to use gender-name associations.", "The use of name-gender associations are problematic when non-English content is considered because databases of anglophone name-gender associations are no longer useful (Mislove et al., 2011).", "We instead used Amazon Mechanical Turk workers to identify the gender of the person shown in the profile picture associated with a user's account (Liu and Ruths, 2013)."], "vector_1": {"identifi": 1, "twitter": 1, "obtain": 1, "dataset": 1, "nonenglish": 1, "namegend": 2, "profil": 1, "use": 4, "pictur": 1, "databas": 1, "content": 1, "way": 1, "instead": 1, "anglophon": 1, "worker": 1, "shown": 1, "amazon": 1, "mechan": 1, "user": 2, "consid": 1, "associ": 4, "account": 1, "domin": 1, "longer": 1, "consist": 1, "highconfid": 1, "genderlabel": 1, "person": 1, "turk": 1, "gender": 1, "gendernam": 1, "problemat": 1}, "marker": "(Mislove et al., 2011)", "article": "D13-1114", "vector_2": [2, 0.2934164330620799, 5, 1, 0, 0]}, {"label": "Neut", "current": "Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985).", "context": ["(Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell.", "Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985).", "To our knowledge, the FraCaS results reported here represent the first such evaluation."], "vector_1": {"represent": 1, "evalu": 1, "natur": 1, "parser": 1, "repres": 1, "result": 1, "close": 1, "languag": 1, "fraca": 1, "describ": 2, "haskel": 1, "categori": 1, "also": 1, "advoc": 1, "draft": 1, "jerri": 1, "knowledg": 1, "preliminari": 1, "fragment": 1, "unpublish": 1, "base": 1, "report": 1, "grammar": 1, "infer": 1, "hobb": 1, "english": 1, "small": 1, "implement": 2, "first": 1, "prolog": 1}, "marker": "(Hobbs, 1985)", "article": "W07-1431", "vector_2": [22, 0.955458784803182, 3, 1, 0, 0]}, {"label": "Pos", "current": "The approach we took is the one in (Ratnaparkhi, 1996), which uses xi(W, T i1 1 ) = {wi, wi1, wi+1, ti1, ti2}.", "context": ["model is built.", "The approach we took is the one in (Ratnaparkhi, 1996), which uses xi(W, T i1 1 ) = {wi, wi1, wi+1, ti1, ti2}.", "We note that the probability model is causal in the sequencing of tags (the probability assignment for ti only depends on previous tags ti1, ti2) which allows for efficient algorithms that search for the most likely tag sequence T(W) = arg maxT P(T |W) as well as ensures a properly normalized conditional probability model P(T|W)."], "vector_1": {"effici": 1, "one": 1, "ptw": 1, "tag": 3, "arg": 1, "ensur": 1, "properli": 1, "causal": 1, "probabl": 3, "use": 1, "depend": 1, "built": 1, "pt": 1, "tw": 1, "maxt": 1, "note": 1, "ti": 5, "condit": 1, "approach": 1, "previou": 1, "w": 1, "normal": 1, "sequenc": 2, "xiw": 1, "took": 1, "wi": 3, "search": 1, "like": 1, "algorithm": 1, "i": 1, "well": 1, "allow": 1, "model": 3, "assign": 1}, "marker": "(Ratnaparkhi, 1996)", "article": "W04-3237", "vector_2": [8, 0.4020992958058373, 1, 3, 0, 0]}, {"label": "Neut", "current": "Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).", "context": ["The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection.", "Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).", "However, we may not have a big background collection that matches the test domain for a reliable IDF estimate."], "vector_1": {"domain": 1, "identifi": 1, "eg": 1, "idea": 1, "extract": 1, "occur": 1, "shown": 1, "appear": 1, "much": 1, "test": 1, "document": 2, "match": 1, "may": 1, "big": 1, "journal": 1, "effect": 1, "entir": 1, "background": 1, "word": 1, "keyword": 1, "scientif": 1, "howev": 1, "work": 1, "tfidf": 1, "collect": 2, "estim": 1, "idf": 1, "reliabl": 1, "frequent": 2}, "marker": "(Frank et al., 1999", "article": "N09-1070", "vector_2": [10, 0.12530221462879984, 3, 2, 0, 0]}, {"label": "Neut", "current": "An alternative approach that we have also explored is to utilize a similar bootstrapping approach with data partially-annotated for grammatical relations (Watson and Briscoe, 2007).", "context": ["We leave for the future a more extensive investigation of these cases which, in principle, would allow us to make more use of this training data.", "An alternative approach that we have also explored is to utilize a similar bootstrapping approach with data partially-annotated for grammatical relations (Watson and Briscoe, 2007).", "5.1 Confidence-Based Approaches"], "vector_1": {"explor": 1, "partiallyannot": 1, "use": 1, "would": 1, "make": 1, "also": 1, "confidencebas": 1, "leav": 1, "futur": 1, "approach": 3, "altern": 1, "investig": 1, "relat": 1, "util": 1, "extens": 1, "train": 1, "data": 2, "case": 1, "grammat": 1, "bootstrap": 1, "us": 1, "allow": 1, "principl": 1, "similar": 1}, "marker": "(Watson and Briscoe, 2007)", "article": "W07-2203", "vector_2": [0, 0.6065189230823093, 1, 1, 3, 1]}, {"label": "Neut", "current": "These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models.", "context": ["Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting.", "These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models.", "Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (>1K) while the confidence-based methods we explore have a total processing overhead equivalent to one iteration of an IOA-based EM algorithm."], "vector_1": {"em": 2, "set": 1, "partofspeech": 1, "process": 1, "one": 1, "tagger": 1, "result": 1, "explor": 2, "total": 1, "use": 1, "techniqu": 1, "equival": 1, "semisupervis": 1, "perform": 1, "neg": 1, "confidencebas": 1, "iter": 1, "baumwelch": 1, "necessarili": 1, "larg": 1, "merialdo": 1, "overhead": 1, "method": 2, "contrast": 1, "altern": 1, "deriv": 1, "deploy": 1, "motiv": 1, "nonit": 1, "hmm": 1, "schabe": 1, "train": 1, "somewhat": 1, "ioabas": 1, "data": 2, "demonstr": 1, "anoth": 1, "algorithm": 1, "fairli": 1, "unsupervis": 1, "space": 1, "requir": 1, "elworthi": 1, "yield": 1, "remain": 1, "partiallybracket": 1, "determin": 1, "suggest": 1, "improv": 1, "model": 1, "pereira": 1, "k": 1}, "marker": "(1992)", "article": "W07-2203", "vector_2": [15, 0.16118755032734658, 3, 3, 0, 0]}, {"label": "Neut", "current": "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).", "context": ["3.1 Internal representation", "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).", "There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures."], "vector_1": {"represent": 1, "tei": 1, "among": 1, "intern": 1, "facil": 1, "shortcom": 1, "develop": 1, "spatiotempor": 1, "graph": 1, "xce": 1, "data": 1, "complex": 1, "speech": 1, "take": 1, "nite": 1, "approach": 3, "difficult": 1, "express": 1, "variou": 1, "format": 1, "acronym": 1, "object": 1, "fiesta": 1, "standard": 1, "extens": 1, "multimod": 1, "transcript": 1, "account": 1, "made": 1, "specif": 1, "annot": 2, "structur": 1, "p": 1, "model": 1}, "marker": "(Evert et al., 2003)", "article": "W13-5507", "vector_2": [10, 0.3696104559891935, 4, 1, 0, 0]}, {"label": "Neut", "current": "based on the SVM-based algorithm proposed for English by Pradhan et al (2003).", "context": ["Email: jurafsky@stanford.edu.", "based on the SVM-based algorithm proposed for English by Pradhan et al (2003).", "We first describe our creation of a small 1100-sentence Chinese corpus labeled according to principles from the English and (in-progress) Chinese PropBanks."], "vector_1": {"corpu": 1, "creation": 1, "al": 1, "propos": 1, "et": 1, "jurafskystanfordedu": 1, "describ": 1, "label": 1, "svmbase": 1, "email": 1, "propbank": 1, "accord": 1, "sentenc": 1, "chines": 2, "base": 1, "pradhan": 1, "inprogress": 1, "algorithm": 1, "english": 2, "small": 1, "principl": 1, "first": 1}, "marker": "(2003)", "article": "N04-1032", "vector_2": [1, 0.061563345877071365, 1, 3, 0, 0]}, {"label": "Pos", "current": "In order to express both established and new data categories and properties, from linguistics as well as from nonlinguistic communication, we developed a new data category registry, which contains links to other resources in the LLOD cloud, in particular to the ISOcat data category repository (Windhouwer and Wright, 2012), but also serves as a place where categories from novel research fields (mainly multimodal communication) can be collected, discussed, until they have settled down and are stable enough for an integration into more authoritative category registries, such as ISOcat.", "context": ["Due to the challenging nature of the data, in particular that it contains annotations on multiple timelines, we developed a new model for the representation of this data, which we call FiESTA.", "In order to express both established and new data categories and properties, from linguistics as well as from nonlinguistic communication, we developed a new data category registry, which contains links to other resources in the LLOD cloud, in particular to the ISOcat data category repository (Windhouwer and Wright, 2012), but also serves as a place where categories from novel research fields (mainly multimodal communication) can be collected, discussed, until they have settled down and are stable enough for an integration into more authoritative category registries, such as ISOcat.", "By means of this we aim to make the re"], "vector_1": {"represent": 1, "natur": 1, "challeng": 1, "timelin": 1, "mainli": 1, "multimod": 1, "stabl": 1, "establish": 1, "cloud": 1, "authorit": 1, "multipl": 1, "commun": 2, "nonlinguist": 1, "make": 1, "isocat": 2, "due": 1, "research": 1, "categori": 5, "also": 1, "enough": 1, "call": 1, "settl": 1, "new": 3, "field": 1, "resourc": 1, "registri": 2, "express": 1, "fiesta": 1, "link": 1, "repositori": 1, "particular": 2, "develop": 2, "data": 5, "discuss": 1, "serv": 1, "novel": 1, "llod": 1, "properti": 1, "well": 1, "annot": 1, "aim": 1, "collect": 1, "place": 1, "integr": 1, "contain": 2, "model": 1, "linguist": 1, "order": 1, "mean": 1}, "marker": "(Windhouwer and Wright, 2012)", "article": "W13-5507", "vector_2": [1, 0.08071994450731992, 1, 2, 0, 0]}, {"label": "Neut", "current": "Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level.", "context": ["Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators - average sentence length and type-token ratio - obtaining good results for her target group.", "Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level.", "He created a tailored C-test that only contains selected gaps in order to better discriminate between the students."], "vector_1": {"calibr": 1, "creat": 1, "classic": 1, "pool": 1, "kleinbraley": 1, "obtain": 1, "indic": 1, "result": 1, "select": 1, "use": 1, "ratio": 1, "compar": 1, "discrimin": 1, "perform": 2, "two": 1, "better": 1, "averag": 1, "test": 1, "build": 1, "analysi": 2, "difficulti": 2, "good": 1, "intend": 1, "linear": 1, "sentenc": 1, "eck": 1, "differ": 1, "rasch": 1, "gap": 2, "tailor": 1, "student": 1, "group": 1, "ctest": 3, "target": 1, "typetoken": 1, "level": 1, "kamimoto": 1, "item": 1, "length": 1, "contain": 1, "regress": 1, "model": 1, "order": 2, "first": 1}, "marker": "(1993)", "article": "Q14-1040", "vector_2": [21, 0.17166096756570828, 3, 1, 0, 0]}, {"label": "Pos", "current": "(Yannakoudakis et al., 2011))-we can determine their relative importance and usefulness.", "context": ["e.g.", "(Yannakoudakis et al., 2011))-we can determine their relative importance and usefulness.", "We run phase 2 (k = 1) using different combinations of phase 1 classifiers (1-best) as input."], "vector_1": {"use": 2, "run": 1, "determin": 1, "eg": 1, "differ": 1, "classifi": 1, "combin": 1, "rel": 1, "input": 1, "phase": 2, "import": 1, "k": 1, "best": 1}, "marker": "(Yannakoudakis et al., 2011)", "article": "W12-2011", "vector_2": [1, 0.9176943454827806, 1, 3, 1, 0]}, {"label": "Neut", "current": "In order to compare our method with results reported by Gabrilovich and Markovitch (2007) and Witten and Milne (2008), we followed the same approach by randomly selecting Algorithm 1 Pseudo code to spread activation depth-first from node vi up to level Lp,max, using global decay d, and threshold T, given an adjacency list graph structure G and a weighting scheme W such that 0 < wig E W < 1.", "context": ["Experimental Method", "In order to compare our method with results reported by Gabrilovich and Markovitch (2007) and Witten and Milne (2008), we followed the same approach by randomly selecting Algorithm 1 Pseudo code to spread activation depth-first from node vi up to level Lp,max, using global decay d, and threshold T, given an adjacency list graph structure G and a weighting scheme W such that 0 < wig E W < 1.", "Require: G, Lp,-ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai < T then > Threshold return end if Add vi to P > To avoid cycles for vj E N(vi) do > Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ < Lp,-ax then aj =aj+ai*wij*d Replace (vj, aj) E A with (vj, aj ) SPREAD UNIDIR(vj, A, P) end if end for return end function"], "vector_1": {"randomli": 1, "wig": 1, "code": 1, "weight": 1, "ai": 2, "vi": 3, "vj": 5, "gabrilovich": 1, "threshold": 2, "list": 1, "jpj": 1, "result": 1, "miln": 1, "follow": 1, "select": 1, "spread": 3, "use": 1, "end": 5, "compar": 1, "decay": 1, "function": 2, "graph": 1, "avoid": 1, "lpmax": 1, "activ": 1, "add": 1, "depthfirst": 1, "adjac": 1, "experiment": 1, "nvi": 1, "replac": 1, "unidirvi": 1, "scheme": 1, "approach": 1, "method": 2, "node": 1, "process": 1, "global": 1, "return": 2, "lpax": 2, "aj": 5, "given": 1, "neighbour": 1, "cycl": 1, "witten": 1, "report": 1, "requir": 1, "unidirvj": 1, "e": 6, "g": 2, "algorithm": 1, "level": 1, "pseudo": 1, "structur": 1, "p": 4, "w": 2, "ajaiwijd": 1, "markovitch": 1, "order": 1}, "marker": "(2007)", "article": "W10-3506", "vector_2": [3, 0.5819706182257638, 2, 6, 1, 0]}, {"label": "Neut", "current": "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "context": ["Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).", "In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.", "In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training."], "vector_1": {"corpu": 1, "satisfactori": 1, "result": 2, "year": 1, "largescal": 1, "intermedi": 1, "research": 1, "build": 1, "method": 1, "machin": 1, "train": 1, "link": 1, "translat": 1, "associ": 1, "requir": 1, "recent": 1, "measur": 1, "word": 1, "align": 2, "employ": 1, "achiev": 1, "statist": 2, "bilingu": 1, "mani": 1, "model": 1, "propos": 1, "order": 1, "first": 1}, "marker": "(Wu, 1997", "article": "P05-1058", "vector_2": [8, 0.039255958147645806, 7, 1, 0, 0]}, {"label": "Weak", "current": "Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed.", "context": ["verbs become leaves instead of governing the sentences.", "Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed.", "The main contribution of this paper is a considerable improvement of unsupervised parsing quality by estimating the Pstop probabilities externally using a very large corpus, and employing this prior knowledge in the standard inference of DMV."], "vector_1": {"corpu": 1, "consider": 1, "hockenmai": 1, "verbocentr": 1, "knowledg": 1, "qualiti": 1, "tag": 2, "probabl": 1, "use": 1, "render": 1, "mark": 1, "identif": 1, "estim": 1, "leav": 1, "useless": 1, "faili": 1, "instead": 1, "main": 1, "boost": 1, "po": 2, "contribut": 1, "sentenc": 1, "standard": 1, "rasooli": 1, "verb": 2, "pars": 1, "govern": 1, "extern": 1, "becom": 1, "effort": 1, "pstop": 1, "requir": 1, "made": 1, "larg": 1, "approach": 1, "howev": 1, "manual": 1, "structur": 1, "infer": 2, "employ": 2, "prior": 1, "paper": 1, "bisk": 1, "dmv": 1, "improv": 1, "unsupervis": 2}, "marker": "(2012)", "article": "P13-1028", "vector_2": [1, 0.07993588192596177, 2, 3, 0, 0]}, {"label": "Neut", "current": "This program is a graphical user interface to insert (or correct) alignments between pairs of syntax trees.4 The TreeAligner can be seen in the line of tools such as I*Link (Ahrenberg et al., 2002) or Cairo (Smith and Jahr, 2000) but it is especially tailored to visualize and align full syntax trees.", "context": ["For this purpose we have developed a \"TreeAligner\".", "This program is a graphical user interface to insert (or correct) alignments between pairs of syntax trees.4 The TreeAligner can be seen in the line of tools such as I*Link (Ahrenberg et al., 2002) or Cairo (Smith and Jahr, 2000) but it is especially tailored to visualize and align full syntax trees.", "The TreeAligner requires three input files."], "vector_1": {"cairo": 1, "syntax": 2, "ilink": 1, "file": 1, "seen": 1, "develop": 1, "three": 1, "program": 1, "interfac": 1, "input": 1, "correct": 1, "full": 1, "especi": 1, "tool": 1, "visual": 1, "tailor": 1, "user": 1, "pair": 1, "line": 1, "requir": 1, "insert": 1, "graphic": 1, "align": 2, "tree": 2, "purpos": 1, "treealign": 3}, "marker": "(Ahrenberg et al., 2002)", "article": "W06-2717", "vector_2": [4, 0.6602886597938145, 2, 1, 0, 0]}, {"label": "Neut", "current": "The grammar we used is extracted from the publicly available parser of Petrov et al (2006).", "context": ["The detailed specifications of the experimental platforms are listed in Table 1.", "The grammar we used is extracted from the publicly available parser of Petrov et al (2006).", "It has 1,120 nonterminal symbols including 636 preterminal symbols."], "vector_1": {"20": 1, "use": 1, "grammar": 1, "pretermin": 1, "specif": 1, "symbol": 2, "parser": 1, "list": 1, "detail": 1, "al": 1, "avail": 1, "platform": 1, "petrov": 1, "experiment": 1, "includ": 1, "publicli": 1, "tabl": 1, "et": 1, "nontermin": 1, "extract": 1}, "marker": "(2006)", "article": "W11-2921", "vector_2": [5, 0.725254587816435, 1, 4, 4, 1]}, {"label": "Neut", "current": "most related method is that of Kita (1999).", "context": ["For instance, a shared task on native language identification took place at an NAACL-HLT 2013 workshop.", "most related method is that of Kita (1999).", "In his method, a variety of languages are modeled by their spelling systems (i.e., character-based n-gram language models)."], "vector_1": {"kita": 1, "spell": 1, "task": 1, "workshop": 1, "relat": 1, "share": 1, "took": 1, "system": 1, "identif": 1, "nativ": 1, "ngram": 1, "instanc": 1, "place": 1, "varieti": 1, "naaclhlt": 1, "characterbas": 1, "model": 2, "ie": 1, "method": 2, "languag": 3}, "marker": "(1999)", "article": "P13-1112", "vector_2": [14, 0.1734396564944027, 1, 3, 0, 0]}, {"label": "CoCo", "current": "In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al (2007).", "context": ["Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system (Marino et al., 2006).", "In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al (2007).", "We present a summary of the TALP-UPC Ngrambased SMT system used for this shared task."], "vector_1": {"campaign": 1, "ngrambas": 3, "share": 1, "al": 1, "past": 1, "year": 1, "et": 1, "shown": 1, "group": 1, "evalu": 1, "compar": 1, "prove": 1, "smt": 3, "system": 3, "approach": 1, "statist": 1, "machin": 1, "koehn": 1, "previou": 1, "use": 1, "translat": 1, "develop": 1, "present": 1, "task": 1, "callisonburch": 1, "phrasebas": 1, "talpupc": 2, "stateoftheart": 1, "monz": 1, "summari": 1}, "marker": "(2007)", "article": "W08-0315", "vector_2": [1, 0.09016393442622951, 3, 1, 1, 0]}, {"label": "CoCo", "current": "Since the governing category information is encoded in the path feature, it may be redundant; indeed this redundancy might explain why the governing category feature was used in Gildea & Jurafsky(2002) but not in Gildea and Palmer(2002).", "context": ["\"the international Olympic Conference held in Paris\" Figure 1 Example of DE construction", "Since the governing category information is encoded in the path feature, it may be redundant; indeed this redundancy might explain why the governing category feature was used in Gildea & Jurafsky(2002) but not in Gildea and Palmer(2002).", "Since the \"DE\" construction caused us to modify the feature for Chinese, we conducted several experiments to test whether the governing category feature is useful or whether it is redundant with the path and position features."], "vector_1": {"featur": 5, "encod": 1, "held": 1, "intern": 1, "sever": 1, "caus": 1, "use": 2, "gildea": 2, "modifi": 1, "explain": 1, "construct": 2, "categori": 3, "figur": 1, "conduct": 1, "test": 1, "experi": 1, "might": 1, "palmer": 1, "may": 1, "chines": 1, "de": 2, "confer": 1, "pari": 1, "govern": 3, "path": 2, "redund": 3, "olymp": 1, "sinc": 2, "jurafski": 1, "whether": 2, "us": 1, "inde": 1, "inform": 1, "exampl": 1, "posit": 1}, "marker": "(2002)", "article": "N04-1032", "vector_2": [2, 0.34782465174622035, 2, 4, 0, 0]}, {"label": "Neut", "current": "(Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell.", "context": ["There has been surprisingly little work on building computational models of natural logic.", "(Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell.", "Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985)."], "vector_1": {"represent": 1, "comput": 1, "natur": 2, "parser": 1, "haskel": 1, "close": 1, "surprisingli": 1, "languag": 1, "describ": 2, "littl": 1, "categori": 1, "also": 1, "draft": 1, "build": 1, "advoc": 1, "infer": 1, "preliminari": 1, "fragment": 1, "logic": 1, "unpublish": 1, "base": 1, "model": 1, "grammar": 1, "work": 1, "jerri": 1, "hobb": 1, "english": 1, "small": 1, "implement": 2, "prolog": 1}, "marker": "(Fyodorov et al., 2003)", "article": "W07-1431", "vector_2": [4, 0.9536071869428062, 3, 1, 1, 0]}, {"label": "Neut", "current": "iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).", "context": ["(Left: German original, right: English translation.)", "iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).", "A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005)."], "vector_1": {"origin": 1, "genpex": 1, "use": 1, "right": 1, "principl": 1, "german": 1, "similar": 1, "thu": 1, "system": 1, "sophist": 1, "text": 1, "translat": 1, "allow": 1, "english": 1, "modelcr": 1, "variat": 1, "linguist": 1, "approach": 1, "left": 1}, "marker": "Holling et al., 2009)", "article": "W11-1403", "vector_2": [2, 0.1438600596735506, 5, 5, 1, 0]}, {"label": "Neut", "current": "NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining for several languages in (Saravanan and Kumaran, 2007).", "context": ["In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences.", "NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining for several languages in (Saravanan and Kumaran, 2007).", "However, such methods miss vast majority of the NETEs due to their dependency on frequency signatures."], "vector_1": {"major": 1, "nete": 4, "mine": 2, "discoveri": 1, "miss": 1, "sever": 1, "even": 1, "use": 1, "depend": 1, "compar": 1, "due": 1, "articl": 1, "languag": 1, "approach": 1, "method": 1, "contrast": 1, "signatur": 1, "nearli": 1, "extend": 1, "may": 1, "sentenc": 1, "transliter": 1, "pair": 1, "parallel": 2, "vast": 1, "frequenc": 1, "howev": 1, "corpora": 1, "time": 1, "model": 1, "seri": 1, "propos": 1}, "marker": "(Klementiev and Roth, 2006)", "article": "E09-1091", "vector_2": [3, 0.9281508481704752, 2, 5, 0, 0]}, {"label": "Neut", "current": "The observation that standard logical entailment and textual entailment deviate in certain respects is not surprising and has also been addressed in a discussion initiated by (Zaenen et al., 2005).", "context": ["Although the hypothesis is logically entailed by the text (if we ignore the report context) - 'kill' implies 'possibly kill' - pragmatic principles seem to block entailment here.", "The observation that standard logical entailment and textual entailment deviate in certain respects is not surprising and has also been addressed in a discussion initiated by (Zaenen et al., 2005).", "Still, there is no consensus regarding the precise mechanisms involved in the latter such as \"general principles of plausibility\" or pragmatic principles."], "vector_1": {"impli": 1, "pragmat": 2, "entail": 4, "text": 1, "kill": 2, "respect": 1, "seem": 1, "surpris": 1, "involv": 1, "still": 1, "consensu": 1, "latter": 1, "textual": 1, "also": 1, "deviat": 1, "certain": 1, "hypothesi": 1, "gener": 1, "regard": 1, "initi": 1, "standard": 1, "mechan": 1, "although": 1, "address": 1, "report": 1, "plausibl": 1, "discuss": 1, "possibl": 1, "precis": 1, "ignor": 1, "context": 1, "logic": 2, "principl": 3, "observ": 1, "block": 1}, "marker": "(Zaenen et al., 2005)", "article": "W07-1402", "vector_2": [2, 0.8766053362688053, 1, 1, 1, 0]}, {"label": "Neut", "current": "Berland and Charniak (1999) used a pattern-based approach to find out part-whole relationships (such as between car and door, or car and engine) in a text.", "context": ["She described a syntagmatic technique for identifying hyponymy relations in free text by using frequently occurring patterns like 'NP0 such as NP1, NP2, ...,NPn'.", "Berland and Charniak (1999) used a pattern-based approach to find out part-whole relationships (such as between car and door, or car and engine) in a text.", "Heuristic approaches rely on language-specific rules which cannot be transferred from one language to another."], "vector_1": {"identifi": 1, "berland": 1, "text": 2, "heurist": 1, "one": 1, "charniak": 1, "partwhol": 1, "find": 1, "languag": 1, "use": 2, "engin": 1, "describ": 1, "pattern": 1, "reli": 1, "occur": 1, "patternbas": 1, "np": 3, "approach": 2, "npn": 1, "door": 1, "relationship": 1, "relat": 1, "free": 1, "hyponymi": 1, "cannot": 1, "languagespecif": 1, "techniqu": 1, "anoth": 1, "like": 1, "car": 2, "rule": 1, "transfer": 1, "syntagmat": 1, "frequent": 1}, "marker": "(1999)", "article": "W12-5209", "vector_2": [13, 0.30068481325890545, 1, 3, 2, 0]}, {"label": "CoCo", "current": "We show that the performance of our approach (using simple lexical features) is comparable to that of the state-of-art statistical MT system (Koehn et al., 2007).", "context": ["We used maximum entropy model in our experiments.", "We show that the performance of our approach (using simple lexical features) is comparable to that of the state-of-art statistical MT system (Koehn et al., 2007).", "When additional syntactic features (POS tags in this paper) are used, there is a boost in the performance which is likely to improve when richer syntactic features are incorporated in the model."], "vector_1": {"featur": 3, "entropi": 1, "show": 1, "richer": 1, "paper": 1, "syntact": 2, "use": 3, "compar": 1, "perform": 2, "system": 1, "approach": 1, "experi": 1, "simpl": 1, "po": 1, "stateofart": 1, "lexic": 1, "addit": 1, "like": 1, "boost": 1, "maximum": 1, "mt": 1, "tag": 1, "statist": 1, "incorpor": 1, "improv": 1, "model": 2}, "marker": "(Koehn et al., 2007)", "article": "W10-3805", "vector_2": [3, 0.04348341700596909, 1, 5, 1, 0]}, {"label": "Neut", "current": "Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).", "context": ["It is important to ensure the quality of the first selection for the subsequent addition of keywords.", "Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).", "Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003)."], "vector_1": {"classif": 1, "qualiti": 1, "solv": 1, "ensur": 1, "extract": 1, "pmi": 2, "select": 2, "supervis": 1, "use": 2, "top": 1, "research": 1, "also": 2, "score": 2, "treat": 1, "import": 1, "subsequ": 1, "approach": 1, "final": 1, "machin": 1, "candid": 1, "pair": 1, "highest": 1, "averag": 1, "addit": 1, "task": 1, "word": 1, "keyword": 4, "k": 1, "learn": 1, "first": 1}, "marker": "(Inkpen and Desilets, 2004)", "article": "N09-1070", "vector_2": [5, 0.15499177976209663, 6, 1, 1, 0]}, {"label": "Pos", "current": "We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data.", "context": ["Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion.", "We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data.", "We evaluate our model on data from the 4FORUMS online debate site (Walker et al., 2012b)."], "vector_1": {"classif": 1, "evalu": 1, "predict": 1, "site": 1, "convey": 1, "onlin": 1, "rebutt": 1, "languag": 1, "leverag": 1, "use": 1, "develop": 1, "sentiment": 1, "repli": 1, "debat": 1, "approach": 2, "infer": 1, "introduc": 1, "model": 1, "psl": 1, "tool": 1, "relat": 1, "agreement": 1, "probabilist": 1, "link": 1, "post": 2, "data": 2, "discuss": 1, "recent": 1, "forum": 1, "consist": 1, "collect": 2, "stanc": 1, "logic": 1, "implement": 1, "soft": 1}, "marker": "(Bach et al., 2013)", "article": "W14-2715", "vector_2": [1, 0.2222222222222222, 2, 3, 3, 0]}, {"label": "Pos", "current": "Unlike Christiansen et al (1998), Yang (2004), and Doyle and Levy (2013), we follow Lignos and Yang (2010) in making the more realistic assumption that the 94 mono-syllabic function words listed by Selkirk (1984) never surface with lexical stress.", "context": ["Our version of the Korman corpus contains, in total, 11413 utterances.", "Unlike Christiansen et al (1998), Yang (2004), and Doyle and Levy (2013), we follow Lignos and Yang (2010) in making the more realistic assumption that the 94 mono-syllabic function words listed by Selkirk (1984) never surface with lexical stress.", "As function words account for roughly 50% of the tokens but only roughly 5% of the types in our corpora, this means that the type and token distribution of stress patterns differs dramatically in all our corpora, as can be seen from Table 2."], "vector_1": {"corpu": 1, "function": 2, "tabl": 1, "al": 1, "seen": 1, "unlik": 1, "differ": 1, "follow": 1, "et": 1, "total": 1, "korman": 1, "pattern": 1, "make": 1, "version": 1, "doyl": 1, "type": 2, "ligno": 1, "assumpt": 1, "distribut": 1, "never": 1, "surfac": 1, "lexic": 1, "levi": 1, "utter": 1, "account": 1, "stress": 2, "word": 2, "realist": 1, "christiansen": 1, "corpora": 2, "list": 1, "dramat": 1, "token": 2, "yang": 2, "contain": 1, "selkirk": 1, "mean": 1, "roughli": 2, "monosyllab": 1}, "marker": "(1998)", "article": "Q14-1008", "vector_2": [16, 0.4789645719105862, 5, 4, 1, 0]}, {"label": "Neut", "current": "Then, we applied a series of rules governing the length of initial and subsequent REs involving a person's name (following Nenkova and McKeown (2003)), as well as 'backoffs' if the predicted type or case were not available.", "context": ["For 2010, we incorporated the case of each RE into our target attribute so that the decision tree classifier would predict both the type and case for the given reference.", "Then, we applied a series of rules governing the length of initial and subsequent REs involving a person's name (following Nenkova and McKeown (2003)), as well as 'backoffs' if the predicted type or case were not available.", "Another improvement we made involved our method of determining whether the use of a pronoun would introduce ambiguity in a given context."], "vector_1": {"context": 1, "pronoun": 1, "predict": 2, "appli": 1, "backoff": 1, "follow": 1, "well": 1, "involv": 2, "given": 2, "would": 2, "anoth": 1, "classifi": 1, "avail": 1, "re": 1, "decis": 1, "subsequ": 1, "type": 2, "method": 1, "introduc": 1, "refer": 1, "nenkova": 1, "determin": 1, "use": 1, "initi": 1, "govern": 1, "name": 1, "case": 3, "made": 1, "target": 1, "whether": 1, "tree": 1, "rule": 1, "ambigu": 1, "person": 1, "length": 1, "incorpor": 1, "mckeown": 1, "improv": 1, "attribut": 1, "seri": 1}, "marker": "(2003)", "article": "W10-4231", "vector_2": [7, 0.34842200180342653, 1, 1, 0, 0]}, {"label": "Neut", "current": "We assume that the set of all individual phenomena and their quantities (e.g., proportion of phenomena classified as 200-level in phase 1) characterize a learner's level (Hawkins and Buttery, 2010).", "context": ["We aggregate the information from phase 1 classification to classify overall learner levels.", "We assume that the set of all individual phenomena and their quantities (e.g., proportion of phenomena classified as 200-level in phase 1) characterize a learner's level (Hawkins and Buttery, 2010).", "The feature types are given in table 4."], "vector_1": {"classif": 1, "set": 1, "level": 3, "phenomena": 2, "individu": 1, "overal": 1, "learner": 2, "given": 1, "classifi": 2, "quantiti": 1, "inform": 1, "featur": 1, "tabl": 1, "aggreg": 1, "type": 1, "phase": 2, "proport": 1, "assum": 1, "eg": 1, "character": 1}, "marker": "(Hawkins and Buttery, 2010)", "article": "W12-2011", "vector_2": [2, 0.6852410097134679, 1, 2, 1, 0]}, {"label": "Neut", "current": "Current state-of-the-art for the shared-task is achieved by Tsai et al (2006), whose improvement depends on careful design of features including the normalization of numeric expressions, and use of post-processing by automatically extracted patterns.", "context": ["Friedrich et al (2006) used CRFs with features from the external gazetteer.", "Current state-of-the-art for the shared-task is achieved by Tsai et al (2006), whose improvement depends on careful design of features including the normalization of numeric expressions, and use of post-processing by automatically extracted patterns.", "210"], "vector_1": {"gazett": 1, "featur": 2, "al": 2, "automat": 1, "design": 1, "et": 2, "extract": 1, "whose": 1, "use": 2, "depend": 1, "pattern": 1, "current": 1, "friedrich": 1, "includ": 1, "sharedtask": 1, "postprocess": 1, "normal": 1, "numer": 1, "express": 1, "extern": 1, "care": 1, "tsai": 1, "achiev": 1, "crf": 1, "stateoftheart": 1, "improv": 1}, "marker": "(2006)", "article": "W07-1033", "vector_2": [1, 0.2819721430832542, 2, 6, 0, 0]}, {"label": "Pos", "current": "We adopted the Q learning paradigm (Watkins, 1989; Mitchell, 1997) to model our problem as a set of possible states, S, and a set of actions, A, which can be performed to alter the current state.", "context": ["4.1 Q Learning", "We adopted the Q learning paradigm (Watkins, 1989; Mitchell, 1997) to model our problem as a set of possible states, S, and a set of actions, A, which can be performed to alter the current state.", "While in state s E S and performing action a E A, the learner receives a reward r(s, a), and advances to state s0 = (s, a)."], "vector_1": {"current": 1, "set": 2, "e": 2, "possibl": 1, "perform": 2, "s": 1, "adopt": 1, "learner": 1, "rs": 1, "advanc": 1, "alter": 1, "q": 2, "receiv": 1, "state": 4, "learn": 2, "action": 2, "problem": 1, "paradigm": 1, "model": 1, "reward": 1}, "marker": "(Watkins, 1989", "article": "W02-1024", "vector_2": [13, 0.32986426985917544, 2, 1, 0, 0]}, {"label": "Neut", "current": "A matching algorithm selects the most similar from the set of the retrieved TL phrases through a comparison process, which is viewed as an assignment problem, using the Gale-Shapley algorithm (Gale and Shapley, 1962).", "context": ["In the second task, the most similar phrases to the TL structure phrases are retrieved from the monolingual corpus to provide local structural information such as word-reordering.", "A matching algorithm selects the most similar from the set of the retrieved TL phrases through a comparison process, which is viewed as an assignment problem, using the Gale-Shapley algorithm (Gale and Shapley, 1962).", "6."], "vector_1": {"corpu": 1, "set": 1, "process": 1, "second": 1, "monolingu": 1, "phrase": 3, "select": 1, "use": 1, "galeshapley": 1, "tl": 2, "local": 1, "match": 1, "comparison": 1, "task": 1, "retriev": 2, "algorithm": 2, "provid": 1, "structur": 2, "wordreord": 1, "inform": 1, "problem": 1, "similar": 2, "assign": 1, "view": 1}, "marker": "(Gale and Shapley, 1962)", "article": "W12-0108", "vector_2": [50, 0.8374675171273328, 1, 1, 0, 0]}, {"label": "Neut", "current": "The cross-product of the stem name and (open-class) reduced core POS tags, plus the CLOSED tag, yields 24 labels for a CRF classifier in Mallet (McCallum, 2002).", "context": ["The classifier is used only to get solutions for the openclass words, although we wish to give the classifier all the words for the sentence.", "The cross-product of the stem name and (open-class) reduced core POS tags, plus the CLOSED tag, yields 24 labels for a CRF classifier in Mallet (McCallum, 2002).", "4 Experiments and Evaluation"], "vector_1": {"evalu": 1, "give": 1, "tag": 2, "close": 1, "use": 1, "mallet": 1, "solut": 1, "classifi": 3, "label": 1, "openclass": 2, "experi": 1, "po": 1, "core": 1, "plu": 1, "get": 1, "sentenc": 1, "stem": 1, "although": 1, "reduc": 1, "crossproduct": 1, "word": 2, "name": 1, "wish": 1, "yield": 1, "crf": 1}, "marker": "(McCallum, 2002)", "article": "P10-2063", "vector_2": [8, 0.6980322003577818, 1, 1, 0, 0]}, {"label": "Neut", "current": "Another type of feature that does not depend on provenance information is Latent Dirichlet allocation (LDA) (Blei et al., 2003), an unsupervised word-based approach that infers a preset number of latent dimensions in a corpus and represents documents as distributions over those dimensions.", "context": ["3.4 Genre adaptation with LDA", "Another type of feature that does not depend on provenance information is Latent Dirichlet allocation (LDA) (Blei et al., 2003), an unsupervised word-based approach that infers a preset number of latent dimensions in a corpus and represents documents as distributions over those dimensions.", "Despite its recent successes in topic adaptation for SMT, we expect such a bag-of-words approach to be insufficient to model genre accurately."], "vector_1": {"dimens": 2, "corpu": 1, "lda": 2, "featur": 1, "number": 1, "proven": 1, "repres": 1, "topic": 1, "expect": 1, "bagofword": 1, "despit": 1, "accur": 1, "depend": 1, "anoth": 1, "smt": 1, "insuffici": 1, "adapt": 2, "document": 1, "approach": 2, "infer": 1, "wordbas": 1, "distribut": 1, "preset": 1, "latent": 2, "recent": 1, "alloc": 1, "dirichlet": 1, "success": 1, "type": 1, "unsupervis": 1, "genr": 2, "inform": 1, "model": 1}, "marker": "(Blei et al., 2003)", "article": "W15-2518", "vector_2": [12, 0.47737414471292106, 1, 2, 0, 0]}, {"label": "Pos", "current": "For a more detailed discussion of the issue of metrics in evaluation, of the metrics we have used, and of an experiment relating human judgments to these metrics, see (Bangalore et al., 2000).", "context": ["This number is subtracted from and then divided by the length of the sentence, yielding a number between 0 and 1, with 1 the best score.", "For a more detailed discussion of the issue of metrics in evaluation, of the metrics we have used, and of an experiment relating human judgments to these metrics, see (Bangalore et al., 2000).", "String accuracy measures the performance of the entire FERGUS system."], "vector_1": {"evalu": 1, "metric": 3, "number": 2, "system": 1, "see": 1, "human": 1, "best": 1, "use": 1, "divid": 1, "perform": 1, "subtract": 1, "detail": 1, "accuraci": 1, "score": 1, "experi": 1, "string": 1, "sentenc": 1, "relat": 1, "entir": 1, "judgment": 1, "discuss": 1, "measur": 1, "yield": 1, "issu": 1, "length": 1, "fergu": 1}, "marker": "(Bangalore et al., 2000)", "article": "P00-1059", "vector_2": [0, 0.6663825520180496, 1, 2, 0, 0]}, {"label": "Neut", "current": "This led Dahlmann and Adolphs (2007) to study pausing within spoken MWEs.", "context": ["In studies of speech, Erman (2007) notes that a pause can be caused by the cognitive demands of lexical retrieval, and Pawley (1985) notes that pauses are much less acceptable within a lexicalized phrase than within a free expression.", "This led Dahlmann and Adolphs (2007) to study pausing within spoken MWEs.", "We augment the previous findings, though, by investigating how varying cognitive demands affect MWE production."], "vector_1": {"paus": 3, "erman": 1, "less": 1, "spoken": 1, "within": 3, "accept": 1, "pawley": 1, "investig": 1, "phrase": 1, "cognit": 2, "find": 1, "vari": 1, "dahlmann": 1, "note": 2, "much": 1, "speech": 1, "mwe": 2, "product": 1, "previou": 1, "led": 1, "though": 1, "express": 1, "adolph": 1, "free": 1, "lexic": 2, "demand": 2, "affect": 1, "retriev": 1, "augment": 1, "caus": 1, "studi": 2}, "marker": "(2007)", "article": "W15-0914", "vector_2": [8, 0.24336661911554922, 3, 6, 0, 0]}, {"label": "CoCo", "current": "Together with syntactic analysis and transformations similar to those of Chandrasekar et al (1996), they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency from the Oxford Psycholinguistic Database (Quinlan, 1992).", "context": ["The PSET project (Carroll et al., 1998) dealt with simplification of news articles in English for aphasic readers.", "Together with syntactic analysis and transformations similar to those of Chandrasekar et al (1996), they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency from the Oxford Psycholinguistic Database (Quinlan, 1992).", "Therefore, the most frequent of a set of synonyms for every content word of the input text was chosen to appear in its simplified version."], "vector_1": {"analysi": 1, "set": 1, "text": 1, "al": 1, "pset": 1, "et": 1, "extract": 1, "wordnet": 1, "psycholinguist": 1, "employ": 1, "appear": 1, "chosen": 1, "databas": 1, "transform": 1, "articl": 1, "content": 1, "version": 1, "reader": 1, "input": 1, "therefor": 1, "aphas": 1, "simplifi": 1, "everi": 1, "syntact": 1, "word": 1, "dealt": 1, "lexic": 1, "chandrasekar": 1, "togeth": 1, "oxford": 1, "news": 1, "kucerafr": 1, "synonym": 2, "look": 1, "simplif": 2, "frequenc": 1, "project": 1, "base": 1, "english": 1, "similar": 1, "frequent": 1}, "marker": "(1996)", "article": "W12-2202", "vector_2": [16, 0.17276769297629793, 3, 2, 0, 0]}, {"label": "Neut", "current": "The SALSA RTE system is based on three main components: (i) a linguistic analysis of text and hypothesis based primarily on LFG and Frame Semantics (Baker et al., 1998), (ii) the computation of a match graph that encodes the \"semantic overlap\" between text and hypothesis, and (iii) a statistical entailment decision.", "context": ["2.1 Architecture", "The SALSA RTE system is based on three main components: (i) a linguistic analysis of text and hypothesis based primarily on LFG and Frame Semantics (Baker et al., 1998), (ii) the computation of a match graph that encodes the \"semantic overlap\" between text and hypothesis, and (iii) a statistical entailment decision.", "Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 10-15, Prague, June 2007."], "vector_1": {"rte": 1, "lfg": 1, "comput": 1, "entail": 2, "primarili": 1, "text": 2, "frame": 1, "encod": 1, "semant": 2, "system": 1, "ii": 1, "textual": 1, "proceed": 1, "graph": 1, "three": 1, "overlap": 1, "compon": 1, "workshop": 1, "decis": 1, "main": 1, "match": 1, "architectur": 1, "analysi": 1, "hypothesi": 2, "june": 1, "base": 2, "salsa": 1, "iii": 1, "pragu": 1, "statist": 1, "linguist": 1, "page": 1, "paraphras": 1}, "marker": "(Baker et al., 1998)", "article": "W07-1402", "vector_2": [9, 0.15463647324002724, 1, 1, 0, 0]}, {"label": "Neut", "current": "Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.", "context": ["One obvious possibility is the extension of Tax and Duin's method to more than one nearest training neighbor for a more accurate estimate of local density.", "Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.", "Our immediate goal is to use unknown sense detection in combination with WSD, to filter out items that the WSD system cannot handle due to missing senses."], "vector_1": {"nearest": 1, "featur": 1, "detect": 2, "obviou": 1, "tax": 1, "one": 2, "miss": 1, "filter": 1, "use": 1, "accur": 1, "goal": 1, "unknown": 1, "system": 1, "estim": 1, "densiti": 1, "test": 1, "sens": 2, "immedi": 1, "local": 1, "method": 1, "neighbor": 1, "outlier": 1, "handl": 1, "duin": 1, "gener": 1, "sophist": 1, "extens": 1, "train": 1, "task": 1, "word": 1, "wsd": 2, "furthermor": 1, "possibl": 1, "approach": 1, "employ": 1, "item": 1, "vector": 1, "combin": 1, "due": 1, "context": 1, "cannot": 1}, "marker": "(Markou and Singh, 2003a", "article": "N06-1017", "vector_2": [3, 0.9739488997649235, 3, 3, 3, 0]}, {"label": "Neut", "current": "Some works on text simplification use parallel corpora of original and simplified sentences (Petersen & Ostendorf, 2007).", "context": ["tion and regeneration, system that lay particular emphasis on the discourse level aspects of syntactic simplification.", "Some works on text simplification use parallel corpora of original and simplified sentences (Petersen & Ostendorf, 2007).", "There are works where text simplification is treated as a \"translation task within a RBMT (Takao and Sumita."], "vector_1": {"origin": 1, "corpora": 1, "emphasi": 1, "simplifi": 1, "text": 2, "within": 1, "aspect": 1, "use": 1, "system": 1, "treat": 1, "simplif": 3, "takao": 1, "regener": 1, "discours": 1, "syntact": 1, "sentenc": 1, "rbmt": 1, "lay": 1, "particular": 1, "parallel": 1, "sumita": 1, "task": 1, "level": 1, "work": 2, "tion": 1, "translat": 1}, "marker": "(Petersen & Ostendorf, 2007)", "article": "W14-5605", "vector_2": [7, 0.22890434606011373, 1, 1, 0, 0]}, {"label": "Pos", "current": "We managed our experiments with LoonyBin (Clark and Lavie, 2010), an open-source tool for defining, modifying, and running complex experimental pipelines.", "context": ["Decoding was carried out in Joshua (Li et al., 2009), an open-source framework for parsing-based MT.", "We managed our experiments with LoonyBin (Clark and Lavie, 2010), an open-source tool for defining, modifying, and running complex experimental pipelines.", "We describe our system-building process in more detail in Section 2."], "vector_1": {"joshua": 1, "carri": 1, "parsingbas": 1, "opensourc": 2, "complex": 1, "describ": 1, "defin": 1, "loonybin": 1, "section": 1, "detail": 1, "process": 1, "experiment": 1, "decod": 1, "experi": 1, "systembuild": 1, "pipelin": 1, "run": 1, "tool": 1, "framework": 1, "modifi": 1, "mt": 1, "manag": 1}, "marker": "(Clark and Lavie, 2010)", "article": "W11-2143", "vector_2": [1, 0.07497656982193064, 2, 1, 3, 0]}, {"label": "Neut", "current": "It certainly conforms to the principles of good documentation established by current research on technical documentation and on the needs of end-users, e.g., (Carroll, 1994; Hammond, 1994), in that it supplies clear and concise information for the task at hand.", "context": ["Furthermore, we know that Macintosh documentation undergoes thorough local quality control.", "It certainly conforms to the principles of good documentation established by current research on technical documentation and on the needs of end-users, e.g., (Carroll, 1994; Hammond, 1994), in that it supplies clear and concise information for the task at hand.", "Finally, we have been assured by French users of the software that they consider this particular manual to be well written and to bear no unnatural trace of its origins."], "vector_1": {"control": 1, "conform": 1, "assur": 1, "softwar": 1, "well": 1, "eg": 1, "qualiti": 1, "suppli": 1, "need": 1, "establish": 1, "user": 1, "written": 1, "thorough": 1, "research": 1, "current": 1, "concis": 1, "certainli": 1, "bear": 1, "endus": 1, "document": 3, "local": 1, "final": 1, "good": 1, "trace": 1, "french": 1, "hand": 1, "task": 1, "know": 1, "consid": 1, "particular": 1, "technic": 1, "origin": 1, "undergo": 1, "furthermor": 1, "clear": 1, "manual": 1, "unnatur": 1, "inform": 1, "principl": 1, "macintosh": 1}, "marker": "(Carroll, 1994", "article": "P96-1026", "vector_2": [2, 0.27779216295529086, 2, 2, 0, 0]}, {"label": "Neut", "current": "Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).", "context": ["In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.", "Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).", "For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option."], "vector_1": {"advantag": 1, "almost": 1, "kleinbraley": 1, "stabl": 1, "indic": 1, "restrict": 1, "prefix": 1, "correl": 1, "thorough": 1, "empir": 1, "raatz": 1, "follow": 2, "guess": 1, "languag": 1, "automat": 2, "score": 1, "space": 1, "solut": 2, "cloze": 2, "valid": 1, "overcom": 1, "test": 4, "analys": 1, "singl": 1, "altern": 1, "option": 1, "benefici": 1, "regard": 1, "weak": 1, "given": 1, "theori": 1, "ctest": 3, "case": 1, "enabl": 1, "provid": 1, "approach": 1, "reliabl": 1, "properti": 1, "without": 1, "principl": 1, "order": 1, "propos": 1}, "marker": "(Babaii and Ansary, 2001", "article": "Q14-1040", "vector_2": [13, 0.12160330210630678, 4, 3, 0, 0]}, {"label": "Neut", "current": "A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible.", "context": ["Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.", "A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible.", "3 MEMM for Sequence Labeling"], "vector_1": {"domain": 1, "featur": 1, "weight": 1, "tag": 1, "techniqu": 1, "perform": 1, "label": 1, "memm": 1, "adapt": 3, "way": 1, "taken": 1, "new": 1, "approach": 2, "final": 1, "contrast": 1, "deriv": 1, "extend": 1, "unclear": 1, "sequenc": 2, "probabilist": 2, "although": 1, "formal": 1, "sound": 1, "remark": 1, "scenario": 1, "possibl": 1, "rulebas": 2, "maxent": 1, "easili": 1, "crf": 1, "allow": 1, "model": 4, "principl": 1}, "marker": "(Brill, 1994)", "article": "W04-3237", "vector_2": [10, 0.3690597457814784, 2, 2, 0, 0]}, {"label": "Pos", "current": "AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.", "context": ["2 System Architecture", "AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.", "In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions."], "vector_1": {"classif": 1, "stump": 1, "predict": 1, "obtain": 1, "rate": 1, "design": 1, "domain": 1, "highli": 1, "use": 1, "accur": 2, "system": 2, "classifi": 1, "version": 1, "decis": 1, "simpl": 1, "method": 1, "architectur": 1, "may": 1, "gener": 2, "weak": 2, "moder": 1, "algorithm": 1, "hypothes": 1, "work": 1, "adaboostmh": 1, "rule": 1, "combin": 1, "adaboost": 2, "partit": 1, "mani": 1, "confid": 1}, "marker": "(Freund and Schapire, 1997)", "article": "W01-0726", "vector_2": [4, 0.08214741387771125, 2, 1, 0, 0]}, {"label": "Neut", "current": "1We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005).", "context": ["Note that these meetings are research discussions, and that the annotators may not be very familiar with", "1We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005).", "the topics discussed and often had trouble deciding the important sentences or keywords."], "vector_1": {"topic": 2, "we": 1, "previou": 1, "often": 1, "sentenc": 1, "may": 1, "familiar": 1, "use": 1, "annot": 1, "troubl": 1, "research": 1, "note": 1, "decid": 1, "summar": 1, "keyword": 1, "import": 1, "meet": 2, "work": 1, "segment": 1, "discuss": 2, "select": 1}, "marker": "Murray et al., 2005)", "article": "N09-1070", "vector_2": [4, 0.298829824957287, 2, 3, 0, 0]}, {"label": "Pos", "current": "Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.", "context": ["Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).", "Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.", "WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them."], "vector_1": {"wikipedia": 4, "semant": 3, "concept": 1, "comput": 2, "creat": 1, "appli": 1, "ir": 1, "outlink": 1, "repres": 1, "cost": 1, "exist": 1, "overlap": 1, "cognit": 1, "network": 1, "compar": 1, "memori": 1, "much": 1, "activ": 1, "due": 1, "coverag": 2, "articl": 3, "linkbas": 1, "spread": 1, "treat": 2, "esa": 1, "text": 2, "hurdl": 1, "method": 2, "analysi": 1, "knowledg": 1, "variou": 1, "regard": 1, "wordnetbas": 1, "foremost": 1, "knowledgebas": 1, "biggest": 1, "base": 3, "link": 2, "theori": 1, "although": 1, "arguabl": 1, "associ": 1, "model": 1, "earlier": 1, "recent": 1, "measur": 1, "success": 1, "level": 1, "outperform": 1, "larger": 1, "explicit": 1, "equal": 1, "conceptu": 2, "adequ": 1, "wlm": 2, "found": 1, "sa": 1, "similar": 1, "anchor": 1}, "marker": "(Gabrilovich and Markovitch, 2007)", "article": "W10-3506", "vector_2": [3, 0.11926432267079573, 6, 6, 1, 0]}, {"label": "Neut", "current": "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).", "context": ["Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of \"normality\" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).", "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).", "One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set."], "vector_1": {"set": 4, "one": 1, "densiti": 1, "around": 1, "detect": 5, "network": 1, "compar": 1, "boundari": 1, "normal": 1, "classifi": 1, "write": 1, "estim": 1, "includ": 1, "deciph": 1, "test": 1, "new": 1, "probabl": 2, "approach": 2, "belong": 1, "applic": 1, "accord": 1, "deriv": 2, "outlier": 4, "object": 2, "hand": 1, "intrus": 1, "train": 2, "standard": 1, "model": 1, "fault": 1, "typic": 1, "nonoutli": 1}, "marker": "(Yeung and Chow, 2002", "article": "N06-1017", "vector_2": [4, 0.4826390226983699, 8, 1, 0, 0]}, {"label": "Neut", "current": "One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).", "context": ["Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.", "One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).", "Our phrasal semantic relatedness approach is inspired from those methods."], "vector_1": {"semant": 6, "featur": 1, "entail": 1, "knowledg": 1, "direct": 1, "related": 4, "one": 1, "set": 1, "path": 1, "wordnet": 1, "use": 2, "group": 1, "network": 2, "reli": 1, "call": 1, "includ": 2, "synset": 2, "approach": 3, "method": 1, "inspir": 1, "knowledgebas": 1, "relat": 1, "hyponymi": 1, "base": 1, "link": 1, "synonym": 1, "measur": 2, "word": 1, "princeton": 1, "type": 1, "meronymi": 1, "structur": 1, "depth": 1, "hypernymi": 1, "phrasal": 1, "frequent": 1}, "marker": "Wu and Palmer, 1994)", "article": "S13-2019", "vector_2": [19, 0.14110836398721582, 5, 1, 0, 0]}, {"label": "Neut", "current": "Initial work in recognizing and indexing abstract configurations of planning relations is discussed in Dolan and Dyer (1985, 1986).", "context": ["We have left the automatic creation of this taxonomy from advisor experiences to future research.", "Initial work in recognizing and indexing abstract configurations of planning relations is discussed in Dolan and Dyer (1985, 1986).", "9.3 OTHER CLASSES OF MISCONCEPTIONS"], "vector_1": {"class": 1, "index": 1, "work": 1, "recogn": 1, "relat": 1, "dolan": 1, "misconcept": 1, "abstract": 1, "creation": 1, "automat": 1, "research": 1, "configur": 1, "futur": 1, "advisor": 1, "dyer": 1, "taxonomi": 1, "plan": 1, "experi": 1, "initi": 1, "discuss": 1, "left": 1}, "marker": "(1985, 1986)", "article": "J88-3004", "vector_2": [3, 0.9282319094377385, 1, 1, 4, 0]}, {"label": "Neut", "current": "Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).", "context": ["For example, the word happens to have a similar 2 score with Shapiro as the word  (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.", "Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).", "They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model."], "vector_1": {"corpu": 1, "shapiro": 2, "partial": 1, "dictionari": 1, "deal": 1, "represent": 1, "fluenci": 1, "phonet": 1, "happen": 2, "total": 1, "special": 1, "use": 1, "cooccurr": 1, "reli": 1, "score": 1, "approach": 1, "pinyin": 1, "chines": 1, "previou": 1, "train": 1, "translat": 1, "transliter": 3, "known": 1, "parallel": 1, "convert": 1, "word": 3, "algorithm": 1, "parenthet": 1, "exampl": 1, "statist": 1, "bilingu": 1, "model": 1, "unrel": 1, "similar": 1}, "marker": "(Cao et al, 2007", "article": "P08-1113", "vector_2": [1, 0.5502692678055565, 3, 6, 0, 0]}, {"label": "Neut", "current": "Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.", "context": ["In the following, we refer to our methods as \"SLBD\" (segmenter leveraging bilingual data).", "Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.", "Moreover, we also evaluated the performance of our sub-models by"], "vector_1": {"evalu": 2, "zc": 1, "follow": 1, "cw": 1, "leverag": 1, "supervis": 1, "perform": 1, "ie": 1, "moreov": 1, "segment": 2, "peng": 1, "also": 1, "method": 2, "refer": 1, "asahara": 1, "whose": 1, "initi": 1, "train": 1, "slbd": 1, "data": 2, "manual": 1, "zhao": 1, "stateoftheart": 1, "bilingu": 1, "model": 1, "submodel": 1}, "marker": "(Peng et al., 2004)", "article": "D15-1142", "vector_2": [11, 0.8553618522450936, 4, 3, 2, 0]}, {"label": "Neut", "current": "Xu (2000) estimated that 71% of the pages (453 million out of 634 million Web pages indexed by the Excite engine at that time) were written in English, followed by Japanese (6.8%), German (5.1%), French (1.8%), Chinese (1.5%), Spanish (1.1%), Italian (0.9%), and Swedish (0.7%).", "context": ["How much of it is English?", "Xu (2000) estimated that 71% of the pages (453 million out of 634 million Web pages indexed by the Excite engine at that time) were written in English, followed by Japanese (6.8%), German (5.1%), French (1.8%), Chinese (1.5%), Spanish (1.1%), Italian (0.9%), and Swedish (0.7%).", "We have measured the counts of some English phrases according to various search engines over time and compared them with counts in the BNC, which we know has 100 million words."], "vector_1": {"phrase": 1, "german": 1, "spanish": 1, "follow": 1, "xu": 1, "excit": 1, "web": 1, "engin": 2, "compar": 1, "japanes": 1, "written": 1, "much": 1, "bnc": 1, "swedish": 1, "accord": 1, "variou": 1, "index": 1, "chines": 1, "million": 3, "french": 1, "know": 1, "count": 2, "measur": 1, "search": 1, "word": 1, "english": 3, "estim": 1, "time": 2, "page": 2, "italian": 1}, "marker": "(2000)", "article": "J03-3001", "vector_2": [3, 0.3858441131053338, 1, 1, 0, 0]}, {"label": "Neut", "current": "Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem.", "context": ["3.1 Architecture and Classifier", "Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem.", "For each (non-aux/non-copula) verb in each sentence, our classifier examines each node in the syntactic parse tree for the sentence and assigns it a semantic role label."], "vector_1": {"semant": 3, "tree": 1, "parser": 1, "classif": 1, "follow": 1, "gildea": 1, "classifi": 2, "label": 1, "role": 1, "treat": 1, "architectur": 2, "node": 1, "nonauxnoncopula": 1, "syntact": 1, "sentenc": 2, "earlier": 1, "verb": 1, "pars": 2, "examin": 1, "jurafski": 1, "task": 1, "like": 1, "ofn": 1, "problem": 1, "assign": 1}, "marker": "(2002)", "article": "N04-1032", "vector_2": [2, 0.19205123126691753, 1, 4, 0, 0]}, {"label": "Pos", "current": "As a measure of informativeness we used Information Density (ID) (Settles and Craven, 2008).", "context": ["We optimised the Gaussian Process hyperparameters every 20 new instances, for both tasks.", "As a measure of informativeness we used Information Density (ID) (Settles and Craven, 2008).", "This measure leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is:"], "vector_1": {"measur": 2, "use": 1, "task": 1, "everi": 1, "among": 1, "process": 1, "region": 1, "space": 1, "gaussian": 1, "locat": 1, "inform": 2, "instanc": 3, "dens": 1, "densiti": 1, "new": 1, "hyperparamet": 1, "varianc": 1, "featur": 1, "id": 1, "optimis": 1, "leverag": 1}, "marker": "(Settles and Craven, 2008)", "article": "W13-2241", "vector_2": [5, 0.5918219749652295, 1, 1, 1, 0]}, {"label": "Pos", "current": "The implementation of the framework has been done entirelv using OpenFst (Allauzen et al., 2007).", "context": ["4 Architecture", "The implementation of the framework has been done entirelv using OpenFst (Allauzen et al., 2007).", "Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile those rules into finite-state transducers compatible with Openfst."], "vector_1": {"compil": 1, "compat": 1, "use": 2, "transduc": 1, "defin": 1, "rule": 2, "finitest": 1, "framework": 1, "done": 1, "entirelv": 1, "openfst": 2, "rewrit": 1, "implement": 1, "thrax": 1, "contextdepend": 1, "architectur": 1}, "marker": "(Allauzen et al., 2007)", "article": "W14-5502", "vector_2": [7, 0.348623012418091, 2, 1, 2, 0]}, {"label": "Pos", "current": "We use a large-coverage syntactic parser for French, FRMG (FRench MetaGrammar) (De La Clergerie et al., 2009).", "context": ["In this section, we address the issue of defining the boundaries of E_M segments, using a syntactic parser.", "We use a large-coverage syntactic parser for French, FRMG (FRench MetaGrammar) (De La Clergerie et al., 2009).", "The main syntactic contexts in which semantic clues can occur are as follows:"], "vector_1": {"em": 1, "metagrammar": 1, "use": 2, "frmg": 1, "syntact": 3, "section": 1, "parser": 2, "semant": 1, "french": 2, "clue": 1, "issu": 1, "defin": 1, "context": 1, "address": 1, "occur": 1, "follow": 1, "main": 1, "segment": 1, "boundari": 1, "largecoverag": 1}, "marker": "(De La Clergerie et al., 2009)", "article": "W13-0302", "vector_2": [4, 0.7849490758204452, 1, 4, 0, 0]}, {"label": "Neut", "current": "LALR(1) and LALR(2) [6], weak precedence [12], LL(0) top-down (recursive descent), LR(0), LR(1) [1] ...).", "context": ["(e.g.", "LALR(1) and LALR(2) [6], weak precedence [12], LL(0) top-down (recursive descent), LR(0), LR(1) [1] ...).", "2. to study the computational behavior of the generated code, and the optimization techniques that could be used on the lin code  and more generally chart parser code  with respect to code size, execution speed and better sharing in the parse forest."], "vector_1": {"code": 4, "comput": 1, "execut": 1, "eg": 1, "parser": 1, "share": 1, "respect": 1, "speed": 1, "size": 1, "use": 1, "descent": 1, "lin": 1, "better": 1, "lr": 2, "forest": 1, "topdown": 1, "optim": 1, "gener": 2, "weak": 1, "chart": 1, "pars": 1, "recurs": 1, "techniqu": 1, "could": 1, "preced": 1, "behavior": 1, "studi": 1, "ll": 1, "lalr": 2}, "marker": "[12]", "article": "P89-1018", "vector_2": [9, 0.636722276085721, 3, 1, 0, 0]}, {"label": "CoCo", "current": "In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al (2003) \"balance\" their corpus using Web documents.", "context": ["The Web is being used to address data sparseness for language modeling.", "In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al (2003) \"balance\" their corpus using Web documents.", "The information retrieval community now has a Web track as a component of its TREC evaluation initiative."], "vector_1": {"therein": 1, "preposit": 1, "evalu": 1, "al": 1, "volk": 1, "keller": 1, "phrase": 1, "languag": 1, "web": 3, "use": 2, "lapata": 1, "villasenorpineda": 1, "compon": 1, "attach": 1, "document": 1, "refer": 1, "spars": 1, "corpu": 1, "et": 1, "lexic": 1, "track": 1, "balanc": 1, "address": 1, "data": 1, "addit": 1, "commun": 1, "retriev": 1, "trec": 1, "gather": 1, "resolv": 1, "issu": 1, "inform": 1, "statist": 1, "model": 1, "initi": 1}, "marker": "(2003)", "article": "J03-3001", "vector_2": [0, 0.22021112320583247, 2, 1, 0, 0]}, {"label": "Neut", "current": "Although the adaptation technology is widely used for other tasks such as language modeling (Iyer et al., 1997), only a few studies, to the best of our knowledge, directly address word alignment adaptation.", "context": ["We implement this by using alignment model adaptation.", "Although the adaptation technology is widely used for other tasks such as language modeling (Iyer et al., 1997), only a few studies, to the best of our knowledge, directly address word alignment adaptation.", "Wu and Wang (2004) adapted the alignment results obtained with the out-of-domain corpus to the results obtained with the in-domain corpus."], "vector_1": {"corpu": 2, "wang": 1, "directli": 1, "knowledg": 1, "obtain": 2, "result": 2, "best": 1, "use": 2, "outofdomain": 1, "adapt": 4, "languag": 1, "model": 2, "wu": 1, "although": 1, "address": 1, "implement": 1, "wide": 1, "task": 1, "word": 1, "indomain": 1, "align": 3, "technolog": 1, "studi": 1}, "marker": "(Iyer et al., 1997)", "article": "P05-1058", "vector_2": [8, 0.08436349544661888, 2, 1, 0, 0]}, {"label": "Neut", "current": "For comparison, we also show results using the supervised approach as in (Liu et al., 2008), which is the average of the 21fold cross validation.", "context": ["Table 1 shows the results using human transcripts for different methods on the 21 test meetings (139 topic segments in total).", "For comparison, we also show results using the supervised approach as in (Liu et al., 2008), which is the average of the 21fold cross validation.", "We only show the maximum F-measure with respect to individual annotations, since the average scores show similar trend."], "vector_1": {"show": 4, "trend": 1, "individu": 1, "annot": 1, "topic": 1, "2fold": 1, "result": 2, "human": 1, "tabl": 1, "respect": 1, "total": 1, "supervis": 1, "use": 2, "score": 1, "transcript": 1, "cross": 1, "also": 1, "2": 1, "test": 1, "approach": 1, "method": 1, "differ": 1, "fmeasur": 1, "segment": 1, "sinc": 1, "averag": 2, "comparison": 1, "39": 1, "maximum": 1, "meet": 1, "similar": 1, "valid": 1}, "marker": "(Liu et al., 2008)", "article": "N09-1070", "vector_2": [1, 0.6663550498049708, 1, 4, 0, 1]}, {"label": "Neut", "current": "tonicity (Sanchez Valencia, 1991).", "context": ["199", "tonicity (Sanchez Valencia, 1991).", "A small current of theoretical work has continued up to the present, for example (Zamansky et al., 2006)."], "vector_1": {"tonic": 1, "work": 1, "continu": 1, "theoret": 1, "current": 1, "exampl": 1, "small": 1, "present": 1}, "marker": "(Sanchez Valencia, 1991)", "article": "W07-1431", "vector_2": [16, 0.9475380606226855, 2, 2, 0, 0]}, {"label": "Neut", "current": "TPLEX is a transductive algorithm (Vapnik, 1998), in that the goal is to perform extraction from a given unlabelled corpus, given a labelled corpus.", "context": ["Rather than relying on redundancy of the fragments, TPLEX exploits redundancy of the learned extraction patterns.", "TPLEX is a transductive algorithm (Vapnik, 1998), in that the goal is to perform extraction from a given unlabelled corpus, given a labelled corpus.", "This is in contrast to the typical machine learning framework, where the goal is a set of extraction patterns (which can of course then be applied to new unlabelled text)."], "vector_1": {"corpu": 2, "set": 1, "appli": 1, "exploit": 1, "extract": 3, "given": 2, "cours": 1, "goal": 2, "perform": 1, "pattern": 2, "tplex": 2, "label": 1, "reli": 1, "text": 1, "new": 1, "contrast": 1, "machin": 1, "fragment": 1, "learn": 2, "framework": 1, "redund": 2, "algorithm": 1, "rather": 1, "unlabel": 2, "transduct": 1, "typic": 1}, "marker": "(Vapnik, 1998)", "article": "W06-2204", "vector_2": [8, 0.12852688417370142, 1, 1, 0, 1]}, {"label": "Neut", "current": "(Specia, 2011) and (Specia et al., 2009, 2010) propose a confidence estimator that relates specifically to the post-editing effort of translators.", "context": ["MT confidence estimation and its relation to existing TM scoring methods, together with how to make the most effective use of both technologies, is an active area of research.", "(Specia, 2011) and (Specia et al., 2009, 2010) propose a confidence estimator that relates specifically to the post-editing effort of translators.", "This research uses regression on both the automatic scores assigned to the MT and scores assigned by posteditors and aims to model post-editors' judgements of the translation quality between good and bad, or among three levels of post-editing effort."], "vector_1": {"among": 1, "qualiti": 1, "automat": 1, "exist": 1, "postedit": 2, "use": 2, "score": 3, "area": 1, "make": 1, "activ": 1, "three": 1, "research": 2, "tm": 1, "method": 1, "confid": 2, "good": 1, "relat": 2, "effect": 1, "judgement": 1, "togeth": 1, "effort": 2, "specif": 1, "level": 1, "posteditor": 2, "technolog": 1, "aim": 1, "mt": 2, "bad": 1, "estim": 2, "regress": 1, "model": 1, "propos": 1, "assign": 2, "translat": 2}, "marker": "(Specia, 2011)", "article": "N13-3003", "vector_2": [2, 0.19322005383580082, 2, 1, 2, 0]}, {"label": "Neut", "current": "Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11.", "context": ["Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.", "Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11.", "The experimental results show that n-grams containing articles are predictive for identifying native languages."], "vector_1": {"identifi": 1, "show": 3, "predict": 2, "text": 1, "expect": 1, "articl": 1, "differ": 1, "languag": 4, "result": 1, "identif": 1, "use": 2, "perform": 3, "research": 1, "better": 1, "also": 1, "experiment": 1, "speaker": 2, "method": 1, "machin": 1, "sourc": 1, "emphas": 1, "relat": 1, "given": 1, "nativ": 3, "ngram": 1, "task": 1, "translat": 2, "although": 1, "contain": 1, "grammat": 1, "learningbas": 1, "contrari": 1, "target": 1, "nonn": 1, "rather": 1, "inform": 1, "achiev": 1, "wong": 1, "determin": 1, "error": 2, "dra": 1, "improv": 1, "typic": 1, "propos": 1}, "marker": "van Halteren, 2008)", "article": "P13-1112", "vector_2": [5, 0.8352399938659715, 3, 1, 0, 0]}, {"label": "CoCo", "current": "TPLEX's boundary detectors are similar to those learned by BWI (Freitag and Kushmerick, 2000).", "context": ["This strategy has previously been employed successfully (Freitag and Kushmerick, 2000; Ciravegna, 2001; Yangarber et al., 2002; Finn and Kushmerick, 2004).", "TPLEX's boundary detectors are similar to those learned by BWI (Freitag and Kushmerick, 2000).", "A boundary detector has two parts, a left pattern and a right pattern."], "vector_1": {"right": 1, "left": 1, "success": 1, "pattern": 2, "tplex": 1, "two": 1, "employ": 1, "part": 1, "learn": 1, "bwi": 1, "detector": 2, "strategi": 1, "similar": 1, "boundari": 2, "previous": 1}, "marker": "(Freitag and Kushmerick, 2000)", "article": "W06-2204", "vector_2": [6, 0.3127994524298426, 5, 5, 0, 0]}, {"label": "Pos", "current": "We share the approach to capitalization as sequence tagging with that of (Lita et al., 2003).", "context": ["2.2 Previous Work", "We share the approach to capitalization as sequence tagging with that of (Lita et al., 2003).", "In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques."], "vector_1": {"program": 1, "capit": 1, "previou": 1, "word": 1, "built": 1, "possibl": 1, "sequenc": 1, "work": 1, "share": 1, "use": 2, "tag": 3, "disambigu": 1, "dynam": 1, "sentenc": 1, "pair": 1, "model": 1, "approach": 2, "assign": 1, "languag": 1, "techniqu": 1}, "marker": "(Lita et al., 2003)", "article": "W04-3237", "vector_2": [1, 0.26272199831702026, 1, 3, 0, 0]}, {"label": "Neut", "current": "It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively.", "context": ["New measures for computing similarity between individual concepts (inter-concept similarity, such as \"France\" and \"Great Britain\"), as well as between documents (inter-document similarity) are proposed and tested.", "It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively.", "Our methods outperform WLM in computing inter-concept similarity, and match ESA for inter-document similarity."], "vector_1": {"semant": 1, "concept": 1, "comput": 2, "explicit": 1, "individu": 1, "dataset": 1, "britain": 1, "respect": 1, "art": 1, "techniqu": 1, "compar": 2, "wikipedia": 1, "achiev": 1, "accuraci": 1, "current": 1, "linkbas": 1, "state": 1, "esa": 2, "test": 1, "new": 1, "document": 1, "method": 2, "match": 1, "analysi": 1, "franc": 1, "interdocu": 3, "demonstr": 1, "measur": 2, "great": 1, "outperform": 1, "well": 1, "interconcept": 3, "wlm": 2, "similar": 7, "propos": 2}, "marker": "(Witten and Milne, 2008)", "article": "W10-3506", "vector_2": [2, 0.0773820763624004, 2, 7, 1, 0]}, {"label": "Neut", "current": "In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.", "context": ["6, this paper reveals several crucial findings that contribute to improving native language identification.", "In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.", "The rest of this paper is structured as follows."], "vector_1": {"show": 1, "rest": 1, "one": 1, "paper": 3, "famili": 1, "follow": 1, "find": 2, "languag": 2, "crucial": 1, "identif": 1, "sever": 1, "contribut": 2, "nativ": 1, "reconstruct": 1, "addit": 1, "reveal": 1, "task": 1, "central": 1, "could": 1, "tree": 1, "histor": 1, "structur": 1, "improv": 1, "linguist": 1}, "marker": "Gray and Atkinson, 2003", "article": "P13-1112", "vector_2": [10, 0.12737310228492563, 5, 2, 0, 0]}, {"label": "Pos", "current": "We computed sentence-level correlations following the benchmark assessment procedure used by WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012; and Bojar, 2013), which use T correlation coefficient, to evaluate the correlation of evaluation metrics against human judgment on ranking the translation adequacy of the three output.", "context": ["output.", "We computed sentence-level correlations following the benchmark assessment procedure used by WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012; and Bojar, 2013), which use T correlation coefficient, to evaluate the correlation of evaluation metrics against human judgment on ranking the translation adequacy of the three output.", "A higher value for T indicates more similarity to the human adequacy rankings by the evaluation metrics."], "vector_1": {"comput": 1, "evalu": 3, "metric": 2, "procedur": 1, "assess": 1, "correl": 3, "human": 2, "follow": 1, "sentencelevel": 1, "use": 2, "three": 1, "wmt": 1, "indic": 1, "higher": 1, "metricsmatr": 1, "benchmark": 1, "coeffici": 1, "rank": 2, "judgment": 1, "valu": 1, "output": 2, "adequaci": 2, "similar": 1, "nist": 1, "translat": 1}, "marker": "(Callison-Burch et al., 2008, 2010, 2011, 2012", "article": "W14-4719", "vector_2": [6, 0.7186359317965898, 2, 1, 2, 0]}, {"label": "Neut", "current": "Cognateness Frequency is not the only indicator for word familiarity and can sometimes even be misleading (Beinborn et al., 2014).", "context": ["As additional feature, we calculate the probability of the POS sequence of the micro context.", "Cognateness Frequency is not the only indicator for word familiarity and can sometimes even be misleading (Beinborn et al., 2014).", "Many solution words are cognates, i.e."], "vector_1": {"even": 1, "featur": 1, "cognat": 2, "sequenc": 1, "mani": 1, "solut": 1, "sometim": 1, "indic": 1, "mislead": 1, "micro": 1, "calcul": 1, "context": 1, "frequenc": 1, "word": 2, "addit": 1, "ie": 1, "po": 1, "familiar": 1, "probabl": 1}, "marker": "(Beinborn et al., 2014)", "article": "Q14-1040", "vector_2": [0, 0.41800376342998197, 1, 1, 5, 0]}, {"label": "Neut", "current": "As evaluations progress, we eliminate any model that is significantly worse than any other model.3 We also note that the Racing technique first randomizes the order of the data points to ensure that prefixes of the dataset are gen3The details of these statistical tests are not so important here since we use different ones in the case of SMT, but we briefly summarize them as follows: Maron and Moore (1994) use a non-parametric method (Hoeffding bounds (Hoeffding, 1963)) for confidence estimation, and places confidence intervals on the mean value of the random variable representing ej(i).", "context": ["The models are evaluated concurrently, and at any given step k E [1, Nd], each model Mj is associated with two pieces of information: the current estimate of its mean error rate, and the estimate of its variance.", "As evaluations progress, we eliminate any model that is significantly worse than any other model.3 We also note that the Racing technique first randomizes the order of the data points to ensure that prefixes of the dataset are gen3The details of these statistical tests are not so important here since we use different ones in the case of SMT, but we briefly summarize them as follows: Maron and Moore (1994) use a non-parametric method (Hoeffding bounds (Hoeffding, 1963)) for confidence estimation, and places confidence intervals on the mean value of the random variable representing ej(i).", "A model is discarded if its confidence interval no longer overlaps with the confidence interval of the current best model."], "vector_1": {"longer": 1, "repres": 1, "evalu": 2, "point": 1, "nonparametr": 1, "interv": 3, "random": 2, "bound": 1, "dataset": 1, "rate": 1, "concurr": 1, "significantli": 1, "ensur": 1, "follow": 1, "differ": 1, "summar": 1, "best": 1, "one": 1, "note": 1, "given": 1, "techniqu": 1, "maron": 1, "genth": 1, "data": 1, "smt": 1, "nd": 1, "two": 1, "overlap": 1, "current": 2, "also": 1, "estim": 3, "test": 1, "progress": 1, "method": 1, "hoeffd": 1, "confid": 4, "variabl": 1, "elimin": 1, "race": 1, "detail": 1, "use": 2, "piec": 1, "step": 1, "import": 1, "varianc": 1, "associ": 1, "sinc": 1, "valu": 1, "eji": 1, "case": 1, "e": 1, "moor": 1, "k": 1, "prefix": 1, "mj": 1, "inform": 1, "wors": 1, "place": 1, "statist": 1, "error": 1, "discard": 1, "model": 6, "first": 1, "briefli": 1, "order": 1, "mean": 2}, "marker": "(1994)", "article": "W12-3159", "vector_2": [18, 0.4863957508643584, 2, 2, 0, 0]}, {"label": "Pos", "current": "In this paper, we use reranking architecture, which was successfully applied to the task of natural language parsing (Collins, 2000; Charniak and", "context": ["As such, a key to further improvement of the performance of bio-entity recognition has been to employ global features, which are effective to capture the features of long names appearing in the bio domain.", "In this paper, we use reranking architecture, which was successfully applied to the task of natural language parsing (Collins, 2000; Charniak and", "209"], "vector_1": {"domain": 1, "featur": 2, "captur": 1, "appli": 1, "global": 1, "paper": 1, "charniak": 1, "languag": 1, "use": 1, "appear": 1, "perform": 1, "long": 1, "recognit": 1, "natur": 1, "architectur": 1, "bio": 1, "rerank": 1, "effect": 1, "pars": 1, "key": 1, "bioentiti": 1, "task": 1, "name": 1, "success": 1, "employ": 1, "improv": 1}, "marker": "(Collins, 2000", "article": "W07-1033", "vector_2": [7, 0.10616492560937005, 1, 2, 0, 0]}, {"label": "Neut", "current": "In our experiments we will use a probabilistic latent variable CFG (Petrov et al., 2006).", "context": ["In this work we focus our attention on constituency parsing and assume that a weighted CFG is available to us.", "In our experiments we will use a probabilistic latent variable CFG (Petrov et al., 2006).", "However, our algorithms can be used with any weighted CFG, including discriminative ones, such as the ones in Petrov and Klein (2007a) and"], "vector_1": {"constitu": 1, "weight": 2, "cfg": 3, "one": 2, "assum": 1, "variabl": 1, "use": 2, "latent": 1, "avail": 1, "includ": 1, "experi": 1, "klein": 1, "probabilist": 1, "petrov": 1, "pars": 1, "attent": 1, "algorithm": 1, "howev": 1, "work": 1, "focu": 1, "us": 1, "discrimin": 1}, "marker": "(Petrov et al., 2006)", "article": "W11-2921", "vector_2": [5, 0.13432288803371817, 2, 4, 4, 1]}, {"label": "Neut", "current": "For example, in (Shinmori et al., 2003) the discourse structure of the patent claim is built by means of a rule-based technique; each discourse segment is then paraphrased.", "context": ["Studies of the first group try to adapt to the patent domain general text simplification techniques and involve lexical and/or structural substitution, pruning, paraphrasing, etc.", "For example, in (Shinmori et al., 2003) the discourse structure of the patent claim is built by means of a rule-based technique; each discourse segment is then paraphrased.", "In (Mille and Wanner, 2008) the claim sentence (by means of lexical and punctuation clues) is segmented into clausal units, that are then compressed into a summary."], "vector_1": {"claim": 2, "domain": 1, "andor": 1, "text": 1, "clausal": 1, "paraphras": 2, "unit": 1, "involv": 1, "group": 1, "techniqu": 2, "patent": 2, "substitut": 1, "adapt": 1, "discours": 2, "sentenc": 1, "gener": 1, "summari": 1, "compress": 1, "lexic": 2, "clue": 1, "punctuat": 1, "segment": 2, "built": 1, "tri": 1, "prune": 1, "simplif": 1, "rulebas": 1, "structur": 2, "etc": 1, "exampl": 1, "studi": 1, "mean": 2, "first": 1}, "marker": "(Shinmori et al., 2003)", "article": "W14-5605", "vector_2": [11, 0.25606722177091795, 2, 1, 1, 0]}, {"label": "CoCo", "current": "Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).", "context": ["Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model.", "Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).", "To ensure a fair comparison, we performed the evaluation in two steps."], "vector_1": {"featur": 1, "fair": 1, "zeng": 1, "ensur": 1, "monolingu": 1, "loglinear": 2, "sentencelevel": 1, "sever": 1, "use": 1, "outer": 1, "evalu": 1, "compar": 1, "semisupervis": 1, "perform": 1, "sun": 1, "moreov": 1, "two": 1, "next": 1, "inner": 1, "method": 2, "includ": 1, "rerank": 1, "effect": 1, "enhanc": 1, "step": 1, "candid": 1, "therebi": 1, "slbd": 1, "segment": 1, "demonstr": 1, "comparison": 1, "achiev": 1, "stateoftheart": 1, "x": 1, "model": 2, "produc": 1}, "marker": "(Zeng et al., 2013b)", "article": "D15-1142", "vector_2": [2, 0.9019077302462581, 3, 4, 2, 0]}, {"label": "Neut", "current": "of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.", "context": ["Table 4: Performance of the reranker.", "of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.", "The information of the previous labels was also quite effective, which indicates that label unigram models (i.e."], "vector_1": {"also": 1, "featur": 1, "unigram": 1, "contribut": 1, "rerank": 1, "perform": 2, "shallow": 1, "previou": 2, "parser": 1, "quit": 1, "indic": 1, "label": 2, "inform": 1, "effect": 1, "larg": 1, "tabl": 1, "studi": 1, "ie": 1, "model": 1}, "marker": "Okanohara et al., 2006", "article": "W07-1033", "vector_2": [1, 0.7420465337132004, 3, 2, 4, 0]}, {"label": "Neut", "current": "Srebro et al (2006) made a similar observation in the context of learning Gaussian mixtures.", "context": ["With more examples, there is less noise in the aggregate statistics, so it might be easier for EM to pick out the salient patterns.", "Srebro et al (2006) made a similar observation in the context of learning Gaussian mixtures.", "They characterized three regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, and the last where both failed (without much data)."], "vector_1": {"em": 3, "less": 1, "global": 1, "al": 1, "one": 1, "cluster": 1, "aggreg": 1, "fail": 2, "et": 1, "data": 2, "srebro": 1, "given": 1, "salient": 1, "much": 1, "nois": 1, "pattern": 1, "character": 1, "three": 1, "without": 1, "lot": 1, "might": 1, "easier": 1, "pick": 1, "mixtur": 1, "gaussian": 1, "regim": 1, "true": 1, "anoth": 1, "made": 1, "last": 1, "success": 2, "recov": 1, "optimum": 1, "exampl": 1, "statist": 1, "context": 1, "learn": 1, "similar": 1, "observ": 1}, "marker": "(2006)", "article": "P08-1100", "vector_2": [2, 0.9481015654055516, 1, 1, 0, 0]}, {"label": "CoCo", "current": "Like the SENNA model, the Skip-gram model (Mikolov et al., 2013) is trained to differentiate between the correct central word of a phrase and a random replacement, which they refer to as negative sampling.", "context": ["2.3 Skip-gram", "Like the SENNA model, the Skip-gram model (Mikolov et al., 2013) is trained to differentiate between the correct central word of a phrase and a random replacement, which they refer to as negative sampling.", "Unlike SENNA, however, the Skipgram model tries to make this prediction using only a single one of the surrounding words at a time and ignores the ordering of those words, i.e."], "vector_1": {"skipgram": 3, "senna": 2, "predict": 1, "replac": 1, "random": 1, "one": 1, "unlik": 1, "sampl": 1, "phrase": 1, "ie": 1, "differenti": 1, "neg": 1, "make": 1, "correct": 1, "singl": 1, "refer": 1, "use": 1, "train": 1, "central": 1, "tri": 1, "word": 3, "like": 1, "howev": 1, "surround": 1, "ignor": 1, "time": 1, "model": 3, "order": 1}, "marker": "(Mikolov et al., 2013)", "article": "W15-2610", "vector_2": [2, 0.3862329756817413, 1, 2, 0, 0]}, {"label": "CoCo", "current": "We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997), similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below.", "context": ["We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data.", "We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997), similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below.", "The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs)."], "vector_1": {"em": 2, "rang": 1, "appli": 1, "treebank": 1, "parser": 1, "pglr": 1, "pcfg": 1, "use": 1, "techniqu": 1, "compar": 1, "semisupervis": 2, "perform": 1, "anoth": 1, "detail": 1, "baumwelch": 1, "confidencebas": 1, "ioa": 1, "hidden": 1, "method": 3, "unlabel": 1, "statist": 1, "gener": 1, "ioabas": 1, "hmm": 1, "schabe": 1, "train": 4, "pars": 1, "data": 1, "markov": 1, "requir": 1, "describ": 1, "forwardbackward": 1, "algorithm": 1, "partiallybracket": 1, "instanc": 1, "contrast": 1, "model": 3, "pereira": 1, "similar": 1}, "marker": "(Inui et al., 1997)", "article": "W07-2203", "vector_2": [10, 0.14238700416622904, 1, 3, 0, 0]}, {"label": "Neut", "current": "The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables.", "context": ["The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary.", "The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables.", "A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language."], "vector_1": {"adopt": 1, "heurist": 1, "obtain": 1, "srilm": 1, "tabl": 1, "phrase": 1, "extract": 1, "languag": 2, "use": 2, "ne": 1, "reorder": 1, "strategi": 1, "train": 1, "bidirect": 1, "translat": 1, "dictionari": 1, "convert": 1, "word": 1, "target": 1, "align": 3, "growdiagfinaland": 1, "smooth": 1, "giza": 1, "combin": 1, "gram": 1, "model": 1, "kneserney": 1}, "marker": "(Koehn et al., 2003)", "article": "D15-1142", "vector_2": [12, 0.5439257457958009, 3, 2, 2, 0]}, {"label": "Pos", "current": "We use hfst-pmatch (Linden et al., 2013), a pattern-matching tool mimicking and extending Xerox fst (Karttunen, 2011), for demonstrating how to develop a semantic frame extractor.", "context": ["Abstract", "We use hfst-pmatch (Linden et al., 2013), a pattern-matching tool mimicking and extending Xerox fst (Karttunen, 2011), for demonstrating how to develop a semantic frame extractor.", "We select a FrameNet (Baker et al., 1998) frame and write shallowly syntactic pattern-matching rules based on part-of-speech information and morphology from either a morphological automaton or tagged text."], "vector_1": {"extractor": 1, "semant": 1, "text": 1, "abstract": 1, "tag": 1, "patternmatch": 2, "select": 1, "morpholog": 2, "use": 1, "develop": 1, "write": 1, "frame": 2, "syntact": 1, "extend": 1, "mimick": 1, "tool": 1, "xerox": 1, "framenet": 1, "base": 1, "partofspeech": 1, "hfstpmatch": 1, "demonstr": 1, "rule": 1, "inform": 1, "fst": 1, "either": 1, "shallowli": 1, "automaton": 1}, "marker": "(Linden et al., 2013)", "article": "W15-1842", "vector_2": [2, 0.030796499390716738, 3, 6, 0, 0]}, {"label": "Pos", "current": "get construction-we use two corpora: the British National Corpus (Burnard, 2000), an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008), approximately one billion words of non-newswire text from the New York Times from the years 1987-2006.", "context": ["62", "get construction-we use two corpora: the British National Corpus (Burnard, 2000), an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008), approximately one billion words of non-newswire text from the New York Times from the years 1987-2006.", "We extract all sentences in these corpora containing the sequence of strings no, is too, and to separated by one or more words."], "vector_1": {"corpu": 3, "text": 1, "approxim": 2, "year": 1, "latetwentieth": 1, "sequenc": 1, "centuri": 1, "extract": 1, "use": 1, "constructionw": 1, "two": 1, "contain": 1, "new": 2, "string": 1, "get": 1, "sentenc": 1, "hundr": 1, "million": 1, "nation": 1, "york": 2, "one": 3, "nonnewswir": 1, "billion": 1, "word": 3, "english": 1, "corpora": 2, "annot": 1, "british": 2, "separ": 1, "time": 2}, "marker": "(Burnard, 2000)", "article": "W10-2109", "vector_2": [10, 0.245115968533194, 2, 1, 0, 0]}, {"label": "Neut", "current": "The similarity-based method(Dagan et al., 1999) and dimension reduction technique can be merged into one model 3 .", "context": ["(k = 3) (dim = 2) (6 = 0.1) (0 = 0) P(swig I coffee) 0 0.09 0 0.1627 0.0756 0.11 P(sip I coffee) 1 1 1 0.2425 0.5536 1 P(drink I coffee) 0 0.18 0.17 0.2359 0.1932 0.28 P(devour I coffee) 0 0.18 0.11 0.1271 0.0726 0.11 P(eat I coffee) 0 0.18 0.11 0.1159 0.0526 0 P(swallow I coffee) 0 0.18 0.11 0.1159 0.0526 0 2.3.3 Method 3: Dimension-reduced similarity-based method", "The similarity-based method(Dagan et al., 1999) and dimension reduction technique can be merged into one model 3 .", "Reduced dimension can be better representation space than the original space for finding similarities between words."], "vector_1": {"dimens": 2, "represent": 1, "one": 1, "reduct": 1, "merg": 1, "pdevour": 1, "pswallow": 1, "space": 2, "45": 1, "find": 1, "better": 1, "5": 2, "7": 4, "9": 2, "psip": 1, "method": 3, "dimensionreduc": 1, "peat": 1, "coffe": 6, "75": 1, "8": 5, "word": 1, "origin": 1, "techniqu": 1, "reduc": 1, "dim": 1, "59": 3, "55": 1, "k": 1, "similaritybas": 2, "pdrink": 1, "model": 1, "pswig": 1, "similar": 1}, "marker": "(Dagan et al., 1999)", "article": "P00-1072", "vector_2": [1, 0.5320525582321322, 1, 8, 0, 0]}, {"label": "CoCo", "current": "Alves et al (2008) similarly concluded that pauses are usually a sign of cognitive competition.", "context": ["Schilperoord (2002) concludes that writers pause for a number of reasons, such as cognitive overload, writing apprehension or fatigue.", "Alves et al (2008) similarly concluded that pauses are usually a sign of cognitive competition.", "Many of the reasons given for pausing during typing are similar to the reasons given for pausing during speech production, thus providing further motivation to use the typing process to test phenomena observed during speech."], "vector_1": {"schilperoord": 1, "paus": 4, "process": 1, "phenomena": 1, "number": 1, "sign": 1, "fatigu": 1, "alv": 1, "et": 1, "cognit": 2, "apprehens": 1, "given": 2, "writer": 1, "overload": 1, "write": 1, "speech": 2, "similarli": 1, "test": 1, "type": 2, "competit": 1, "product": 1, "conclud": 2, "motiv": 1, "use": 1, "al": 1, "reason": 3, "provid": 1, "thu": 1, "mani": 1, "similar": 1, "observ": 1, "usual": 1}, "marker": "(2008)", "article": "W15-0914", "vector_2": [7, 0.34175667413898514, 2, 4, 0, 0]}, {"label": "Neut", "current": "This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.", "context": ["If a system has seen only positive examples, how does it recognize a negative example?", "This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.", "Outlier detection approaches typically derive some model of \"normal\" objects from the training set and use a distance measure and a threshold to detect abnormal items."], "vector_1": {"set": 1, "distanc": 1, "threshold": 1, "seen": 2, "differ": 1, "detect": 5, "unknown": 1, "normal": 1, "system": 1, "also": 1, "call": 1, "approach": 1, "model": 1, "recogn": 1, "deriv": 1, "outlier": 2, "object": 1, "use": 1, "train": 2, "address": 1, "data": 1, "typic": 1, "measur": 1, "novel": 1, "novelti": 1, "item": 2, "exampl": 2, "abnorm": 1, "neg": 1, "posit": 1, "problem": 1}, "marker": "(Markou and Singh, 2003a", "article": "N06-1017", "vector_2": [3, 0.026821842845581718, 3, 3, 3, 0]}, {"label": "Pos", "current": "The existence of such preferred formulations is in line with the results of Cahill and Forst (2010), who carried out an experiment in which native speakers of German evaluated a number of alternative realisations of the same sentence.", "context": ["As a consequence, the system frequently applies too much or too little ellipsis to the generated sentences, with less than ideal (though not ungrammatical) results.", "The existence of such preferred formulations is in line with the results of Cahill and Forst (2010), who carried out an experiment in which native speakers of German evaluated a number of alternative realisations of the same sentence.", "Their subjects accepted some variation in word order, but showed a clear preference for some of the alternatives."], "vector_1": {"cahil": 1, "evalu": 1, "less": 1, "carri": 1, "appli": 1, "prefer": 2, "number": 1, "accept": 1, "ellipsi": 1, "exist": 1, "result": 2, "realis": 1, "subject": 1, "show": 1, "system": 1, "littl": 1, "german": 1, "ideal": 1, "much": 1, "speaker": 1, "variat": 1, "formul": 1, "experi": 1, "altern": 2, "though": 1, "sentenc": 2, "gener": 1, "nativ": 1, "line": 1, "word": 1, "forst": 1, "consequ": 1, "clear": 1, "ungrammat": 1, "order": 1, "frequent": 1}, "marker": "(2010)", "article": "W11-1403", "vector_2": [1, 0.8429766571110981, 1, 1, 0, 0]}, {"label": "Pos", "current": "For the present study, we adopted an SVM-based classifier, described in (Zamal et al., 2012), that incorporated nearly all features used in prior work and showed comparable (and sometimes better) accuracy than other methods.", "context": ["We followed prior work in this regard, particularly since our intent here is to evaluate the relevance of existing gender inference machinery on other languages.", "For the present study, we adopted an SVM-based classifier, described in (Zamal et al., 2012), that incorporated nearly all features used in prior work and showed comparable (and sometimes better) accuracy than other methods.", "Parameter values and kernel choices for the SVM are discussed in the source paper."], "vector_1": {"kernel": 1, "adopt": 1, "evalu": 1, "show": 1, "featur": 1, "particularli": 1, "follow": 1, "accuraci": 1, "languag": 1, "paramet": 1, "better": 1, "use": 1, "describ": 1, "compar": 1, "sometim": 1, "classifi": 1, "sinc": 1, "svmbase": 1, "valu": 1, "infer": 1, "discuss": 1, "sourc": 1, "nearli": 1, "regard": 1, "intent": 1, "relev": 1, "present": 1, "choic": 1, "svm": 1, "machineri": 1, "gender": 1, "work": 2, "method": 1, "prior": 2, "paper": 1, "incorpor": 1, "exist": 1, "studi": 1}, "marker": "(Zamal et al., 2012)", "article": "D13-1114", "vector_2": [1, 0.33004926108374383, 1, 10, 4, 1]}, {"label": "Neut", "current": "Relatedly and in line with the finding that stress cues are used by infants before phonotactic cues (Jusczyk et al., 1999a), we observe that phonotactic cues require more input than stress cues to be used efficiently.", "context": ["Overall, we find that stress cues add roughly 6% token f-score to a model that does not account for phonotactics and 4% to a model that already incorporates phonotactics.", "Relatedly and in line with the finding that stress cues are used by infants before phonotactic cues (Jusczyk et al., 1999a), we observe that phonotactic cues require more input than stress cues to be used efficiently.", "A closer look at the knowledge acquired by our models shows that the Unique Stress Constraint of Yang (2004) can be acquired jointly with segmenting the input instead"], "vector_1": {"infant": 1, "effici": 1, "relatedli": 1, "show": 1, "knowledg": 1, "alreadi": 1, "phonotact": 4, "find": 2, "closer": 1, "use": 2, "acquir": 2, "cue": 5, "add": 1, "jointli": 1, "input": 2, "fscore": 1, "overal": 1, "account": 1, "instead": 1, "line": 1, "segment": 1, "requir": 1, "stress": 4, "look": 1, "constraint": 1, "uniqu": 1, "roughli": 1, "token": 1, "yang": 1, "incorpor": 1, "model": 3, "observ": 1}, "marker": "(Jusczyk et al., 1999a)", "article": "Q14-1008", "vector_2": [15, 0.07191058625052721, 2, 3, 8, 0]}, {"label": "Neut", "current": "A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).", "context": ["In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner.", "A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).", "The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly."], "vector_1": {"corpu": 1, "infant": 1, "constitu": 1, "evalu": 1, "intermedi": 1, "transit": 2, "number": 1, "procedur": 1, "daysold": 1, "result": 1, "human": 1, "onlin": 1, "artifici": 1, "surprisingli": 1, "languag": 1, "probabl": 2, "even": 1, "syllabifi": 1, "colloc": 1, "would": 1, "support": 1, "perfectli": 1, "learner": 2, "young": 1, "tp": 2, "research": 1, "identif": 1, "reli": 1, "experiment": 2, "unit": 1, "basic": 1, "input": 1, "approach": 1, "realiti": 1, "poor": 1, "assumpt": 1, "syntact": 1, "use": 4, "syllab": 1, "base": 1, "segment": 2, "studi": 1, "addit": 1, "linguist": 1, "word": 2, "level": 2, "local": 1, "clear": 1, "work": 1, "psychologicallymotiv": 1, "syllabl": 2, "inform": 1, "alreadi": 1, "learn": 2, "model": 1, "psycholog": 2}, "marker": "Saffran et al., 1996b)", "article": "W10-2912", "vector_2": [14, 0.16909708650290148, 5, 1, 1, 0]}, {"label": "Neut", "current": "SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.", "context": ["This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).", "SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.", "More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output."], "vector_1": {"set": 1, "number": 1, "templat": 1, "minimum": 1, "featur": 3, "och": 1, "use": 4, "smt": 1, "also": 1, "larg": 1, "approach": 2, "wellformed": 1, "function": 2, "syntact": 1, "team": 1, "differ": 1, "train": 1, "output": 1, "align": 1, "mt": 1, "error": 1, "improv": 1, "order": 1}, "marker": "(2003)", "article": "N04-1023", "vector_2": [1, 0.22354257494502103, 3, 8, 0, 0]}, {"label": "Pos", "current": "These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003).", "context": ["There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.", "These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003).", "One of the most pressing problems is the restriction to a single, flat stream or sequence ofprimary data (called \"text\" in some approaches), or a single, flat timeline."], "vector_1": {"stream": 1, "text": 1, "timelin": 1, "one": 1, "restrict": 1, "establish": 1, "commun": 1, "ofprimari": 1, "also": 1, "powla": 1, "complex": 1, "call": 1, "approach": 2, "singl": 2, "difficult": 1, "flat": 2, "sequenc": 1, "laf": 1, "express": 1, "link": 1, "theori": 1, "press": 1, "multimod": 1, "data": 3, "problem": 1, "made": 1, "structur": 1, "shortcom": 2, "found": 1, "model": 1}, "marker": "(Ide et al., 2003)", "article": "W13-5507", "vector_2": [10, 0.37815340805374026, 2, 1, 1, 0]}, {"label": "Neut", "current": "This in itself is not surprising, given the fact that Fokkens et al (2013) showed that even replicating the results that Pedersen (2010) reports can be challenging.", "context": ["Finally, the differences between the scores from the WordNet::Similarity package and the WordNetTools show that we did not reproduce the results exactly.", "This in itself is not surprising, given the fact that Fokkens et al (2013) showed that even replicating the results that Pedersen (2010) reports can be challenging.", "They showed that even if the main properties are kept stable, such as software and versions of software, variations in minor properties can lead to completely different outcomes."], "vector_1": {"softwar": 2, "version": 1, "show": 3, "challeng": 1, "al": 1, "packag": 1, "result": 2, "stabl": 1, "et": 1, "surpris": 1, "even": 2, "differ": 2, "lead": 1, "pedersen": 1, "score": 1, "main": 1, "reproduc": 1, "final": 1, "minor": 1, "complet": 1, "fokken": 1, "variat": 1, "given": 1, "report": 1, "exactli": 1, "replic": 1, "kept": 1, "properti": 2, "wordnettool": 1, "outcom": 1, "wordnetsimilar": 1, "fact": 1}, "marker": "(2010)", "article": "W14-0118", "vector_2": [4, 0.899283053129207, 2, 5, 6, 0]}, {"label": "Neut", "current": "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "context": ["The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\"", "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level."], "vector_1": {"featur": 1, "profici": 1, "focus": 1, "connect": 1, "touch": 1, "skill": 1, "languag": 3, "involv": 1, "rather": 1, "question": 1, "construct": 1, "valid": 1, "advoc": 1, "factor": 1, "test": 2, "instead": 1, "analys": 1, "tightli": 1, "difficulti": 2, "gener": 1, "argu": 1, "gap": 1, "vocabulari": 1, "earliest": 1, "ctest": 4, "reduc": 1, "measur": 2, "search": 1, "grammar": 1, "level": 3, "aim": 1, "paragraph": 1, "combin": 1, "determin": 1, "model": 1, "other": 1}, "marker": "Klein-Braley, 1985)", "article": "Q14-1040", "vector_2": [29, 0.1619488901928252, 6, 1, 10, 0]}, {"label": "Neut", "current": "In parse reranking, each training sentence has a ranked list of 27 candidates on average (Collins, 2000), but for machine translation, the number of candidate translations in the -best list is much higher.", "context": ["Although we might invent metrics to define the quality of a translation, standard reranking algorithms cannot be directly applied to MT.", "In parse reranking, each training sentence has a ranked list of 27 candidates on average (Collins, 2000), but for machine translation, the number of candidate translations in the -best list is much higher.", "(SMT Team, 2003) show that to get a reasonable improvement in the BLEU score at least 1000 candidates need to be considered in the -best list."], "vector_1": {"bleu": 1, "consid": 1, "directli": 1, "show": 1, "appli": 1, "metric": 1, "qualiti": 1, "number": 1, "rank": 1, "need": 1, "best": 2, "score": 1, "defin": 1, "get": 1, "least": 1, "much": 1, "might": 1, "higher": 1, "machin": 1, "candid": 3, "rerank": 2, "sentenc": 1, "standard": 1, "reason": 1, "cannot": 1, "pars": 1, "invent": 1, "although": 1, "averag": 1, "algorithm": 1, "list": 3, "mt": 1, "train": 1, "improv": 1, "translat": 3}, "marker": "(Collins, 2000)", "article": "N04-1023", "vector_2": [4, 0.4049153130907828, 2, 2, 1, 0]}, {"label": "CoCo", "current": "When compared with the sample queries in (Cao et al., 2007), the queries in our sample seem to contain more phrasal words and technical terminology.", "context": ["We use * to mark incorrect translations.", "When compared with the sample queries in (Cao et al., 2007), the queries in our sample seem to contain more phrasal words and technical terminology.", "It is interesting to see that even though parenthetical translations tend to be out-of-vocabulary words, as we have remarked in the introduction, the sheer size of the web means that occasionally translations of common words such as 'use' are sometimes included as well."], "vector_1": {"queri": 2, "terminolog": 1, "see": 1, "sampl": 2, "seem": 1, "tend": 1, "well": 1, "size": 1, "incorrect": 1, "web": 1, "use": 2, "compar": 1, "sometim": 1, "mark": 1, "includ": 1, "interest": 1, "even": 1, "though": 1, "sheer": 1, "translat": 3, "technic": 1, "introduct": 1, "remark": 1, "word": 3, "parenthet": 1, "outofvocabulari": 1, "common": 1, "contain": 1, "occasion": 1, "phrasal": 1, "mean": 1}, "marker": "(Cao et al., 2007)", "article": "P08-1113", "vector_2": [1, 0.8742822497235992, 1, 6, 0, 0]}, {"label": "Neut", "current": "Htun et al (2012) extended the many-to-many alignment for the sample-wise transliteration mining, but its noise model only handles the sample-wise noise and cannot distinguish partial noise.", "context": ["We introduce a noise symbol to handle partial noise in the many-to-many alignment model.", "Htun et al (2012) extended the many-to-many alignment for the sample-wise transliteration mining, but its noise model only handles the sample-wise noise and cannot distinguish partial noise.", "We model partial noise in the CRP-based joint substring model."], "vector_1": {"partial": 3, "nois": 6, "extend": 1, "joint": 1, "handl": 2, "symbol": 1, "substr": 1, "al": 1, "mine": 1, "align": 2, "cannot": 1, "crpbase": 1, "transliter": 1, "et": 1, "samplewis": 2, "htun": 1, "model": 4, "distinguish": 1, "manytomani": 2, "introduc": 1}, "marker": "(2012)", "article": "D13-1021", "vector_2": [1, 0.347566235366605, 1, 1, 10, 0]}, {"label": "Neut", "current": "Nagata et al (2001) made the first proposal to mine translations from the web.", "context": ["6 Related Work", "Nagata et al (2001) made the first proposal to mine translations from the web.", "Their work was concentrated on terminologies, and assumed the English terms were given as input."], "vector_1": {"nagata": 1, "web": 1, "term": 1, "relat": 1, "input": 1, "english": 1, "given": 1, "work": 2, "al": 1, "terminolog": 1, "mine": 1, "made": 1, "translat": 1, "concentr": 1, "et": 1, "propos": 1, "assum": 1, "first": 1}, "marker": "(2001)", "article": "P08-1113", "vector_2": [7, 0.9432219408680766, 1, 2, 0, 0]}, {"label": "Neut", "current": "Recently, Nickolls et al (2008) introduced the Compute Unified Device Architecture (CUDA).", "context": ["3.1 Compute Unified Device Architecture", "Recently, Nickolls et al (2008) introduced the Compute Unified Device Architecture (CUDA).", "It allows programmers to utilize GPUs to accelerate applications in domains other than graphics."], "vector_1": {"applic": 1, "acceler": 1, "graphic": 1, "comput": 2, "gpu": 1, "domain": 1, "devic": 2, "al": 1, "util": 1, "unifi": 2, "cuda": 1, "allow": 1, "programm": 1, "et": 1, "nickol": 1, "recent": 1, "introduc": 1, "architectur": 2}, "marker": "(2008)", "article": "W11-2921", "vector_2": [3, 0.22838294196183145, 1, 3, 0, 0]}, {"label": "Pos", "current": "Hence we are reusing and developing our theoretical [18] and experimental [36] approach in this much more general setting which is more likely to be effectively usable for natural language parsing.", "context": ["We have formally extended the concept of PDA into that of Logical PDA which is an operational push-down stack device for parsing unification based grammars [17,18] or other non-CF grammars such as Tree Adjoining Grammars [19].", "Hence we are reusing and developing our theoretical [18] and experimental [36] approach in this much more general setting which is more likely to be effectively usable for natural language parsing.", "This is certainly the case for all left-to-right CF chart parsing schemata."], "vector_1": {"oper": 1, "concept": 1, "natur": 1, "unif": 1, "cf": 1, "set": 1, "pushdown": 1, "languag": 1, "develop": 1, "much": 1, "devic": 1, "theoret": 1, "certainli": 1, "experiment": 1, "adjoin": 1, "approach": 1, "noncf": 1, "extend": 1, "henc": 1, "gener": 1, "effect": 1, "usabl": 1, "chart": 1, "base": 1, "pars": 3, "stack": 1, "formal": 1, "case": 1, "grammar": 3, "like": 1, "tree": 1, "lefttoright": 1, "reus": 1, "schemata": 1, "logic": 1, "pda": 2}, "marker": "[18]", "article": "P89-1018", "vector_2": [2, 0.9353080806392298, 5, 3, 0, 0]}, {"label": "Neut", "current": "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "context": ["The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\"", "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level."], "vector_1": {"featur": 1, "profici": 1, "focus": 1, "connect": 1, "touch": 1, "skill": 1, "languag": 3, "involv": 1, "rather": 1, "question": 1, "construct": 1, "valid": 1, "advoc": 1, "factor": 1, "test": 2, "instead": 1, "analys": 1, "tightli": 1, "difficulti": 2, "gener": 1, "argu": 1, "gap": 1, "vocabulari": 1, "earliest": 1, "ctest": 4, "reduc": 1, "measur": 2, "search": 1, "grammar": 1, "level": 3, "aim": 1, "paragraph": 1, "combin": 1, "determin": 1, "model": 1, "other": 1}, "marker": "(Eckes and Grotjahn, 2006", "article": "Q14-1040", "vector_2": [8, 0.1619488901928252, 6, 2, 3, 0]}, {"label": "Neut", "current": "The improvement observed by (Hickl et al., 2006) was achieved by going to 10.000 items.", "context": ["3In terms of machine learning, extending a training set by factor 2 (from 800 to 1.600 items) does not make a qualitative difference.", "The improvement observed by (Hickl et al., 2006) was achieved by going to 10.000 items.", "As can be seen from Table 2, in most of the cases, IV performs best, e.g."], "vector_1": {"set": 1, "eg": 1, "tabl": 1, "iv": 1, "in": 1, "go": 1, "seen": 1, "best": 1, "differ": 1, "perform": 1, "make": 1, "factor": 1, "machin": 1, "extend": 1, "train": 1, "case": 1, "term": 1, "qualit": 1, "item": 2, "achiev": 1, "learn": 1, "improv": 1, "observ": 1}, "marker": "(Hickl et al., 2006)", "article": "W07-1402", "vector_2": [1, 0.6422917649525607, 1, 2, 0, 0]}, {"label": "Neut", "current": "Another line of research for keyword extraction has adopted graph-based methods similar to Google's PageRank algorithm (Brin and Page, 1998).", "context": ["Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reasonable performance for this task.", "Another line of research for keyword extraction has adopted graph-based methods similar to Google's PageRank algorithm (Brin and Page, 1998).", "In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences."], "vector_1": {"set": 1, "assumpt": 1, "seen": 1, "seem": 1, "simultan": 1, "extract": 2, "summar": 1, "even": 1, "supervis": 1, "use": 1, "googl": 1, "perform": 1, "anoth": 1, "research": 1, "sentenc": 2, "import": 2, "approach": 2, "method": 1, "graphbas": 1, "though": 1, "big": 1, "contain": 1, "reinforc": 1, "reason": 1, "pagerank": 1, "particular": 1, "line": 1, "data": 1, "task": 1, "attempt": 1, "algorithm": 1, "keyword": 4, "adopt": 1, "achiev": 1, "learn": 1, "studi": 1, "similar": 1, "usual": 2}, "marker": "(Brin and Page, 1998)", "article": "N09-1070", "vector_2": [11, 0.19212791334902163, 2, 1, 0, 0]}, {"label": "Pos", "current": "We combine the following freely available data, leading to a large corpus of positive and negative tweets: - 1.6 million automatically labeled tweets from the Sentiment140 data set (Go et al., 2009), collected by searching for positive and negative emoticons;", "context": ["We compute the LMI over a corpus of positive, respectively negative tweets, in order to obtain positive (LMIpos) and negative (LMIneg) bigram scores.", "We combine the following freely available data, leading to a large corpus of positive and negative tweets: - 1.6 million automatically labeled tweets from the Sentiment140 data set (Go et al., 2009), collected by searching for positive and negative emoticons;", "1An online demo illustrating the score values and distributional term similarities in this Twitter space can be found at the LT website http://maggie.lt.informatik."], "vector_1": {"corpu": 2, "set": 1, "comput": 1, "illustr": 1, "demo": 1, "twitter": 1, "obtain": 1, "automat": 1, "an": 1, "respect": 1, "follow": 1, "websit": 1, "lead": 1, "collect": 1, "neg": 4, "tweet": 3, "label": 1, "avail": 1, "lt": 1, "score": 2, "larg": 1, "emoticon": 1, "httpmaggieltinformatik": 1, "distribut": 1, "found": 1, "million": 1, "term": 1, "sentiment": 1, "lmi": 1, "onlin": 1, "data": 2, "valu": 1, "search": 1, "lmineg": 1, "space": 1, "posit": 4, "combin": 1, "lmipo": 1, "freeli": 1, "bigram": 1, "similar": 1, "order": 1}, "marker": "(Go et al., 2009)", "article": "W15-2911", "vector_2": [6, 0.3340106800245735, 1, 1, 0, 0]}, {"label": "Weak", "current": "Although some versions of the grid use an automatic coreference resolver, this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their target domains, and actually hurts for readability prediction.", "context": ["We follow the -COREFERENCE setting from Barzilay and Lapata (2005) and perform heuristic coreference resolution by linking mentions which share a head noun.", "Although some versions of the grid use an automatic coreference resolver, this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their target domains, and actually hurts for readability prediction.", "Their results, moreover, rely on running coreference on the document in its original order; in a summarization task, the correct order is not known, which will cause even more resolver errors."], "vector_1": {"origin": 1, "domain": 1, "set": 1, "often": 1, "predict": 1, "share": 1, "automat": 1, "one": 1, "result": 3, "heurist": 1, "fail": 1, "follow": 1, "summar": 1, "even": 1, "use": 1, "lapata": 2, "perform": 1, "moreov": 1, "readabl": 1, "corefer": 5, "reli": 1, "version": 1, "document": 1, "correct": 1, "head": 1, "run": 1, "resolut": 1, "hurt": 1, "mention": 1, "grid": 1, "although": 1, "improv": 2, "known": 1, "actual": 1, "task": 1, "noun": 1, "target": 1, "resolv": 2, "caus": 1, "barzilay": 2, "error": 1, "link": 1, "order": 2}, "marker": "(2005)", "article": "P11-2022", "vector_2": [6, 0.28803878594673576, 2, 5, 8, 0]}, {"label": "Neut", "current": "One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).", "context": ["Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).", "One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).", "Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011)."], "vector_1": {"rang": 2, "help": 1, "intellig": 1, "challeng": 1, "one": 1, "topic": 1, "see": 1, "abil": 1, "morphologicallycomplex": 1, "increasingli": 1, "sever": 1, "detect": 2, "hoo": 1, "learner": 1, "research": 1, "write": 1, "languag": 3, "type": 1, "assist": 1, "learn": 1, "autom": 1, "ical": 1, "becom": 1, "strand": 1, "recent": 1, "task": 1, "rare": 1, "computerassist": 1, "work": 1, "focu": 1, "exampl": 1, "determin": 1, "error": 3, "popular": 1}, "marker": "(Rozovskaya and Roth, 2011", "article": "W12-2011", "vector_2": [1, 0.03127188575256539, 6, 1, 0, 0]}, {"label": "Neut", "current": "The word alignment is automatically computed by using GIZA++4(Och and Ney, 2000) in both directions, which are symmetrized by using the union operation.", "context": ["Word Alignment.", "The word alignment is automatically computed by using GIZA++4(Och and Ney, 2000) in both directions, which are symmetrized by using the union operation.", "Instead of aligning words themselves, stems are used for aligning."], "vector_1": {"oper": 1, "use": 3, "comput": 1, "union": 1, "align": 4, "direct": 1, "automat": 1, "giza": 1, "instead": 1, "word": 3, "symmetr": 1, "stem": 1}, "marker": "(Och and Ney, 2000)", "article": "W08-0315", "vector_2": [8, 0.748039914468995, 1, 1, 1, 0]}, {"label": "CoCo", "current": "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).", "context": ["The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.", "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).", "The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation."], "vector_1": {"xi": 1, "effici": 1, "evalu": 2, "obtain": 1, "indic": 2, "dataset": 1, "monolingu": 2, "decreas": 1, "significantli": 1, "tabl": 1, "leverag": 1, "use": 1, "accur": 1, "develop": 1, "primarili": 1, "semisupervis": 3, "perform": 1, "data": 1, "smt": 2, "highli": 1, "much": 3, "includ": 1, "xu": 1, "method": 5, "unlabel": 2, "rather": 1, "perplex": 1, "optim": 1, "step": 1, "zeng": 1, "slbd": 4, "preferenti": 1, "segment": 3, "demonstr": 2, "comparison": 1, "subsequ": 1, "provid": 1, "outperform": 1, "larger": 1, "stronger": 1, "focu": 1, "final": 1, "caus": 1, "inform": 1, "present": 1, "either": 1, "bilingu": 1, "produc": 1, "result": 2}, "marker": "(Xi et al., 2012)", "article": "D15-1142", "vector_2": [3, 0.9519128677603863, 4, 3, 0, 0]}, {"label": "Neut", "current": "In order to increase the efficiency by exploiting this parallelism, typical GPUs (Lindholm et al., 2008) have hundreds of processing cores.", "context": ["Graphics Processor Units (GPUs) were originally designed for processing graphics applications, where millions of operations can be executed in parallel.", "In order to increase the efficiency by exploiting this parallelism, typical GPUs (Lindholm et al., 2008) have hundreds of processing cores.", "For example, the NVIDIA GTX480 GPU has 480 processing cores called stream processors (SP)."], "vector_1": {"origin": 1, "oper": 1, "effici": 1, "execut": 1, "core": 2, "process": 3, "exploit": 1, "design": 1, "unit": 1, "stream": 1, "call": 1, "gtx": 1, "gpu": 3, "applic": 1, "hundr": 1, "million": 1, "increas": 1, "parallel": 2, "graphic": 2, "sp": 1, "processor": 2, "exampl": 1, "nvidia": 1, "typic": 1, "order": 1}, "marker": "(Lindholm et al., 2008)", "article": "W11-2921", "vector_2": [3, 0.20738762794837562, 1, 3, 0, 0]}, {"label": "Neut", "current": "Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting.", "context": ["Blitzer et al (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products.", "Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting.", "Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007; Wang et al., 2007)."], "vector_1": {"domain": 1, "weight": 1, "focus": 2, "corpusbas": 1, "et": 1, "vote": 1, "onlin": 1, "differ": 1, "sentiment": 2, "review": 1, "system": 1, "classifi": 3, "research": 1, "adapt": 1, "bergler": 1, "new": 1, "type": 1, "includ": 1, "analysi": 1, "product": 1, "investig": 1, "chines": 1, "al": 1, "lexiconbas": 1, "andreevskaia": 1, "blitzer": 1, "present": 1, "consist": 1, "work": 1, "ensembl": 1, "precisionbas": 1}, "marker": "(2008)", "article": "D08-1058", "vector_2": [0, 0.2263481131740566, 6, 2, 0, 0]}, {"label": "Neut", "current": "We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings.", "context": ["3 Data", "We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings.", "All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005)."], "vector_1": {"use": 1, "extract": 1, "natur": 1, "transcrib": 1, "annot": 1, "topic": 1, "record": 1, "icsi": 1, "dialog": 1, "act": 1, "meet": 4, "da": 1, "data": 2, "occur": 1, "summari": 1}, "marker": "(Janin et al., 2003)", "article": "N09-1070", "vector_2": [6, 0.25308661874214244, 3, 1, 0, 0]}, {"label": "Pos", "current": "The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English.", "context": ["POS information for the source and the target languages was considered for both translation tasks that we have participated.", "The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English.", "The number of classes for English is 44, while Spanish is considered as a more inflectional language, and the tag set contains 376 different tags."], "vector_1": {"softwar": 1, "set": 1, "number": 1, "tag": 2, "spanish": 2, "languag": 2, "differ": 1, "perform": 1, "particip": 1, "avail": 1, "freel": 1, "inflect": 1, "sourc": 1, "tool": 1, "contain": 1, "postag": 1, "translat": 1, "consid": 2, "class": 1, "tnt": 1, "task": 1, "target": 1, "po": 1, "inform": 1, "english": 2}, "marker": "(Brants, 2000)", "article": "W08-0315", "vector_2": [8, 0.7208838203848895, 2, 1, 0, 0]}, {"label": "Pos", "current": "Collins et al (Collins, 2001a; Collins, 2001b) proposed an efficient method to calculate Tree Kernel by using C(n1, n2) as follows.", "context": ["2.2 Algorithm to calculate similarity", "Collins et al (Collins, 2001a; Collins, 2001b) proposed an efficient method to calculate Tree Kernel by using C(n1, n2) as follows.", " If the productions at n1 and n2 are different C(n1, n2) = 0  If the productions at n1 and n2 are the same, and n1 and n2 are pre-terminals, then C(n1, n2) = 1  Else if the productions at n1 and n2 are the same and n1 and n2 are not pre-terminals,"], "vector_1": {"kernel": 1, "use": 1, "effici": 1, "cn": 3, "algorithm": 1, "collin": 1, "tree": 1, "al": 1, "n": 13, "product": 3, "pretermin": 2, "calcul": 2, "els": 1, "follow": 1, "et": 1, "differ": 1, "similar": 1, "method": 1, "propos": 1}, "marker": "(Collins, 2001a", "article": "P06-2052", "vector_2": [5, 0.22268386122052555, 2, 3, 3, 0]}, {"label": "Neut", "current": "The approach is in the spirit of Smadja (1993) on retrieving collocations from text corpora, but is more integrated with parsing.", "context": ["In the rest of this paper, we will present a learning procedure that learns those relations by processing a large corpus with a chart-filter, a treefilter and an LLR filter.", "The approach is in the spirit of Smadja (1993) on retrieving collocations from text corpora, but is more integrated with parsing.", "We will show in the evaluation section how much the learned knowledge can help improve sentence analysis."], "vector_1": {"corpu": 1, "help": 1, "evalu": 1, "show": 1, "process": 1, "llr": 1, "knowledg": 1, "procedur": 1, "paper": 1, "smadja": 1, "colloc": 1, "section": 1, "much": 1, "larg": 1, "text": 1, "approach": 1, "analysi": 1, "chartfilt": 1, "sentenc": 1, "relat": 1, "pars": 1, "rest": 1, "spirit": 1, "present": 1, "treefilt": 1, "retriev": 1, "corpora": 1, "filter": 1, "integr": 1, "learn": 3, "improv": 1}, "marker": "(1993)", "article": "W03-1717", "vector_2": [10, 0.14370860927152318, 1, 1, 0, 0]}, {"label": "Neut", "current": "To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).", "context": ["Most use the vector space model to represent the context as dimensions in a vector space, where the feature are frequency of co-occurrence of the context words, and the comparison is usually the cosine similarity.", "To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).", "In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008)."], "vector_1": {"dimens": 1, "semant": 1, "featur": 1, "composit": 1, "creat": 1, "inspir": 1, "repres": 2, "go": 1, "phrase": 1, "beyond": 1, "use": 3, "cooccurr": 1, "tensor": 1, "space": 2, "sadrzadeh": 1, "complex": 1, "grefenstett": 1, "product": 1, "variou": 1, "lexic": 1, "multipl": 1, "mitchel": 2, "addit": 1, "comparison": 1, "account": 1, "word": 2, "specif": 1, "frequenc": 1, "lapata": 2, "work": 4, "vector": 3, "cosin": 1, "context": 2, "widdow": 1, "model": 4, "similar": 1, "order": 1, "usual": 1}, "marker": "(2008)", "article": "S13-2019", "vector_2": [5, 0.5225833685099198, 4, 3, 0, 0]}, {"label": "Neut", "current": "(Fulcher, 1997)), while at the same time incorporating more linguistic processing for more complex input.", "context": ["e.g.", "(Fulcher, 1997)), while at the same time incorporating more linguistic processing for more complex input.", "For example, with question formation exercises, no closed set of correct answers exists, and one must use parse tree distance to delineate features."], "vector_1": {"set": 1, "process": 1, "eg": 1, "one": 1, "distanc": 1, "exist": 1, "close": 1, "featur": 1, "use": 1, "question": 1, "complex": 1, "exercis": 1, "answer": 1, "input": 1, "correct": 1, "format": 1, "pars": 1, "delin": 1, "must": 1, "tree": 1, "exampl": 1, "incorpor": 1, "time": 1, "linguist": 1}, "marker": "(Fulcher, 1997)", "article": "W12-2011", "vector_2": [15, 0.9774671904022411, 1, 3, 0, 0]}, {"label": "Neut", "current": "The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.", "context": ["1 Introduction", "The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.", "Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements."], "vector_1": {"entropi": 1, "ten": 1, "appli": 1, "foundat": 1, "socal": 1, "signific": 1, "year": 1, "techniqu": 1, "gradient": 1, "noisychannel": 1, "smt": 1, "introduct": 1, "method": 1, "machin": 2, "rerank": 1, "translat": 2, "recent": 1, "provid": 1, "maximum": 1, "mt": 1, "statist": 1, "improv": 1, "model": 2}, "marker": "(Brown et al., 1990)", "article": "N04-1023", "vector_2": [14, 0.04213125506385277, 3, 2, 0, 0]}, {"label": "Neut", "current": "A statistical prediction engine provides the completions to what a human translator types (Foster et al., 1997; Och et al., 2003).", "context": ["a statistical interactive machine translation system, to the CAT system is another useful feature.", "A statistical prediction engine provides the completions to what a human translator types (Foster et al., 1997; Och et al., 2003).", "Then, one possible procedure for skilled human translators is to provide the oral translation of a given source text and then to post-edit the recognized text."], "vector_1": {"featur": 1, "predict": 1, "text": 2, "procedur": 1, "one": 1, "human": 2, "skill": 1, "postedit": 1, "use": 1, "engin": 1, "anoth": 1, "interact": 1, "system": 2, "type": 1, "complet": 1, "machin": 1, "sourc": 1, "recogn": 1, "given": 1, "translat": 4, "possibl": 1, "provid": 2, "cat": 1, "statist": 2, "oral": 1}, "marker": "(Foster et al., 1997", "article": "P06-2061", "vector_2": [9, 0.050150375939849626, 2, 1, 3, 0]}, {"label": "Neut", "current": "Two studies in this area are Sekine (1997) and Gildea (2001), both of which show substantial variation in model performance", "context": ["There is little work on assessing how well one language model fares when applied to a text type that is different from that of the training corpus.", "Two studies in this area are Sekine (1997) and Gildea (2001), both of which show substantial variation in model performance", "341"], "vector_1": {"corpu": 1, "show": 1, "appli": 1, "one": 1, "assess": 1, "substanti": 1, "languag": 1, "differ": 1, "gildea": 1, "area": 1, "perform": 1, "two": 1, "littl": 1, "sekin": 1, "type": 1, "fare": 1, "variat": 1, "train": 1, "studi": 1, "work": 1, "well": 1, "text": 1, "model": 2}, "marker": "(2001)", "article": "J03-3001", "vector_2": [2, 0.6984026529630661, 2, 2, 0, 0]}, {"label": "Neut", "current": "First, we pre-train word embeddings using the open-source toolkit Word2Vec (Mikolov et al., 2013) on the Chinese (segmented using characterlevel features only) and English sentences separately, thereby obtaining the vocabularies Vch and Ven and their corresponding embedding matrixes Lch E Rn|Vch |and Len E Rn|Ven|.", "context": ["English-Chinese semantic gap feature To guarantee that the semantic meanings of the Chinese segmentation match those of the corresponding English sentences as closely as possible, we propose to use a feature based on the EnglishChinese semantic gap to ensure the retention of semantic meaning during the segmentation process.", "First, we pre-train word embeddings using the open-source toolkit Word2Vec (Mikolov et al., 2013) on the Chinese (segmented using characterlevel features only) and English sentences separately, thereby obtaining the vocabularies Vch and Ven and their corresponding embedding matrixes Lch E Rn|Vch |and Len E Rn|Ven|.", "Given a Chinese word wn with an index i in the vocabulary, it is then straightforward to retrieve the word's vector representation via simple multiplication with a binary vector d that is equal to zero at all positions except that with index i:"], "vector_1": {"represent": 1, "semant": 4, "featur": 3, "vocabulari": 2, "process": 1, "retriev": 1, "opensourc": 1, "toolkit": 1, "pretrain": 1, "wordvec": 1, "zero": 1, "via": 1, "guarante": 1, "ensur": 1, "close": 1, "index": 2, "use": 3, "multipl": 1, "matrix": 1, "vch": 1, "except": 1, "rnven": 1, "binari": 1, "equal": 1, "wn": 1, "mean": 2, "straightforward": 1, "match": 1, "rnvch": 1, "sentenc": 2, "chines": 3, "given": 1, "len": 1, "gap": 2, "englishchines": 2, "base": 1, "therebi": 1, "ven": 1, "e": 2, "segment": 3, "lch": 1, "characterlevel": 1, "word": 3, "possibl": 1, "simpl": 1, "retent": 1, "correspond": 2, "separ": 1, "vector": 2, "english": 2, "posit": 1, "embed": 2, "propos": 1, "obtain": 1, "first": 1}, "marker": "(Mikolov et al., 2013)", "article": "D15-1142", "vector_2": [2, 0.5944446347227454, 1, 1, 0, 0]}, {"label": "Neut", "current": "2001a, 2001b; Biber et al.", "context": ["1997; Michiels and Dufour 1998; Merkel and Andersson 2000; Piao and McEnery 2001; Sag et al.", "2001a, 2001b; Biber et al.", "2003)."], "vector_1": {"merkel": 1, "sag": 1, "al": 2, "biber": 1, "dufour": 1, "michiel": 1, "piao": 1, "et": 2, "mceneri": 1, "andersson": 1}, "marker": "a, 2001b", "article": "W03-1807", "vector_2": [2, 0.5933037912358444, 1, 3, 0, 0]}, {"label": "Neut", "current": "The first method is descri ed in (Wu and Wang, 2004).", "context": ["6 Evaluation", "The first method is descri ed in (Wu and Wang, 2004).", "We call it \"Result Adaptation (ResAdapt)\"."], "vector_1": {"evalu": 1, "descri": 1, "ed": 1, "adapt": 1, "call": 1, "result": 1, "resadapt": 1, "method": 1, "first": 1}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.48517729122263126, 1, 13, 0, 1]}, {"label": "Neut", "current": "However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse.", "context": ["The worst size complexity of such a chart is only a square function of the size of the input2.", "However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse.", "Furthermore it makes the extraction of parse-trees a trivial matter."], "vector_1": {"constitu": 2, "often": 1, "advantag": 1, "share": 1, "one": 1, "parsetre": 1, "ambigu": 2, "extract": 1, "sever": 1, "size": 2, "explicitli": 1, "squar": 1, "distinct": 1, "trivial": 1, "make": 1, "particip": 1, "complex": 2, "way": 1, "input": 1, "function": 1, "subtre": 1, "fragment": 1, "sentenc": 1, "relat": 1, "chart": 2, "worst": 1, "retain": 1, "associ": 1, "case": 1, "actual": 1, "practic": 1, "algorithm": 1, "possibl": 1, "howev": 1, "structur": 2, "nontermin": 1, "matter": 1, "instanc": 1, "common": 1, "furthermor": 1, "pars": 3, "produc": 1}, "marker": "[24]", "article": "P89-1018", "vector_2": [13, 0.11443950386145565, 5, 2, 0, 0]}, {"label": "CoCo", "current": "Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.", "context": ["Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).", "Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.", "WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them."], "vector_1": {"wikipedia": 4, "semant": 3, "concept": 1, "comput": 2, "creat": 1, "appli": 1, "ir": 1, "outlink": 1, "repres": 1, "cost": 1, "exist": 1, "overlap": 1, "cognit": 1, "network": 1, "compar": 1, "memori": 1, "much": 1, "activ": 1, "due": 1, "coverag": 2, "articl": 3, "linkbas": 1, "spread": 1, "treat": 2, "esa": 1, "text": 2, "hurdl": 1, "method": 2, "analysi": 1, "knowledg": 1, "variou": 1, "regard": 1, "wordnetbas": 1, "foremost": 1, "knowledgebas": 1, "biggest": 1, "base": 3, "link": 2, "theori": 1, "although": 1, "arguabl": 1, "associ": 1, "model": 1, "earlier": 1, "recent": 1, "measur": 1, "success": 1, "level": 1, "outperform": 1, "larger": 1, "explicit": 1, "equal": 1, "conceptu": 2, "adequ": 1, "wlm": 2, "found": 1, "sa": 1, "similar": 1, "anchor": 1}, "marker": "(Budanitsky and Hirst, 2001)", "article": "W10-3506", "vector_2": [9, 0.11926432267079573, 6, 1, 0, 0]}, {"label": "Neut", "current": "Malik et al (2008) implemented a Hindi-Urdu transliteration svstem with finite-state transducers using a universal intermediate transcription (UIT).", "context": ["Within Indic transliteration, there have been several attempts on rule-based approaches.", "Malik et al (2008) implemented a Hindi-Urdu transliteration svstem with finite-state transducers using a universal intermediate transcription (UIT).", "It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script."], "vector_1": {"graphem": 1, "within": 1, "al": 1, "indic": 1, "uit": 1, "equival": 1, "et": 1, "sever": 1, "use": 1, "persoarab": 1, "transduc": 1, "script": 2, "univers": 1, "intermedi": 1, "approach": 1, "svstem": 1, "base": 1, "transliter": 2, "devanagari": 1, "transcript": 1, "attempt": 1, "hindiurdu": 1, "rulebas": 1, "finitest": 1, "malik": 1, "implement": 1}, "marker": "(2008)", "article": "W14-5502", "vector_2": [6, 0.20148467213490354, 1, 1, 2, 0]}, {"label": "CoCo", "current": "Textual inference problems from the PASCAL RTE Challenge (Dagan et al., 2005) differ from FraCaS problems in several important ways.", "context": ["5 Experiments with RTE data", "Textual inference problems from the PASCAL RTE Challenge (Dagan et al., 2005) differ from FraCaS problems in several important ways.", "(See table 5 for examples.)"], "vector_1": {"rte": 2, "differ": 1, "fraca": 1, "challeng": 1, "see": 1, "textual": 1, "pascal": 1, "exampl": 1, "way": 1, "tabl": 1, "import": 1, "problem": 2, "experi": 1, "data": 1, "infer": 1, "sever": 1}, "marker": "(Dagan et al., 2005)", "article": "W07-1431", "vector_2": [2, 0.7497599780551365, 1, 1, 1, 0]}, {"label": "Neut", "current": "In previous work, Brown (1989) calculates the frequency of the target word on the basis of the current test text.", "context": ["We therefore calculate the frequency of the solution and also its length as more frequent words tend to be shorter in English.", "In previous work, Brown (1989) calculates the frequency of the target word on the basis of the current test text.", "This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty."], "vector_1": {"identifi": 1, "difficulti": 1, "text": 1, "clearli": 1, "indic": 1, "bias": 1, "still": 1, "current": 1, "solut": 1, "calcul": 2, "cloze": 1, "tend": 1, "shorter": 1, "also": 1, "estim": 1, "test": 1, "therefor": 1, "brown": 1, "good": 1, "previou": 1, "gap": 1, "basi": 1, "word": 2, "target": 1, "frequenc": 3, "work": 1, "length": 1, "english": 1, "frequent": 1}, "marker": "(1989)", "article": "Q14-1040", "vector_2": [25, 0.37383404487789085, 1, 5, 0, 0]}, {"label": "Neut", "current": "Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.", "context": ["4 Conclusion", "Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.", "Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html)."], "vector_1": {"domain": 1, "lexical": 1, "nouvel": 1, "ce": 2, "vu": 3, "et": 3, "nombr": 1, "facil": 1, "ont": 1, "causer": 1, "le": 6, "jour": 1, "autr": 1, "semantiqu": 1, "la": 2, "avec": 1, "gnralis": 1, "cogalex": 1, "vectoriel": 1, "silenc": 1, "pa": 1, "soumiss": 1, "sur": 1, "francophon": 1, "distributionel": 1, "ressourc": 1, "normment": 1, "memoir": 1, "contrast": 1, "pu": 1, "travail": 1, "consacr": 1, "nest": 1, "de": 4, "succ": 1, "comm": 1, "voisin": 1, "savoir": 1, "vivier": 1, "qui": 1, "nou": 1, "vnement": 1, "du": 3, "problem": 1, "conclus": 1, "faibl": 1, "distributionnel": 1, "mond": 1, "surpri": 1, "dun": 1, "httppagepersolifunivmrsfrmichaelzockcogalexivcogalexwebpageindexhtml": 1, "car": 1, "od": 1, "tre": 2, "thori": 1, "method": 1, "etc": 1, "dynamism": 1, "sont": 1, "tion": 1, "se": 1, "il": 2}, "marker": "(Sahlgren, 2008)", "article": "W14-6700", "vector_2": [6, 0.9209515096065873, 4, 1, 0, 0]}, {"label": "Neut", "current": "(Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993).", "context": ["We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component.", "(Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993).", "Questioning the traditional n-grams, Magerman already advocates a heavier reliance on contextual information."], "vector_1": {"featur": 2, "ad": 1, "wall": 1, "treebank": 1, "move": 1, "penn": 1, "syntax": 1, "magerman": 1, "street": 1, "significantli": 1, "beyond": 1, "languag": 1, "use": 1, "question": 1, "system": 1, "compon": 1, "advoc": 1, "decis": 1, "sentenc": 1, "spatter": 1, "heavier": 1, "simpl": 1, "introduc": 1, "machin": 1, "knowledg": 1, "deriv": 1, "extend": 1, "far": 1, "sequenc": 1, "journal": 1, "express": 1, "sophist": 1, "ngram": 1, "train": 1, "pars": 2, "background": 1, "particular": 1, "increas": 1, "tradit": 1, "relianc": 1, "work": 1, "tree": 1, "contextu": 1, "inform": 1, "alreadi": 1, "limit": 1, "learn": 1, "action": 2, "model": 1, "similar": 1}, "marker": "(Magerman, 1995)", "article": "P97-1062", "vector_2": [2, 0.8782098057611453, 2, 1, 0, 0]}, {"label": "Neut", "current": "Guzman et al (2009) analyzed the role of the word alignment in the phrase extraction process.", "context": ["4.3 Unaligned Word Feature", "Guzman et al (2009) analyzed the role of the word alignment in the phrase extraction process.", "To better model the relation between word alignment and the phrase extraction process, they introduced two new features into the log-linear model."], "vector_1": {"better": 1, "featur": 2, "word": 3, "process": 2, "align": 2, "relat": 1, "al": 1, "new": 1, "two": 1, "guzman": 1, "role": 1, "analyz": 1, "phrase": 2, "loglinear": 1, "et": 1, "model": 2, "extract": 2, "introduc": 1, "unalign": 1}, "marker": "(2009)", "article": "W10-1719", "vector_2": [1, 0.5447305889152912, 1, 1, 5, 0]}, {"label": "CoCo", "current": "Comparison between the classifications of Kay [14] and Griffith dc Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers'.", "context": ["The idea of this approach is to separate the dynamic programming constructs needed for efficient chart parsing from the chosen parsing schema.", "Comparison between the classifications of Kay [14] and Griffith dc Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers'.", "These PDTs are usually non-deterministic and cannot be used as produced for actual parsing."], "vector_1": {"classif": 1, "effici": 1, "show": 1, "parser": 1, "idea": 1, "pushdown": 1, "need": 1, "griffith": 1, "dynam": 1, "use": 1, "transduc": 1, "chosen": 1, "construct": 2, "program": 1, "approach": 1, "strategi": 1, "nondeterminist": 1, "may": 1, "express": 1, "dc": 1, "chart": 1, "cannot": 1, "pars": 5, "kay": 1, "formal": 1, "comparison": 1, "actual": 1, "pdt": 2, "well": 1, "lefttoright": 1, "separ": 1, "cf": 1, "petrick": 1, "studi": 1, "schema": 2, "produc": 1, "usual": 1}, "marker": "[10]", "article": "P89-1018", "vector_2": [19, 0.23466283307144528, 2, 1, 0, 0]}, {"label": "Neut", "current": "(Milne et al., 2007) proposed a system called \"KORU\" for query expansion using Wikipedia's most relevant articles to user's query.", "context": ["Synonyms from WordNet are used to expand the question in order to extract the most semantically relevant passages to the question.", "(Milne et al., 2007) proposed a system called \"KORU\" for query expansion using Wikipedia's most relevant articles to user's query.", "The system allows the user to refine the set of Wikipedia pages to be used for expansion."], "vector_1": {"semant": 1, "set": 1, "wikipedia": 2, "extract": 1, "wordnet": 1, "use": 3, "passag": 1, "question": 2, "system": 2, "articl": 1, "call": 1, "queri": 2, "expans": 2, "user": 2, "refin": 1, "koru": 1, "relev": 2, "expand": 1, "synonym": 1, "page": 1, "allow": 1, "order": 1, "propos": 1}, "marker": "(Milne et al., 2007)", "article": "W14-3611", "vector_2": [7, 0.2967125936669084, 1, 1, 0, 0]}, {"label": "Weak", "current": "Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed.", "context": ["verbs become leaves instead of governing the sentences.", "Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed.", "The main contribution of this paper is a considerable improvement of unsupervised parsing quality by estimating the Pstop probabilities externally using a very large corpus, and employing this prior knowledge in the standard inference of DMV."], "vector_1": {"corpu": 1, "consider": 1, "hockenmai": 1, "verbocentr": 1, "knowledg": 1, "qualiti": 1, "tag": 2, "probabl": 1, "use": 1, "render": 1, "mark": 1, "identif": 1, "estim": 1, "leav": 1, "useless": 1, "faili": 1, "instead": 1, "main": 1, "boost": 1, "po": 2, "contribut": 1, "sentenc": 1, "standard": 1, "rasooli": 1, "verb": 2, "pars": 1, "govern": 1, "extern": 1, "becom": 1, "effort": 1, "pstop": 1, "requir": 1, "made": 1, "larg": 1, "approach": 1, "howev": 1, "manual": 1, "structur": 1, "infer": 2, "employ": 2, "prior": 1, "paper": 1, "bisk": 1, "dmv": 1, "improv": 1, "unsupervis": 2}, "marker": "(2012)", "article": "P13-1028", "vector_2": [1, 0.07993588192596177, 2, 3, 0, 0]}, {"label": "Pos", "current": "Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available).", "context": ["a parser to a new domain with less annotation effort.", "Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available).", "The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents."], "vector_1": {"em": 1, "correct": 1, "domain": 1, "constitu": 1, "less": 1, "parser": 1, "high": 1, "tag": 1, "effort": 1, "proport": 1, "find": 1, "compat": 1, "nois": 1, "semisupervis": 1, "suggest": 1, "support": 1, "avail": 1, "alway": 1, "confidencebas": 1, "merialdo": 1, "new": 1, "po": 1, "introduc": 1, "suitabl": 1, "deriv": 2, "especi": 1, "elworthi": 1, "train": 2, "data": 1, "outweigh": 1, "success": 1, "level": 1, "indomain": 1, "annot": 1, "method": 2, "benefit": 1, "bracket": 1, "incorpor": 1, "contain": 1}, "marker": "(1994)", "article": "W07-2203", "vector_2": [13, 0.9437033925007877, 2, 4, 0, 0]}, {"label": "Pos", "current": "For evaluation, we used Kyoto Text Corpus Version 4.0 (Kurohashi and Nagao, 2003), Mainichi news articles in 1995 that have been manually annotated with dependency relations.6 The training, development, and test sets included 24,283, 4833, and 9284 sentences, and 234,685, 47,571, and 89,874 bunsetsus, respectively.", "context": ["This task is therefore a better measure of our method than simple sequential labeling such as POS tagging or named-entity recognition.", "For evaluation, we used Kyoto Text Corpus Version 4.0 (Kurohashi and Nagao, 2003), Mainichi news articles in 1995 that have been manually annotated with dependency relations.6 The training, development, and test sets included 24,283, 4833, and 9284 sentences, and 234,685, 47,571, and 89,874 bunsetsus, respectively.", "The training samples generated from the training set included 150,064 positive and 146,712 negative samples."], "vector_1": {"corpu": 1, "set": 2, "evalu": 1, "text": 1, "23485": 1, "14712": 1, "tag": 1, "sampl": 2, "respect": 1, "articl": 1, "use": 1, "depend": 1, "namedent": 1, "neg": 1, "sequenti": 1, "develop": 1, "label": 1, "better": 1, "recognit": 1, "version": 1, "includ": 2, "sentenc": 1, "test": 1, "therefor": 1, "simpl": 1, "po": 1, "mainichi": 1, "kyoto": 1, "gener": 1, "relat": 1, "bunsetsu": 1, "train": 3, "news": 1, "measur": 1, "task": 1, "manual": 1, "annot": 1, "method": 1, "15004": 1, "posit": 1}, "marker": "(Kurohashi and Nagao, 2003)", "article": "D09-1160", "vector_2": [6, 0.6086224833241539, 1, 1, 0, 0]}, {"label": "Neut", "current": "The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between human-human and human-computer dialogues (Doran et al., 2001).", "context": ["A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses.", "The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between human-human and human-computer dialogues (Doran et al., 2001).", "While there is a large degree of overlap across schemes, the set of DA types will differ due to differences in the nature of the communicative goals; thus information-seeking versus task-oriented dialogues differ in the set of speech acts and their relative frequencies."], "vector_1": {"repair": 1, "major": 1, "set": 2, "natur": 1, "eg": 1, "informationseek": 1, "humancomput": 1, "due": 1, "overlap": 1, "ground": 1, "dialogu": 2, "goal": 1, "commun": 1, "interact": 1, "frequenc": 1, "system": 1, "classifi": 1, "speech": 4, "rel": 1, "larg": 1, "scheme": 1, "type": 2, "cite": 1, "contrast": 1, "recogn": 1, "distribut": 1, "gener": 2, "differ": 3, "da": 1, "taskori": 1, "comparison": 1, "specif": 1, "analys": 1, "thu": 1, "work": 1, "focu": 2, "across": 1, "humanhuman": 1, "degre": 1, "versu": 1, "act": 4}, "marker": "(Traum and Heeman, 1996", "article": "W09-3953", "vector_2": [13, 0.10271662940931307, 3, 2, 0, 0]}, {"label": "Neut", "current": "The second algorithm that we will use for MT reranking is the -insensitive ordinal regression with uneven margin, which was proposed in (Shen and Joshi, 2004), as shown in Algorithm 2.", "context": ["4.2 Ordinal Regression", "The second algorithm that we will use for MT reranking is the -insensitive ordinal regression with uneven margin, which was proposed in (Shen and Joshi, 2004), as shown in Algorithm 2.", "In Algorithm 2, the function is used to control the level of insensitivity, and the function is used to control the learning margin between pairs of translations with different ranks as described in Section 3.5."], "vector_1": {"control": 2, "ordin": 2, "rank": 1, "second": 1, "differ": 1, "use": 3, "insensit": 2, "describ": 1, "section": 1, "function": 2, "rerank": 1, "shown": 1, "uneven": 1, "translat": 1, "pair": 1, "algorithm": 3, "level": 1, "mt": 1, "learn": 1, "regress": 2, "margin": 2, "propos": 1}, "marker": "(Shen and Joshi, 2004)", "article": "N04-1023", "vector_2": [0, 0.7281145105906863, 1, 4, 2, 1]}, {"label": "Neut", "current": "constructed from this backbone (Tomita, 1987).", "context": ["24", "constructed from this backbone (Tomita, 1987).", "The residue of features not incorporated into the backbone are unified on each reduce action and if unification fails the associated derivation paths also fail."], "vector_1": {"reduc": 1, "featur": 1, "backbon": 2, "also": 1, "unif": 1, "construct": 1, "residu": 1, "fail": 2, "unifi": 1, "incorpor": 1, "action": 1, "path": 1, "associ": 1, "deriv": 1}, "marker": "(Tomita, 1987)", "article": "W07-2203", "vector_2": [20, 0.2570458285194132, 1, 1, 0, 0]}, {"label": "Neut", "current": "Bautista et al (2009) also rely on a dictionary of synonyms, but their criterion for choosing the most appropriate one is wordlength rather than frequency.", "context": ["The above approach to lexical simplification has been repeated in a number of works (Lal and Ruger, 2002; Burstein et al., 2007).", "Bautista et al (2009) also rely on a dictionary of synonyms, but their criterion for choosing the most appropriate one is wordlength rather than frequency.", "Caseli et al (2009) analyse lexical operations on a parallel corpus of original and manually simplified texts in Portuguese, using lists of simple words and discourse markers as resources."], "vector_1": {"oper": 1, "corpu": 1, "text": 1, "number": 1, "one": 1, "origin": 1, "list": 1, "bautista": 1, "marker": 1, "et": 2, "simpl": 1, "use": 1, "rather": 1, "also": 1, "reli": 1, "approach": 1, "criterion": 1, "analys": 1, "simplifi": 1, "discours": 1, "repeat": 1, "resourc": 1, "lexic": 2, "al": 2, "dictionari": 1, "word": 1, "parallel": 1, "wordlength": 1, "case": 1, "appropri": 1, "synonym": 1, "simplif": 1, "frequenc": 1, "work": 1, "manual": 1, "portugues": 1, "choos": 1}, "marker": "(2009)", "article": "W12-2202", "vector_2": [3, 0.187008992745629, 4, 1, 2, 0]}, {"label": "Neut", "current": "The appropriate movement and marking of local focus, and the appropriate choice of the form of a noun phrase (NP) based on local focus information, are considered to contribute to the local coherence exhibited by discourse (Sidner [1979], Grosz, Joshi, and Weinstein [1983, 1995], Carter [1987], and others).", "context": ["By \"local focus,\" we refer to the person, object, property, or concept that a sentence is most centrally about within the discourse context in which it occurs.", "The appropriate movement and marking of local focus, and the appropriate choice of the form of a noun phrase (NP) based on local focus information, are considered to contribute to the local coherence exhibited by discourse (Sidner [1979], Grosz, Joshi, and Weinstein [1983, 1995], Carter [1987], and others).", "In addition, local focus information is one source of information that is used by readers and hearers for interpreting pronouns."], "vector_1": {"exhibit": 1, "concept": 1, "carter": 1, "pronoun": 1, "within": 1, "one": 1, "joshi": 1, "coher": 1, "phrase": 1, "occur": 1, "use": 1, "inform": 3, "grosz": 1, "hearer": 1, "sidner": 1, "mark": 1, "other": 1, "reader": 1, "np": 1, "local": 5, "movement": 1, "discours": 2, "sourc": 1, "contribut": 1, "form": 1, "sentenc": 1, "object": 1, "base": 1, "consid": 1, "noun": 1, "interpret": 1, "addit": 1, "choic": 1, "appropri": 2, "central": 1, "weinstein": 1, "properti": 1, "focu": 4, "person": 1, "context": 1, "refer": 1}, "marker": "[1979]", "article": "J99-2001", "vector_2": [20, 0.02070471205702471, 4, 2, 29, 1]}, {"label": "CoCo", "current": "The results from our linguistic analysis are consistent with other research on sublanguages in the instructions domain, in both French and English, e.g., (Kosseim and Lapalme, 1994; Paris and Scott, 1994).", "context": ["9 Related Work", "The results from our linguistic analysis are consistent with other research on sublanguages in the instructions domain, in both French and English, e.g., (Kosseim and Lapalme, 1994; Paris and Scott, 1994).", "Our analysis goes beyond previous work by identifying within the discourse context the means for exercising explicit control over a text generator."], "vector_1": {"control": 1, "domain": 1, "identifi": 1, "text": 1, "eg": 1, "within": 1, "instruct": 1, "result": 1, "beyond": 1, "goe": 1, "research": 1, "exercis": 1, "sublanguag": 1, "analysi": 2, "discours": 1, "previou": 1, "gener": 1, "relat": 1, "french": 1, "consist": 1, "work": 2, "explicit": 1, "context": 1, "english": 1, "linguist": 1, "mean": 1}, "marker": "(Kosseim and Lapalme, 1994", "article": "P96-1026", "vector_2": [2, 0.8891334369066114, 2, 3, 0, 0]}, {"label": "Neut", "current": "This score is often used in extractive summarization to select summary sentences (Radev et al., 2001).", "context": ["The sentence score is calculated based on its cosine similarity to the entire meeting.", "This score is often used in extractive summarization to select summary sentences (Radev et al., 2001).", "The cosine similarity between two vectors, D1 and D2, is defined as:"], "vector_1": {"use": 1, "vector": 1, "often": 1, "entir": 1, "sentenc": 2, "two": 1, "d": 2, "defin": 1, "calcul": 1, "score": 2, "cosin": 2, "base": 1, "meet": 1, "extract": 1, "similar": 2, "summar": 1, "select": 1, "summari": 1}, "marker": "(Radev et al., 2001)", "article": "N09-1070", "vector_2": [8, 0.5004674252925437, 1, 1, 0, 0]}, {"label": "Neut", "current": "Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).", "context": ["There is also a rich body of theoretical work on learning latent-variable models.", "Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).", "But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV."], "vector_1": {"em": 1, "certain": 1, "except": 1, "littl": 1, "famili": 1, "complex": 1, "special": 1, "provabl": 1, "capac": 2, "theoret": 2, "also": 1, "discret": 1, "pcfg": 1, "rich": 1, "dmv": 1, "dasgupta": 1, "gener": 2, "weak": 1, "bodi": 1, "hmm": 1, "understand": 1, "strong": 1, "constrain": 1, "schulman": 1, "latentvari": 1, "term": 2, "algorithm": 1, "work": 1, "alon": 1, "hiddenvari": 1, "learn": 2, "let": 1, "model": 3, "other": 1}, "marker": "Feldman et al., 2005)", "article": "P08-1100", "vector_2": [3, 0.9644994421340907, 6, 1, 0, 0]}, {"label": "Pos", "current": "In our particular setting, weak rules are extended to arbitrarily deep decision trees following the suggestion of (Schapire and Singer, 1999) and the definition presented in (Carreras and Marquez, 2001).", "context": ["This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP-attachment disambiguation, text categorization, and word sense disambiguation.", "In our particular setting, weak rules are extended to arbitrarily deep decision trees following the suggestion of (Schapire and Singer, 1999) and the definition presented in (Carreras and Marquez, 2001).", "These more complex weak rules allow the algorithm to work in a higher dimensional feature space that contains conjunctions of simple features, and this fact has turned out to be crucial for improving results in the present domain."], "vector_1": {"nlp": 1, "domain": 1, "featur": 3, "effici": 1, "appli": 1, "number": 1, "deep": 1, "high": 1, "signific": 1, "tag": 1, "result": 1, "follow": 1, "conjunct": 1, "definit": 1, "space": 2, "suggest": 1, "abl": 1, "complex": 1, "disambigu": 3, "decis": 1, "text": 1, "sens": 1, "set": 1, "boost": 1, "po": 1, "higher": 1, "crucial": 1, "extend": 1, "weak": 2, "particular": 2, "categor": 1, "present": 2, "ppattach": 1, "arbitrarili": 1, "task": 1, "word": 1, "algorithm": 2, "success": 1, "simpl": 1, "work": 2, "tree": 1, "rule": 2, "turn": 1, "allow": 1, "contain": 1, "improv": 1, "dimension": 2, "fact": 1}, "marker": "(Schapire and Singer, 1999)", "article": "W01-0726", "vector_2": [2, 0.12012955147708312, 2, 2, 0, 0]}, {"label": "Neut", "current": "Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form.", "context": ["Parse trees have also been used in alignment models.", "Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form.", "(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank."], "vector_1": {"use": 3, "grammar": 1, "form": 1, "constraint": 1, "contextfre": 1, "synchron": 1, "align": 2, "tree": 1, "probabilist": 1, "syntax": 1, "restrict": 1, "also": 1, "wu": 1, "pars": 1, "treebank": 1, "selforgan": 1, "model": 2, "chomskynorm": 1, "implicit": 1, "introduc": 1}, "marker": "(1997)", "article": "N04-1023", "vector_2": [7, 0.16594004398317835, 2, 2, 0, 0]}, {"label": "Pos", "current": "The results show that for both gold standards, we approach the correlations that are reported by Pedersen (2010), but that there are probably still differences in the implementation of the measures that lead to different output values.", "context": ["7The depth parameter is set to 19, For more information, we refer to section 6.", "The results show that for both gold standards, we approach the correlations that are reported by Pedersen (2010), but that there are probably still differences in the implementation of the measures that lead to different output values.", "6 Discussion"], "vector_1": {"set": 1, "gold": 1, "show": 1, "correl": 1, "result": 1, "still": 1, "probabl": 1, "differ": 2, "lead": 1, "section": 1, "pedersen": 1, "discuss": 1, "paramet": 1, "approach": 1, "refer": 1, "standard": 1, "report": 1, "implement": 1, "valu": 1, "measur": 1, "inform": 1, "depth": 1, "output": 1, "the": 1}, "marker": "(2010)", "article": "W14-0118", "vector_2": [4, 0.77305031151185, 1, 5, 6, 0]}, {"label": "Neut", "current": "The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.", "context": ["A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).", "The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.", "Precision is 41.6%, and recall is 23.3%, which we will show is worse than a simple baseline of assuming every syllable is a word."], "vector_1": {"corpu": 1, "infant": 1, "evalu": 1, "show": 1, "transit": 2, "wors": 1, "number": 1, "procedur": 1, "daysold": 1, "result": 1, "onlin": 1, "artifici": 1, "assum": 1, "surprisingli": 1, "languag": 1, "probabl": 2, "even": 1, "syllabifi": 1, "use": 4, "support": 1, "perfectli": 1, "learner": 1, "young": 1, "tp": 2, "research": 1, "identif": 1, "reli": 1, "experiment": 2, "unit": 1, "basic": 1, "input": 1, "precis": 1, "baselin": 1, "local": 1, "poor": 1, "assumpt": 1, "recal": 1, "everi": 1, "syllab": 1, "base": 1, "segment": 2, "studi": 1, "word": 2, "16": 1, "simpl": 1, "work": 1, "psychologicallymotiv": 1, "syllabl": 3, "inform": 1, "alreadi": 1, "learn": 2, "model": 1, "psycholog": 1}, "marker": "(Yang, 2004)", "article": "W10-2912", "vector_2": [6, 0.17841787185423494, 5, 9, 0, 0]}, {"label": "Weak", "current": "Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact decoder (Chang and Collins, 2011).", "context": ["First, the Nbest error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum1 of the N-best error surface is not guaranteed to be any close to an optimum of the true error surface.", "Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact decoder (Chang and Collins, 2011).", "MERT calculates an envelope from candidate translations and assumes all translations on the envelope are reachable by the decoder, but these translations may become unreachable due to search errors."], "vector_1": {"unreach": 1, "mert": 3, "one": 1, "second": 1, "explor": 1, "guarante": 1, "close": 1, "optimum": 2, "yet": 1, "surfac": 5, "exact": 1, "make": 1, "smt": 1, "due": 1, "decod": 4, "reachabl": 1, "may": 1, "gener": 1, "envelop": 2, "differ": 1, "candid": 1, "translat": 3, "becom": 1, "true": 2, "nbest": 2, "search": 2, "errorpron": 1, "rate": 1, "calcul": 1, "ignor": 1, "assum": 1, "error": 8, "first": 1, "fact": 1, "mean": 1}, "marker": "(Chang and Collins, 2011)", "article": "W12-3159", "vector_2": [1, 0.053013980057122816, 1, 1, 0, 0]}, {"label": "Neut", "current": "For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.", "context": ["Thirdly, many (lexical) parameter estimates do not generalize well between domains.", "For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.", "Tadayoshi et al (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format."], "vector_1": {"corpu": 2, "domain": 3, "manual": 1, "parser": 2, "ptb": 1, "al": 1, "biomed": 1, "et": 1, "select": 2, "paramet": 2, "thirdli": 1, "gildea": 1, "collin": 1, "accuraci": 1, "estim": 1, "adapt": 1, "test": 2, "correct": 1, "brown": 1, "contribut": 1, "format": 1, "gener": 1, "lexic": 1, "bilex": 1, "retrain": 1, "train": 1, "pars": 1, "tadayoshi": 1, "improv": 1, "report": 1, "data": 2, "wsjderiv": 1, "deriv": 1, "augment": 1, "wsj": 1, "well": 1, "yield": 1, "instanc": 1, "statist": 1, "mani": 1, "model": 1, "genia": 1}, "marker": "(1999)", "article": "W07-2203", "vector_2": [8, 0.06809508805097504, 3, 2, 0, 0]}, {"label": "CoCo", "current": "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "context": ["The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\"", "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level."], "vector_1": {"featur": 1, "profici": 1, "focus": 1, "connect": 1, "touch": 1, "skill": 1, "languag": 3, "involv": 1, "rather": 1, "question": 1, "construct": 1, "valid": 1, "advoc": 1, "factor": 1, "test": 2, "instead": 1, "analys": 1, "tightli": 1, "difficulti": 2, "gener": 1, "argu": 1, "gap": 1, "vocabulari": 1, "earliest": 1, "ctest": 4, "reduc": 1, "measur": 2, "search": 1, "grammar": 1, "level": 3, "aim": 1, "paragraph": 1, "combin": 1, "determin": 1, "model": 1, "other": 1}, "marker": "Singleton and Little, 1991)", "article": "Q14-1040", "vector_2": [23, 0.1619488901928252, 6, 1, 0, 0]}, {"label": "Pos", "current": "We used an implementation of Brill's (1992) part-of-speech tagger to find adjectives and modifiers; for parsing, we used the Stanford dependency parser (Klein and Manning, 2003).", "context": ["Each document in each corpus was preprocessed into individual sentences, lower-cased, and tokenized.", "We used an implementation of Brill's (1992) part-of-speech tagger to find adjectives and modifiers; for parsing, we used the Stanford dependency parser (Klein and Manning, 2003).", "6 Evaluating Extraction"], "vector_1": {"corpu": 1, "use": 2, "extract": 1, "depend": 1, "evalu": 1, "stanford": 1, "sentenc": 1, "individu": 1, "parser": 1, "token": 1, "brill": 1, "lowercas": 1, "adject": 1, "tagger": 1, "partofspeech": 1, "preprocess": 1, "pars": 1, "implement": 1, "document": 1, "modifi": 1, "find": 1}, "marker": "(1992)", "article": "N07-1039", "vector_2": [15, 0.6270997044192921, 2, 1, 0, 0]}, {"label": "Pos", "current": "In the first maximum alignment based approach we will consider, the definitions of sz,pred and sz, are inspired by Mihalcea et al (2006) who normalize phrasal similarities according to the phrase length.", "context": ["3.2 Maximum alignment (precision-recall average)", "In the first maximum alignment based approach we will consider, the definitions of sz,pred and sz, are inspired by Mihalcea et al (2006) who normalize phrasal similarities according to the phrase length.", "1 Si,pred = 2 1precei,pred,fi,pred + recei,pred,fi,pred 1 =2 1precei,j,fi,j + recei,j ,fi,j"], "vector_1": {"szpred": 1, "inspir": 1, "al": 1, "sipr": 1, "et": 1, "phrase": 1, "definit": 1, "preceipredfipr": 1, "precisionrecal": 1, "receij": 1, "averag": 1, "fij": 1, "approach": 1, "receipredfipr": 1, "accord": 1, "normal": 1, "base": 1, "consid": 1, "mihalcea": 1, "sz": 1, "align": 2, "maximum": 2, "length": 1, "phrasal": 1, "preceijfij": 1, "similar": 1, "first": 1}, "marker": "(2006)", "article": "W14-4719", "vector_2": [8, 0.4131206560328016, 1, 5, 0, 0]}, {"label": "CoCo", "current": "The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P(s|t) are smoothed using the Good-Turing technique (Foster et al., 2006).", "context": ["Portage's model for P(t1s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature.", "The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P(s|t) are smoothed using the Good-Turing technique (Foster et al., 2006).", "The distortion model is also very similar to Koehn's, with the exception of a final cost to account for sentence endings."], "vector_1": {"featur": 1, "one": 2, "four": 1, "cost": 1, "loglinear": 1, "phrase": 2, "probabl": 1, "sentencelength": 1, "use": 1, "end": 1, "techniqu": 1, "pt": 1, "except": 2, "compon": 1, "also": 1, "estim": 1, "goodtur": 1, "main": 1, "final": 1, "koehn": 2, "sentenc": 1, "distort": 2, "ngram": 1, "translat": 2, "pst": 1, "portag": 1, "account": 1, "smooth": 1, "phrasebas": 1, "targetlanguag": 1, "combin": 1, "model": 6, "similar": 2, "wordreord": 1}, "marker": "(Foster et al., 2006)", "article": "N07-1064", "vector_2": [1, 0.4447607779338904, 1, 1, 0, 0]}, {"label": "Neut", "current": "This measure is closely related to the BLEU-1 evaluation metric and the Meteor (Banerjee and Lavie, 2005) metric (when it is evaluated without considering near-matches and the distortion penalty).", "context": ["This amounts to evaluating translation quality in terms of unigram precision and recall, which are highly correlated with human judgements (Lavie et al., ).", "This measure is closely related to the BLEU-1 evaluation metric and the Meteor (Banerjee and Lavie, 2005) metric (when it is evaluated without considering near-matches and the distortion penalty).", "We also believe that hypotheses that maximize the unigram precision and recall at the sentence level yield corpus level BLEU-4 scores close the maximal achievable."], "vector_1": {"bleu": 2, "corpu": 1, "evalu": 3, "metric": 2, "qualiti": 1, "al": 1, "correl": 1, "human": 1, "close": 2, "nearmatch": 1, "et": 1, "highli": 1, "penalti": 1, "lavi": 1, "achiev": 1, "also": 1, "score": 1, "meteor": 1, "recal": 2, "sentenc": 1, "relat": 1, "distort": 1, "maxim": 2, "judgement": 1, "translat": 1, "consid": 1, "measur": 1, "term": 1, "unigram": 2, "level": 2, "hypothes": 1, "yield": 1, "precis": 2, "amount": 1, "without": 1, "believ": 1}, "marker": "(Banerjee and Lavie, 2005)", "article": "D10-1091", "vector_2": [5, 0.2491735146878247, 1, 1, 0, 0]}, {"label": "Pos", "current": "50 word-pairs from the WordSimilarity-353 dataset (Gabrilovich, 2002) and correlating our method's scores with the human-assigned scores.", "context": ["Require: G, Lp,-ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai < T then > Threshold return end if Add vi to P > To avoid cycles for vj E N(vi) do > Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ < Lp,-ax then aj =aj+ai*wij*d Replace (vj, aj) E A with (vj, aj ) SPREAD UNIDIR(vj, A, P) end if end for return end function", "50 word-pairs from the WordSimilarity-353 dataset (Gabrilovich, 2002) and correlating our method's scores with the human-assigned scores.", "To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout (Witten and Frank, 2005): Given a sample test set of N pairs of words with human-assigned ratings of relatedness, randomly divide this set into k parts of roughly equal size3."], "vector_1": {"randomli": 1, "jpj": 1, "set": 3, "process": 1, "vi": 2, "vj": 5, "aj": 5, "related": 1, "dataset": 1, "rate": 1, "correl": 1, "sampl": 2, "threshold": 1, "happen": 1, "holdout": 1, "size": 1, "given": 1, "end": 5, "score": 2, "divid": 1, "perform": 1, "avoid": 1, "favour": 1, "ai": 2, "add": 1, "spread": 2, "5": 1, "requir": 1, "replac": 1, "unidirvi": 1, "wordpair": 1, "unidirvj": 1, "method": 1, "function": 2, "repeat": 1, "return": 2, "lpax": 2, "overestim": 1, "neighbour": 1, "cycl": 1, "test": 1, "pair": 1, "word": 1, "humanassign": 2, "nvi": 1, "techniqu": 3, "reduc": 1, "wordsimilar": 1, "e": 5, "furthermor": 1, "g": 1, "possibl": 1, "k": 1, "equal": 1, "n": 1, "p": 4, "part": 1, "ajaiwijd": 1, "implement": 1, "roughli": 1}, "marker": "(Gabrilovich, 2002)", "article": "W10-3506", "vector_2": [8, 0.5918274859322482, 2, 1, 6, 0]}, {"label": "Neut", "current": "To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).", "context": ["Most use the vector space model to represent the context as dimensions in a vector space, where the feature are frequency of co-occurrence of the context words, and the comparison is usually the cosine similarity.", "To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).", "In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008)."], "vector_1": {"dimens": 1, "semant": 1, "featur": 1, "composit": 1, "creat": 1, "inspir": 1, "repres": 2, "go": 1, "phrase": 1, "beyond": 1, "use": 3, "cooccurr": 1, "tensor": 1, "space": 2, "sadrzadeh": 1, "complex": 1, "grefenstett": 1, "product": 1, "variou": 1, "lexic": 1, "multipl": 1, "mitchel": 2, "addit": 1, "comparison": 1, "account": 1, "word": 2, "specif": 1, "frequenc": 1, "lapata": 2, "work": 4, "vector": 3, "cosin": 1, "context": 2, "widdow": 1, "model": 4, "similar": 1, "order": 1, "usual": 1}, "marker": "(2011)", "article": "S13-2019", "vector_2": [2, 0.5225833685099198, 4, 1, 0, 0]}, {"label": "Pos", "current": "By representing the English and Chinese sentences as eI1 = e1e2...eI and cJ1 = c1c2...cJ, respectively, where ei and cj represent single elements of the sentences, we define their alignment as aK1 , of which each element is a span ak =< s, t > and represents the alignment of the English word es with the Chinese character ct. Then, the corpus of unlabeled bilingual data can be represented as the set of sentence tuples <eI1, cJ1 , aK1 > To obtain the character-based alignment, we employ an open-source toolkit Pialign1 ((Neubig et al., 2011); (Neubig et al., 2012)) which uses Bayesian learning and inversion transduction grammars.", "context": ["Then, they are associated with English words using a statistical word aligner.", "By representing the English and Chinese sentences as eI1 = e1e2...eI and cJ1 = c1c2...cJ, respectively, where ei and cj represent single elements of the sentences, we define their alignment as aK1 , of which each element is a span ak =< s, t > and represents the alignment of the English word es with the Chinese character ct. Then, the corpus of unlabeled bilingual data can be represented as the set of sentence tuples <eI1, cJ1 , aK1 > To obtain the character-based alignment, we employ an open-source toolkit Pialign1 ((Neubig et al., 2011); (Neubig et al., 2012)) which uses Bayesian learning and inversion transduction grammars.", "3.2.2 Features Obtained from the"], "vector_1": {"corpu": 1, "cj": 3, "set": 1, "ei": 3, "ak": 3, "obtain": 2, "repres": 4, "featur": 1, "learn": 1, "opensourc": 1, "respect": 1, "element": 2, "data": 1, "es": 1, "ct": 1, "use": 2, "span": 1, "pialign": 1, "associ": 1, "characterbas": 1, "3": 1, "invers": 1, "tupl": 1, "toolkit": 1, "singl": 1, "sentenc": 3, "chines": 2, "bilingu": 1, "unlabel": 1, "grammar": 1, "bayesian": 1, "cccj": 1, "eeei": 1, "word": 3, "align": 4, "charact": 1, "employ": 1, "defin": 1, "statist": 1, "english": 3, "transduct": 1}, "marker": "(Neubig et al., 2012)", "article": "D15-1142", "vector_2": [3, 0.4062061170668219, 2, 1, 1, 0]}, {"label": "Pos", "current": "We follow Taskar et al (2005) by using the first 100 test sentences for training and the remaining 347 for testing.", "context": ["Unlike the unsupervised entrants in the 2003 task, we require word-aligned training data, and therefore must cannibalise the test set for this purpose.", "We follow Taskar et al (2005) by using the first 100 test sentences for training and the remaining 347 for testing.", "This means that our results should not be directly compared to those entrants, other than in an approximate manner."], "vector_1": {"set": 1, "directli": 1, "approxim": 1, "al": 1, "unlik": 1, "result": 1, "follow": 1, "manner": 1, "et": 1, "use": 1, "compar": 1, "entrant": 2, "test": 3, "therefor": 1, "wordalign": 1, "taskar": 1, "sentenc": 1, "train": 2, "data": 1, "requir": 1, "must": 1, "task": 1, "cannibalis": 1, "mean": 1, "unsupervis": 1, "remain": 1, "purpos": 1, "first": 1}, "marker": "(2005)", "article": "P06-1009", "vector_2": [1, 0.6036014569220819, 1, 4, 0, 0]}, {"label": "Neut", "current": "Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).", "context": ["2 Related Work and Overview", "Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).", "Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage."], "vector_1": {"wikipedia": 3, "semant": 3, "larger": 1, "comput": 2, "creat": 1, "overview": 1, "ir": 1, "cost": 1, "cognit": 1, "network": 1, "memori": 1, "activ": 1, "due": 1, "coverag": 2, "linkbas": 1, "spread": 1, "knowledg": 1, "esa": 1, "appli": 1, "hurdl": 1, "method": 2, "analysi": 1, "regard": 1, "variou": 1, "relat": 1, "wordnetbas": 1, "foremost": 1, "knowledgebas": 1, "biggest": 1, "base": 2, "theori": 1, "although": 1, "arguabl": 1, "associ": 1, "model": 1, "earlier": 1, "recent": 1, "measur": 1, "success": 1, "level": 1, "outperform": 1, "work": 1, "explicit": 1, "conceptu": 2, "adequ": 1, "text": 1, "wlm": 1, "found": 1, "sa": 1, "similar": 1}, "marker": "(Preece, 1982)", "article": "W10-3506", "vector_2": [28, 0.11850900713773178, 6, 1, 0, 0]}, {"label": "Neut", "current": "In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.", "context": ["2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others.", "In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.", "As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000)."], "vector_1": {"among": 1, "work": 1, "directli": 1, "nlp": 1, "text": 2, "continu": 1, "one": 1, "pass": 1, "et": 1, "cavaglia": 1, "use": 1, "develop": 1, "kilgarriff": 1, "system": 1, "subcorpora": 1, "other": 1, "roland": 1, "sekin": 1, "quantifi": 1, "analys": 1, "accord": 1, "relat": 1, "specifi": 1, "al": 1, "mention": 1, "address": 1, "gildea": 1, "line": 1, "workstat": 1, "present": 1, "biberstyl": 1, "type": 2, "corpora": 1, "item": 1, "similar": 1, "first": 1}, "marker": "(2002)", "article": "J03-3001", "vector_2": [1, 0.7751816317748019, 5, 1, 1, 0]}, {"label": "Neut", "current": "corresponding normalized inside-outside weight for each node (Watson et al., 2005).", "context": ["$represents the statistical significance of the system against the baseline model.", "corresponding normalized inside-outside weight for each node (Watson et al., 2005).", "We perform EM starting from two initial models; either a uniform probability model, IL(), or from models derived from unambiguous training data, 'y."], "vector_1": {"em": 1, "weight": 1, "repres": 1, "signific": 1, "il": 1, "probabl": 1, "perform": 1, "system": 1, "uniform": 1, "insideoutsid": 1, "start": 1, "unambigu": 1, "baselin": 1, "statist": 1, "node": 1, "deriv": 1, "normal": 1, "initi": 1, "train": 1, "two": 1, "data": 1, "correspond": 1, "either": 1, "model": 4}, "marker": "(Watson et al., 2005)", "article": "W07-2203", "vector_2": [2, 0.7494660924972867, 1, 1, 4, 1]}, {"label": "Neut", "current": "Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.", "context": ["The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.", "Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.", "Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks."], "vector_1": {"domain": 1, "concept": 4, "identifi": 2, "creat": 1, "process": 1, "ontolog": 3, "challeng": 1, "major": 1, "exist": 1, "construct": 1, "hierarchi": 2, "extract": 2, "involv": 2, "use": 1, "top": 1, "subsumpt": 1, "two": 1, "construt": 1, "identif": 1, "basic": 1, "document": 1, "method": 1, "good": 1, "variou": 1, "relat": 1, "relev": 1, "term": 3, "task": 2, "specif": 1, "algorithm": 1, "level": 1, "learn": 2}, "marker": "(Ahmad et al., 1999", "article": "W12-5209", "vector_2": [13, 0.09967198020371756, 5, 3, 0, 0]}, {"label": "Pos", "current": "Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).", "context": ["national project on item generation for testing student competencies in solving probability problems.", "Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).", "The goal of our item generation project is to develop a model to support optimal problem and test construction."], "vector_1": {"control": 1, "set": 1, "automat": 1, "predefin": 1, "paramet": 1, "develop": 1, "goal": 1, "support": 1, "construct": 2, "compet": 1, "way": 1, "test": 2, "probabl": 1, "difficulti": 1, "optim": 1, "gener": 3, "effect": 1, "nation": 1, "base": 1, "student": 1, "solv": 1, "model": 1, "task": 1, "project": 2, "item": 4, "mani": 1, "problem": 2}, "marker": "Arendasy et al., 2006", "article": "W11-1403", "vector_2": [5, 0.05554905516878254, 4, 3, 0, 0]}, {"label": "Neut", "current": "Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "context": ["We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.", "Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed."], "vector_1": {"often": 1, "process": 1, "natur": 1, "parser": 2, "focus": 1, "finegrain": 1, "exploit": 1, "result": 1, "massiv": 1, "languag": 1, "differ": 1, "anoth": 1, "priorit": 1, "intermedi": 1, "sentenc": 1, "previou": 1, "speedup": 1, "pars": 1, "inher": 1, "parallel": 2, "agendabas": 2, "refin": 1, "work": 1, "iter": 1, "order": 1, "queue": 1, "achiev": 1, "maintain": 1, "combin": 1, "whole": 1, "magnitud": 1}, "marker": "Manousopoulou et al., 1997)", "article": "W11-2921", "vector_2": [14, 0.8912013403492238, 4, 3, 0, 0]}, {"label": "Neut", "current": "criterial features in (Hawkins and Buttery, 2010)), giving less noise to phase 2.", "context": ["The second approach counteracts this confusion by selecting the most prototypical level for an individual phenomenon (cf.", "criterial features in (Hawkins and Buttery, 2010)), giving less noise to phase 2.", "We may lose important non-best level information, but as we show in sec."], "vector_1": {"featur": 1, "phenomenon": 1, "less": 1, "individu": 1, "cf": 1, "nonbest": 1, "second": 1, "sec": 1, "counteract": 1, "select": 1, "nois": 1, "give": 1, "show": 1, "import": 1, "approach": 1, "may": 1, "criteri": 1, "prototyp": 1, "phase": 1, "level": 2, "inform": 1, "lose": 1, "confus": 1}, "marker": "(Hawkins and Buttery, 2010)", "article": "W12-2011", "vector_2": [2, 0.6473006303096739, 1, 2, 1, 0]}, {"label": "Pos", "current": "We also experimented with an ordinal regression algorithm proposed in (Shen and Joshi, 2004).", "context": ["classification algorithm.", "We also experimented with an ordinal regression algorithm proposed in (Shen and Joshi, 2004).", "For the sake of completeness, we will briefly describe the algorithm here."], "vector_1": {"classif": 1, "describ": 1, "algorithm": 3, "sake": 1, "ordin": 1, "also": 1, "briefli": 1, "complet": 1, "regress": 1, "experi": 1, "propos": 1}, "marker": "(Shen and Joshi, 2004)", "article": "N04-1023", "vector_2": [0, 0.5972838458273854, 1, 4, 2, 1]}, {"label": "Neut", "current": "Caraballo (1999) combined the lexico-syntactic patterns and distributional similarity based methods to construct ontology.", "context": ["Hybrid approaches leverage the strengths of both statistical and heuristic based approaches and often use evidences from existing knowledge bases such as wordnet, wikipedia, etc.", "Caraballo (1999) combined the lexico-syntactic patterns and distributional similarity based methods to construct ontology.", "Similarity between two nouns is calculated by computing the cosine between their respective vectors and used for hierarchical bottom-up clustering."], "vector_1": {"comput": 1, "often": 1, "hybrid": 1, "knowledg": 1, "lexicosyntact": 1, "cluster": 1, "exist": 1, "heurist": 1, "respect": 1, "wordnet": 1, "leverag": 1, "hierarch": 1, "use": 2, "strength": 1, "pattern": 1, "caraballo": 1, "wikipedia": 1, "two": 1, "construct": 1, "ontolog": 1, "approach": 2, "method": 1, "cosin": 1, "distribut": 1, "evid": 1, "base": 3, "noun": 1, "etc": 1, "calcul": 1, "vector": 1, "combin": 1, "statist": 1, "bottomup": 1, "similar": 2}, "marker": "(1999)", "article": "W12-5209", "vector_2": [13, 0.3581170512746734, 1, 2, 3, 0]}, {"label": "Neut", "current": "In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language.", "context": ["For example, Wu (1997) used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms, including multiword expressions.", "In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language.", "Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system."], "vector_1": {"identifi": 3, "parser": 1, "it": 1, "dufour": 1, "michiel": 1, "framework": 1, "bilingu": 1, "languag": 1, "employ": 1, "use": 2, "system": 1, "idiom": 1, "includ": 1, "multiword": 2, "wehrli": 1, "gener": 1, "express": 2, "french": 1, "englishchines": 1, "wu": 1, "base": 1, "translat": 1, "dictionari": 1, "compound": 1, "defi": 1, "term": 1, "grammar": 2, "english": 1, "project": 1, "mt": 1, "exampl": 1, "transduct": 1, "stochast": 1, "englishfrench": 1}, "marker": "(1998)", "article": "W03-1807", "vector_2": [5, 0.18078122435581814, 3, 3, 0, 0]}, {"label": "Pos", "current": "Following Cutler and Carter (1987)'s observation that stressed syllables tend to occur at the beginnings of words in English, Jusczyk et al (1993) investigated whether infants acquiring English take advantage of this fact.", "context": ["Lexical stress is the \"accentuation of syllables within words\" (Cutler, 2005) and has long been argued to play an important role in adult word recognition.", "Following Cutler and Carter (1987)'s observation that stressed syllables tend to occur at the beginnings of words in English, Jusczyk et al (1993) investigated whether infants acquiring English take advantage of this fact.", "Their study demonstrated that this is indeed the case for 9 month olds, although they found no indication of using stressed syllables as cues for word boundaries in 6 month olds."], "vector_1": {"infant": 1, "old": 2, "carter": 1, "advantag": 1, "within": 1, "accentu": 1, "begin": 1, "indic": 1, "month": 2, "et": 1, "follow": 1, "occur": 1, "inde": 1, "use": 1, "acquir": 1, "boundari": 1, "long": 1, "tend": 1, "recognit": 1, "cue": 1, "role": 1, "take": 1, "import": 1, "play": 1, "investig": 1, "cutler": 1, "argu": 1, "lexic": 1, "al": 1, "jusczyk": 1, "although": 1, "demonstr": 1, "case": 1, "stress": 3, "word": 4, "whether": 1, "syllabl": 3, "english": 2, "found": 1, "studi": 1, "observ": 1, "fact": 1, "adult": 1}, "marker": "(1987)", "article": "Q14-1008", "vector_2": [27, 0.10446541543652467, 3, 1, 7, 0]}, {"label": "CoCo", "current": "In order to allow for direct comparison with prior work, we used the same subset of these data as Beigman Klebanov et al (2014), in the same crossvalidation setting.", "context": ["The data is annotated according to the MIPVU procedure (Steen et al., 2010) with the interannotator reliability of  > 0.8.", "In order to allow for direct comparison with prior work, we used the same subset of these data as Beigman Klebanov et al (2014), in the same crossvalidation setting.", "The total of 90 fragments are used in cross-validation: 10-fold on News, 9-fold on Conversation, 11 on Fiction, and 12 on Academic."], "vector_1": {"subset": 1, "klebanov": 1, "set": 1, "crossvalid": 2, "al": 1, "procedur": 1, "fold": 2, "et": 1, "total": 1, "use": 2, "fiction": 1, "beigman": 1, "accord": 1, "fragment": 1, "direct": 1, "news": 1, "data": 2, "comparison": 1, "convers": 1, "reliabl": 1, "work": 1, "annot": 1, "prior": 1, "allow": 1, "mipvu": 1, "academ": 1, "interannot": 1, "order": 1}, "marker": "(2014)", "article": "W15-1402", "vector_2": [1, 0.09931052771148237, 2, 8, 5, 0]}, {"label": "Pos", "current": "We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014).", "context": ["Our difficulty prediction approach is based on the model described in the previous section.", "We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014).", "We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18"], "vector_1": {"featur": 1, "core": 1, "predict": 1, "natur": 1, "dataset": 1, "extract": 1, "languag": 1, "use": 2, "weka": 1, "describ": 1, "perform": 1, "section": 1, "classifi": 1, "process": 1, "experi": 1, "approach": 1, "tc": 1, "difficulti": 1, "tool": 1, "previou": 1, "differ": 1, "framework": 1, "base": 1, "provid": 1, "model": 1, "dkpro": 2}, "marker": "(de Castilho and Gurevych, 2014)", "article": "Q14-1040", "vector_2": [0, 0.7400198288246363, 3, 1, 4, 0]}, {"label": "Neut", "current": "Sag et al (2001b) introduced Head-driven Phrase Structure Grammar for analyzing MWEs.", "context": ["Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system.", "Sag et al (2001b) introduced Head-driven Phrase Structure Grammar for analyzing MWEs.", "Like pure statistical approaches, purely knowledgebased symbolic approaches also face problems."], "vector_1": {"identifi": 1, "sag": 1, "it": 1, "headdriven": 1, "et": 1, "phrase": 1, "also": 1, "system": 1, "idiom": 1, "analyz": 1, "mwe": 1, "wehrli": 1, "approach": 2, "symbol": 1, "introduc": 1, "pure": 2, "knowledgebas": 1, "gener": 1, "al": 1, "framework": 1, "englishfrench": 1, "compound": 1, "grammar": 2, "like": 1, "structur": 1, "face": 1, "employ": 1, "mt": 1, "statist": 1, "problem": 1}, "marker": "(2001b)", "article": "W03-1807", "vector_2": [2, 0.19161332676842277, 2, 3, 0, 0]}, {"label": "Neut", "current": "We find the exact top N consistent' most likely local model labelings using a simple dynamic program described in (Toutanova et al., 2005).", "context": ["The model is trained to re-rank a set of N likely labelings according to the local model.", "We find the exact top N consistent' most likely local model labelings using a simple dynamic program described in (Toutanova et al., 2005).", "'A labeling is consistent if satisfies the constraint that argument phrases do not overlap."], "vector_1": {"set": 1, "argument": 1, "phrase": 1, "find": 1, "dynam": 1, "use": 1, "describ": 1, "top": 1, "overlap": 1, "label": 3, "program": 1, "simpl": 1, "accord": 1, "rerank": 1, "satisfi": 1, "train": 1, "exact": 1, "like": 2, "consist": 2, "constraint": 1, "local": 2, "n": 2, "model": 3}, "marker": "(Toutanova et al., 2005)", "article": "W05-0623", "vector_2": [0, 0.5309851488997746, 1, 5, 0, 0]}, {"label": "Pos", "current": "In our tests, the best performing structure was a decision list (Rivest, 1987) of hierarchical decision trees, whose simplified basic structure is illustrated in figure 3.", "context": ["We extended the standard ID3 model (Quinlan, 1986) to more general hybrid decision structures.", "In our tests, the best performing structure was a decision list (Rivest, 1987) of hierarchical decision trees, whose simplified basic structure is illustrated in figure 3.", "Note that in the 'reduce operation tree', the system first decides whether or not to perform a reduction before deciding on a specific reduction."], "vector_1": {"oper": 1, "illustr": 1, "list": 1, "id": 1, "best": 1, "hierarch": 1, "whose": 1, "decid": 2, "perform": 2, "hybrid": 1, "system": 1, "reduct": 2, "note": 1, "figur": 1, "decis": 3, "basic": 1, "test": 1, "simplifi": 1, "extend": 1, "gener": 1, "standard": 1, "reduc": 1, "specif": 1, "whether": 1, "tree": 2, "structur": 3, "model": 1, "first": 1}, "marker": "(Rivest, 1987)", "article": "P97-1062", "vector_2": [10, 0.49153896938740055, 2, 1, 0, 0]}, {"label": "Pos", "current": "Six features from (Och, 2003) were used as baseline features.", "context": ["In (SMT Team, 2003), 450 features were generated.", "Six features from (Och, 2003) were used as baseline features.", "Each of the 450 features was evaluated independently by combining it with 6 baseline features and assessing on the test data with the minimum error training."], "vector_1": {"use": 1, "featur": 5, "evalu": 1, "independ": 1, "train": 1, "gener": 1, "six": 1, "assess": 1, "minimum": 1, "combin": 1, "error": 1, "test": 1, "data": 1, "baselin": 2}, "marker": "(Och, 2003)", "article": "N04-1023", "vector_2": [1, 0.8004166827423898, 2, 6, 5, 1]}, {"label": "Neut", "current": "Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).", "context": ["Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).", "Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).", "In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features."], "vector_1": {"classif": 1, "set": 1, "pair": 1, "featur": 1, "need": 1, "extract": 1, "pmi": 2, "select": 1, "supervis": 1, "use": 3, "neg": 1, "top": 1, "classifi": 1, "research": 1, "also": 2, "score": 2, "treat": 1, "document": 1, "approach": 2, "final": 1, "machin": 1, "candid": 2, "solv": 1, "highest": 1, "averag": 1, "task": 1, "word": 2, "algorithm": 1, "keyword": 3, "k": 1, "exampl": 1, "learn": 3, "posit": 1}, "marker": "Kerner et al., 2005", "article": "N09-1070", "vector_2": [4, 0.15750620547371136, 6, 2, 0, 0]}, {"label": "Neut", "current": "A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "context": ["6 Related Work", "A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization."], "vector_1": {"finegrain": 1, "none": 1, "relat": 2, "last": 1, "compar": 1, "directli": 1, "possibl": 1, "provid": 1, "natur": 1, "howev": 1, "work": 3, "parser": 1, "two": 1, "bodi": 1, "accumul": 1, "much": 1, "substanti": 1, "gpu": 1, "decad": 1, "parallel": 2, "languag": 1}, "marker": "Giachin and Rullent, 1989", "article": "W11-2921", "vector_2": [22, 0.8736092567868269, 4, 2, 0, 0]}, {"label": "Neut", "current": "We use these terms in Lehmann's reading: \"Primary linguistic data are [...] representations of [...] speech events with their spatio-temporal coordinates\" (Lehmann, 2005, p. 187).", "context": ["Based on this automatically generated data, several annotations have been created: 1Terms like primary and secondary data are problematic when we go beyond classical face-to-face dialogues preserved in audio and video recordings.", "We use these terms in Lehmann's reading: \"Primary linguistic data are [...] representations of [...] speech events with their spatio-temporal coordinates\" (Lehmann, 2005, p. 187).", "However, his distinction between raw (=non-symbolic) and processed (=symbolic) data (Lehmann, 2005, pp."], "vector_1": {"represent": 1, "creat": 1, "classic": 1, "process": 1, "automat": 1, "raw": 1, "linguist": 1, "video": 1, "go": 1, "beyond": 1, "primari": 2, "sever": 1, "dialogu": 1, "nonsymbol": 1, "spatiotempor": 1, "distinct": 1, "pp": 1, "event": 1, "symbol": 1, "read": 1, "speech": 1, "lehmann": 1, "87": 1, "preserv": 1, "gener": 1, "use": 1, "p": 1, "secondari": 1, "coordin": 1, "base": 1, "data": 4, "term": 2, "facetofac": 1, "like": 1, "howev": 1, "annot": 1, "record": 1, "audio": 1, "problemat": 1}, "marker": "(Lehmann, 2005, ", "article": "W13-5507", "vector_2": [8, 0.24734401810813772, 2, 2, 0, 0]}, {"label": "Neut", "current": "Previous simulations in word segmentation using the same type of distributional information as many statistical optimization-based learners but without an optimization model suggest that statistics alone are not sufficient for learning to succeed in a computationally efficient online manner; further constraints on the search space are needed (Yang, 2004).", "context": ["Statistical models provide excellent information about the features, distributional cues, and priors that can be used in learning, but provide little information about how a child learner can use this information and how her knowledge of language develops as the learning process evolves.", "Previous simulations in word segmentation using the same type of distributional information as many statistical optimization-based learners but without an optimization model suggest that statistics alone are not sufficient for learning to succeed in a computationally efficient online manner; further constraints on the search space are needed (Yang, 2004).", "Previous computational models have demanded tremendous memory and computational capacity from human learners."], "vector_1": {"tremend": 1, "featur": 1, "comput": 3, "process": 1, "excel": 1, "knowledg": 1, "manner": 1, "human": 1, "onlin": 1, "need": 1, "effici": 1, "languag": 1, "use": 3, "develop": 1, "suffici": 1, "inform": 4, "suggest": 1, "learner": 3, "capac": 1, "littl": 1, "optim": 1, "cue": 1, "evolv": 1, "type": 1, "distribut": 2, "optimizationbas": 1, "previou": 2, "succeed": 1, "child": 1, "segment": 1, "demand": 1, "memori": 1, "search": 1, "word": 1, "constraint": 1, "provid": 2, "space": 1, "alon": 1, "simul": 1, "prior": 1, "without": 1, "statist": 3, "learn": 3, "mani": 1, "model": 3}, "marker": "(Yang, 2004)", "article": "W10-2912", "vector_2": [6, 0.09014101446225081, 1, 9, 0, 0]}, {"label": "CoCo", "current": "This result is different from that in (Wu and Wang, 2004), where their method achieved an error rate reduction of 21.96% as compared with the method \"Gen+Spec\".", "context": ["Compared with the method \"ResAdapt\", our method achieves an error rate reduction of 10.15%.", "This result is different from that in (Wu and Wang, 2004), where their method achieved an error rate reduction of 21.96% as compared with the method \"Gen+Spec\".", "The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004)."], "vector_1": {"corpu": 2, "differ": 2, "compar": 2, "genspec": 1, "indomain": 1, "rate": 2, "reduct": 2, "paper": 1, "test": 1, "achiev": 2, "train": 1, "result": 1, "resadapt": 1, "error": 2, "reason": 1, "main": 1, "method": 4}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.7919782987793064, 2, 13, 0, 1]}, {"label": "Pos", "current": "To transform the semantic relatedness measure to a semantic similarity classification one, we first calculated the semantic relatedness of each word-phrase in the training set, and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999), in order to learn a set of rules that can differentiate between a positive semantic similarity and a negative one.", "context": ["The task provided an English dataset of 15,628 word-phrases, 60% annotated for training and 40% for testing, with the goal of classifying each word-phrase as either positive or negative.", "To transform the semantic relatedness measure to a semantic similarity classification one, we first calculated the semantic relatedness of each word-phrase in the training set, and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999), in order to learn a set of rules that can differentiate between a positive semantic similarity and a negative one.", "The classifier resulted in rules for the semantic network model based relatedness that could be summarized as follows: If the semantic relatedness of the word-phrase is over 61% then the similarity is positive, otherwise it is negative."], "vector_1": {"semant": 6, "set": 2, "cohen": 1, "classif": 1, "related": 4, "dataset": 1, "jrip": 1, "result": 1, "follow": 1, "summar": 1, "use": 1, "weka": 1, "goal": 1, "neg": 3, "transform": 1, "classifi": 2, "test": 1, "learn": 2, "differenti": 1, "train": 2, "ripper": 1, "wordphras": 4, "one": 2, "model": 1, "network": 1, "measur": 1, "task": 1, "algorithm": 1, "provid": 1, "could": 1, "annot": 1, "rule": 3, "calcul": 1, "base": 1, "either": 1, "english": 1, "posit": 3, "otherwis": 1, "implement": 1, "similar": 3, "order": 1, "first": 1}, "marker": "(Cohen and Singer, 1999)", "article": "S13-2019", "vector_2": [14, 0.622384369535066, 2, 2, 0, 0]}, {"label": "Neut", "current": "2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al.", "context": ["2005; Hu et al.", "2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al.", "2004 Oliver 2004, Chapter 4-5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation Hammarstrom 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentation Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101-139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al."], "vector_1": {"pauw": 1, "keshava": 1, "unclear": 2, "arabsorkhi": 1, "golcher": 1, "heali": 1, "nunzio": 1, "warlpiri": 1, "al": 3, "gikuyu": 1, "bacchin": 1, "set": 1, "bordag": 1, "pitler": 1, "freitag": 1, "et": 3, "demberg": 1, "ur": 1, "griffith": 1, "keselj": 1, "melucci": 1, "goldwat": 3, "finnishturkishenglish": 7, "englishchildenglish": 1, "robert": 1, "choudri": 1, "finnishturkish": 1, "ng": 1, "atwel": 1, "samestem": 1, "trr": 1, "bernhard": 1, "cnc": 4, "dasgupta": 2, "hammarstrom": 1, "rehman": 1, "englishturkish": 1, "lexicon": 2, "chapter": 1, "maori": 1, "johnsen": 1, "wagacha": 1, "xia": 1, "chan": 2, "relat": 1, "nc": 1, "hussain": 1, "hu": 1, "arab": 1, "catalan": 1, "analysi": 1, "englishgerman": 3, "jordan": 1, "bengali": 1, "italianenglish": 1, "segment": 15, "naradowski": 1, "a": 2, "c": 17, "b": 1, "word": 1, "tepper": 2, "johnson": 1, "oliv": 1, "xantho": 1, "majumd": 1, "ferro": 1, "de": 1, "persian": 1, "english": 2, "dang": 1, "paradigm": 4, "page": 1, "shamsfard": 1}, "marker": "a, 2002b", "article": "J11-2002", "vector_2": [9, 0.418429938426667, 5, 39, 6, 1]}, {"label": "Neut", "current": "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "context": ["Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.", "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc."], "vector_1": {"phrase": 1, "domain": 1, "en": 1, "aspect": 2, "modelis": 2, "celui": 1, "al": 1, "ce": 1, "accessibilitd": 1, "distanc": 1, "travaux": 1, "il": 1, "semblent": 1, "et": 2, "eu": 1, "sen": 1, "diver": 1, "le": 2, "la": 2, "associ": 1, "montrant": 1, "capter": 1, "densitd": 1, "dictionnair": 1, "dernier": 1, "preter": 1, "structur": 2, "effet": 1, "de": 5, "lexic": 1, "mot": 2, "compri": 1, "du": 2, "langu": 1, "mond": 1, "merveil": 1, "nombreux": 2, "pour": 2, "etc": 1, "gaum": 1, "entr": 1, "leur": 1, "lexical": 1, "dynamiqu": 1, "graph": 1, "pertin": 1, "ou": 1, "moyenn": 1, "se": 1}, "marker": "(Schvaneveldt, 1989, ", "article": "W14-6700", "vector_2": [25, 0.2616651418115279, 11, 4, 0, 1]}, {"label": "Neut", "current": "Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).", "context": ["A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle.", "Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).", "Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years."], "vector_1": {"represent": 1, "rang": 1, "explor": 1, "approxim": 1, "monoton": 1, "semant": 2, "confound": 1, "particularli": 1, "confirm": 1, "impoverish": 1, "year": 1, "rabi": 1, "extract": 1, "deepbutbrittl": 1, "involv": 1, "use": 1, "acquir": 1, "p": 1, "broad": 1, "reli": 1, "overlap": 1, "patternbas": 1, "imprecis": 1, "approach": 2, "method": 1, "match": 1, "polar": 1, "infect": 1, "relat": 1, "effect": 1, "lexic": 1, "spectrum": 1, "fairli": 1, "broadli": 1, "robust": 1, "case": 1, "measur": 1, "shallowbutrobust": 1, "success": 1, "indigen": 1, "ubiquitu": 1, "past": 1, "predicateargu": 1, "structur": 1, "infer": 1, "easili": 1, "context": 1, "neg": 1}, "marker": "(Jijkoun and de Rijke, 2005)", "article": "W07-1431", "vector_2": [2, 0.04954738718968591, 3, 1, 0, 0]}, {"label": "Pos", "current": "We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003).", "context": ["While the log-likelihood cannot be maximised for the parameters, A, in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters.", "We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003).", "Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters."], "vector_1": {"global": 1, "convex": 1, "loglinear": 1, "close": 1, "find": 1, "optimis": 2, "paramet": 3, "use": 1, "resort": 1, "perform": 1, "loglikelihood": 1, "numer": 1, "valu": 1, "method": 1, "function": 1, "quasinewton": 1, "form": 1, "optim": 1, "object": 1, "cannot": 1, "respect": 1, "requir": 1, "lbfg": 2, "maximis": 1, "thu": 1, "well": 1, "iter": 2, "gradient": 1, "train": 1, "model": 2}, "marker": "(Malouf, 2002", "article": "P06-1009", "vector_2": [4, 0.25288490996357693, 2, 1, 0, 0]}, {"label": "Weak", "current": "3 There are several other published implementation of chart parsers [23,20,33], but they often do not give much detail on the output of the parsing process, or even side-step the problem altogether [33].", "context": ["  2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are only of theoretical interest here [35,5].", "3 There are several other published implementation of chart parsers [23,20,33], but they often do not give much detail on the output of the parsing process, or even side-step the problem altogether [33].", "We do not consider here the well formed stastring tables of Shell [26] which falls somewhere in between in our classification."], "vector_1": {"classif": 1, "often": 1, "give": 1, "process": 1, "parser": 1, "cf": 1, "shell": 1, "publish": 1, "tabl": 1, "complex": 1, "sever": 1, "even": 1, "detail": 1, "theoret": 1, "much": 1, "interest": 1, "lowest": 1, "sidestep": 1, "recogn": 1, "form": 1, "asymptot": 1, "altogeth": 1, "chart": 1, "stastr": 1, "pars": 1, "consid": 2, "fall": 1, "problem": 1, "somewher": 1, "well": 1, "output": 1, "implement": 1}, "marker": "[20]", "article": "P89-1018", "vector_2": [9, 0.1401825415399017, 7, 1, 0, 0]}, {"label": "Neut", "current": "We use the same evaluation metrics as described in (Wu and Wang, 2004).", "context": ["6.2 Evaluation Metrics", "We use the same evaluation metrics as described in (Wu and Wang, 2004).", "If we use to represent SG the set of alignment links identified by the proposed methods and to denote the reference SC alignment set, the methods to calculate the precision, recall, f-measure, and alignment error rate (AER) are shown in Equation (13), (14), (15), and (16)."], "vector_1": {"set": 2, "identifi": 1, "evalu": 2, "metric": 2, "repres": 1, "rate": 1, "denot": 1, "use": 2, "describ": 1, "method": 2, "refer": 1, "recal": 1, "shown": 1, "fmeasur": 1, "link": 1, "aer": 1, "align": 3, "precis": 1, "calcul": 1, "equat": 1, "error": 1, "sc": 1, "sg": 1, "propos": 1}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.6099980623910095, 1, 13, 0, 1]}, {"label": "Neut", "current": "Bautista et al (2009) also rely on a dictionary of synonyms, but their criterion for choosing the most appropriate one is wordlength rather than frequency.", "context": ["The above approach to lexical simplification has been repeated in a number of works (Lal and Ruger, 2002; Burstein et al., 2007).", "Bautista et al (2009) also rely on a dictionary of synonyms, but their criterion for choosing the most appropriate one is wordlength rather than frequency.", "Caseli et al (2009) analyse lexical operations on a parallel corpus of original and manually simplified texts in Portuguese, using lists of simple words and discourse markers as resources."], "vector_1": {"oper": 1, "corpu": 1, "text": 1, "number": 1, "one": 1, "origin": 1, "list": 1, "bautista": 1, "marker": 1, "et": 2, "simpl": 1, "use": 1, "rather": 1, "also": 1, "reli": 1, "approach": 1, "criterion": 1, "analys": 1, "simplifi": 1, "discours": 1, "repeat": 1, "resourc": 1, "lexic": 2, "al": 2, "dictionari": 1, "word": 1, "parallel": 1, "wordlength": 1, "case": 1, "appropri": 1, "synonym": 1, "simplif": 1, "frequenc": 1, "work": 1, "manual": 1, "portugues": 1, "choos": 1}, "marker": "(2009)", "article": "W12-2202", "vector_2": [3, 0.187008992745629, 4, 1, 2, 0]}, {"label": "Neut", "current": "We rely on the Stanford parser (Klein and Manning, 2003), a Treebank-trained statistical parser, for tokenization, part-of-speech tagging, and phrase-structure parsing.", "context": ["Relative to other textual inference systems, the NatLog system does comparatively little linguistic preprocessing.", "We rely on the Stanford parser (Klein and Manning, 2003), a Treebank-trained statistical parser, for tokenization, part-of-speech tagging, and phrase-structure parsing.", "By far the most important analysis performed at this stage is monotonicity marking, in which we compute the effective mono"], "vector_1": {"comput": 1, "parser": 2, "tag": 1, "littl": 1, "compar": 1, "perform": 1, "system": 2, "textual": 1, "mark": 1, "reli": 1, "rel": 1, "preprocess": 1, "import": 1, "infer": 1, "analysi": 1, "natlog": 1, "far": 1, "treebanktrain": 1, "phrasestructur": 1, "effect": 1, "pars": 1, "partofspeech": 1, "stage": 1, "monoton": 1, "stanford": 1, "mono": 1, "token": 1, "statist": 1, "linguist": 1}, "marker": "(Klein and Manning, 2003)", "article": "W07-1431", "vector_2": [4, 0.33270470443011935, 1, 1, 2, 0]}, {"label": "Neut", "current": "(Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view.", "context": ["(Pantel et al., 2012) models latent intent to mine entity type distributions.", "(Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view.", "(Lin et al., 2012) also pays attention to refiners, but restricted to limited domains, while our method is more general."], "vector_1": {"domain": 1, "knowledg": 1, "mine": 1, "restrict": 1, "entiti": 1, "latent": 1, "graph": 1, "also": 1, "type": 1, "method": 1, "resourc": 1, "distribut": 1, "gener": 1, "util": 1, "intent": 1, "attent": 1, "refin": 1, "hetrogen": 1, "limit": 1, "pay": 1, "model": 1, "view": 1}, "marker": "(Ren et al., 2014)", "article": "D14-1114", "vector_2": [0, 0.9575459652706844, 3, 3, 2, 0]}, {"label": "Pos", "current": "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "context": ["Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.", "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort."], "vector_1": {"control": 1, "creat": 1, "al": 1, "assess": 1, "exist": 1, "expect": 1, "need": 1, "et": 1, "probabl": 1, "use": 1, "develop": 2, "pay": 1, "system": 1, "textual": 1, "genpex": 1, "also": 1, "larg": 2, "statement": 1, "test": 3, "instead": 1, "difficulti": 1, "exercis": 1, "variat": 1, "given": 1, "initi": 1, "effect": 1, "mention": 1, "holl": 1, "know": 1, "onetim": 1, "littl": 1, "effort": 1, "afterward": 1, "valu": 1, "count": 1, "great": 2, "level": 1, "invest": 1, "materi": 1, "item": 1, "amount": 2, "learn": 1}, "marker": "Holling et al., 2008", "article": "W11-1403", "vector_2": [3, 0.9378107997425846, 7, 1, 5, 0]}, {"label": "CoCo", "current": "Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners.", "context": ["The word class has been studied as a difficulty indicator by several researchers but with mixed results.", "Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners.", "Sigott (1995) could not confirm any effect of the word class on C-test difficulty."], "vector_1": {"preposit": 1, "claim": 1, "often": 1, "brown": 1, "kleinbraley": 1, "indic": 1, "result": 1, "find": 1, "sever": 1, "confirm": 1, "learner": 1, "research": 1, "mix": 1, "easier": 1, "function": 1, "difficulti": 2, "sigott": 1, "effect": 1, "solv": 1, "class": 2, "ctest": 1, "word": 3, "could": 1, "harder": 1, "studi": 1}, "marker": "(1989)", "article": "Q14-1040", "vector_2": [25, 0.407522813265079, 3, 5, 0, 0]}, {"label": "Neut", "current": "The cross-product of the stem name and (open-class) reduced core POS tags, plus the CLOSED tag, yields 24 labels for a CRF classifier in Mallet (McCallum, 2002).", "context": ["The classifier is used only to get solutions for the openclass words, although we wish to give the classifier all the words for the sentence.", "The cross-product of the stem name and (open-class) reduced core POS tags, plus the CLOSED tag, yields 24 labels for a CRF classifier in Mallet (McCallum, 2002).", "4 Experiments and Evaluation"], "vector_1": {"evalu": 1, "give": 1, "tag": 2, "close": 1, "use": 1, "mallet": 1, "solut": 1, "classifi": 3, "label": 1, "openclass": 2, "experi": 1, "po": 1, "core": 1, "plu": 1, "get": 1, "sentenc": 1, "stem": 1, "although": 1, "reduc": 1, "crossproduct": 1, "word": 2, "name": 1, "wish": 1, "yield": 1, "crf": 1}, "marker": "(McCallum, 2002)", "article": "P10-2063", "vector_2": [8, 0.6980322003577818, 1, 1, 0, 0]}, {"label": "Neut", "current": "(Ren et al., 2014) uses an unsupervised heterogeneous clustering.", "context": ["(Tan et al., 2012) encode intent in language models, aware of long-lasting interests.", "(Ren et al., 2014) uses an unsupervised heterogeneous clustering.", "(Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics."], "vector_1": {"captur": 1, "certain": 1, "encod": 1, "mine": 1, "topic": 1, "cluster": 1, "motiv": 1, "aspect": 1, "phrase": 1, "taxonomi": 1, "languag": 1, "entiti": 1, "awar": 1, "use": 1, "latent": 1, "heterogen": 1, "broad": 1, "interest": 1, "around": 1, "relationship": 1, "gener": 1, "intent": 5, "modifi": 1, "longlast": 1, "name": 1, "unsupervis": 1, "tree": 1, "model": 3, "similar": 1}, "marker": "(Ren et al., 2014)", "article": "D14-1114", "vector_2": [0, 0.9191458120531154, 4, 3, 2, 0]}, {"label": "Pos", "current": "Moving forward, we hope to expand our feature set by including the morphology of words immediately surrounding the reference, as well as a more extensive reference history, as suggested by (Favre and Bohnet, 2009).", "context": ["5 Future Work", "Moving forward, we hope to expand our feature set by including the morphology of words immediately surrounding the reference, as well as a more extensive reference history, as suggested by (Favre and Bohnet, 2009).", "We suspect that these features may play a significant role in determining the type of referenced used, the prediction of which acts as a 'bottleneck' in generating exact REs."], "vector_1": {"set": 1, "predict": 1, "move": 1, "signific": 1, "featur": 2, "use": 1, "suggest": 1, "morpholog": 1, "re": 1, "suspect": 1, "includ": 1, "forward": 1, "role": 1, "immedi": 1, "type": 1, "hope": 1, "futur": 1, "play": 1, "referenc": 1, "may": 1, "gener": 1, "extens": 1, "bottleneck": 1, "exact": 1, "expand": 1, "word": 1, "work": 1, "well": 1, "surround": 1, "histori": 1, "determin": 1, "act": 1, "refer": 2}, "marker": "(Favre and Bohnet, 2009)", "article": "W10-4231", "vector_2": [1, 0.9255184851217313, 1, 2, 0, 0]}, {"label": "Neut", "current": "We select a FrameNet (Baker et al., 1998) frame and write shallowly syntactic pattern-matching rules based on part-of-speech information and morphology from either a morphological automaton or tagged text.", "context": ["We use hfst-pmatch (Linden et al., 2013), a pattern-matching tool mimicking and extending Xerox fst (Karttunen, 2011), for demonstrating how to develop a semantic frame extractor.", "We select a FrameNet (Baker et al., 1998) frame and write shallowly syntactic pattern-matching rules based on part-of-speech information and morphology from either a morphological automaton or tagged text.", "1 Introduction"], "vector_1": {"extractor": 1, "semant": 1, "text": 1, "frame": 2, "tag": 1, "patternmatch": 2, "select": 1, "morpholog": 2, "use": 1, "develop": 1, "write": 1, "syntact": 1, "extend": 1, "mimick": 1, "tool": 1, "xerox": 1, "framenet": 1, "base": 1, "partofspeech": 1, "hfstpmatch": 1, "demonstr": 1, "introduct": 1, "rule": 1, "inform": 1, "fst": 1, "either": 1, "shallowli": 1, "automaton": 1}, "marker": "(Baker et al., 1998)", "article": "W15-1842", "vector_2": [17, 0.034119862634319265, 3, 6, 0, 0]}, {"label": "Neut", "current": "Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "context": ["2.2 Bilingual Semi-supervised CWS Methods", "Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).", "These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004)."], "vector_1": {"dictionari": 1, "individu": 1, "focus": 1, "one": 1, "cw": 1, "leverag": 2, "semisupervis": 1, "perform": 1, "segment": 2, "smt": 1, "construct": 1, "label": 2, "better": 1, "consecut": 1, "approach": 1, "method": 1, "unlabel": 1, "either": 1, "machin": 1, "map": 1, "form": 1, "sequenc": 1, "chines": 2, "previou": 1, "train": 1, "translat": 1, "although": 1, "dataset": 1, "data": 1, "model": 1, "maximummatch": 1, "word": 2, "english": 1, "work": 1, "charact": 1, "achiev": 1, "statist": 1, "bilingu": 2, "studi": 1}, "marker": "(Chung et al., 2009)", "article": "D15-1142", "vector_2": [6, 0.2968798164194952, 7, 2, 0, 0]}, {"label": "Neut", "current": "Other method similar to our work is proposed in Fountain and Lapata (2012).", "context": ["Ryu and Choi (2006) also used term frequency as a measure of domain specificity, but instead of partitioning they combined term frequency and distributional similarity to construct hierarchy.", "Other method similar to our work is proposed in Fountain and Lapata (2012).", "Fountain and Lapata (2012) proposed a graph based approach that does not require a separate term extraction step."], "vector_1": {"domain": 1, "hierarchi": 1, "extract": 1, "use": 1, "lapata": 2, "graph": 1, "construct": 1, "also": 1, "fountain": 2, "instead": 1, "ryu": 1, "approach": 1, "method": 1, "distribut": 1, "step": 1, "base": 1, "requir": 1, "measur": 1, "term": 3, "specif": 1, "choi": 1, "work": 1, "separ": 1, "combin": 1, "partit": 1, "frequenc": 2, "similar": 2, "propos": 2}, "marker": "(2012)", "article": "W12-5209", "vector_2": [0, 0.4300512171260862, 3, 3, 0, 0]}, {"label": "Neut", "current": "There have been attempts in applying other algorithms in Chinese parsing (Bikel and Chiang, 2000; Chiang and Bikel 2002; Levy and Manning 2003), but there has been no report on applying the Collins parser on Chinese.", "context": ["1999).", "There have been attempts in applying other algorithms in Chinese parsing (Bikel and Chiang, 2000; Chiang and Bikel 2002; Levy and Manning 2003), but there has been no report on applying the Collins parser on Chinese.", "The Collins parser is a lexicalized statistical parser based on a head-driven extended PCFG model; thus the choice of head node is crucial to the success of the parser."], "vector_1": {"appli": 2, "parser": 4, "bikel": 1, "headdriven": 1, "pcfg": 1, "collin": 2, "crucial": 1, "node": 1, "head": 1, "extend": 1, "chines": 2, "lexic": 1, "levi": 1, "base": 1, "pars": 1, "report": 1, "man": 1, "choic": 1, "attempt": 1, "success": 1, "algorithm": 1, "thu": 1, "statist": 1, "chiang": 1, "model": 1}, "marker": "(Bikel and Chiang, 2000", "article": "N04-1032", "vector_2": [4, 0.5920974450386215, 1, 1, 0, 0]}, {"label": "Neut", "current": "Our natural logic system, dubbed the NatLog system, has a three-stage architecture similar to those in (Marsi and Krahmer, 2005; MacCartney et al., 2006), comprising (1) linguistic pre-preprocessing, (2) alignment, and (3) entailment classification.", "context": ["3 The NatLog System", "Our natural logic system, dubbed the NatLog system, has a three-stage architecture similar to those in (Marsi and Krahmer, 2005; MacCartney et al., 2006), comprising (1) linguistic pre-preprocessing, (2) alignment, and (3) entailment classification.", "3.1 Linguistic pre-processing"], "vector_1": {"prepreprocess": 1, "classif": 1, "natlog": 2, "entail": 1, "natur": 1, "dub": 1, "system": 3, "threestag": 1, "compris": 1, "logic": 1, "preprocess": 1, "linguist": 2, "similar": 1, "align": 1, "architectur": 1}, "marker": "MacCartney et al., 2006)", "article": "W07-1431", "vector_2": [1, 0.3256412014812783, 2, 2, 1, 0]}, {"label": "CoCo", "current": "These features are similar to the position features proposed by Collobert et al (2011) for the Semantic Role Labeling task.", "context": ["Zeng et al (2014) propose the use of word position embeddings (position features) which help the CNN by keeping track of how close words are to the target nouns.", "These features are similar to the position features proposed by Collobert et al (2011) for the Semantic Role Labeling task.", "In this work we also experiment with the word position embeddings (WPE) proposed by Zeng et al (2014)."], "vector_1": {"semant": 1, "featur": 3, "help": 1, "al": 3, "zeng": 2, "et": 3, "close": 1, "use": 1, "label": 1, "also": 1, "role": 1, "cnn": 1, "experi": 1, "track": 1, "noun": 1, "wpe": 1, "task": 1, "word": 3, "target": 1, "work": 1, "keep": 1, "posit": 4, "collobert": 1, "embed": 2, "similar": 1, "propos": 3}, "marker": "(2011)", "article": "P15-1061", "vector_2": [4, 0.20082843399251737, 3, 3, 0, 0]}, {"label": "Neut", "current": "Besides, Wong and Dras (2009) show that there are no significant differences, between mother tongues, in the misuse of certain syntactic features such as subject-verb agreement that have different tendencies depending on their mother tongues.", "context": ["A similar argument can be made about some parts of gender, tense, and aspect systems.", "Besides, Wong and Dras (2009) show that there are no significant differences, between mother tongues, in the misuse of certain syntactic features such as subject-verb agreement that have different tendencies depending on their mother tongues.", "Considering these, one could not be so sure which argument is correct."], "vector_1": {"featur": 1, "show": 1, "certain": 1, "argument": 2, "one": 1, "signific": 1, "aspect": 1, "differ": 2, "subjectverb": 1, "misus": 1, "system": 1, "besid": 1, "tens": 1, "correct": 1, "sure": 1, "syntact": 1, "agreement": 1, "part": 1, "consid": 1, "depend": 1, "tendenc": 1, "made": 1, "gender": 1, "could": 1, "wong": 1, "tongu": 2, "mother": 2, "dra": 1, "similar": 1}, "marker": "(2009)", "article": "P13-1112", "vector_2": [4, 0.08655114246281245, 1, 3, 0, 0]}, {"label": "Neut", "current": "2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just \"regular\" readers (Graesser et al., 2004).", "context": ["The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.", "2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just \"regular\" readers (Graesser et al., 2004).", "Text simplification is most often performed on the sentence level."], "vector_1": {"often": 1, "text": 3, "al": 1, "impair": 1, "mild": 1, "need": 1, "et": 1, "cognit": 1, "special": 1, "differ": 1, "develop": 1, "sentenc": 1, "perform": 1, "learner": 1, "elderli": 1, "reader": 3, "languag": 1, "type": 1, "gener": 1, "poor": 1, "mainstream": 1, "peopl": 2, "methodolog": 1, "tool": 1, "dell": 1, "literaci": 1, "regular": 1, "address": 1, "aluisio": 1, "simplif": 2, "level": 2}, "marker": "(Graesser et al., 2004)", "article": "W14-5605", "vector_2": [10, 0.18475832656376928, 4, 1, 0, 0]}, {"label": "Neut", "current": "Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].", "context": [" Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].", "Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].", "Extracting word correspondences [Gale and Church, 1991a]."], "vector_1": {"word": 1, "sentenc": 1, "align": 1, "correspond": 1, "automat": 1, "disambigu": 1, "wordsens": 1, "extract": 1}, "marker": "Church and Gale, 1991]", "article": "P93-1003", "vector_2": [2, 0.05604523532346582, 7, 4, 2, 0]}, {"label": "CoCo", "current": "4While it may be surprising that disallowing reestimation of the transition function is helpful here, the same has been observed in acoustic modeling (Rabiner and Juang, 1993).", "context": ["where P, is the common word distribution, and  is", "4While it may be surprising that disallowing reestimation of the transition function is helpful here, the same has been observed in acoustic modeling (Rabiner and Juang, 1993).", "375"], "vector_1": {"function": 1, "acoust": 1, "word": 1, "distribut": 1, "may": 1, "transit": 1, "disallow": 1, "p": 1, "while": 1, "reestim": 1, "common": 1, "model": 1, "surpris": 1, "observ": 1, "help": 1}, "marker": "(Rabiner and Juang, 1993)", "article": "P05-1046", "vector_2": [12, 0.6330399223731633, 1, 1, 1, 0]}, {"label": "Pos", "current": "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).", "context": ["3.1 Internal representation", "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).", "There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures."], "vector_1": {"represent": 1, "tei": 1, "among": 1, "intern": 1, "facil": 1, "shortcom": 1, "develop": 1, "spatiotempor": 1, "graph": 1, "xce": 1, "data": 1, "complex": 1, "speech": 1, "take": 1, "nite": 1, "approach": 3, "difficult": 1, "express": 1, "variou": 1, "format": 1, "acronym": 1, "object": 1, "fiesta": 1, "standard": 1, "extens": 1, "multimod": 1, "transcript": 1, "account": 1, "made": 1, "specif": 1, "annot": 2, "structur": 1, "p": 1, "model": 1}, "marker": "(Ide et al., 2000)", "article": "W13-5507", "vector_2": [13, 0.3696104559891935, 4, 1, 1, 0]}, {"label": "Neut", "current": "Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).", "context": ["Since then, there has been a large body of work addressing the flaws of the EM-based approach.", "Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).", "Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure."], "vector_1": {"procedur": 1, "empir": 1, "pcfg": 1, "develop": 1, "smith": 2, "learnabl": 1, "larg": 1, "new": 2, "approach": 1, "introduc": 1, "function": 1, "flaw": 1, "syntact": 1, "object": 1, "bodi": 1, "embas": 1, "train": 1, "address": 1, "sinc": 1, "work": 1, "eisner": 2, "model": 1, "propos": 1}, "marker": "(Clark, 2001", "article": "P08-1100", "vector_2": [7, 0.04385164147817561, 4, 1, 1, 0]}, {"label": "Neut", "current": "We make no assumptions about the size of the grammar and we demonstrate the efficacy of our approach by implementing a decoder for the state-of-the-art latent variable grammars of Petrov et al (2006) (a.k.a.", "context": ["We present a general approach for parallelizing the CKY algorithm that can handle arbitrary context-free grammars (Section 2).", "We make no assumptions about the size of the grammar and we demonstrate the efficacy of our approach by implementing a decoder for the state-of-the-art latent variable grammars of Petrov et al (2006) (a.k.a.", "Berkeley Parser) on a Graphics Processor Unit (GPU)."], "vector_1": {"parser": 1, "al": 1, "et": 1, "unit": 1, "size": 1, "variabl": 1, "latent": 1, "section": 1, "decod": 1, "present": 1, "gpu": 1, "approach": 2, "assumpt": 1, "handl": 1, "gener": 1, "petrov": 1, "efficaci": 1, "berkeley": 1, "aka": 1, "parallel": 1, "demonstr": 1, "graphic": 1, "grammar": 3, "algorithm": 1, "arbitrari": 1, "contextfre": 1, "cki": 1, "processor": 1, "stateoftheart": 1, "implement": 1, "make": 1}, "marker": "(2006)", "article": "W11-2921", "vector_2": [5, 0.06233147464593314, 1, 4, 4, 1]}, {"label": "Neut", "current": "We next present two algorithms, MAXSIM and WIKISPREAD, for computing document similarity, and test these over the Lee (2005) document similarity dataset, a set of 50 documents between 51 and 126 words each, with the averaged gold standard similarity ratings produced by 83 test subjects (see (Lee et al., 2005)).", "context": ["This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document.", "We next present two algorithms, MAXSIM and WIKISPREAD, for computing document similarity, and test these over the Lee (2005) document similarity dataset, a set of 50 documents between 51 and 126 words each, with the averaged gold standard similarity ratings produced by 83 test subjects (see (Lee et al., 2005)).", "The first metric we propose is called MAXSIM (see Algorithm 2) and is based on the idea of measuring document similarity by pairing up each Wikipedia concept in one document's concept vector with its most similar concept in the other document."], "vector_1": {"concept": 5, "identifi": 1, "weight": 1, "vi": 1, "metric": 1, "idea": 1, "dataset": 1, "strongli": 1, "rate": 1, "set": 1, "comput": 1, "articl": 1, "id": 2, "subject": 1, "denot": 1, "gold": 1, "wikipedia": 2, "two": 1, "next": 1, "current": 1, "5": 1, "averag": 1, "6": 1, "test": 2, "call": 1, "document": 8, "lee": 1, "form": 1, "measur": 1, "relat": 1, "wi": 1, "standard": 1, "base": 1, "pair": 1, "one": 1, "present": 1, "maxsim": 2, "wikispread": 1, "word": 1, "algorithm": 2, "see": 2, "vector": 2, "w": 2, "idi": 1, "propos": 1, "similar": 5, "produc": 2, "first": 1}, "marker": "(2005)", "article": "W10-3506", "vector_2": [5, 0.7981419237886627, 2, 4, 0, 0]}, {"label": "CoCo", "current": "This method for correcting misconceptions suggests a model of natural language generation that is similar to that put forth by McKeown (1982) but which differs from McKeown's model in several ways.", "context": ["The highlighting and similarity metric used by ROMPER will be discussed below.", "This method for correcting misconceptions suggests a model of natural language generation that is similar to that put forth by McKeown (1982) but which differs from McKeown's model in several ways.", "Both McKeown and this work concentrate on determining the content and textual shape of a response."], "vector_1": {"natur": 1, "metric": 1, "shape": 1, "respons": 1, "sever": 1, "concentr": 1, "use": 1, "suggest": 1, "textual": 1, "content": 1, "misconcept": 1, "way": 1, "languag": 1, "forth": 1, "correct": 1, "gener": 1, "differ": 1, "put": 1, "discuss": 1, "work": 1, "method": 1, "determin": 1, "mckeown": 3, "highlight": 1, "model": 2, "similar": 2, "romper": 1}, "marker": "(1982)", "article": "J88-3005", "vector_2": [6, 0.492833984231081, 1, 1, 0, 0]}, {"label": "Neut", "current": "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).", "context": ["Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of \"normality\" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).", "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).", "One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set."], "vector_1": {"set": 4, "one": 1, "densiti": 1, "around": 1, "detect": 5, "network": 1, "compar": 1, "boundari": 1, "normal": 1, "classifi": 1, "write": 1, "estim": 1, "includ": 1, "deciph": 1, "test": 1, "new": 1, "probabl": 2, "approach": 2, "belong": 1, "applic": 1, "accord": 1, "deriv": 2, "outlier": 4, "object": 2, "hand": 1, "intrus": 1, "train": 2, "standard": 1, "model": 1, "fault": 1, "typic": 1, "nonoutli": 1}, "marker": "(Tax and Duin, 1998", "article": "N06-1017", "vector_2": [8, 0.4826390226983699, 8, 1, 2, 0]}, {"label": "CoCo", "current": "This feature is comparable to the semantic cache used by Brown (1989).", "context": ["not as a gap) because it facilitates the correct production for the student.", "This feature is comparable to the semantic cache used by Brown (1989).", "Phonetic complexity Wrong answer variants for C-test gaps are often rooted in phonetic problems."], "vector_1": {"brown": 1, "product": 1, "featur": 1, "often": 1, "compar": 1, "use": 1, "variant": 1, "semant": 1, "phonet": 2, "gap": 2, "wrong": 1, "cach": 1, "facilit": 1, "student": 1, "answer": 1, "problem": 1, "complex": 1, "root": 1, "correct": 1, "ctest": 1}, "marker": "(1989)", "article": "Q14-1040", "vector_2": [25, 0.46970034194605753, 1, 5, 0, 0]}, {"label": "Pos", "current": "The algorithm described here is an instance of a general approach to statistical estimation, represented by the EM algorithm [Dempster et al., 1977].", "context": ["An arbitrarily large corpus can be accommodated by segmenting it appropriately.", "The algorithm described here is an instance of a general approach to statistical estimation, represented by the EM algorithm [Dempster et al., 1977].", "In contrast to reservations that have been expressed [Gale and Church, 1991a] about using the EM algorithm to provide word correspondences, there have been no indications that prohibitive amounts of memory might be required, or that the approach lacks robustness."], "vector_1": {"corpu": 1, "em": 2, "prohibit": 1, "lack": 1, "indic": 1, "repres": 1, "arbitrarili": 1, "reserv": 1, "use": 1, "describ": 1, "memori": 1, "estim": 1, "larg": 1, "approach": 2, "contrast": 1, "gener": 1, "express": 1, "robust": 1, "segment": 1, "requir": 1, "appropri": 1, "word": 1, "algorithm": 3, "provid": 1, "might": 1, "correspond": 1, "accommod": 1, "amount": 1, "instanc": 1, "statist": 1}, "marker": "Dempster et al., 1977]", "article": "P93-1003", "vector_2": [16, 0.4676534175952104, 2, 10, 0, 0]}, {"label": "Neut", "current": "We can also add the Unique Stress Constraint (USC) (Yang, 2004) by excluding all variants of rule (18) that generate two or more stressed syllables.", "context": ["This yields the colloc3-phon-stress model.", "We can also add the Unique Stress Constraint (USC) (Yang, 2004) by excluding all variants of rule (18) that generate two or more stressed syllables.", "For example, while the lexical generator for the colloc3-nophon-stress model will include the rule Word  SSyll SSyll, the lexical generator embodying the USC lacks this rule."], "vector_1": {"lack": 1, "collocphonstress": 1, "rule": 3, "usc": 2, "two": 1, "also": 1, "add": 1, "includ": 1, "ssyll": 2, "gener": 3, "variant": 1, "lexic": 2, "collocnophonstress": 1, "stress": 2, "word": 1, "constraint": 1, "embodi": 1, "uniqu": 1, "yield": 1, "exclud": 1, "syllabl": 1, "exampl": 1, "model": 2}, "marker": "(Yang, 2004)", "article": "Q14-1008", "vector_2": [10, 0.42426718684099535, 1, 9, 3, 0]}, {"label": "Pos", "current": "To train the model, we used a negative sampling rate of 25 words, sampled from a multinomial of unigram word probabilities over all the vocabulary (Goldberg and Levy, 2014).", "context": ["The words that occurred less than 40 times in the data were discarded from the vocabulary.", "To train the model, we used a negative sampling rate of 25 words, sampled from a multinomial of unigram word probabilities over all the vocabulary (Goldberg and Levy, 2014).", "Embeddings of 50, 200, 400 and 600 dimensions were trained."], "vector_1": {"dimens": 1, "unigram": 1, "use": 1, "word": 3, "multinomi": 1, "less": 1, "neg": 1, "embed": 1, "0": 1, "rate": 1, "train": 2, "sampl": 2, "vocabulari": 2, "time": 1, "discard": 1, "model": 1, "data": 1, "occur": 1, "probabl": 1}, "marker": "(Goldberg and Levy, 2014)", "article": "S15-2109", "vector_2": [1, 0.5932728023111845, 1, 1, 0, 0]}, {"label": "Neut", "current": "In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.", "context": ["6, this paper reveals several crucial findings that contribute to improving native language identification.", "In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.", "The rest of this paper is structured as follows."], "vector_1": {"show": 1, "rest": 1, "one": 1, "paper": 3, "famili": 1, "follow": 1, "find": 2, "languag": 2, "crucial": 1, "identif": 1, "sever": 1, "contribut": 2, "nativ": 1, "reconstruct": 1, "addit": 1, "reveal": 1, "task": 1, "central": 1, "could": 1, "tree": 1, "histor": 1, "structur": 1, "improv": 1, "linguist": 1}, "marker": "Nakhleh et al., 2005)", "article": "P13-1112", "vector_2": [8, 0.12737310228492563, 5, 2, 2, 0]}, {"label": "Neut", "current": "Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009).", "context": ["934", "Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009).", "We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3)."], "vector_1": {"machin": 1, "use": 1, "set": 1, "identifi": 1, "section": 1, "erron": 1, "oracl": 1, "system": 1, "also": 1, "combin": 1, "multisourc": 1, "decod": 1, "difficult": 1, "refer": 1, "translat": 2}, "marker": "Koehn, 2007)", "article": "D10-1091", "vector_2": [3, 0.2207660338150562, 2, 2, 14, 0]}, {"label": "Neut", "current": "To transform the semantic relatedness measure to a semantic similarity classification one, we first calculated the semantic relatedness of each word-phrase in the training set, and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999), in order to learn a set of rules that can differentiate between a positive semantic similarity and a negative one.", "context": ["The task provided an English dataset of 15,628 word-phrases, 60% annotated for training and 40% for testing, with the goal of classifying each word-phrase as either positive or negative.", "To transform the semantic relatedness measure to a semantic similarity classification one, we first calculated the semantic relatedness of each word-phrase in the training set, and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999), in order to learn a set of rules that can differentiate between a positive semantic similarity and a negative one.", "The classifier resulted in rules for the semantic network model based relatedness that could be summarized as follows: If the semantic relatedness of the word-phrase is over 61% then the similarity is positive, otherwise it is negative."], "vector_1": {"semant": 6, "set": 2, "cohen": 1, "classif": 1, "related": 4, "dataset": 1, "jrip": 1, "result": 1, "follow": 1, "summar": 1, "use": 1, "weka": 1, "goal": 1, "neg": 3, "transform": 1, "classifi": 2, "test": 1, "learn": 2, "differenti": 1, "train": 2, "ripper": 1, "wordphras": 4, "one": 2, "model": 1, "network": 1, "measur": 1, "task": 1, "algorithm": 1, "provid": 1, "could": 1, "annot": 1, "rule": 3, "calcul": 1, "base": 1, "either": 1, "english": 1, "posit": 3, "otherwis": 1, "implement": 1, "similar": 3, "order": 1, "first": 1}, "marker": "(Witten et al., 1999)", "article": "S13-2019", "vector_2": [14, 0.622384369535066, 2, 2, 0, 0]}, {"label": "Weak", "current": "Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006).", "context": ["The volume of information available to users on the World Wide Web is growing at an exponential rate (Lyman and Varian, 2003).", "Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006).", "In recent years, much research attention has therefore been given to semantic techniques of information retrieval."], "vector_1": {"semant": 1, "synonymi": 1, "natur": 1, "ir": 1, "one": 1, "rate": 1, "year": 1, "suffer": 1, "sever": 1, "web": 1, "differ": 2, "accur": 1, "multipl": 1, "appear": 1, "exponenti": 1, "system": 1, "research": 1, "avail": 1, "much": 1, "larg": 1, "languag": 1, "techniqu": 1, "therefor": 1, "notabl": 1, "polysemi": 1, "inabl": 1, "word": 3, "given": 1, "volum": 1, "user": 1, "attent": 1, "govern": 1, "world": 1, "grow": 1, "recent": 1, "wide": 1, "retriev": 2, "current": 1, "ambigu": 1, "keywordmatch": 1, "inform": 3, "limit": 1, "context": 1, "model": 1, "mean": 2}, "marker": "(Metzler and Croft, 2006)", "article": "W10-3506", "vector_2": [4, 0.033536009668038824, 2, 1, 0, 0]}, {"label": "Pos", "current": "For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).", "context": ["Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model.", "For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).", "Following the learning stage, each vector in the test data set is labeled with a predicted word and sense."], "vector_1": {"semant": 1, "set": 1, "predict": 2, "follow": 1, "label": 1, "previous": 1, "use": 2, "memori": 1, "next": 1, "process": 1, "vector": 1, "disambigu": 1, "timbl": 1, "test": 2, "sens": 3, "run": 1, "made": 1, "base": 1, "data": 1, "stage": 1, "task": 1, "word": 3, "algorithm": 1, "separ": 1, "exampl": 1, "learn": 4, "found": 1, "model": 1}, "marker": "(Mihalcea, 2002)", "article": "P05-3014", "vector_2": [3, 0.7244623655913979, 3, 2, 2, 1]}, {"label": "Neut", "current": "Information Quality Dimensions (Source: Strong, Lee, Wang, 1997, p.39)", "context": ["Table 1.", "Information Quality Dimensions (Source: Strong, Lee, Wang, 1997, p.39)", "Empirical attempts to assess quality have primarily focused on counting hyperlinks in a networked environment."], "vector_1": {"dimens": 1, "count": 1, "sourc": 1, "attempt": 1, "network": 1, "primarili": 1, "p": 1, "hyperlink": 1, "qualiti": 2, "focus": 1, "assess": 1, "inform": 1, "environ": 1, "empir": 1, "tabl": 1}, "marker": "Strong, Lee, Wang, 1997, ", "article": "N03-2033", "vector_2": [6, 0.24273086764363805, 1, 1, 1, 0]}, {"label": "Neut", "current": "The study of natural logic was formalized by Johan van Benthem, who crucially connected it with categorial grammar (van Benthem, 1986), and later was brought to fruition by Victor Sanchez Valencia, who first gave a precise definition of a calculus of mono", "context": ["While the roots of natural logic can be traced back to Aristotle's syllogisms, the modern conception of natural logic began with George Lakoff, who proposed \"a logic for natural language\" which could \"characterize all the valid inferences that can be made in natural language\" (Lakoff, 1970).", "The study of natural logic was formalized by Johan van Benthem, who crucially connected it with categorial grammar (van Benthem, 1986), and later was brought to fruition by Victor Sanchez Valencia, who first gave a precise definition of a calculus of mono", "199"], "vector_1": {"sanchez": 1, "concept": 1, "benthem": 1, "natur": 5, "modern": 1, "back": 1, "brought": 1, "connect": 1, "gave": 1, "languag": 2, "victor": 1, "definit": 1, "van": 1, "began": 1, "character": 1, "calculu": 1, "georg": 1, "categori": 1, "valid": 1, "valencia": 1, "fruition": 1, "infer": 1, "first": 1, "crucial": 1, "syllog": 1, "trace": 1, "lakoff": 1, "formal": 1, "made": 1, "grammar": 1, "mono": 1, "could": 1, "later": 1, "precis": 1, "aristotl": 1, "logic": 4, "studi": 1, "johan": 1, "root": 1, "propos": 1}, "marker": "(van Benthem, 1986)", "article": "W07-1431", "vector_2": [21, 0.934131120559594, 2, 1, 0, 0]}, {"label": "Neut", "current": "In his Xtract system, Smadja (1993) first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance, strength and spread, and then examined concordances of the bi-grams to find longer frequent multiword units.", "context": ["In this tool, they first collect candidate nominal terms with a POS syntactic pattern filter, then use concordances to identify frequently co-occurring multiword units.", "In his Xtract system, Smadja (1993) first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance, strength and spread, and then examined concordances of the bi-grams to find longer frequent multiword units.", "Similarly, Merkel and Andersson (2000) compared frequency-based and entropy based algorithms, each of which was combined with a language filter."], "vector_1": {"distanc": 1, "identifi": 1, "concord": 2, "entropi": 1, "within": 1, "frequencybas": 1, "signific": 1, "merkel": 1, "bigram": 1, "extract": 1, "find": 1, "unit": 2, "use": 2, "strength": 1, "nomin": 1, "pattern": 1, "system": 1, "spread": 1, "score": 1, "call": 1, "smadja": 1, "multiword": 2, "similarli": 1, "algorithm": 1, "languag": 1, "xtract": 1, "po": 1, "singl": 1, "compar": 1, "syntact": 2, "tool": 1, "cooccur": 2, "candid": 1, "examin": 1, "pair": 1, "term": 1, "word": 1, "longer": 1, "consist": 1, "andersson": 1, "structur": 1, "filter": 2, "collect": 1, "base": 1, "combin": 1, "statist": 1, "frequent": 2, "first": 2}, "marker": "(1993)", "article": "W03-1807", "vector_2": [10, 0.1399146561628098, 2, 2, 0, 0]}, {"label": "Neut", "current": "There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).", "context": ["They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).", "There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).", "(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data."], "vector_1": {"semant": 2, "show": 2, "process": 1, "eg": 1, "signific": 1, "extract": 1, "leverag": 1, "use": 2, "ratio": 1, "compar": 1, "perform": 2, "also": 1, "speech": 2, "rel": 1, "approach": 1, "resourc": 1, "broadcast": 1, "base": 1, "news": 1, "data": 1, "task": 1, "retriev": 1, "keyword": 2, "frequenc": 1, "work": 1, "yield": 1, "combin": 1, "verif": 1, "idf": 1, "improv": 2, "similar": 1}, "marker": "Rogina, 2002)", "article": "N09-1070", "vector_2": [7, 0.2224622030237581, 6, 2, 0, 0]}, {"label": "Neut", "current": "Following Pradhan et al (2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software.", "context": ["Since SVMs are binary classifiers, we represent this 1-of-19 classification problem (18 roles plus NULL) by training 19 binary one-versus-all classifiers.", "Following Pradhan et al (2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software.", "The system uses a polynominal kernel with degree 2; the cost per unit violation of the margin, C=1; tolerance of the termination criterion e=0.001."], "vector_1": {"softwar": 1, "classif": 1, "kernel": 1, "oneversusal": 1, "al": 1, "repres": 1, "matsumoto": 1, "cost": 1, "per": 1, "follow": 1, "et": 1, "toler": 1, "null": 1, "unit": 1, "use": 2, "tinysvm": 1, "violat": 1, "polynomin": 1, "system": 1, "classifi": 2, "binari": 2, "e000": 1, "role": 1, "yamcha": 1, "9": 1, "8": 1, "of9": 1, "plu": 1, "kudo": 1, "00": 1, "test": 1, "train": 2, "termin": 1, "pradhan": 1, "along": 1, "sinc": 1, "c": 1, "svm": 2, "criterion": 1, "degre": 1, "problem": 1, "margin": 1}, "marker": "(2003)", "article": "N04-1032", "vector_2": [1, 0.20858915956955174, 1, 3, 0, 0]}, {"label": "Neut", "current": "Bordim et al (2002) present a CKY parser that is implemented on a field-programmable gate array (FPGA) and report a speedup of up to 750x.", "context": ["Chart-based parsing on the other hand allows us to expose and exploit the abundant parallelism of the dynamic program.", "Bordim et al (2002) present a CKY parser that is implemented on a field-programmable gate array (FPGA) and report a speedup of up to 750x.", "However, this hardware approach suffers from insufficient memory or logic elements and limits the number of rules in the grammar to 2,048 and the number of non-terminal symbols."], "vector_1": {"abund": 1, "nontermin": 1, "parser": 1, "al": 1, "exploit": 1, "rule": 1, "et": 1, "array": 1, "suffer": 1, "dynam": 1, "number": 2, "expos": 1, "memori": 1, "speedup": 1, "insuffici": 1, "program": 1, "bordim": 1, "fieldprogramm": 1, "gate": 1, "approach": 1, "fpga": 1, "symbol": 1, "hand": 1, "hardwar": 1, "pars": 1, "chartbas": 1, "report": 1, "parallel": 1, "present": 1, "grammar": 1, "howev": 1, "cki": 1, "us": 1, "element": 1, "limit": 1, "allow": 1, "logic": 1, "x": 1, "implement": 1}, "marker": "(2002)", "article": "W11-2921", "vector_2": [9, 0.9071703447734234, 1, 1, 0, 0]}, {"label": "Weak", "current": "Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations.", "context": ["Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics.", "Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations.", "Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy."], "vector_1": {"bleu": 2, "semant": 2, "evalu": 1, "metric": 2, "foundat": 1, "automat": 1, "done": 1, "significantli": 1, "human": 1, "ter": 1, "orient": 1, "sever": 1, "use": 1, "explicitli": 1, "fact": 1, "littl": 1, "research": 1, "wer": 1, "metaevalu": 1, "larg": 1, "meteor": 1, "decad": 1, "main": 1, "refer": 1, "machin": 2, "surfaceform": 1, "niel": 1, "disagre": 1, "lexic": 1, "driven": 1, "scale": 1, "translat": 3, "half": 1, "report": 1, "judgment": 1, "recent": 1, "reflect": 1, "like": 1, "cder": 1, "mt": 2, "common": 1, "adequaci": 1, "similar": 1, "nist": 1}, "marker": "(Papineni et al., 2002)", "article": "W14-4719", "vector_2": [12, 0.08985449272463623, 8, 2, 0, 0]}, {"label": "Pos", "current": "In (Shen and Joshi, 2004), we have introduced a new perceptron-like ordinal regression algorithm for parse reranking.", "context": ["The reranking problem is reduced to a classification problem by using pairwise samples.", "In (Shen and Joshi, 2004), we have introduced a new perceptron-like ordinal regression algorithm for parse reranking.", "In that algorithm, pairwise samples are used for training and margins are defined as the distance between parses of different ranks."], "vector_1": {"reduc": 1, "use": 2, "distanc": 1, "algorithm": 2, "rerank": 2, "margin": 1, "pars": 2, "classif": 1, "rank": 1, "perceptronlik": 1, "defin": 1, "train": 1, "pairwis": 2, "new": 1, "regress": 1, "sampl": 2, "problem": 2, "differ": 1, "ordin": 1, "introduc": 1}, "marker": "(Shen and Joshi, 2004)", "article": "N04-1023", "vector_2": [0, 0.268297388016513, 1, 4, 2, 1]}, {"label": "Neut", "current": "The DTM2 model differs from other phrasebased SMT models in that it avoids the redundancy present in other systems by extracting from a word aligned parallel corpora a set of minimal phrases such that no two phrases overlap with each other (Hassan et al., 2009).", "context": ["e = arg max Pr(e|f) e M (3) = arg max mm(f, e) e m=1", "The DTM2 model differs from other phrasebased SMT models in that it avoids the redundancy present in other systems by extracting from a word aligned parallel corpora a set of minimal phrases such that no two phrases overlap with each other (Hassan et al., 2009).", "The decoding strategy in DTM2 (Ittycheriah and Roukos, 2007) is similar to a phrase-based decoder except that the score of a particular translation block is obtained from the maximum entropy model using the set of feature functions."], "vector_1": {"pref": 1, "entropi": 1, "obtain": 1, "two": 1, "set": 2, "arg": 2, "phrase": 2, "extract": 1, "differ": 1, "minim": 1, "avoid": 1, "smt": 1, "system": 1, "overlap": 1, "score": 1, "decod": 2, "featur": 1, "except": 1, "strategi": 1, "function": 1, "max": 2, "use": 1, "translat": 1, "particular": 1, "word": 1, "redund": 1, "mmf": 1, "parallel": 1, "present": 1, "e": 4, "dtm": 2, "align": 1, "corpora": 1, "m": 1, "maximum": 1, "phrasebas": 2, "model": 3, "similar": 1, "block": 1}, "marker": "(Hassan et al., 2009)", "article": "W10-3805", "vector_2": [1, 0.3164802150452623, 2, 2, 0, 0]}, {"label": "Neut", "current": "of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.", "context": ["Table 4: Performance of the reranker.", "of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.", "The information of the previous labels was also quite effective, which indicates that label unigram models (i.e."], "vector_1": {"also": 1, "featur": 1, "unigram": 1, "contribut": 1, "rerank": 1, "perform": 2, "shallow": 1, "previou": 2, "parser": 1, "quit": 1, "indic": 1, "label": 2, "inform": 1, "effect": 1, "larg": 1, "tabl": 1, "studi": 1, "ie": 1, "model": 1}, "marker": "(Kim et al., 2004", "article": "W07-1033", "vector_2": [3, 0.7420465337132004, 3, 3, 3, 0]}, {"label": "Neut", "current": "The Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) built in Sheffield was used to evaluate our approach.", "context": ["We use an English semantic tagger (USAS) developed at Lancaster University to identify multiword units which depict single semantic concepts.", "The Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) built in Sheffield was used to evaluate our approach.", "In our evaluation, this approach extracted a total of 4,195 MWE candidates, of which, after manual checking, 3,792 were accepted as valid MWEs, producing a precision of 90.39% and an estimated recall of 39.38%."], "vector_1": {"corpu": 1, "semant": 2, "concept": 1, "identifi": 1, "evalu": 2, "meter": 1, "tagger": 1, "total": 1, "extract": 1, "check": 1, "unit": 1, "use": 2, "lancast": 1, "develop": 1, "built": 1, "usa": 1, "univers": 1, "valid": 1, "multiword": 1, "approach": 2, "singl": 1, "recal": 1, "mwe": 2, "sheffield": 1, "depict": 1, "accept": 1, "manual": 1, "precis": 1, "candid": 1, "estim": 1, "english": 1, "produc": 1}, "marker": "Clough et al., 2002)", "article": "W03-1807", "vector_2": [1, 0.03692762186115214, 2, 3, 2, 0]}, {"label": "Neut", "current": "In the original formulation for English in Gildea and Jurafsky (2002), it answers the question: Is the NP governed by IP or VP?", "context": ["feature is only applicable for NPs.", "In the original formulation for English in Gildea and Jurafsky (2002), it answers the question: Is the NP governed by IP or VP?", "An NP governed by an IP is likely to be a subject, while an NP governed by a VP is more likely to be an object."], "vector_1": {"origin": 1, "applic": 1, "featur": 1, "gildea": 1, "like": 2, "jurafski": 1, "ip": 2, "govern": 3, "question": 1, "object": 1, "vp": 2, "answer": 1, "formul": 1, "english": 1, "np": 4, "subject": 1}, "marker": "(2002)", "article": "N04-1032", "vector_2": [2, 0.32389251997095136, 1, 4, 0, 0]}, {"label": "Neut", "current": "The Berkeley Parser4 (Petrov et al., 2006), in contrast, is based on a method for automatically finding useful subcategorizations during training by splitting and merging the original nodes.", "context": ["Their subcategorizations were developed by hand based on linguistic intuitions and a careful error analysis.", "The Berkeley Parser4 (Petrov et al., 2006), in contrast, is based on a method for automatically finding useful subcategorizations during training by splitting and merging the original nodes.", "The model is an unlexicalized generative PCFG, but the granularity of the terminal and nonterminal categories found in training give it a much greater sensitivity to the syntactic behaviour of words and phrases than is possible using standard POS tags."], "vector_1": {"origin": 1, "node": 1, "give": 1, "parser": 1, "automat": 1, "unlexic": 1, "tag": 1, "phrase": 1, "find": 1, "pcfg": 1, "merg": 1, "categori": 1, "use": 2, "develop": 1, "sensit": 1, "granular": 1, "much": 1, "split": 1, "subcategor": 2, "termin": 1, "method": 1, "contrast": 1, "analysi": 1, "syntact": 1, "greater": 1, "gener": 1, "intuit": 1, "hand": 1, "behaviour": 1, "train": 2, "po": 1, "berkeley": 1, "standard": 1, "care": 1, "word": 1, "possibl": 1, "nontermin": 1, "base": 2, "error": 1, "found": 1, "model": 1, "linguist": 1}, "marker": "(Petrov et al., 2006)", "article": "W15-2610", "vector_2": [9, 0.4834137793217942, 1, 2, 8, 0]}, {"label": "Neut", "current": "The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus.", "context": ["All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).", "The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus.", "We align the human transcripts and ASR output, then map the human annotated DA boundaries and topic boundaries to the ASR words, such that we have human annotation of these information for the ASR output."], "vector_1": {"corpu": 2, "telephon": 1, "map": 1, "obtain": 1, "system": 1, "topic": 2, "rate": 1, "human": 3, "extract": 1, "boundari": 2, "transcrib": 1, "speech": 1, "asr": 4, "act": 1, "da": 2, "entir": 1, "meet": 1, "transcript": 1, "word": 2, "convers": 1, "align": 1, "sri": 1, "annot": 3, "inform": 1, "stateoftheart": 1, "dialog": 1, "error": 1, "output": 3, "summari": 1}, "marker": "(Zhu et al., 2005)", "article": "N09-1070", "vector_2": [4, 0.2575674543051481, 3, 1, 2, 0]}, {"label": "Pos", "current": "In another current effort, we work on an interface to upper-level ontologies (Reiter, 2007) in order to access more \"world-knowledge\" which is a desideratum in natural language processing in general, as in many approaches to textual entailment.", "context": ["As we cannot expect the necessary amount of training data to be available in the near future, we currently investigate the data more closely in order to arrive at a more controlled model of textual entailment.", "In another current effort, we work on an interface to upper-level ontologies (Reiter, 2007) in order to access more \"world-knowledge\" which is a desideratum in natural language processing in general, as in many approaches to textual entailment.", "Acknowledgements"], "vector_1": {"control": 1, "entail": 2, "process": 1, "ontolog": 1, "upperlevel": 1, "arriv": 1, "desideratum": 1, "expect": 1, "close": 1, "data": 2, "languag": 1, "avail": 1, "anoth": 1, "access": 1, "amount": 1, "textual": 2, "current": 2, "interfac": 1, "futur": 1, "natur": 1, "approach": 1, "investig": 1, "acknowledg": 1, "gener": 1, "cannot": 1, "worldknowledg": 1, "effort": 1, "work": 1, "near": 1, "train": 1, "necessari": 1, "mani": 1, "model": 1, "order": 2}, "marker": "(Reiter, 2007)", "article": "W07-1402", "vector_2": [0, 0.9745767154164701, 1, 1, 0, 0]}, {"label": "Pos", "current": "The system of McDonald et al (2006) achieved the best average parsing performance over 13 languages (excluding English) in CoNLL-2006 shared tasks.", "context": ["different languages.", "The system of McDonald et al (2006) achieved the best average parsing performance over 13 languages (excluding English) in CoNLL-2006 shared tasks.", "Its average UAS and LAS are 87.03% and 80.83%, respectively, while our average UAS and LAS excluding English are 87.79% and 81.29%."], "vector_1": {"conll": 1, "differ": 1, "task": 1, "la": 2, "mcdonald": 1, "share": 1, "system": 1, "al": 1, "perform": 1, "achiev": 1, "pars": 1, "best": 1, "english": 2, "averag": 3, "et": 1, "exclud": 2, "ua": 2, "respect": 1, "languag": 2}, "marker": "(2006)", "article": "D15-1154", "vector_2": [9, 0.7876468182083796, 1, 8, 2, 0]}, {"label": "Pos", "current": "We use the SALSA tool developed at Saarbrucken University (Erk and Pado, 2004) which also assumes TIGER-XML input.", "context": ["For example, we are currently experimenting with the annotation of semantic frames on top of the treebanks.", "We use the SALSA tool developed at Saarbrucken University (Erk and Pado, 2004) which also assumes TIGER-XML input.", ""], "vector_1": {"semant": 1, "univers": 1, "develop": 1, "saarbrucken": 1, "tigerxml": 1, "frame": 1, "use": 1, "tool": 1, "annot": 1, "current": 1, "also": 1, "treebank": 1, "exampl": 1, "input": 1, "salsa": 1, "experi": 1, "assum": 1, "top": 1}, "marker": "(Erk and Pado, 2004)", "article": "W06-2717", "vector_2": [2, 0.9782268041237113, 1, 1, 0, 0]}, {"label": "Neut", "current": "We lemmatized and POS-tagged the German GUR350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict", "context": ["The first task is predicting semantic similarity.", "We lemmatized and POS-tagged the German GUR350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict", "2Downloadable from: http://goo.gl/bFokI"], "vector_1": {"semant": 1, "set": 1, "creat": 1, "predict": 2, "goodenough": 1, "rubenstein": 1, "dataset": 2, "human": 1, "download": 1, "analog": 1, "german": 1, "wellknown": 1, "postag": 1, "pair": 1, "judgment": 1, "task": 1, "word": 1, "lemmat": 1, "gur": 1, "httpgooglbfoki": 1, "english": 1, "similar": 2, "first": 1}, "marker": "(Zesch et al., 2007)", "article": "P13-2128", "vector_2": [6, 0.6074579240442971, 2, 4, 0, 0]}, {"label": "Pos", "current": "We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18", "context": ["We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014).", "We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18", "6.1 Classification vs Regression"], "vector_1": {"core": 1, "use": 2, "featur": 1, "weka": 1, "tc": 1, "framework": 1, "process": 1, "natur": 1, "tool": 1, "differ": 1, "classifi": 1, "dataset": 1, "provid": 1, "vs": 1, "regress": 1, "perform": 1, "experi": 1, "extract": 1, "dkpro": 2, "languag": 1, "classif": 1}, "marker": "(Hall et al., 2009)", "article": "Q14-1040", "vector_2": [5, 0.7420634117718471, 3, 1, 0, 0]}, {"label": "Neut", "current": "Collins (1999) is a detailed exposition of one such ongoing line of research which utilizes the Wall Street Journal (WSJ) sections of the Penn Treebank (PTB).", "context": ["Extant statistical parsers require extensive and detailed treebanks, as many of their lexical and structural parameters are estimated in a fullysupervised fashion from treebank derivations.", "Collins (1999) is a detailed exposition of one such ongoing line of research which utilizes the Wall Street Journal (WSJ) sections of the Penn Treebank (PTB).", "However, there are disadvantages to this approach."], "vector_1": {"fashion": 1, "wall": 1, "treebank": 3, "parser": 1, "ptb": 1, "one": 1, "street": 1, "disadvantag": 1, "extant": 1, "paramet": 1, "fullysupervis": 1, "collin": 1, "section": 1, "detail": 2, "research": 1, "estim": 1, "penn": 1, "approach": 1, "ongo": 1, "deriv": 1, "journal": 1, "lexic": 1, "util": 1, "extens": 1, "exposit": 1, "line": 1, "requir": 1, "howev": 1, "wsj": 1, "structur": 1, "statist": 1, "mani": 1}, "marker": "(1999)", "article": "W07-2203", "vector_2": [8, 0.038966495116059235, 1, 2, 0, 0]}, {"label": "Neut", "current": "Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves.", "context": ["The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked.", "Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves.", "However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results."], "vector_1": {"set": 2, "circumv": 1, "hard": 1, "automat": 1, "bias": 1, "result": 1, "need": 1, "close": 1, "open": 1, "select": 1, "use": 1, "easi": 1, "space": 1, "make": 1, "solut": 4, "distractor": 4, "valid": 1, "sever": 1, "approach": 1, "correct": 1, "sure": 1, "option": 1, "presenc": 1, "guess": 1, "candid": 1, "enabl": 1, "provid": 1, "howev": 1, "lead": 1, "random": 1, "pick": 1, "propos": 1}, "marker": "Zesch and Melamud, 2014)", "article": "Q14-1040", "vector_2": [0, 0.11231612811848735, 2, 1, 3, 0]}, {"label": "Pos", "current": "We also plan to apply self-training of n-best tagger which successfully boosted the performance of one of the best existing English syntactic parser (McClosky et al., 2006).", "context": ["We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase.", "We also plan to apply self-training of n-best tagger which successfully boosted the performance of one of the best existing English syntactic parser (McClosky et al., 2006).", ""], "vector_1": {"featur": 1, "appli": 2, "parser": 1, "readili": 1, "tagger": 1, "expect": 1, "best": 1, "syntact": 1, "selftrain": 1, "perform": 1, "avail": 1, "also": 1, "boost": 1, "architectur": 1, "qualifi": 1, "string": 1, "rerank": 2, "candid": 1, "plan": 1, "elabor": 1, "phase": 1, "one": 1, "nbest": 1, "success": 1, "algorithm": 1, "dictionarybas": 1, "stringmatch": 1, "accommod": 1, "english": 1, "exist": 1}, "marker": "(McClosky et al., 2006)", "article": "W07-1033", "vector_2": [1, 0.980650522317189, 1, 1, 0, 0]}, {"label": "Neut", "current": "Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).", "context": ["1 Introduction and Motivation", "Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).", "One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011)."], "vector_1": {"rang": 2, "help": 1, "intellig": 1, "challeng": 1, "one": 1, "topic": 1, "see": 1, "motiv": 1, "abil": 1, "increasingli": 1, "sever": 1, "detect": 1, "hoo": 1, "learner": 1, "research": 1, "write": 1, "languag": 2, "recent": 1, "type": 1, "assist": 1, "learn": 1, "autom": 1, "ical": 1, "becom": 1, "strand": 1, "introduct": 1, "task": 1, "computerassist": 1, "focu": 1, "exampl": 1, "determin": 1, "error": 2, "popular": 1}, "marker": "Yannakoudakis et al., 2011)", "article": "W12-2011", "vector_2": [1, 0.028805456593891783, 5, 3, 1, 0]}, {"label": "Neut", "current": "Keystroke dynamics looks at the speed at which a users hands move across a keyboard (Bergadano et al., 2002).", "context": ["(Cohen Priva et al., 2010)).", "Keystroke dynamics looks at the speed at which a users hands move across a keyboard (Bergadano et al., 2002).", "It has the distinct advantage of using written text, with clear word and sentence boundaries, while combining it with dynamic production features, allowing for greater insight into the language creation process."], "vector_1": {"featur": 1, "creation": 1, "advantag": 1, "process": 1, "text": 1, "move": 1, "keyboard": 1, "speed": 1, "dynam": 2, "use": 1, "distinct": 1, "boundari": 1, "written": 1, "languag": 1, "across": 1, "product": 1, "insight": 1, "greater": 1, "sentenc": 1, "hand": 1, "user": 1, "keystrok": 1, "word": 1, "look": 1, "clear": 1, "combin": 1, "allow": 1}, "marker": "(Bergadano et al., 2002)", "article": "W15-0914", "vector_2": [13, 0.0659058487874465, 2, 1, 0, 0]}, {"label": "Neut", "current": "Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.", "context": ["One obvious possibility is the extension of Tax and Duin's method to more than one nearest training neighbor for a more accurate estimate of local density.", "Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.", "Our immediate goal is to use unknown sense detection in combination with WSD, to filter out items that the WSD system cannot handle due to missing senses."], "vector_1": {"nearest": 1, "featur": 1, "detect": 2, "obviou": 1, "tax": 1, "one": 2, "miss": 1, "filter": 1, "use": 1, "accur": 1, "goal": 1, "unknown": 1, "system": 1, "estim": 1, "densiti": 1, "test": 1, "sens": 2, "immedi": 1, "local": 1, "method": 1, "neighbor": 1, "outlier": 1, "handl": 1, "duin": 1, "gener": 1, "sophist": 1, "extens": 1, "train": 1, "task": 1, "word": 1, "wsd": 2, "furthermor": 1, "possibl": 1, "approach": 1, "employ": 1, "item": 1, "vector": 1, "combin": 1, "due": 1, "context": 1, "cannot": 1}, "marker": "Marsland, 2003)", "article": "N06-1017", "vector_2": [3, 0.9739488997649235, 3, 3, 0, 0]}, {"label": "CoCo", "current": "These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008) (p = 0.69) and ESA (Gabrilovich and Markovitch, 2007) (p = 0.75).", "context": ["Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (pmax,wlm = 0.60 vs pmax,cos = 0.70).", "These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008) (p = 0.69) and ESA (Gabrilovich and Markovitch, 2007) (p = 0.75).", "Maximum path length Lp,max is related to how far one node can spread its activation in the network."], "vector_1": {"set": 1, "aaco": 1, "one": 1, "secondli": 1, "vs": 1, "sampl": 1, "result": 2, "network": 1, "compar": 1, "lpmax": 1, "activ": 1, "favour": 1, "spread": 2, "esa": 1, "length": 1, "strategi": 2, "node": 1, "far": 1, "relat": 1, "pmaxco": 1, "aawlm": 1, "report": 1, "path": 1, "pmaxwlm": 1, "outperform": 1, "maximum": 1, "p": 2, "interconcept": 1, "wlm": 1, "similar": 1, "significantli": 1}, "marker": "(Gabrilovich and Markovitch, 2007)", "article": "W10-3506", "vector_2": [3, 0.6805392952906076, 2, 6, 1, 0]}, {"label": "Neut", "current": "Although literate people produce text on a nearly daily basis, researchers have gone so far as to call the writing process one of the most complex and demanding activities that humans engage in (Alves et al., 2008, p. 2).", "context": ["On the cognitive side, a typist must undertake the cognitively demanding task of text production.", "Although literate people produce text on a nearly daily basis, researchers have gone so far as to call the writing process one of the most complex and demanding activities that humans engage in (Alves et al., 2008, p. 2).", "The act of typing involves juggling both the high-level text creation process, and low-level motor execution."], "vector_1": {"gone": 1, "engag": 1, "execut": 1, "process": 2, "text": 3, "creation": 1, "one": 1, "human": 1, "cognit": 2, "involv": 1, "activ": 1, "research": 1, "write": 1, "complex": 1, "call": 1, "juggl": 1, "lowlevel": 1, "must": 1, "type": 1, "product": 1, "liter": 1, "nearli": 1, "far": 1, "peopl": 1, "although": 1, "demand": 2, "basi": 1, "motor": 1, "task": 1, "side": 1, "p": 1, "undertak": 1, "act": 1, "highlevel": 1, "produc": 1, "daili": 1, "typist": 1}, "marker": "(Alves et al., 2008, ", "article": "W15-0914", "vector_2": [7, 0.27263093539841043, 1, 4, 0, 0]}, {"label": "Neut", "current": "We could have partially alleviated this effect by employing head-word triggers as done in Zhou et al (2004), but we decided to use backward tagging because the results of a number of preliminary experiments, including the ones shown in Table 2 above, seemed to be showing that the backward tagging is preferable in this task setting.", "context": ["In biomedical named-entity tagging, right boundaries are usually easier to detect, and it may be the reason of the superiority of the backward tagging.", "We could have partially alleviated this effect by employing head-word triggers as done in Zhou et al (2004), but we decided to use backward tagging because the results of a number of preliminary experiments, including the ones shown in Table 2 above, seemed to be showing that the backward tagging is preferable in this task setting.", "3.2 Feature set"], "vector_1": {"right": 1, "partial": 1, "namedent": 1, "show": 1, "prefer": 1, "al": 1, "biomed": 1, "one": 1, "tag": 4, "done": 1, "result": 1, "tabl": 1, "et": 1, "seem": 1, "featur": 1, "superior": 1, "decid": 1, "detect": 1, "boundari": 1, "3": 1, "trigger": 1, "includ": 1, "set": 2, "experi": 1, "easier": 1, "zhou": 1, "preliminari": 1, "may": 1, "use": 1, "effect": 1, "number": 1, "reason": 1, "task": 1, "shown": 1, "allevi": 1, "could": 1, "employ": 1, "headword": 1, "backward": 3, "usual": 1}, "marker": "(2004)", "article": "W07-1033", "vector_2": [3, 0.45572174738841403, 1, 5, 0, 0]}, {"label": "Pos", "current": "We use version 1.3 of DERIVBASE (Zeller et al., 2013),1 a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 nonsingleton derivational families.", "context": ["For German, there are several resources with derivational information.", "We use version 1.3 of DERIVBASE (Zeller et al., 2013),1 a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 nonsingleton derivational families.", "It has a precision of 84% and a recall of 71%."], "vector_1": {"precis": 1, "use": 1, "7000": 1, "resourc": 2, "deriv": 2, "german": 1, "noun": 1, "derivbas": 1, "group": 1, "avail": 1, "inform": 1, "version": 1, "nonsingleton": 1, "7": 1, "famili": 1, "verb": 1, "adject": 1, "freeli": 1, "recal": 1, "sever": 1}, "marker": "(Zeller et al., 2013)", "article": "P13-2128", "vector_2": [0, 0.3267847087530501, 1, 1, 4, 0]}, {"label": "Pos", "current": "Okanohara et al (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state.", "context": ["After the shared task, several researchers tackled the problem using the CRFs and their extensions.", "Okanohara et al (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state.", "Friedrich et al (2006) used CRFs with features from the external gazetteer."], "vector_1": {"gazett": 1, "featur": 1, "appli": 1, "share": 1, "al": 2, "et": 2, "sever": 1, "use": 2, "multipl": 1, "tackl": 1, "research": 1, "friedrich": 1, "state": 1, "treat": 1, "okanohara": 1, "singl": 1, "extens": 1, "semicrf": 1, "extern": 1, "task": 1, "word": 1, "correspond": 1, "crf": 2, "problem": 1}, "marker": "(2006)", "article": "W07-1033", "vector_2": [1, 0.27148622981956316, 3, 2, 4, 0]}, {"label": "Neut", "current": "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).", "context": ["Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of \"normality\" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).", "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).", "One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set."], "vector_1": {"set": 4, "one": 1, "densiti": 1, "around": 1, "detect": 5, "network": 1, "compar": 1, "boundari": 1, "normal": 1, "classifi": 1, "write": 1, "estim": 1, "includ": 1, "deciph": 1, "test": 1, "new": 1, "probabl": 2, "approach": 2, "belong": 1, "applic": 1, "accord": 1, "deriv": 2, "outlier": 4, "object": 2, "hand": 1, "intrus": 1, "train": 2, "standard": 1, "model": 1, "fault": 1, "typic": 1, "nonoutli": 1}, "marker": "(Hickinbotham and Austin, 2000)", "article": "N06-1017", "vector_2": [6, 0.4826390226983699, 8, 1, 0, 0]}, {"label": "Neut", "current": "To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).", "context": ["Most use the vector space model to represent the context as dimensions in a vector space, where the feature are frequency of co-occurrence of the context words, and the comparison is usually the cosine similarity.", "To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).", "In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008)."], "vector_1": {"dimens": 1, "semant": 1, "featur": 1, "composit": 1, "creat": 1, "inspir": 1, "repres": 2, "go": 1, "phrase": 1, "beyond": 1, "use": 3, "cooccurr": 1, "tensor": 1, "space": 2, "sadrzadeh": 1, "complex": 1, "grefenstett": 1, "product": 1, "variou": 1, "lexic": 1, "multipl": 1, "mitchel": 2, "addit": 1, "comparison": 1, "account": 1, "word": 2, "specif": 1, "frequenc": 1, "lapata": 2, "work": 4, "vector": 3, "cosin": 1, "context": 2, "widdow": 1, "model": 4, "similar": 1, "order": 1, "usual": 1}, "marker": "(2008)", "article": "S13-2019", "vector_2": [5, 0.5225833685099198, 4, 3, 0, 0]}, {"label": "Pos", "current": "We used the StackingC implementation of WEKA (Seewald, 2002) to combine the models, with a linear regression model as our meta classifier.", "context": ["We trained three classification models on the entire feature set, using the same train-test sets as explained before and trained an ensemble model with three classifiers.", "We used the StackingC implementation of WEKA (Seewald, 2002) to combine the models, with a linear regression model as our meta classifier.", "Table 4 shows the classification accuracies for the individual classifiers as well as the ensemble on a 10-fold CV of the training set and on the held out test set."], "vector_1": {"classif": 2, "featur": 1, "show": 1, "train": 3, "traintest": 1, "fold": 1, "set": 4, "tabl": 1, "cv": 1, "use": 2, "weka": 1, "explain": 1, "three": 2, "classifi": 3, "accuraci": 1, "test": 1, "individu": 1, "linear": 1, "entir": 1, "model": 4, "held": 1, "stackingc": 1, "well": 1, "meta": 1, "combin": 1, "ensembl": 2, "regress": 1, "implement": 1}, "marker": "(Seewald, 2002)", "article": "W13-1708", "vector_2": [11, 0.6185211816166337, 1, 1, 0, 0]}, {"label": "Pos", "current": "To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout (Witten and Frank, 2005): Given a sample test set of N pairs of words with human-assigned ratings of relatedness, randomly divide this set into k parts of roughly equal size3.", "context": ["50 word-pairs from the WordSimilarity-353 dataset (Gabrilovich, 2002) and correlating our method's scores with the human-assigned scores.", "To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout (Witten and Frank, 2005): Given a sample test set of N pairs of words with human-assigned ratings of relatedness, randomly divide this set into k parts of roughly equal size3.", "Hold out one part of the data and iteratively evaluate the performance of the algorithm on the remaining k1 parts until all k parts have been held out once."], "vector_1": {"randomli": 1, "set": 3, "evalu": 1, "related": 1, "dataset": 1, "held": 1, "rate": 1, "correl": 1, "sampl": 2, "holdout": 1, "happen": 1, "size": 1, "one": 1, "given": 1, "hold": 1, "techniqu": 3, "divid": 1, "perform": 2, "favour": 1, "score": 2, "wordpair": 1, "test": 1, "method": 1, "repeat": 1, "overestim": 1, "wordsimilar": 1, "part": 4, "pair": 1, "humanassign": 2, "data": 1, "reduc": 1, "word": 1, "algorithm": 1, "furthermor": 1, "possibl": 1, "k": 3, "equal": 1, "iter": 1, "n": 1, "remain": 1, "implement": 1, "roughli": 1}, "marker": "(Witten and Frank, 2005)", "article": "W10-3506", "vector_2": [5, 0.6083688961063485, 2, 1, 7, 0]}, {"label": "Pos", "current": "In order to calculate relative frequencies of the English words, the English sense-tagged corpus SemCor (Miller et al., 1993) was used.", "context": ["In addition, the relative frequencies of the English word and its translation were checked.", "In order to calculate relative frequencies of the English words, the English sense-tagged corpus SemCor (Miller et al., 1993) was used.", "For Dutch, such a resource was not available."], "vector_1": {"corpu": 1, "use": 1, "word": 2, "frequenc": 2, "dutch": 1, "order": 1, "avail": 1, "semcor": 1, "calcul": 1, "translat": 1, "rel": 2, "english": 3, "resourc": 1, "sensetag": 1, "check": 1, "addit": 1}, "marker": "(Miller et al., 1993)", "article": "W14-0118", "vector_2": [21, 0.33665195203656745, 1, 1, 2, 0]}, {"label": "Pos", "current": "Using the switchboard corpus, (Stolcke et al., 2000) achieved good dialogue act labeling accuracy (71% on manual transcriptions) for a set of 42 dialogue act types, and constructed probabilistic models of dialogue act sequencing in order to test the hypothesis that dialogue act sequence information could boost speech recognition performance.", "context": ["Their maximum entropy ranking approach achieved 90% accuracy on the 4-way classification into agreement, disagreement, backchannel and other.", "Using the switchboard corpus, (Stolcke et al., 2000) achieved good dialogue act labeling accuracy (71% on manual transcriptions) for a set of 42 dialogue act types, and constructed probabilistic models of dialogue act sequencing in order to test the hypothesis that dialogue act sequence information could boost speech recognition performance.", "There has been far less work on developing manual and automatic dialogue act annotation schemes for email."], "vector_1": {"corpu": 1, "classif": 1, "set": 1, "entropi": 1, "less": 1, "rank": 1, "annot": 1, "type": 1, "label": 1, "backchannel": 1, "dialogu": 5, "develop": 1, "perform": 1, "construct": 1, "accuraci": 2, "recognit": 1, "2": 1, "speech": 1, "way": 1, "test": 1, "scheme": 1, "disagr": 1, "email": 1, "good": 1, "hypothesi": 1, "far": 1, "sequenc": 2, "use": 1, "probabilist": 1, "agreement": 1, "boost": 1, "automat": 1, "switchboard": 1, "transcript": 1, "approach": 1, "could": 1, "work": 1, "manual": 2, "maximum": 1, "inform": 1, "achiev": 2, "act": 5, "model": 1, "order": 1}, "marker": "(Stolcke et al., 2000)", "article": "W09-3953", "vector_2": [9, 0.15833743795703406, 1, 2, 2, 0]}, {"label": "Neut", "current": "We introduced a simple procedure for recognition of reducible sequences in (Marecek and Zabokrtsky, 2012): The particular sequence of words is removed from the sentence and if the remainder of the sentence exists elsewhere in the corpus, the sequence is considered reducible.", "context": ["3.1 Recognition of reducible sequences", "We introduced a simple procedure for recognition of reducible sequences in (Marecek and Zabokrtsky, 2012): The particular sequence of words is removed from the sentence and if the remainder of the sentence exists elsewhere in the corpus, the sequence is considered reducible.", "We provide an example in Figure 2."], "vector_1": {"reduc": 3, "corpu": 1, "word": 1, "sentenc": 2, "provid": 1, "sequenc": 4, "remov": 1, "elsewher": 1, "procedur": 1, "recognit": 2, "figur": 1, "exampl": 1, "exist": 1, "consid": 1, "particular": 1, "simpl": 1, "introduc": 1, "remaind": 1}, "marker": "(Marecek and Zabokrtsky, 2012)", "article": "P13-1028", "vector_2": [1, 0.259224534236632, 1, 8, 1, 0]}]