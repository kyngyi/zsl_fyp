{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FYP -SpaceCnn",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWvybEnjh0sf",
        "colab_type": "code",
        "outputId": "a8487cfe-68e4-417c-f4bc-d21b0180120f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8IbBDQwoGu8",
        "colab_type": "code",
        "outputId": "2b8b4dec-c2cf-4e3b-f6d6-9ecf07b29cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "pip install json-lines"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting json-lines\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/0f/79c96c0d26b276c583484fe8209e5ebbb416a920309568650325f6e1de73/json_lines-0.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from json-lines) (1.12.0)\n",
            "Installing collected packages: json-lines\n",
            "Successfully installed json-lines-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8ekuuz7nMNA",
        "colab_type": "code",
        "outputId": "1d84b195-06ab-4b69-bde8-b77056bc0037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "pip install sentence-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/91/c85ddef872d5bb39949386930c1f834ac382e145fcd30155b09d6fb65c5a/sentence-transformers-0.2.5.tar.gz (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
            "\u001b[?25hCollecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.28.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.17.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 54.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (1.11.15)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.14.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0->sentence-transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5-cp36-none-any.whl size=64942 sha256=5835c3537011e47e1f13a890a153320ca542c6bd6b9e843cdf7bfcc6f7826c7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/ce/39/5bbda8ac34eb52df8c6531382ca077773fbfcbfb6386e5d66c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=eef7a92aa83bcc42796f1d8e9f4fea86abe6db1e21d0d881ae47ddbdd0f58c79\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.38 sentence-transformers-0.2.5 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1H_Y6_-dhNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnxfWXxjL4LR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Config\n",
        "\"\"\"\n",
        "Configuration file for the project.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Base directory.\n",
        "\"\"\"\n",
        "PWD = '/content/drive/My Drive/KY, FYP/Code/'\n",
        "\n",
        "\"\"\"\n",
        "File directories.\n",
        "\"\"\"\n",
        "# Directory for the word embeddings\n",
        "GLOVE_DIR = PWD + '/glove.6B'\n",
        "\n",
        "# Directory for storing citation function data\n",
        "DATA_DIR = PWD + '/data/data'\n",
        "\n",
        "\"\"\"\n",
        "Data files: the citation and provenance dataset.\n",
        "MTL refers to the aligned dataset.\n",
        "\"\"\"\n",
        "DATA_FILES = {\n",
        "    'func': {\n",
        "        'golden_train': 'processed/golden_train.func.json',\n",
        "        'golden_test': 'processed/golden_test.func.json',\n",
        "    },\n",
        "    'scicite': {\n",
        "        'train': 'scicite/train.jsonl',\n",
        "        'test': 'scicite/test.jsonl',\n",
        "        'dev': 'scicite/dev.jsonl'\n",
        "    },\n",
        "    'acl-arc': {\n",
        "        'train': 'acl-arc/train.jsonl',\n",
        "        'test': 'acl-arc/test.jsonl',\n",
        "        'dev': 'acl-arc/dev.jsonl'\n",
        "    },\n",
        "    'prov': {\n",
        "        'golden_train': 'processed/golden_train.prov.json',\n",
        "        'golden_test': 'processed/golden_test.prov.json',\n",
        "    },\n",
        "    'mtl': {\n",
        "        'golden_train': 'processed/golden_train.mtl.json',\n",
        "        'golden_test': 'processed/golden_test.mtl.json'\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yy6scFMoNxK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Data\n",
        "\"\"\"\n",
        "Common data operations.\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "import json_lines\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def read_json_data(filename):\n",
        "    \"\"\"\n",
        "    Read the given JSON file.\n",
        "    \"\"\"\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    with open(path, 'rb') as fp:\n",
        "        content = json.load(fp)\n",
        "        return content\n",
        "\n",
        "def read_jsonl_data(filename):\n",
        "    \"\"\"\n",
        "    Read the given JSONL file.\n",
        "    \"\"\"\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    content = []\n",
        "    print (type(content))\n",
        "    with open(path, 'rb') as fp:\n",
        "        for item in json_lines.reader(fp):\n",
        "            content.append(item)\n",
        "        return content\n",
        "\n",
        "\"\"\"\n",
        "Custom cross validation.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def compress_y(ys):\n",
        "    \"\"\"\n",
        "    For each y in ys, if y is of the form [0 0 ... 1 ... 0], compress it to a\n",
        "    single integer.\n",
        "    \"\"\"\n",
        "    if len(ys) < 1:\n",
        "        return ys\n",
        "\n",
        "    if isinstance(ys[0], np.ndarray):\n",
        "        # A hack >.<\n",
        "        return map(lambda x: x.tolist().index(1), ys)\n",
        "    else:\n",
        "        return ys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZDbEnBl0GFy",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "c3a7912c-c0a4-4912-d1f4-5b58fd035f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title acl-arc (6 Classes)\n",
        "\n",
        "# Integrated Cosine Sim into the model\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "# import lib.logger, os, sys, random, math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def embed_sentence(sentence):\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    message_embeddings = session.run(embed(sentence))\n",
        "    return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "    # Flatten feature vector\n",
        "    flat_dim_size = np.prod(model.output_shape[1:])\n",
        "    x = Reshape(target_shape=(flat_dim_size,),\n",
        "                name='features_flat')(model.output)\n",
        "\n",
        "    # Dot product between feature vector and reference vectors\n",
        "    x = Dense(units=output_size,\n",
        "              activation='linear',\n",
        "              use_bias=False)(x)\n",
        "\n",
        "    classifier = Model(inputs=[model.input], outputs=x)\n",
        "    return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "seed = 1020\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "               'Extends': 5}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['acl-arc']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "# random.shuffle(dataset_func)\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "  \n",
        "s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "words = {}\n",
        "\n",
        "# ## Raw Description\n",
        "# words[0] = [\"P provides relevant information for this domain.\"]\n",
        "# words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "# words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "# words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "# words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "# words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "# Summarized\n",
        "words[0] = [\"provides relevant information\"]\n",
        "words[1] = [\"Illustrates need\"]\n",
        "words[2] = [\"Uses\"]\n",
        "words[3] = [\"Extends\"]\n",
        "words[4] = [\"similarity differences\"]\n",
        "words[5] = [\"Potential Future\"]\n",
        "\n",
        "# Class Title\n",
        "# words[0] = [\"Background\"]\n",
        "# words[1] = [\"Motivation\"]\n",
        "# words[2] = [\"Uses\"]\n",
        "# words[3] = [\"Extension\"]\n",
        "# words[4] = [\"Compare Or Contrast\"]\n",
        "# words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # words[i] = embed_sentence(words[i])\n",
        "    words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "for i,element in enumerate(ys):\n",
        "    ys[i] = words[ys[i]]\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_prov_all = []\n",
        "y_test_prov_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_only_prov_all = []\n",
        "y_test_only_prov_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# -------------------------\n",
        "texts = map(lambda d: d['text'], dataset_func)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "ys = np.asarray(ys)\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "encoded_classes = words[0]\n",
        "for i in range(len(words)-1):\n",
        "    encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=42)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "indices = []\n",
        "indices_type = []\n",
        "\n",
        "# ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerA\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "# preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "preds = Dense(768, name=\"output_layer\")(x)\n",
        "output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, output_reshape)\n",
        "\n",
        "model.compile(loss=cosine_proximity,\n",
        "            # optimizer='adam',\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# import datetime\n",
        "# from keras.callbacks import TensorBoard\n",
        "\n",
        "# log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "# model.load_weights('model-scicite.h5', by_name=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "# model.save_weights('model-acl.h5')\n",
        "\n",
        "new_model = build_knn(model, encoded_classes.shape[1])\n",
        "print(new_model.summary())\n",
        "encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "temp_weights = new_model.get_weights()\n",
        "temp_weights[-1] = encoded_classes_norm\n",
        "new_model.set_weights(temp_weights)\n",
        "\n",
        "y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test_list = []\n",
        "sim = {}\n",
        "\n",
        "for i, sample in enumerate(y_pred_probs):\n",
        "    for j in range(len(funcs_index)):\n",
        "        if np.array_equal(y_test[i], words[j]):\n",
        "            y_test_list.append(j)\n",
        "\n",
        "y_test = y_test_list\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "        # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:38<00:00, 10.5MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 1827 texts.\n",
            "Found 6073 unique tokens.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerA (Embedding) (None, 50, 100)           607400    \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 768)               99072     \n",
            "_________________________________________________________________\n",
            "reshape_layer (Reshape)      (None, 1, 768)            0         \n",
            "=================================================================\n",
            "Total params: 787,112\n",
            "Trainable params: 787,112\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 1168 samples, validate on 293 samples\n",
            "Epoch 1/15\n",
            "1168/1168 [==============================] - 1s 1ms/sample - loss: -0.8382 - acc: 0.6036 - val_loss: -0.8980 - val_acc: 0.9590\n",
            "Epoch 2/15\n",
            "1168/1168 [==============================] - 1s 781us/sample - loss: -0.9070 - acc: 0.9683 - val_loss: -0.8996 - val_acc: 0.9590\n",
            "Epoch 3/15\n",
            "1168/1168 [==============================] - 1s 793us/sample - loss: -0.9119 - acc: 0.9683 - val_loss: -0.9131 - val_acc: 0.9590\n",
            "Epoch 4/15\n",
            "1168/1168 [==============================] - 1s 784us/sample - loss: -0.9253 - acc: 0.9683 - val_loss: -0.9241 - val_acc: 0.9590\n",
            "Epoch 5/15\n",
            "1168/1168 [==============================] - 1s 799us/sample - loss: -0.9385 - acc: 0.9683 - val_loss: -0.9302 - val_acc: 0.9590\n",
            "Epoch 6/15\n",
            "1168/1168 [==============================] - 1s 795us/sample - loss: -0.9503 - acc: 0.9675 - val_loss: -0.9204 - val_acc: 0.9590\n",
            "Epoch 7/15\n",
            "1168/1168 [==============================] - 1s 789us/sample - loss: -0.9607 - acc: 0.9683 - val_loss: -0.9284 - val_acc: 0.9590\n",
            "Epoch 8/15\n",
            "1168/1168 [==============================] - 1s 794us/sample - loss: -0.9681 - acc: 0.9649 - val_loss: -0.9309 - val_acc: 0.9590\n",
            "Epoch 9/15\n",
            "1168/1168 [==============================] - 1s 799us/sample - loss: -0.9753 - acc: 0.9666 - val_loss: -0.9280 - val_acc: 0.9522\n",
            "Epoch 10/15\n",
            "1168/1168 [==============================] - 1s 811us/sample - loss: -0.9807 - acc: 0.9769 - val_loss: -0.9354 - val_acc: 0.9590\n",
            "Epoch 11/15\n",
            "1168/1168 [==============================] - 1s 794us/sample - loss: -0.9845 - acc: 0.9906 - val_loss: -0.9333 - val_acc: 0.9590\n",
            "Epoch 12/15\n",
            "1168/1168 [==============================] - 1s 830us/sample - loss: -0.9884 - acc: 0.9966 - val_loss: -0.9380 - val_acc: 0.9590\n",
            "Epoch 13/15\n",
            "1168/1168 [==============================] - 1s 817us/sample - loss: -0.9902 - acc: 0.9949 - val_loss: -0.9407 - val_acc: 0.9590\n",
            "Epoch 14/15\n",
            "1168/1168 [==============================] - 1s 811us/sample - loss: -0.9917 - acc: 0.9983 - val_loss: -0.9339 - val_acc: 0.9590\n",
            "Epoch 15/15\n",
            "1168/1168 [==============================] - 1s 811us/sample - loss: -0.9923 - acc: 0.9974 - val_loss: -0.9353 - val_acc: 0.9590\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerA (Embedding) (None, 50, 100)           607400    \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 768)               99072     \n",
            "_________________________________________________________________\n",
            "reshape_layer (Reshape)      (None, 1, 768)            0         \n",
            "_________________________________________________________________\n",
            "features_flat (Reshape)      (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 4608      \n",
            "=================================================================\n",
            "Total params: 791,720\n",
            "Trainable params: 791,720\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Plain_Func\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5556    0.3846    0.4545        65\n",
            "           1     0.6667    0.9231    0.7742       182\n",
            "           2     0.5000    0.0769    0.1333        13\n",
            "           3     0.7255    0.4868    0.5827        76\n",
            "           4     1.0000    0.1875    0.3158        16\n",
            "           5     0.5385    0.5000    0.5185        14\n",
            "\n",
            "    accuracy                         0.6585       366\n",
            "   macro avg     0.6644    0.4265    0.4632       366\n",
            "weighted avg     0.6629    0.6585    0.6251       366\n",
            "\n",
            "Finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uQhU4hEdvmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# files.download('SpacePrediction_title_SBERT.h5') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SRrQ-rX73tI",
        "colab_type": "code",
        "outputId": "92d26671-df04-4117-895e-75d6c720a874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'drive', 'model-acl.h5', 'model-scicite.h5', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjvAuPzspsx8",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Commented out ( f(x,y) = 0,1)) unfinished\n",
        "\n",
        "\n",
        "# # Experiment f(x,y) = 0,1 -------- EXP2\n",
        "# # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "# \"\"\"\n",
        "# Perform the experiments on bootstrapped data and actual annotated data.\n",
        "# \"\"\"\n",
        "# # import lib.logger, os, sys, random, math\n",
        "# import numpy as np\n",
        "# import os\n",
        "\n",
        "# from functools import reduce\n",
        "\n",
        "# # import config.config as config\n",
        "# # import data.data as data\n",
        "# # import data.data_func as data_func\n",
        "# import sklearn.metrics as metrics\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from sklearn.model_selection import KFold\n",
        "# import pandas as pd\n",
        "\n",
        "# # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "# from sklearn.utils import class_weight\n",
        "\n",
        "# # import keras.backend as K\n",
        "# from tensorflow.keras import utils\n",
        "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "#     GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "# from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "# from tensorflow.keras.models import Model, Sequential, load_model\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.losses import cosine_proximity\n",
        "\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# # import matplotlib.pyplot as plt\n",
        "# # from sklearn.decomposition import PCA\n",
        "\n",
        "# import random\n",
        "# \"\"\"\n",
        "# Set random seed and fix bug on Dropout usage.\n",
        "# \"\"\"\n",
        "# import tensorflow as tf\n",
        "\n",
        "# import tensorflow_hub as hub\n",
        "\n",
        "# def embed_sentence(sentence):\n",
        "#   with tf.Session() as session:\n",
        "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "#     message_embeddings = session.run(embed(sentence))\n",
        "#     return message_embeddings\n",
        "\n",
        "# def ilen(iterable):\n",
        "#     return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "# def build_knn(model, output_size):\n",
        "#     # Flatten feature vector\n",
        "#     flat_dim_size = np.prod(model.output_shape[1:])\n",
        "#     x = Reshape(target_shape=(flat_dim_size,),\n",
        "#                 name='features_flat')(model.output)\n",
        "\n",
        "#     # Dot product between feature vector and reference vectors\n",
        "#     x = Dense(units=output_size,\n",
        "#               activation='linear',\n",
        "#               use_bias=False)(x)\n",
        "\n",
        "#     classifier = Model(inputs=[model.input], outputs=x)\n",
        "#     return classifier\n",
        "\n",
        "# def normalize_encodings(encodings):\n",
        "#     ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "#     return encodings / ref_norms\n",
        "\n",
        "# seed = 1020\n",
        "# np.random.seed(seed)\n",
        "# # tf.python.control_flow_ops = tf\n",
        "# tf.compat.v1.set_random_seed(seed)\n",
        "# random.seed(seed)\n",
        "\n",
        "# MAX_NB_WORDS = 20000\n",
        "# MAX_SEQUENCE_LENGTH = 50\n",
        "# EMBEDDING_DIM = 100\n",
        "\n",
        "# \"\"\"\n",
        "# Data reading and saving from disk (so that data processing is done only once).\n",
        "# \"\"\"\n",
        "# directory = DATA_DIR\n",
        "# funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "#                'Extends': 5}\n",
        "\n",
        "\n",
        "# # Function dataset start\n",
        "# datafiles = DATA_FILES['acl-arc']\n",
        "# test = read_jsonl_data(datafiles['test'])\n",
        "# train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "# dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "# dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "# train_data = list(filter(lambda d: d['intent'] != 'Error', train))\n",
        "# test_data = list(filter(lambda d: d['intent'] != 'Error', test))\n",
        "\n",
        "# #\n",
        "# # dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "# # embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# # print(\"loaded Hub Module\")\n",
        "\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "  \n",
        "# s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# words = {}\n",
        "\n",
        "# # ## Raw Description\n",
        "# # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "# # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "# # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "# # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "# # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "# # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "# # Summarized\n",
        "# # words[0] = [\"provides relevant information\"]\n",
        "# # words[1] = [\"Illustrates need\"]\n",
        "# # words[2] = [\"Uses\"]\n",
        "# # words[3] = [\"Extends\"]\n",
        "# # words[4] = [\"similarity differences\"]\n",
        "# # words[5] = [\"Potential Future\"]\n",
        "\n",
        "# # Class Title\n",
        "# words[0] = [\"Background\"]\n",
        "# words[1] = [\"Motivation\"]\n",
        "# words[2] = [\"Uses\"]\n",
        "# words[3] = [\"Extension\"]\n",
        "# words[4] = [\"Compare Or Contrast\"]\n",
        "# words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "# for i, word in enumerate(words):\n",
        "#     # words[i] = embed_sentence(words[i])\n",
        "#     words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# # Function dataset end\n",
        "# #############################################################################3\n",
        "\n",
        "# # texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "# train_texts = list(map(lambda d: d['text'], train_data))\n",
        "# test_texts = list(map(lambda d: d['text'], test_data))\n",
        "\n",
        "# train_y = list(map(lambda d: funcs_index[d['intent']], train_data))\n",
        "# test_y = list(map(lambda d: funcs_index[d['intent']], test_data))\n",
        "\n",
        "# # ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "# # for i,element in enumerate(ys):\n",
        "# #     ys[i] = words[ys[i]]\n",
        "\n",
        "# for i,element in enumerate(train_y):\n",
        "#     train_y[i] = words[train_y[i]]\n",
        "\n",
        "# for i,element in enumerate(test_y):\n",
        "#     test_y[i] = words[test_y[i]]\n",
        "\n",
        "# print('Found %s texts.' % (len(train_texts) + len(test_texts)))\n",
        "\n",
        "# texts = train_texts + test_texts\n",
        "\n",
        "# tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "# tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# word_index = tokenizer.word_index\n",
        "# print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# kf = KFold(n_splits=5)\n",
        "\n",
        "# y_pred_func_all = []\n",
        "# y_test_func_all = []\n",
        "# y_pred_prov_all = []\n",
        "# y_test_prov_all = []\n",
        "# y_pred_only_func_all = []\n",
        "# y_test_only_func_all = []\n",
        "# y_pred_only_prov_all = []\n",
        "# y_test_only_prov_all = []\n",
        "# y_pred_func = []\n",
        "# y_test_func = []\n",
        "\n",
        "# embeddings_index = {}\n",
        "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "# for line in f:\n",
        "#     values = line.split()\n",
        "#     word = values[0]\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "\n",
        "# print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# # for word, i in word_index.items():\n",
        "# #     embedding_vector = embeddings_index.get(word)\n",
        "# #     if embedding_vector is not None:\n",
        "# #         # words not found in embedding index will be all-zeros.\n",
        "# #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# # -------------------------\n",
        "# texts = map(lambda d: d['text'], dataset_func)\n",
        "# sequences = tokenizer.texts_to_sequences(texts)\n",
        "# xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "# ys = np.asarray(ys)\n",
        "\n",
        "# x_train = pad_sequences(sequences, )\n",
        "\n",
        "# batch_num = 0\n",
        "# average_list = {}\n",
        "# encoded_classes = words[0]\n",
        "# for i in range(len(words)-1):\n",
        "#     encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "# encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "# for train_index, test_index in kf.split(xs):\n",
        "#     x_train, x_test = xs[train_index], xs[test_index]\n",
        "#     y_train, y_test = ys[train_index], ys[test_index]\n",
        "\n",
        "#     x_train = np.array(x_train)\n",
        "#     x_test = np.array(x_test)\n",
        "#     y_train = np.array(y_train)\n",
        "#     y_test = np.array(y_test)\n",
        "\n",
        "\n",
        "#     # indexes = []\n",
        "#     # for i, sample in enumerate(y_test):\n",
        "#     #     if np.array_equal(y_test[i],word2):\n",
        "#     #         indexes.append(i)\n",
        "#     # y_test = np.delete(y_test,indexes,0)\n",
        "#     # x_test = np.delete(x_test,indexes,0)\n",
        "#     # x_test = np.asarray(x_test)\n",
        "#     # y_test = np.asarray(y_test)\n",
        "\n",
        "#     # y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "\n",
        "#     print (x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "#     NB_FILTER = 128\n",
        "#     BATCH_SIZE = 32\n",
        "#     count = 0\n",
        "#     EPOCH = 15 # 20\n",
        "#     indices = []\n",
        "#     indices_type = []\n",
        "\n",
        "#     # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "#     embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "#     for word, i in word_index.items():\n",
        "#         embedding_vector = embeddings_index.get(word)\n",
        "#         if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#             embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#     embedding_layer = Embedding(len(word_index) + 1,\n",
        "#                                 EMBEDDING_DIM,\n",
        "#                                 weights=[embedding_matrix],\n",
        "#                                 input_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "#     sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "#     embedded_sequences = embedding_layer(sequence_input)\n",
        "#     x = Convolution1D(filters=NB_FILTER,\n",
        "#                     kernel_size=5,\n",
        "#                     padding='valid',\n",
        "#                     activation='relu')(embedded_sequences)\n",
        "\n",
        "#     x = GlobalMaxPooling1D()(x)\n",
        "#     x = Dense(NB_FILTER, activation='relu')(x)\n",
        "#     # x = Dropout(0.3)(x)\n",
        "#     # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "#     preds = Dense(768)(x)\n",
        "#     output_reshape = Reshape((1,768))(preds)\n",
        "\n",
        "#     model = Model(sequence_input, output_reshape)\n",
        "\n",
        "#     model.compile(loss=cosine_proximity,\n",
        "#                 # optimizer='adam',\n",
        "#                 optimizer='rmsprop',\n",
        "#                 metrics=['acc'])\n",
        "\n",
        "#     print(model.summary())\n",
        "\n",
        "#     # import datetime\n",
        "#     # from keras.callbacks import TensorBoard\n",
        "\n",
        "#     # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "#     # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "#     model.fit(x_train, y_train,\n",
        "#             nb_epoch=EPOCH, batch_size=BATCH_SIZE)\n",
        "\n",
        "#     new_model = build_knn(model, encoded_classes.shape[1])\n",
        "#     print(new_model.summary())\n",
        "#     encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "#     temp_weights = new_model.get_weights()\n",
        "#     temp_weights[-1] = encoded_classes_norm\n",
        "#     new_model.set_weights(temp_weights)\n",
        "\n",
        "#     y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "#     y_pred_func = []\n",
        "\n",
        "#     y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "#     y_test_list = []\n",
        "#     sim = {}\n",
        "\n",
        "#     for i, sample in enumerate(y_pred_probs):\n",
        "#         for j in range(len(funcs_index)):\n",
        "#             # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "#             if np.array_equal(y_test[i], words[j]):\n",
        "#                 y_test_list.append(j)\n",
        "#         # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "#         # y_pred_func.append(greatest_sim)\n",
        "\n",
        "#     y_test = y_test_list\n",
        "\n",
        "\n",
        "#     # y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "#     # new_y_pred = [1] * len(y_pred_func)\n",
        "#         # Generate classificat\n",
        "#     # y_pred_func = new_y_pred\n",
        "#     # y_test = data.compress_y(y_test)\n",
        "\n",
        "#             #print('y_pred_func_A')\n",
        "#             #print(y_pred_func)\n",
        "\n",
        "#     y_pred_only_func_all += y_pred_func\n",
        "#     y_test_only_func_all += y_test\n",
        "\n",
        "#         # ---------- End of citation function ----------\n",
        "\n",
        "# print('Plain_Func')\n",
        "# # print(average_list)\n",
        "# print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "# print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOoiAKkk9t_p",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "bd3b65d3-960b-4d1e-a718-b4cb13ed039e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Transferred model with Training Acl anthology (4 classes)\n",
        "\n",
        "# New file authored 28 Jan 2018\n",
        "# For citation provenance\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "\"\"\"\n",
        "This file tests the model that is trained (on acl-arc dataset, 6 classes) that outputs a 512 dimensional vector based on USE\n",
        "Using the 4 Citation taxonomy dataset, (golden_test) as the target test set\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Perform the experiments on bootstrapped data and actual annotated data.\n",
        "\"\"\"\n",
        "# import lib.logger, os, sys, random, math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# import tensorflow_hub as hub\n",
        "\n",
        "# def embed_sentence(sentence):\n",
        "#   with tf.Session() as session:\n",
        "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "#     message_embeddings = session.run(embed(sentence))\n",
        "#     return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "  # Flatten feature vector\n",
        "  flat_dim_size = np.prod(model.output_shape[1:])\n",
        "  x = Reshape(target_shape=(flat_dim_size,),\n",
        "              name='features_flat')(model.output)\n",
        "\n",
        "  # Dot product between feature vector and reference vectors\n",
        "  x = Dense(units=output_size,\n",
        "            activation='linear',\n",
        "            use_bias=False)(x)\n",
        "\n",
        "  classifier = Model(inputs=[model.input], outputs=x)\n",
        "  return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "seed = 1020\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "# GLOVE_DIR = GLOVE_DIR\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['func']\n",
        "test = read_json_data(datafiles['golden_test'])\n",
        "# train = data.read_jsonl_data((datafiles['train']))\n",
        "train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_train + dataset_test\n",
        "# dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "# random.shuffle(dataset_func)\n",
        "\n",
        "\n",
        "# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# print(\"loaded Hub Module\")\n",
        "\n",
        "s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "words = {}\n",
        "\n",
        "## Title ##\n",
        "# words[0] = [\"Weak\"]\n",
        "# words[1] = [\"Compare and Contrast\"]\n",
        "# words[2] = [\"Positive\"]\n",
        "# words[3] = [\"Neutral\"]\n",
        "\n",
        "## Description ##\n",
        "# words[0] = [\"The citation points to weaknesses or problems of the cited paper\"]\n",
        "# words[1] = [\"The citation compares or contrasts the results or methodology from the cited paper with another work. \"]\n",
        "# words[2] = [\"The citation expresses approval of the cited paper. For example, the citing paper adopts an idea,\"\n",
        "#             \" method or dataset from the cited paper, or it shows compliment of the cited paper. \"]\n",
        "# words[3] = [\"The citation serves a neutral purpose: background, mere mentioning, etc; or its function is not decidable.\"]\n",
        "\n",
        "## Mod Description ##\n",
        "words[0] = [\"points Weaknesses problems\"]\n",
        "words[1] = [\"Compares Contrasts\"]\n",
        "words[2] = [\"Expresses Approval\"]\n",
        "words[3] = [\"serves Neutral Purpose Function not decidable\"]\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # words[i] = embed_sentence(words[i])\n",
        "    words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts_train = list(map(lambda d: d['context'][0], dataset_train))\n",
        "\n",
        "texts_test = list(map(lambda d: d['context'][0], dataset_test))\n",
        "\n",
        "texts = texts_train + texts_test\n",
        "\n",
        "y_train = list(map(lambda d: funcs_index[d['label']], dataset_train))\n",
        "y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n",
        "\n",
        "ys = y_train + y_test\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=42)\n",
        "\n",
        "y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "for i,element in enumerate(y_train):\n",
        "  y_train[i] = words[y_train[i]]\n",
        "\n",
        "\n",
        "for i,element in enumerate(y_test):\n",
        "  y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# -------------------------\n",
        "texts = map(lambda d: d['context'][0], dataset)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "ys = np.asarray(ys)\n",
        "xs = np.asarray(xs)\n",
        "\n",
        "print(xs.shape, ys.shape)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_train = np.asarray(y_train)\n",
        "x_train = np.asarray(x_train)\n",
        "\n",
        "\n",
        "# Proportional Reduction\n",
        "# ------------------------------------\n",
        "\n",
        "# new_x_train = []\n",
        "# new_y_train = []\n",
        "# arr = {}\n",
        "# for index in range(len(funcs_index)):\n",
        "#     print(index)\n",
        "#     print(\"-----------------------\")\n",
        "#     arr[index] = []\n",
        "#     for i, value in enumerate(y_train):\n",
        "#         if (value == index):\n",
        "#             arr[index].append(i)\n",
        "#     print(len(arr[index]))\n",
        "#     sample_length = len(arr[index]) / 20  # 5% of data\n",
        "#     sample_length = int(sample_length)\n",
        "#     for j in range(sample_length):\n",
        "#         new_x_train.append(x_train[arr[index][j]])\n",
        "#         new_y_train.append(y_train[arr[index][j]])\n",
        "#\n",
        "# new_x_train = np.asarray(new_x_train)\n",
        "# new_y_train = np.asarray(new_y_train)\n",
        "# x_train = new_x_train\n",
        "# y_train = new_y_train\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "# One Shot\n",
        "# ------------------------------------------\n",
        "\n",
        "# x_train_unique = [x_train[i] for i in indices]\n",
        "# x_train_unique = np.asarray(x_train_unique)\n",
        "# y_train_unique = [y_train[i] for i in indices]\n",
        "# y_train_unique = np.asarray(y_train_unique)\n",
        "# x_train = x_train_unique\n",
        "# y_train = y_train_unique\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_test)\n",
        "x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_test = np.asarray(y_test)\n",
        "x_test = np.asarray(x_test)\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "\n",
        "encoded_classes = words[0]\n",
        "for i in range(len(words)-1):\n",
        "  encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerC\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "# preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "preds = Dense(768, name=\"output_layer\")(x)\n",
        "output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, output_reshape)\n",
        "\n",
        "model.compile(loss=cosine_proximity,\n",
        "            # optimizer='adam',\n",
        "\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "model.load_weights('model-aclarc_scicite.h5', by_name=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2)\n",
        "\n",
        "new_model = build_knn(model, encoded_classes.shape[1])\n",
        "print(new_model.summary())\n",
        "encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "temp_weights = new_model.get_weights()\n",
        "temp_weights[-1] = encoded_classes_norm\n",
        "new_model.set_weights(temp_weights)\n",
        "\n",
        "y_pred_probs = new_model.predict(x_test)\n",
        "# y_test = ys\n",
        "\n",
        "# total_diff = 0\n",
        "# sample_count = 0\n",
        "# for i, sample in enumerate(y_pred_probs):\n",
        "#     total_diff += 1-cosine_similarity(y_pred_probs[i],ys[i])\n",
        "#     sample_count += 1\n",
        "# average = total_diff/sample_count\n",
        "# print(\"The average cosine difference between the predictions and test are :\")\n",
        "# print(average)\n",
        "# average_list[batch_num] = average\n",
        "# batch_num += 1\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test_list = []\n",
        "sim = {}\n",
        "\n",
        "# for i, sample in enumerate(y_pred_probs):\n",
        "#     for j in range(len(funcs_index)):\n",
        "#         sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "#         if np.array_equal(ys[i], words[j]):\n",
        "#             y_test_list.append(j)\n",
        "#     greatest_sim = max(sim, key=sim.get)\n",
        "#     y_pred_func.append(greatest_sim)\n",
        "\n",
        "# y_test = y_test_list  \n",
        "\n",
        "for i, sample in enumerate(y_pred_probs):\n",
        "    for j in range(len(funcs_index)):\n",
        "        if np.array_equal(y_test[i], words[j]):\n",
        "            y_test_list.append(j)\n",
        "    # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "    # y_pred_func.append(greatest_sim)\n",
        "\n",
        "y_test = y_test_list\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "    # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "# print(average_list)\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "# print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1432 texts.\n",
            "Found 4194 unique tokens.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "(1432, 50) (1432,)\n",
            "Train on 916 samples, validate on 229 samples\n",
            "Epoch 1/15\n",
            "916/916 [==============================] - 1s 1ms/sample - loss: -0.7913 - acc: 0.1747 - val_loss: -0.8744 - val_acc: 0.1354\n",
            "Epoch 2/15\n",
            "916/916 [==============================] - 1s 767us/sample - loss: -0.8639 - acc: 0.1976 - val_loss: -0.8737 - val_acc: 0.1092\n",
            "Epoch 3/15\n",
            "916/916 [==============================] - 1s 791us/sample - loss: -0.8894 - acc: 0.2413 - val_loss: -0.8728 - val_acc: 0.1266\n",
            "Epoch 4/15\n",
            "916/916 [==============================] - 1s 803us/sample - loss: -0.9200 - acc: 0.4465 - val_loss: -0.8594 - val_acc: 0.1528\n",
            "Epoch 5/15\n",
            "916/916 [==============================] - 1s 801us/sample - loss: -0.9400 - acc: 0.5721 - val_loss: -0.8628 - val_acc: 0.2926\n",
            "Epoch 6/15\n",
            "916/916 [==============================] - 1s 759us/sample - loss: -0.9504 - acc: 0.6004 - val_loss: -0.8585 - val_acc: 0.3362\n",
            "Epoch 7/15\n",
            "916/916 [==============================] - 1s 770us/sample - loss: -0.9584 - acc: 0.6550 - val_loss: -0.8649 - val_acc: 0.2620\n",
            "Epoch 8/15\n",
            "916/916 [==============================] - 1s 792us/sample - loss: -0.9648 - acc: 0.6998 - val_loss: -0.8715 - val_acc: 0.4760\n",
            "Epoch 9/15\n",
            "916/916 [==============================] - 1s 780us/sample - loss: -0.9668 - acc: 0.7325 - val_loss: -0.8568 - val_acc: 0.3886\n",
            "Epoch 10/15\n",
            "916/916 [==============================] - 1s 775us/sample - loss: -0.9676 - acc: 0.7391 - val_loss: -0.8627 - val_acc: 0.3013\n",
            "Epoch 11/15\n",
            "916/916 [==============================] - 1s 786us/sample - loss: -0.9721 - acc: 0.7937 - val_loss: -0.8572 - val_acc: 0.2489\n",
            "Epoch 12/15\n",
            "916/916 [==============================] - 1s 795us/sample - loss: -0.9723 - acc: 0.8013 - val_loss: -0.8573 - val_acc: 0.3100\n",
            "Epoch 13/15\n",
            "916/916 [==============================] - 1s 785us/sample - loss: -0.9756 - acc: 0.8461 - val_loss: -0.8685 - val_acc: 0.5197\n",
            "Epoch 14/15\n",
            "916/916 [==============================] - 1s 757us/sample - loss: -0.9757 - acc: 0.8526 - val_loss: -0.8252 - val_acc: 0.2314\n",
            "Epoch 15/15\n",
            "916/916 [==============================] - 1s 771us/sample - loss: -0.9767 - acc: 0.8483 - val_loss: -0.8697 - val_acc: 0.4978\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerC (Embedding) (None, 50, 100)           419500    \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 768)               99072     \n",
            "_________________________________________________________________\n",
            "reshape_layer (Reshape)      (None, 1, 768)            0         \n",
            "_________________________________________________________________\n",
            "features_flat (Reshape)      (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4)                 3072      \n",
            "=================================================================\n",
            "Total params: 602,284\n",
            "Trainable params: 602,284\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Plain_Func\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000         5\n",
            "           1     0.7273    0.4444    0.5517        18\n",
            "           2     0.4667    0.1400    0.2154        50\n",
            "           3     0.7854    0.9579    0.8632       214\n",
            "\n",
            "    accuracy                         0.7666       287\n",
            "   macro avg     0.4948    0.3856    0.4076       287\n",
            "weighted avg     0.7126    0.7666    0.7157       287\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCKKTxPT-zWL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Transferred model with Training SciCite (3 Classes)\n",
        "\n",
        "# New file authored 28 Jan 2018\n",
        "# For citation provenance\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "\"\"\"\n",
        "This file tests the model that is trained (on acl-arc dataset, 6 classes) that outputs a 512 dimensional vector based on USE\n",
        "Using the 4 Citation taxonomy dataset, (golden_test) as the target test set\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Perform the experiments on bootstrapped data and actual annotated data.\n",
        "\"\"\"\n",
        "# import lib.logger, os, sys, random, math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# import tensorflow_hub as hub\n",
        "\n",
        "# def embed_sentence(sentence):\n",
        "#   with tf.Session() as session:\n",
        "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "#     message_embeddings = session.run(embed(sentence))\n",
        "#     return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "  # Flatten feature vector\n",
        "  flat_dim_size = np.prod(model.output_shape[1:])\n",
        "  x = Reshape(target_shape=(flat_dim_size,),\n",
        "              name='features_flat')(model.output)\n",
        "\n",
        "  # Dot product between feature vector and reference vectors\n",
        "  x = Dense(units=output_size,\n",
        "            activation='linear',\n",
        "            use_bias=False)(x)\n",
        "\n",
        "  classifier = Model(inputs=[model.input], outputs=x)\n",
        "  return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "seed = 1020\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "# GLOVE_DIR = GLOVE_DIR\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'background': 0, 'method': 1, 'result': 2}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['scicite']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data(datafiles['train'])\n",
        "\n",
        "dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_train + dataset_test\n",
        "\n",
        "# dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "# random.shuffle(dataset_func)\n",
        "\n",
        "\n",
        "# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# print(\"loaded Hub Module\")\n",
        "\n",
        "s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "words = {}\n",
        "\n",
        "## Title ##\n",
        "# words[0] = [\"Background Information\"]\n",
        "# words[1] = [\"Method\"]\n",
        "# words[2] = [\"Result Comparison\"]\n",
        "\n",
        "## Description ##\n",
        "# words[0] = [\"The citation states, mentions, or points to the background\"\n",
        "#               \" information giving more context about a problem,concept, approach, topic,\"\n",
        "#               \"or importance of the problem in the field\"]\n",
        "# words[1] = [\"Making use of a method, tool, approach or dataset\"]\n",
        "# words[2] = [\"Comparison of the paper’s results/findings\"\n",
        "#               \"with the results/findings of other work\"]\n",
        "\n",
        "## Mod Description ##\n",
        "words[0] = [\"states background\"]\n",
        "words[1] = [\"making use method approach\"]\n",
        "words[2] = [\"Comparison results findings\"]\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # words[i] = embed_sentence(words[i])\n",
        "    words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts_train = list(map(lambda d: d['string'], dataset_train))\n",
        "\n",
        "texts_test = list(map(lambda d: d['string'], dataset_test))\n",
        "\n",
        "texts = texts_train + texts_test\n",
        "\n",
        "y_train = list(map(lambda d: funcs_index[d['label']], dataset_train))\n",
        "y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n",
        "\n",
        "ys = y_train + y_test\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=42)\n",
        "\n",
        "y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "for i,element in enumerate(y_train):\n",
        "  y_train[i] = words[y_train[i]]\n",
        "\n",
        "\n",
        "for i,element in enumerate(y_test):\n",
        "  y_test[i] = words[y_test[i]]\n",
        "\n",
        "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# -------------------------\n",
        "# texts = map(lambda d: d['string'], dataset)\n",
        "# sequences = tokenizer.texts_to_sequences(texts)\n",
        "# xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "# ys = np.asarray(ys)\n",
        "# xs = np.asarray(xs)\n",
        "\n",
        "# print(xs.shape, ys.shape)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_train = np.asarray(y_train)\n",
        "x_train = np.asarray(x_train)\n",
        "\n",
        "\n",
        "# Proportional Reduction\n",
        "# ------------------------------------\n",
        "\n",
        "# new_x_train = []\n",
        "# new_y_train = []\n",
        "# arr = {}\n",
        "# for index in range(len(funcs_index)):\n",
        "#     print(index)\n",
        "#     print(\"-----------------------\")\n",
        "#     arr[index] = []\n",
        "#     for i, value in enumerate(y_train):\n",
        "#         if (value == index):\n",
        "#             arr[index].append(i)\n",
        "#     print(len(arr[index]))\n",
        "#     sample_length = len(arr[index]) / 20  # 5% of data\n",
        "#     sample_length = int(sample_length)\n",
        "#     for j in range(sample_length):\n",
        "#         new_x_train.append(x_train[arr[index][j]])\n",
        "#         new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "\n",
        "# for i,element in enumerate(new_x_train):\n",
        "#   new_y_train[i] = words[new_y_train[i]]\n",
        "\n",
        "# new_x_train = np.asarray(new_x_train)\n",
        "# new_y_train = np.asarray(new_y_train)\n",
        "# x_train = new_x_train\n",
        "# y_train = new_y_train\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "# One Shot\n",
        "# ------------------------------------------\n",
        "\n",
        "x_train_unique = [x_train[i] for i in indices]\n",
        "x_train_unique = np.asarray(x_train_unique)\n",
        "y_train_unique = [y_train[i] for i in indices]\n",
        "y_train_unique = np.asarray(y_train_unique)\n",
        "x_train = x_train_unique\n",
        "y_train = y_train_unique\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_test)\n",
        "x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_test = np.asarray(y_test)\n",
        "x_test = np.asarray(x_test)\n",
        "\n",
        "\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "\n",
        "encoded_classes = words[0]\n",
        "for i in range(len(words)-1):\n",
        "  encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerB\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "# preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "preds = Dense(768, name=\"output_layer\")(x)\n",
        "output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, output_reshape)\n",
        "\n",
        "model.compile(loss=cosine_proximity,\n",
        "            # optimizer='adam',\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "model.load_weights('model-acl.h5', by_name=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.0)\n",
        "\n",
        "# model.save_weights('model-scicite.h5')\n",
        "\n",
        "new_model = build_knn(model, encoded_classes.shape[1])\n",
        "print(new_model.summary())\n",
        "encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "temp_weights = new_model.get_weights()\n",
        "temp_weights[-1] = encoded_classes_norm\n",
        "new_model.set_weights(temp_weights)\n",
        "\n",
        "y_pred_probs = new_model.predict(x_test)\n",
        "# y_test = ys\n",
        "\n",
        "# total_diff = 0\n",
        "# sample_count = 0\n",
        "# for i, sample in enumerate(y_pred_probs):\n",
        "#     total_diff += 1-cosine_similarity(y_pred_probs[i],ys[i])\n",
        "#     sample_count += 1\n",
        "# average = total_diff/sample_count\n",
        "# print(\"The average cosine difference between the predictions and test are :\")\n",
        "# print(average)\n",
        "# average_list[batch_num] = average\n",
        "# batch_num += 1\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test_list = []\n",
        "sim = {}\n",
        "\n",
        "# for i, sample in enumerate(y_pred_probs):\n",
        "#     for j in range(len(funcs_index)):\n",
        "#         sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "#         if np.array_equal(ys[i], words[j]):\n",
        "#             y_test_list.append(j)\n",
        "#     greatest_sim = max(sim, key=sim.get)\n",
        "#     y_pred_func.append(greatest_sim)\n",
        "\n",
        "# y_test = y_test_list  \n",
        "\n",
        "for i, sample in enumerate(y_pred_probs):\n",
        "    for j in range(len(funcs_index)):\n",
        "        # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "        if np.array_equal(y_test[i], words[j]):\n",
        "            y_test_list.append(j)\n",
        "    # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "    # y_pred_func.append(greatest_sim)\n",
        "\n",
        "y_test = y_test_list\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "    # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "# print(average_list)\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "# print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcSoS-izTpjV",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "285069f5-57a5-4fb8-8068-6af58d2303d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Combination (Aclarc + Scicite)\n",
        "\n",
        "# Integrated Cosine Sim into the model\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "# import lib.logger, os, sys, random, math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.utils.random import sample_without_replacement\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def embed_sentence(sentence):\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    message_embeddings = session.run(embed(sentence))\n",
        "    return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "    # Flatten feature vector\n",
        "    flat_dim_size = np.prod(model.output_shape[1:])\n",
        "    x = Reshape(target_shape=(flat_dim_size,),\n",
        "                name='features_flat')(model.output)\n",
        "\n",
        "    # Dot product between feature vector and reference vectors\n",
        "    x = Dense(units=output_size,\n",
        "              activation='linear',\n",
        "              use_bias=False)(x)\n",
        "\n",
        "    classifier = Model(inputs=[model.input], outputs=x)\n",
        "    return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "seed = 1020\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "               'Extends': 5, 'background': 6, 'method': 7, 'result': 8}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['acl-arc']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "s_datafiles = DATA_FILES['scicite']\n",
        "s_test = read_jsonl_data(s_datafiles['test'])\n",
        "s_train = read_jsonl_data(s_datafiles['train'])\n",
        "\n",
        "s_dataset_train = list(filter(lambda x: x['label'] != 'Error',s_train))\n",
        "s_dataset_test = list(filter(lambda x: x['label'] != 'Error', s_test))\n",
        "\n",
        "texts_train = list(map(lambda d: d['string'], s_train))\n",
        "\n",
        "texts_test = list(map(lambda d: d['string'], s_test))\n",
        "\n",
        "s_texts = texts_train + texts_test\n",
        "\n",
        "y_train = list(map(lambda d: funcs_index[d['label']], s_dataset_train))\n",
        "y_test = list(map(lambda d: funcs_index[d['label']], s_dataset_test))\n",
        "\n",
        "s_ys = y_train + y_test\n",
        "\n",
        "temp_list0 = []\n",
        "temp_list1 = []\n",
        "r_indices = sample_without_replacement(len(s_test + s_train),len(test + train),random_state=42)\n",
        "for index in r_indices:\n",
        "  temp_list0.append(s_texts[index])\n",
        "  temp_list1.append(s_ys[index])\n",
        "s_texts = temp_list0\n",
        "s_ys = temp_list1\n",
        "\n",
        "dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "random.shuffle(dataset_func)\n",
        "\n",
        "\n",
        "# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# print(\"loaded Hub Module\")\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "  \n",
        "s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "words = {}\n",
        "\n",
        "# ## Raw Description\n",
        "# words[0] = [\"P provides relevant information for this domain.\"]\n",
        "# words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "# words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "# words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "# words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "# words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "# Summarized\n",
        "words[0] = [\"provides relevant information\"]\n",
        "words[1] = [\"Illustrates need\"]\n",
        "words[2] = [\"Uses\"]\n",
        "words[3] = [\"Extends\"]\n",
        "words[4] = [\"similarity differences\"]\n",
        "words[5] = [\"Potential Future\"]\n",
        "## Mod Description ##\n",
        "words[6] = [\"states background\"]\n",
        "words[7] = [\"making use method approach\"]\n",
        "words[8] = [\"Comparison results findings\"]\n",
        "\n",
        "\n",
        "# Class Title\n",
        "# words[0] = [\"Background\"]\n",
        "# words[1] = [\"Motivation\"]\n",
        "# words[2] = [\"Uses\"]\n",
        "# words[3] = [\"Extension\"]\n",
        "# words[4] = [\"Compare Or Contrast\"]\n",
        "# words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # words[i] = embed_sentence(words[i])\n",
        "    words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts = list(map(lambda d: d['text'], dataset_func))\n",
        "texts = texts + s_texts\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "ys = ys + s_ys\n",
        "\n",
        "for i,element in enumerate(ys):\n",
        "    ys[i] = words[ys[i]]\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_prov_all = []\n",
        "y_test_prov_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_only_prov_all = []\n",
        "y_test_only_prov_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# -------------------------\n",
        "# texts = map(lambda d: d['text'], dataset_func)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "ys = np.asarray(ys)\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "encoded_classes = words[0]\n",
        "for i in range(len(words)-1):\n",
        "    encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=42)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "indices = []\n",
        "indices_type = []\n",
        "\n",
        "# ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerA\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "# preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "preds = Dense(768, name=\"output_layer\")(x)\n",
        "output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, output_reshape)\n",
        "\n",
        "model.compile(loss=cosine_proximity,\n",
        "            # optimizer='adam',\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# import datetime\n",
        "# from keras.callbacks import TensorBoard\n",
        "\n",
        "# log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "# model.load_weights('model-scicite.h5', by_name=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "model.save_weights('model-aclarc_scicite.h5')\n",
        "\n",
        "new_model = build_knn(model, encoded_classes.shape[1])\n",
        "print(new_model.summary())\n",
        "encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "temp_weights = new_model.get_weights()\n",
        "temp_weights[-1] = encoded_classes_norm\n",
        "new_model.set_weights(temp_weights)\n",
        "\n",
        "y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test_list = []\n",
        "sim = {}\n",
        "\n",
        "for i, sample in enumerate(y_pred_probs):\n",
        "    for j in range(len(funcs_index)):\n",
        "        # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "        if np.array_equal(y_test[i], words[j]):\n",
        "            y_test_list.append(j)\n",
        "    # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "    # y_pred_func.append(greatest_sim)\n",
        "\n",
        "y_test = y_test_list\n",
        "\n",
        "\n",
        "# y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "# new_y_pred = [1] * len(y_pred_func)\n",
        "    # Generate classificat\n",
        "# y_pred_func = new_y_pred\n",
        "# y_test = data.compress_y(y_test)\n",
        "\n",
        "        #print('y_pred_func_A')\n",
        "        #print(y_pred_func)\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "        # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "# print(average_list)\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Found 3654 texts.\n",
            "Found 15996 unique tokens.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerA (Embedding) (None, 50, 100)           1599700   \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 768)               99072     \n",
            "_________________________________________________________________\n",
            "reshape_layer (Reshape)      (None, 1, 768)            0         \n",
            "=================================================================\n",
            "Total params: 1,779,412\n",
            "Trainable params: 1,779,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 2338 samples, validate on 585 samples\n",
            "Epoch 1/15\n",
            "2338/2338 [==============================] - 3s 1ms/sample - loss: -0.8370 - acc: 0.5043 - val_loss: -0.8271 - val_acc: 0.4496\n",
            "Epoch 2/15\n",
            "2338/2338 [==============================] - 2s 992us/sample - loss: -0.8929 - acc: 0.5915 - val_loss: -0.8737 - val_acc: 0.4718\n",
            "Epoch 3/15\n",
            "2338/2338 [==============================] - 2s 985us/sample - loss: -0.9086 - acc: 0.5552 - val_loss: -0.8883 - val_acc: 0.5248\n",
            "Epoch 4/15\n",
            "2338/2338 [==============================] - 2s 1000us/sample - loss: -0.9289 - acc: 0.6035 - val_loss: -0.8970 - val_acc: 0.5761\n",
            "Epoch 5/15\n",
            "2338/2338 [==============================] - 2s 980us/sample - loss: -0.9474 - acc: 0.6801 - val_loss: -0.9082 - val_acc: 0.6359\n",
            "Epoch 6/15\n",
            "2338/2338 [==============================] - 2s 958us/sample - loss: -0.9609 - acc: 0.7352 - val_loss: -0.9168 - val_acc: 0.6274\n",
            "Epoch 7/15\n",
            "2338/2338 [==============================] - 2s 1ms/sample - loss: -0.9712 - acc: 0.7665 - val_loss: -0.9048 - val_acc: 0.5692\n",
            "Epoch 8/15\n",
            "2338/2338 [==============================] - 2s 998us/sample - loss: -0.9782 - acc: 0.7938 - val_loss: -0.9156 - val_acc: 0.5932\n",
            "Epoch 9/15\n",
            "2338/2338 [==============================] - 2s 968us/sample - loss: -0.9840 - acc: 0.8169 - val_loss: -0.9181 - val_acc: 0.6222\n",
            "Epoch 10/15\n",
            "2338/2338 [==============================] - 2s 969us/sample - loss: -0.9886 - acc: 0.8417 - val_loss: -0.9199 - val_acc: 0.6256\n",
            "Epoch 11/15\n",
            "2338/2338 [==============================] - 2s 993us/sample - loss: -0.9915 - acc: 0.8674 - val_loss: -0.9171 - val_acc: 0.6120\n",
            "Epoch 12/15\n",
            "2338/2338 [==============================] - 2s 992us/sample - loss: -0.9938 - acc: 0.8867 - val_loss: -0.9081 - val_acc: 0.6923\n",
            "Epoch 13/15\n",
            "2338/2338 [==============================] - 2s 967us/sample - loss: -0.9953 - acc: 0.9033 - val_loss: -0.9233 - val_acc: 0.6444\n",
            "Epoch 14/15\n",
            "2338/2338 [==============================] - 2s 984us/sample - loss: -0.9959 - acc: 0.9251 - val_loss: -0.9244 - val_acc: 0.6427\n",
            "Epoch 15/15\n",
            "2338/2338 [==============================] - 2s 958us/sample - loss: -0.9967 - acc: 0.9508 - val_loss: -0.9182 - val_acc: 0.6769\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerA (Embedding) (None, 50, 100)           1599700   \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 768)               99072     \n",
            "_________________________________________________________________\n",
            "reshape_layer (Reshape)      (None, 1, 768)            0         \n",
            "_________________________________________________________________\n",
            "features_flat (Reshape)      (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 9)                 6912      \n",
            "=================================================================\n",
            "Total params: 1,786,324\n",
            "Trainable params: 1,786,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Plain_Func\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7561    0.3780    0.5041        82\n",
            "           1     0.6078    0.8942    0.7237       208\n",
            "           2     0.0492    0.3333    0.0857         9\n",
            "           3     0.5217    0.3692    0.4324        65\n",
            "           4     0.6250    0.3846    0.4762        13\n",
            "           5     1.0000    0.0833    0.1538        12\n",
            "           6     0.7500    0.8011    0.7747       176\n",
            "           7     0.8000    0.2414    0.3709       116\n",
            "           8     0.7333    0.6600    0.6947        50\n",
            "\n",
            "    accuracy                         0.6183       731\n",
            "   macro avg     0.6492    0.4606    0.4685       731\n",
            "weighted avg     0.6900    0.6183    0.6059       731\n",
            "\n",
            "Finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll8Od_QDJr9c",
        "colab_type": "code",
        "outputId": "da435dc3-2a84-4814-bee4-32672dde2dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(s_texts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehHN4PhKYIAR",
        "colab_type": "code",
        "outputId": "b2fcef02-cb91-4a57-9825-5116996430e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = [[1,2],[2,2]]\n",
        "b = [[3,2],[4,2]]\n",
        "print(a+b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2], [2, 2], [3, 2], [4, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}