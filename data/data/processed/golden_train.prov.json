[{"function": "Neut", "cited": "C02-1025", "provenance": ["Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc.", "Token Information: This group consists of 10 features based on the string , as listed in Table 1."], "label": "Prov", "citing": "P02-1061", "vector": [5, 0, 0, 0.06711560552140244], "context": ["", "More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).", ""], "marker": "Chieu and Ng, 2002", "vector_1": {"case": 1, "given": 1, "perform": 1, "detail": 1, "mix": 1, "ner": 1}, "vector_2": [0, 0.3285224531256613, 1, 1, 0, 0]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc.", "On its own, a NER can also provide users who are looking for person or organization names with quick information."], "label": "Prov", "citing": "P03-1028", "vector": [3, 0, 0, 0.0], "context": ["", "They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).", ""], "marker": "Chieu and Ng, 2002a", "vector_1": {"use": 1, "deriv": 1, "metric": 1, "automat": 1, "correl": 1, "base": 1, "valu": 1}, "vector_2": [1, 0.7137619341489091, 1, 6, 3, 1]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing .", "Our system is built on a maximum entropy classifier."], "label": "Prov", "citing": "P03-1028", "vector": [2, 0, 0, 0.0], "context": ["", "Several benchmark data sets have been used to evaluate IE approaches on semi- structured texts (Soderland, 1999; Ciravegna, 2001; Chieu and Ng, 2002a).", ""], "marker": "Chieu and Ng, 2002a", "vector_1": {"use": 1, "set": 1, "semi": 1, "evalu": 1, "ie": 1, "benchmark": 1, "structur": 1, "approach": 1, "text": 1, "data": 1, "sever": 1}, "vector_2": [1, 0.07826623588775547, 3, 6, 3, 1]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["Such constraints are derived from training data, expressing some relationship between features and outcome."], "label": "Prov", "citing": "P03-1028", "vector": [2, 0, 0, 0.0], "context": ["", "More recently, machine learning approaches have been used for IE from semi-structured texts (Califf and Mooney, 1999; Soderland, 1999; Roth and Yih, 2001; Ciravegna, 2001; Chieu and Ng, 2002a), named entity extraction (Chieu and Ng, 2002b), template element extraction, and template relation extraction (Miller et al., 1998).", ""], "marker": "Chieu and Ng, 2002b", "vector_1": {"machin": 1, "use": 1, "extract": 3, "semistructur": 1, "approach": 1, "relat": 1, "templat": 2, "name": 1, "text": 1, "learn": 1, "element": 1, "entiti": 1, "ie": 1, "recent": 1}, "vector_2": [1, 0.15627836062003123, 7, 2, 7, 1]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.", "Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier."], "label": "Prov", "citing": "P03-1028", "vector": [11, 0, 0, 0.1805787796286538], "context": ["", "Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", ""], "marker": "2002a", "vector_1": {"machin": 1, "chieu": 1, "land": 1, "attempt": 1, "sentenc": 1, "assum": 1, "one": 2, "templat": 1, "ng": 1, "inform": 1, "soder": 1, "version": 1, "learn": 1, "need": 1, "scaleddown": 1, "st": 1, "approach": 1, "task": 1, "came": 1, "fill": 1}, "vector_2": [1, 0.18161687298072385, 2, 0, 0, 0]}, {"function": "CoCo", "cited": "C02-1025", "provenance": ["We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing .", "Our system is built on a maximum entropy classifier."], "label": "Prov", "citing": "P03-1028", "vector": [8, 0, 0, 0.061313933948496574], "context": ["", "The task we tackle is considerably more complex than that of (Soderland, 1999; Chieu and Ng, 2002a), since we need to deal with merging information from multiple sentences to fill one template.", ""], "marker": "Chieu and Ng, 2002a", "vector_1": {"task": 1, "multipl": 1, "consider": 1, "deal": 1, "sentenc": 1, "tackl": 1, "templat": 1, "one": 1, "inform": 1, "complex": 1, "need": 1, "fill": 1, "sinc": 1, "merg": 1}, "vector_2": [1, 0.19127309688895341, 2, 6, 3, 1]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["Global features are extracted from other occurrences of the same token in the whole document."], "label": "Prov", "citing": "P05-1045", "vector": [10, 0, 8, 0.48795003647426655], "context": ["", "Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.", ""], "marker": "2002", "vector_1": {"chieu": 1, "featur": 1, "solut": 1, "ng": 1, "defin": 1, "token": 2, "taken": 1, "problem": 1, "addit": 1, "occurr": 1, "document": 1, "propos": 1}, "vector_2": [3, 0.8518130736746589, 1, 0, 0, 0]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["Global features are extracted from other occurrences of the same token in the whole document."], "label": "Prov", "citing": "P05-1051", "vector": [7, 0, 3, 0.15152288168283162], "context": ["", "(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.", ""], "marker": "Chieu and Ng, 2002", "vector_1": {"featur": 2, "appli": 1, "pro": 1, "tokenrec": 1, "second": 1, "tag": 3, "pass": 2, "recov": 1, "use": 3, "corefer": 2, "low": 1, "method": 1, "confid": 1, "sequenc": 1, "pose": 1, "base": 1, "svm": 1, "made": 1, "name": 3, "rule": 1, "correct": 1, "filter": 1, "inform": 3, "token": 1, "instanc": 1, "assign": 1, "first": 1}, "vector_2": [3, 0.16776805836739372, 3, 1, 0, 0]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing .", "Our system is built on a maximum entropy classifier."], "label": "Prov", "citing": "P06-1141", "vector": [3, 0, 0, 0.08671099695241198], "context": ["", " Most work has looked to model non-local dependencies only within a document (Finkel et al., 2005; Chieu and Ng, 2002; Sutton and McCallum, 2004; Bunescu and Mooney, 2004).", ""], "marker": "Chieu and Ng, 2002", "vector_1": {"work": 1, "depend": 1, "look": 1, "within": 1, "nonloc": 1, "model": 1, "document": 1}, "vector_2": [4, 0.6447444925382777, 4, 2, 0, 0]}, {"function": "Pos", "cited": "C02-1025", "provenance": ["In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data."], "label": "Prov", "citing": "W03-0423", "vector": [4, 0, 0, 0.17888543819998318], "context": ["", "Such global features enhance the performance of NER (Chieu and Ng, 2002b).", ""], "marker": "Chieu and Ng, 2002b", "vector_1": {"perform": 1, "ner": 1, "global": 1, "featur": 1, "enhanc": 1}, "vector_2": [1, 0.08101868047295879, 1, 4, 1, 0]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["For all lists except locations, the lists are processed into a list of tokens (unigrams).", "Location list is processed into a list of unigrams and bigrams (e.g., New York).", "For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams."], "label": "Prov", "citing": "W03-0423", "vector": [8, 0, 2, 0.18090680674665816], "context": ["", "Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.", ""], "marker": "Chieu and Ng, 2002a", "vector_1": {"correl": 1, "compil": 1, "use": 2, "unigram": 1, "name": 2, "word": 1, "metric": 1, "top": 1, "list": 1, "rank": 1, "preced": 1, "uni": 1, "class": 2}, "vector_2": [1, 0.2662142307423214, 1, 1, 4, 0]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["The features we used can be divided into 2 classes: local and global.", "Local features are features that are based on neighboring tokens, as well as the token itself.", "Global features are extracted from other occurrences of the same token in the whole document."], "label": "Prov", "citing": "W03-0423", "vector": [11, 1, 6, 0.5020790110464023], "context": ["", "The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).", ""], "marker": "Chieu and Ng, 2002b", "vector_1": {"me": 2, "use": 1, "featur": 1, "divid": 1, "global": 1, "two": 1, "basic": 1, "local": 1, "class": 1}, "vector_2": [1, 0.366473098719653, 1, 4, 1, 0]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc.", "Token Information: This group consists of 10 features based on the string , as listed in Table 1."], "label": "Prov", "citing": "W03-0423", "vector": [9, 0, 4, 0.3287979746107146], "context": ["", "Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).", ""], "marker": "Chieu and Ng, 2002b", "vector_1": {"featur": 1, "string": 1, "etc": 1, "inform": 1, "token": 1, "base": 1, "w": 1, "containsdollarsign": 1, "containsdigit": 1}, "vector_2": [1, 0.4965367662492129, 1, 4, 1, 0]}, {"function": "Pos", "cited": "C02-1025", "provenance": ["The features we used can be divided into 2 classes: local and global.", "Local features are features that are based on neighboring tokens, as well as the token itself.", "Global features are extracted from other occurrences of the same token in the whole document."], "label": "Prov", "citing": "W03-0423", "vector": [6, 0, 1, 0.18257418583505533], "context": ["", "In this paper, wi refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).", ""], "marker": "Chieu and Ng, 2002b", "vector_1": {"use": 2, "featur": 1, "word": 2, "ith": 2, "wi": 2, "paper": 1, "w": 2, "similar": 1, "refer": 2}, "vector_2": [1, 0.37563842440355416, 1, 4, 1, 0]}, {"function": "Pos", "cited": "C02-1025", "provenance": ["The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.", "For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own."], "label": "Prov", "citing": "W03-0432", "vector": [13, 0, 7, 0.225], "context": ["", "Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a weaker classifier that did not use case information at all (Chieu and Ng, 2002b).", ""], "marker": "Chieu and Ng, 2002a", "vector_1": {"case": 1, "chieu": 1, "use": 3, "weaker": 1, "word": 1, "capitalis": 1, "also": 1, "global": 1, "mixedcas": 1, "classifi": 2, "ng": 1, "inform": 2, "teach": 1, "document": 1, "occurr": 1}, "vector_2": [1, 0.35680428134556574, 2, 1, 1, 0]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information)."], "label": "Prov", "citing": "W04-0705", "vector": [1, 0, 0, 0.10660035817780521], "context": ["", "AMaximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002)", ""], "marker": "Chieu and Ng 2002", "vector_1": {"chieu": 1, "borthwick": 1, "entropi": 1, "al": 1, "ng": 1, "et": 1, "amaximum": 1, "method": 1}, "vector_2": [2, 0.04020301856551356, 0, 0, 1, 0]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["Global features are extracted from other occurrences of the same token in the whole document."], "label": "Prov", "citing": "W04-0705", "vector": [7, 0, 4, 0.2760262237369417], "context": ["", "Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).", ""], "marker": "Chieu and Ng 2002", "vector_1": {"chieu": 1, "use": 2, "made": 1, "instanc": 1, "sequenc": 1, "system": 1, "ng": 1, "inform": 2, "second": 1, "tag": 2, "token": 2, "pass": 2, "borthwick": 1, "featur": 2, "assign": 1, "first": 1}, "vector_2": [2, 0.876229909621121, 0, 0, 1, 0]}, {"function": "Neut", "cited": "C02-1025", "provenance": ["In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data."], "label": "Prov", "citing": "W06-0119", "vector": [6, 0, 1, 0.1414213562373095], "context": ["", "To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;", ""], "marker": "Chieu et al. 2002", "vector_1": {"featur": 2, "entropi": 1, "al": 1, "et": 1, "extract": 1, "use": 1, "strength": 1, "develop": 1, "unknown": 1, "three": 1, "research": 1, "approach": 2, "match": 1, "common": 1, "mutual": 1, "uwi": 1, "detect": 1, "associ": 1, "chieu": 1, "word": 1, "maximum": 1, "ambigu": 1, "inform": 1, "multistatist": 1, "statist": 2}, "vector_2": [4, 0.184751231108847, 0, 0, 2, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data."], "label": "Prov", "citing": "D12-1133", "vector": [4, 0, 0, 0.056613851707229795], "context": ["", "For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"german": 2, "rftagger": 1, "knowledg": 1, "obtain": 1, "accuraci": 1, "achiev": 1, "tag": 1, "tagger": 1, "close": 1, "best": 1}, "vector_2": [4, 0.5622081767452454, 1, 2, 0, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset."], "label": "Prov", "citing": "D12-1133", "vector": [2, 1, 0, 0.0], "context": ["", "For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"corpu": 1, "word": 2, "german": 1, "k": 1, "tag": 1, "occur": 1, "see": 1, "train": 2, "greatest": 1, "found": 1, "improv": 1, "tional": 1, "data": 1, "final": 1, "addit": 1}, "vector_2": [4, 0.5855255665641727, 1, 2, 0, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.)", "computes the most probable POS tag sequence tN = t1, ..., tN for a given word sequence wN .", "POS taggers are usually trained on corpora with between 50 and 150 different POS tags."], "label": "Prov", "citing": "D13-1032", "vector": [4, 0, 0, 0.07312724241271307], "context": ["", "We use the following baselines: SVMTool (Gimenez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"model": 1, "use": 1, "develop": 1, "gimenez": 1, "di": 1, "svmtool": 1, "rftagger": 1, "posmorph": 1, "svmbase": 1, "hmm": 1, "ngram": 1, "tag": 1, "tagger": 2, "markov": 1, "follow": 1, "crimin": 1, "hidden": 1, "baselin": 1}, "vector_2": [5, 0.875781150363474, 2, 1, 2, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities."], "label": "Prov", "citing": "D13-1033", "vector": [0, 0, 0, 0.0], "context": ["", "For German, we show results for RFTagger (Schmid and Laws, 2008).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"german": 1, "rftagger": 1, "result": 1, "show": 1}, "vector_2": [5, 0.551747549086169, 1, 1, 1, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["Our tagger generates a predictor for each feature (such as base POS, number, gender etc.)", "Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc.", "for base POS), the tagger builds a separate decision tree for each value.", "A typical context attribute is 1:ART.Nom which states that the preceding tag is an article with the attribute Nom."], "label": "Prov", "citing": "P10-1020", "vector": [4, 0, 0, 0.03959037912324479], "context": ["", "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"case": 1, "morpholog": 1, "use": 1, "noun": 1, "howev": 1, "accuraci": 1, "better": 1, "achiev": 1, "tag": 1, "found": 1, "rftagger": 1}, "vector_2": [2, 0.601132449944545, 1, 1, 0, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "W10-1704", "vector": [7, 0, 0, 0.20025046972870353], "context": ["", "These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"use": 2, "comput": 2, "lemma": 1, "pattern": 1, "rftagger": 1, "normal": 1, "finegrain": 1, "inform": 2, "tag": 1, "tagset": 1, "treetagg": 1, "approxim": 1, "contain": 1, "po": 1}, "vector_2": [2, 0.28745523853528937, 1, 1, 1, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood."], "label": "Prov", "citing": "W10-1727", "vector": [8, 0, 1, 0.37573457465108967], "context": ["", "For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"case": 1, "use": 1, "noun": 1, "german": 1, "gender": 1, "rftagger": 1, "number": 1, "morpholog": 2, "inform": 1, "verb": 1, "tag": 1, "rich": 1, "contain": 1, "tens": 1}, "vector_2": [2, 0.2820042264486709, 1, 1, 1, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "W11-2135", "vector": [4, 0, 0, 0.24333213169614376], "context": ["", "For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"use": 1, "comput": 1, "german": 1, "rftagger": 1, "finegrain": 1, "inform": 1, "preprocess": 1, "po": 1}, "vector_2": [3, 0.331369798971482, 1, 1, 1, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).", "It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood."], "label": "Prov", "citing": "W11-2145", "vector": [12, 0, 2, 0.41917001364979695], "context": ["", "The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus.", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"case": 1, "corpu": 2, "use": 1, "word": 1, "po": 1, "person": 1, "german": 1, "gender": 1, "gener": 1, "differ": 2, "assign": 1, "finegrain": 2, "also": 1, "tag": 3, "includ": 1, "partofspeech": 1, "treetagg": 2, "k": 1, "rftagger": 2, "produc": 2, "inform": 1}, "vector_2": [3, 0.6076122931442081, 1, 1, 1, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.)", "computes the most probable POS tag sequence tN = t1, ..., tN for a given word sequence wN .", "POS taggers are usually trained on corpora with between 50 and 150 different POS tags.", "Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features."], "label": "Prov", "citing": "W11-2147", "vector": [9, 0, 0, 0.2860794159408606], "context": ["", "For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"case": 1, "verb": 1, "noun": 1, "inform": 1, "german": 1, "gender": 1, "rftagger": 1, "obtain": 1, "number": 1, "morpholog": 2, "provid": 1, "tag": 1, "tens": 1, "po": 1}, "vector_2": [3, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood."], "label": "Prov", "citing": "W12-3144", "vector": [9, 0, 1, 0.4383570037596046], "context": ["", "The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"case": 1, "inform": 1, "german": 1, "gender": 1, "gener": 1, "po": 1, "finegrain": 1, "person": 1, "tag": 2, "includ": 1, "rftagger": 1, "produc": 1}, "vector_2": [4, 0.6782878525143744, 1, 1, 1, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "W12-3402", "vector": [7, 0, 1, 0.15624999999999997], "context": ["", "Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"use": 1, "accur": 1, "allow": 1, "normal": 1, "sequenc": 1, "rftagger": 1, "tree": 1, "annot": 1, "agreement": 1, "morpholog": 3, "inform": 1, "window": 1, "base": 1, "tagger": 2, "stateoftheart": 1, "context": 1, "larg": 1, "model": 1, "trigrambas": 1, "decis": 1}, "vector_2": [4, 0.525107856660699, 1, 1, 1, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "W13-2204", "vector": [6, 0, 0, 0.1777046633277277], "context": ["", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"finegrain": 1, "use": 1, "preprocess": 1, "german": 1, "rftagger": 1, "corpora": 1, "obtain": 1, "po": 1, "postag": 1, "also": 1, "treetagg": 1, "need": 1, "label": 1, "parallel": 1, "addit": 1}, "vector_2": [5, 0.28342944674716164, 2, 1, 1, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "W13-2210", "vector": [5, 0, 3, 0.1777046633277277], "context": ["", "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"corpu": 1, "rule": 1, "align": 1, "treetagg": 1, "tag": 1, "probabilist": 1, "po": 1, "train": 2, "posbas": 1, "base": 1, "learn": 1, "model": 1, "order": 1, "reorder": 1}, "vector_2": [5, 0.2719642976087391, 1, 2, 0, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["The probability of an attribute (such as Nom) is always conditioned on the respective base POS (such as N) (unless the predicted attribute is theFigure 1: Probability estimation tree for the nomi native case of nouns."], "label": "Prov", "citing": "W13-2210", "vector": [3, 0, 0, 0.07715167498104596], "context": ["", "The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"use": 1, "german": 1, "gener": 1, "tag": 1, "rftagger": 1, "po": 1}, "vector_2": [5, 0.5844268300805968, 1, 2, 0, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging"], "label": "Prov", "citing": "W13-2211", "vector": [2, 0, 0, 0.0], "context": ["", "We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"use": 1, "bild": 2, "noun": 1, "pronoun": 1, "form": 1, "german": 1, "eg": 1, "remov": 1, "tag": 1, "lemmat": 1, "articl": 1, "adject": 1, "grain": 1, "lexic": 1, "partofspeech": 1, "gener": 1, "posit": 1, "redund": 1, "fine": 1, "order": 1, "rftagger": 1}, "vector_2": [5, 0.715072463768116, 1, 1, 1, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "W13-2228", "vector": [3, 0, 0, 0.29019050004400465], "context": ["", "6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"use": 1, "tag": 2, "po": 2, "rftagger": 1}, "vector_2": [5, 0.7390230377577255, 1, 1, 11, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "W13-2230", "vector": [5, 0, 0, 0.05564148840746571], "context": ["", "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (Abeille et al., 2003).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"use": 1, "word": 1, "normal": 1, "tree": 1, "rftagger": 1, "tag": 1, "step": 1, "french": 1, "annot": 1, "postag": 1, "lemma": 1, "second": 1, "train": 2, "partofspeech": 1, "data": 1, "bank": 1}, "vector_2": [5, 0.2296147672552167, 2, 3, 9, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "W13-2230", "vector": [7, 0, 1, 0.0936585811581694], "context": ["", "Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"analysi": 2, "finegrain": 1, "use": 1, "resourc": 1, "rftagger": 1, "tag": 1, "obtain": 1, "french": 1, "annot": 1, "morpholog": 3, "train": 1, "design": 1, "data": 1}, "vector_2": [5, 0.3489566613162119, 1, 3, 9, 0]}, {"function": "Pos", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "W13-2230", "vector": [5, 0, 0, 0.0], "context": ["", "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"use": 1, "tag": 3, "rftagger": 1, "version": 1, "error": 1}, "vector_2": [5, 0.49181380417335474, 1, 3, 9, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "W13-2302", "vector": [3, 0, 0, 0.13245323570650439], "context": ["", "Part-of-speech (POS) tagging of modern language data is a well-explored field, commonly achieving accuracies around 97% (Brants, 2000; Schmid and Laws, 2008).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"wellexplor": 1, "around": 1, "modern": 1, "accuraci": 1, "field": 1, "achiev": 1, "tag": 1, "commonli": 1, "partofspeech": 1, "data": 1, "po": 1, "languag": 1}, "vector_2": [5, 0.023665546845838, 2, 2, 0, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.", "3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset."], "label": "Prov", "citing": "W13-2302", "vector": [1, 0, 0, 0.0], "context": ["", "The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"achiev": 1, "use": 1, "result": 1, "present": 1, "rftagger": 1}, "vector_2": [5, 0.3908547965658828, 1, 2, 0, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags."], "label": "Prov", "citing": "W13-2708", "vector": [2, 0, 1, 0.18490006540840973], "context": ["", "So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"concept": 1, "far": 1, "builder": 1, "tag": 1, "lemmatis": 1, "token": 1, "complex": 1, "partofspeech": 1, "implement": 1}, "vector_2": [5, 0.5218585721553333, 3, 1, 2, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["6 Joint Segmentation and Parsing."], "label": "Prov", "citing": "D12-1046", "vector": [4, 0, 3, 0.7071067811865476], "context": ["", "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).", ""], "marker": "Green and Manning, 2010", "vector_1": {"investig": 1, "also": 1, "arab": 1, "joint": 1, "pars": 1, "segment": 1}, "vector_2": [2, 0.13362849017580145, 1, 1, 1, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["6 Joint Segmentation and Parsing."], "label": "Prov", "citing": "J13-1007", "vector": [4, 0, 3, 0.46188021535170065], "context": ["", "Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).", ""], "marker": "Green and Manning 2010", "vector_1": {"recov": 1, "use": 2, "joint": 1, "chines": 1, "arab": 1, "green": 1, "element": 1, "inde": 1, "chiang": 1, "cai": 1, "other": 1, "pars": 2, "english": 1, "solv": 1, "goldberg": 1, "problem": 1, "null": 1, "segment": 1, "man": 1}, "vector_2": [3, 0.5, 0, 2, 2, 0]}, {"function": "Pos", "cited": "C10-1045", "provenance": ["But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline."], "label": "Prov", "citing": "J13-1007", "vector": [6, 0, 1, 0.1126872339638022], "context": ["", "One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser. This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).", ""], "marker": "Green and Manning 2010", "vector_1": {"parser": 1, "unobserv": 1, "one": 1, "second": 1, "fed": 1, "solut": 1, "system": 2, "tokensegment": 1, "liu": 1, "input": 1, "man": 1, "approach": 1, "pipelin": 1, "chines": 1, "initi": 2, "arab": 1, "pars": 1, "huang": 1, "model": 2, "charg": 1, "stage": 1, "jiang": 1, "possibl": 1, "wordsequ": 1, "green": 1, "popular": 1, "output": 1, "problem": 1}, "vector_2": [3, 0.5, 0, 2, 2, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["6 Joint Segmentation and Parsing."], "label": "Prov", "citing": "J13-1007", "vector": [1, 0, 0, 0.0], "context": ["", "This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaan, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It is also used for Arabic (Green and Manning 2010)", ""], "marker": "Green and Manning 2010", "vector_1": {"represent": 1, "cohen": 1, "barhaim": 1, "goldberg": 3, "hebrew": 1, "utter": 1, "use": 1, "multipl": 1, "winter": 1, "smith": 1, "morpholog": 1, "also": 1, "elhadad": 2, "simaan": 1, "standard": 1, "arab": 1, "fairli": 1, "segment": 1, "adler": 3, "man": 1, "green": 1, "tsarfati": 1}, "vector_2": [3, 0.5, 0, 2, 2, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 25% F1."], "label": "Prov", "citing": "J13-1007", "vector": [5, 0, 0, 0.11605177063713189], "context": ["", "Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al (1999), Simaan (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).", ""], "marker": "2010", "vector_1": {"green": 1, "simaan": 1, "explor": 1, "disambigu": 1, "et": 1, "signal": 1, "smith": 1, "al": 1, "cohen": 1, "wordsegment": 1, "joint": 1, "speech": 1, "context": 2, "lattic": 1, "tsarfati": 1, "pars": 2, "goldberg": 1, "man": 1, "hall": 1, "chappeli": 1, "syntact": 1}, "vector_2": [3, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (6)."], "label": "Prov", "citing": "J13-1007", "vector": [5, 0, 0, 0.11180339887498948], "context": ["", "Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.", ""], "marker": "2010", "vector_1": {"pcfgla": 1, "set": 1, "gold": 1, "accuraci": 1, "assum": 1, "sever": 1, "use": 2, "stanfordpars": 1, "experi": 1, "extens": 1, "pars": 1, "refin": 1, "report": 2, "segment": 1, "recent": 1, "kind": 1, "word": 1, "f": 2, "tree": 1, "annot": 1, "green": 1, "man": 1, "berkeleypars": 1}, "vector_2": [3, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (6).", "Parent Head Modif er Dir # gold F1 Label # gold F1 NP NP TAG R 946 0.54 ADJP 1216 59.45 S S S R 708 0.57 SBAR 2918 69.81 NP NP ADJ P R 803 0.64 FRAG 254 72.87 NP NP N P R 2907 0.66 VP 5507 78.83 NP NP SBA R R 1035 0.67 S 6579 78.91 NP NP P P R 2713 0.67 PP 7516 80.93 VP TAG P P R 3230 0.80 NP 34025 84.95 NP NP TAG L 805 0.85 ADVP 1093 90.64 VP TAG SBA R R 772 0.86 WHN P 787 96.00 S VP N P L 961 0.87 (a) Major phrasal categories (b) Major POS categories (c) Ten lowest scoring (Collins, 2003)-style dependencies occurring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths  70 (dev set, gold segmentation).", "Table 9: Dev set results for sentences of length  70."], "label": "Prov", "citing": "J13-1007", "vector": [16, 2, 6, 0.09816956803310375], "context": ["", "The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).", ""], "marker": "Green and Manning 2010", "vector_1": {"constitu": 1, "gold": 1, "appli": 1, "parser": 1, "obtain": 1, "report": 1, "tagger": 1, "result": 2, "best": 1, "use": 1, "statesplit": 1, "wordsegment": 1, "fscore": 1, "200": 1, "pipelin": 1, "sentenc": 1, "arab": 1, "pars": 1, "known": 1, "segment": 1, "man": 1, "word": 2, "f": 1, "howev": 1, "manual": 1, "prior": 1, "green": 1, "model": 1}, "vector_2": [3, 0.5, 0, 2, 2, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply Arabic) because of the unusual opportunity it presents for comparison to English parsing results."], "label": "Prov", "citing": "J13-1008", "vector": [5, 0, 0, 0.2581988897471611], "context": ["", "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)", ""], "marker": "Green and Manning 2010", "vector_1": {"work": 1, "diab": 1, "arab": 1, "gabbard": 1, "patb": 1, "green": 1, "result": 1, "report": 1, "marcu": 1, "man": 1, "msa": 1, "kulick": 1}, "vector_2": [3, 0.28304034676766127, 0, 0, 20, 1]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (3).", "We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (4).", "When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct  ?f iDafa.", "mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences."], "label": "Prov", "citing": "J13-1008", "vector": [8, 0, 1, 0.19569842191603265], "context": ["", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.", ""], "marker": "2010", "vector_1": {"constitu": 1, "grammar": 1, "verbal": 1, "consist": 1, "equat": 1, "short": 1, "construct": 1, "claus": 1, "annot": 1, "includ": 1, "enhanc": 1, "label": 1, "patb": 1, "green": 1, "analyz": 1, "splitstat": 1, "recent": 1, "idafa": 1, "introduc": 1, "man": 1}, "vector_2": [3, 0.285568860964557, 1, 0, 20, 1]}, {"function": "Pos", "cited": "C10-1045", "provenance": ["We propose a limit of 70 words for Arabic parsing evaluations."], "label": "Prov", "citing": "J13-1008", "vector": [4, 1, 0, 0.10540925533894598], "context": ["", "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.", ""], "marker": "2010", "vector_1": {"comparison": 1, "adopt": 1, "evalu": 1, "suggest": 1, "work": 1, "qualiti": 1, "long": 1, "better": 1, "token": 1, "other": 1, "green": 1, "sentenc": 1, "pars": 1, "made": 1, "man": 1}, "vector_2": [3, 0.7708793975349723, 1, 0, 20, 1]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["In our grammar, features are realized as annotations to basic category labels.", "We start with noun features since written Arabic contains a very high proportion of NPs."], "label": "Prov", "citing": "J13-1009", "vector": [5, 0, 1, 0.3458572319330373], "context": ["", "The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.", ""], "marker": "2010", "vector_1": {"featur": 1, "grammar": 1, "similar": 1, "tabl": 1, "arab": 1, "green": 1, "ablat": 1, "contain": 1, "studi": 1, "come": 1, "man": 1}, "vector_2": [3, 0.276662294420341, 1, 0, 0, 1]}, {"function": "Pos", "cited": "C10-1045", "provenance": ["8 We use head-finding rules specified by a native speaker."], "label": "Prov", "citing": "J13-1009", "vector": [4, 0, 2, 0.5000000000000001], "context": ["", "For Arabic, we use the head-finding rules from Green and Manning (2010).", ""], "marker": "2010", "vector_1": {"use": 1, "rule": 1, "arab": 1, "green": 1, "headfind": 1, "man": 1}, "vector_2": [3, 0.27927302996670367, 1, 0, 0, 1]}, {"function": "Pos", "cited": "C10-1045", "provenance": ["By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English."], "label": "Prov", "citing": "J13-1009", "vector": [6, 0, 1, 0.2842676218074806], "context": ["", "We previously showed that the Kulick tag set is very effective for basic Arabic parsing (Green and Manning 2010).", ""], "marker": "Green and Manning 2010", "vector_1": {"set": 1, "show": 1, "effect": 1, "arab": 1, "tag": 1, "pars": 1, "kulick": 1, "basic": 1, "green": 1, "man": 1, "previous": 1}, "vector_2": [3, 0.4450862677832711, 0, 0, 0, 1]}, {"function": "Weak", "cited": "C10-1045", "provenance": ["Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1."], "label": "Prov", "citing": "J13-1009", "vector": [6, 1, 1, 0.36084391824351614], "context": ["", "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).", ""], "marker": "Green and Manning 2010", "vector_1": {"show": 1, "f": 1, "green": 1, "accuraci": 1, "arab": 1, "0": 1, "decreas": 1, "error": 1, "pars": 1, "man": 1, "segment": 1, "previous": 1}, "vector_2": [3, 0.5356548279689234, 0, 0, 0, 1]}, {"function": "Pos", "cited": "C10-1045", "provenance": ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply Arabic) because of the unusual opportunity it presents for comparison to English parsing results."], "label": "Prov", "citing": "J13-1009", "vector": [4, 0, 0, 0.08944271909999159], "context": ["", "We previously showed optimal Berkeley parser (Petrov et al 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al 2011) data sets", ""], "marker": "Green and Manning 2010", "vector_1": {"set": 1, "show": 1, "optim": 1, "parser": 1, "al": 2, "french": 1, "arab": 1, "rameter": 1, "petrov": 1, "pa": 1, "green": 2, "berkeley": 1, "et": 2, "man": 1, "data": 1, "previous": 1}, "vector_2": [3, 0.5452149127232369, 0, 0, 0, 1]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply Arabic) because of the unusual opportunity it presents for comparison to English parsing results."], "label": "Prov", "citing": "P11-1159", "vector": [3, 0, 0, 0.29999999999999993], "context": ["", "As for work on Arabic, results have been reported on PATB (Kulick et al., 2006; Diab, 2007; Green and Manning, 2010)", ""], "marker": "reen and Manning, 2010", "vector_1": {"arab": 1, "report": 1, "work": 1, "result": 1, "patb": 1}, "vector_2": [1, 0.4196944851989298, 3, 2, 0, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply Arabic) because of the unusual opportunity it presents for comparison to English parsing results."], "label": "Prov", "citing": "P11-1159", "vector": [2, 0, 0, 0.0], "context": ["", "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency", ""], "marker": "2010", "vector_1": {"consist": 1, "annot": 1, "patb": 1, "green": 1, "analyz": 1, "recent": 1, "man": 1}, "vector_2": [1, 0.42536175599090936, 1, 0, 14, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply Arabic) because of the unusual opportunity it presents for comparison to English parsing results."], "label": "Prov", "citing": "P11-2037", "vector": [5, 0, 0, 0.10540925533894598], "context": ["", "We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).", ""], "marker": "Green and Manning, 2010", "vector_1": {"task": 1, "commun": 1, "process": 1, "appli": 1, "gener": 1, "parser": 1, "empti": 1, "element": 1, "arab": 1, "joint": 1, "syntacticpars": 1, "allow": 1, "hebrew": 1, "cliticsegment": 1, "recent": 1, "produc": 1, "latticepars": 1, "mean": 1}, "vector_2": [1, 0.432488986784141, 6, 1, 0, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c)."], "label": "Prov", "citing": "P11-2122", "vector": [3, 0, 0, 0.09805806756909201], "context": ["", "Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)", ""], "marker": "Green and Manning, 2010", "vector_1": {"detect": 1, "treebank": 1, "work": 1, "focus": 1, "error": 1, "import": 1, "therefor": 1, "recent": 1}, "vector_2": [1, 0.05182403433476395, 1, 3, 0, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency"], "label": "Prov", "citing": "P11-2122", "vector": [5, 0, 1, 0.30151134457776363], "context": ["", "Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB)", ""], "marker": "2010", "vector_1": {"atb": 1, "consist": 1, "treebank": 1, "annot": 1, "arab": 1, "green": 1, "penn": 1, "discuss": 1, "man": 1}, "vector_2": [1, 0.6219957081545064, 1, 0, 2, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c)."], "label": "Prov", "citing": "P11-2122", "vector": [3, 0, 0, 0.15384615384615385], "context": ["", "Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as gold errors.", ""], "marker": "2010", "vector_1": {"even": 1, "measur": 1, "use": 1, "recal": 1, "gold": 1, "tricki": 1, "green": 1, "error": 2, "identifi": 1, "man": 1}, "vector_2": [1, 0.866362660944206, 1, 0, 2, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.", "We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton."], "label": "Prov", "citing": "P11-2124", "vector": [2, 0, 0, 0.07372097807744857], "context": ["", "Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.", ""], "marker": "2010", "vector_1": {"effect": 1, "arab": 1, "green": 1, "pars": 1, "recent": 1, "latticepars": 1, "demonstr": 1, "man": 1}, "vector_2": [1, 0.14235909355026147, 1, 0, 3, 0]}, {"function": "Pos", "cited": "C10-1045", "provenance": ["Better Arabic Parsing: Baselines, Evaluations, and Analysis"], "label": "Prov", "citing": "P12-1016", "vector": [1, 0, 0, 0.16666666666666669], "context": ["", "The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).", ""], "marker": "Green and Manning, 2010", "vector_1": {"stanford": 1, "parser": 1, "arab": 1, "packag": 1, "preprocess": 1, "data": 1}, "vector_2": [2, 0.7311106090436691, 1, 1, 3, 1]}, {"function": "Pos", "cited": "C10-1045", "provenance": ["Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text."], "label": "Prov", "citing": "W13-4904", "vector": [3, 0, 1, 0.07856742013183861], "context": ["", "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted", ""], "marker": "2010", "vector_1": {"node": 1, "head": 1, "sentenc": 1, "delet": 1, "other": 1, "green": 1, "x": 1, "follow": 1, "man": 1}, "vector_2": [3, 0.46200055881531155, 1, 0, 3, 0]}, {"function": "CoCo", "cited": "C10-1045", "provenance": ["Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1."], "label": "Prov", "citing": "W13-4904", "vector": [4, 1, 0, 0.16770509831248423], "context": ["", "Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).", ""], "marker": "2010", "vector_1": {"797": 1, "underperform": 1, "pipelin": 1, "f": 2, "point": 1, "opposit": 1, "parser": 1, "obtain": 1, "system": 1, "arab": 1, "vs": 1, "green": 1, "result": 1, "lattic": 1, "pars": 1, "experi": 1, "man": 1}, "vector_2": [3, 0.8812517462978485, 1, 0, 3, 0]}, {"function": "Error", "cited": "C10-1045", "provenance": ["Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1."], "label": "Prov", "citing": "W13-4904", "vector": [7, 1, 0, 0.16770509831248423], "context": ["", "Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.", ""], "marker": "2010", "vector_1": {"use": 1, "drop": 1, "gold": 1, "mada": 1, "provid": 1, "f": 1, "automat": 1, "constitu": 1, "token": 2, "score": 1, "green": 1, "result": 1, "pars": 1, "instead": 1, "work": 1, "find": 1, "man": 1}, "vector_2": [3, 0.9151438949427214, 2, 0, 3, 0]}, {"function": "Pos", "cited": "D10-1083", "provenance": ["TheFigure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration"], "label": "Prov", "citing": "D11-1056", "vector": [3, 0, 1, 0.0778498944161523], "context": ["", "Following Lee et al (2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.", ""], "marker": "2010", "vector_1": {"set": 1, "lee": 1, "f": 1, "median": 1, "al": 1, "base": 1, "valu": 1, "et": 1, "report": 1, "follow": 1, "hyperparamet": 1, "score": 1, "infer": 1, "best": 1, "addit": 1}, "vector_2": [1, 0.7661660958402857, 1, 0, 1, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon."], "label": "Prov", "citing": "D13-1004", "vector": [1, 0, 0, 0.0], "context": ["", "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)", ""], "marker": "Lee et al., 2010", "vector_1": {"use": 1, "featur": 1, "morphemelik": 1, "often": 1, "make": 1, "system": 1, "charact": 1, "categori": 1, "wordfin": 1, "induc": 1, "syntact": 1}, "vector_2": [3, 0.12470456156936895, 4, 1, 4, 0]}, {"function": "Weak", "cited": "D10-1083", "provenance": ["First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).", "Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution  over tag assignments drawn from DIRICHLET(, K ).", "During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior:"], "label": "Prov", "citing": "P11-1087", "vector": [12, 0, 1, 0.20241022618818408], "context": ["", "Recently Lee et al (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al (1992)s one-class HMM.", ""], "marker": "2010", "vector_1": {"omit": 1, "approxim": 1, "al": 2, "one": 1, "sampl": 1, "result": 1, "et": 2, "event": 1, "lee": 1, "per": 1, "sampler": 1, "type": 1, "collaps": 1, "brown": 1, "gibb": 1, "deriv": 1, "form": 1, "underperform": 1, "hmm": 2, "interdepend": 1, "class": 1, "recent": 1, "sparsiti": 1, "dirichlet": 1, "word": 1, "constraint": 1, "oneclass": 1, "howev": 1, "work": 1, "prior": 1, "achiev": 1, "combin": 1, "model": 2}, "vector_2": [1, 0.21839604446470526, 3, 0, 0, 0]}, {"function": "CoCo", "cited": "D10-1083", "provenance": ["5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5)."], "label": "Prov", "citing": "P11-1087", "vector": [3, 0, 0, 0.05698028822981897], "context": ["", "It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al (2010). That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%", ""], "marker": "2010", "vector_1": {"independ": 1, "bigram": 1, "lee": 1, "compar": 1, "incorrectli": 1, "pyphmm": 1, "relat": 1, "al": 1, "condit": 1, "also": 1, "sampl": 1, "result": 1, "interest": 1, "et": 1, "close": 1, "model": 2, "accuraci": 1, "assum": 1, "distribut": 1}, "vector_2": [1, 0.8365185678717958, 1, 0, 0, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model."], "label": "Prov", "citing": "P13-1150", "vector": [3, 0, 0, 0.11547005383792514], "context": ["", "Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)", ""], "marker": "Lee et al., 2010", "vector_1": {"develop": 1, "tag": 1, "similar": 1, "partofspeech": 1, "constraint": 1}, "vector_2": [3, 0.24278718898888302, 2, 1, 1, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model."], "label": "Prov", "citing": "W12-1914", "vector": [4, 0, 0, 0.1076763804116331], "context": ["", "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al (2010), Lamar et al", ""], "marker": "2010", "vector_1": {"lamar": 1, "task": 1, "lee": 1, "knight": 1, "unsupervis": 1, "work": 1, "al": 2, "categori": 1, "cast": 1, "second": 1, "tag": 1, "ravi": 1, "includ": 1, "partofspeech": 1, "learn": 1, "et": 2, "recent": 1}, "vector_2": [2, 0.07699508542007957, 2, 0, 0, 0]}, {"function": "Pos", "cited": "E09-2008", "provenance": ["Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications."], "label": "Prov", "citing": "N13-1140", "vector": [1, 0, 0, 0.05679618342470648], "context": ["", "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.", ""], "marker": "Hulden, 2009", "vector_1": {"use": 1, "develop": 1, "guesser": 1, "tool": 1, "foma": 1, "russian": 1, "paper": 1, "opensourc": 1, "releas": 1, "recent": 1}, "vector_2": [4, 0.8648809302671174, 1, 1, 0, 0]}, {"function": "Pos", "cited": "E09-2008", "provenance": ["The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&Ts fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005)."], "label": "Prov", "citing": "W11-2605", "vector": [5, 0, 0, 0.20025046972870353], "context": ["", "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).", ""], "marker": "Hulden, 2009a", "vector_1": {"avail": 1, "convert": 1, "foma": 1, "transduc": 1, "equival": 1, "format": 1, "phonolog": 1, "replac": 1, "later": 1, "use": 1, "rule": 2, "socal": 1, "finitest": 1, "learn": 1, "toolkit": 1, "freeli": 1}, "vector_2": [2, 0.35484068065866253, 2, 1, 1, 1]}, {"function": "Neut", "cited": "E09-2008", "provenance": ["Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers."], "label": "Prov", "citing": "W12-6202", "vector": [9, 0, 0, 0.036084391824351615], "context": ["", "Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.", ""], "marker": "2009a", "vector_1": {"function": 3, "decid": 1, "violat": 1, "effici": 1, "algorithm": 1, "transduc": 2, "constraint": 1, "foma": 1, "question": 2, "given": 1, "test": 1, "calcul": 1, "permut": 1, "command": 1, "includ": 1, "hulden": 1, "address": 1, "known": 1, "necessari": 1, "sinc": 1, "marker": 1}, "vector_2": [3, 0.5930456112437019, 2, 0, 0, 0]}, {"function": "Neut", "cited": "E09-2008", "provenance": ["This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately."], "label": "Prov", "citing": "W12-6213", "vector": [4, 0, 0, 0.0], "context": ["", "This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).", ""], "marker": "Hulden, 2009a", "vector_1": {"applic": 1, "use": 1, "lexicon": 1, "transduc": 1, "weight": 1, "frequenc": 1, "spell": 1, "reflect": 1, "inform": 1, "exampl": 1, "integr": 1, "error": 1, "model": 1, "check": 1, "er": 1}, "vector_2": [3, 0.8482956584140653, 2, 1, 1, 1]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["The focus of our work is on the use of contextual role knowledge for coreference resolution."], "label": "Prov", "citing": "E12-1054", "vector": [6, 0, 1, 0.23717082451262844], "context": ["", "Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and Desilets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)", ""], "marker": "Bean and Riloff, 2004", "vector_1": {"applic": 1, "measur": 1, "term": 1, "context": 1, "fit": 1, "nlp": 1, "optic": 1, "resolut": 1, "differ": 1, "contextu": 1, "charact": 1, "compon": 1, "recognit": 2, "speech": 1, "key": 1, "corefer": 1, "like": 1}, "vector_2": [8, 0.03858202275923795, 3, 1, 0, 0]}, {"function": "Pos", "cited": "N04-1038", "provenance": ["Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes."], "label": "Prov", "citing": "N13-1104", "vector": [2, 0, 0, 0.0], "context": ["", "the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).", ""], "marker": "2004", "vector_1": {"follow": 1, "head": 2, "depend": 2, "casefram": 1, "relat": 1, "argument": 1, "emit": 1, "bean": 1, "call": 1, "riloff": 1, "depij": 1, "pair": 1, "instead": 1, "model": 1, "event": 3}, "vector_2": [9, 0.4869364086525932, 1, 0, 1, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution"], "label": "Prov", "citing": "P05-1020", "vector": [4, 0, 0, 0.2036532699906392], "context": ["", "A learning-based coreference system can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set 2 Examples of such scoring functions include the DempsterShafer rule (see Kehler (1997) and Bean and Riloff (2004))", ""], "marker": "2004", "vector_1": {"set": 1, "creat": 1, "four": 1, "see": 1, "featur": 1, "use": 1, "score": 1, "learner": 1, "system": 1, "classifi": 1, "corefer": 2, "dempstershaf": 1, "exampl": 1, "includ": 1, "kehler": 1, "method": 1, "function": 1, "bean": 1, "train": 2, "learningbas": 1, "algorithm": 1, "rule": 1, "element": 1, "defin": 1, "instanc": 1, "riloff": 1, "learn": 1}, "vector_2": [1, 0.2554170073589534, 2, 0, 0, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision."], "label": "Prov", "citing": "P05-1020", "vector": [1, 0, 0, 0.0], "context": ["", "(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).", ""], "marker": "2004", "vector_1": {"cardi": 1, "unsupervis": 1, "bean": 1, "riloff": 1, "wagstaff": 1, "approach": 1, "eg": 1}, "vector_2": [1, 0.9342804578904333, 3, 0, 0, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution We generate these caseframes automatically by running AutoSlog over the training corpus exhaustively so that it literally generates a pattern to extract every noun phrase in the corpus."], "label": "Prov", "citing": "P05-1021", "vector": [7, 0, 1, 0.22360679774997896], "context": ["", "Recently, Bean and Riloff (2004) presented an unsupervised approach to coreference resolution, which mined the co-referring NP pairs with similar predicate- arguments from a large corpus using a bootstrapping method.", ""], "marker": "2004", "vector_1": {"corpu": 1, "use": 1, "bootstrap": 1, "argument": 1, "similar": 1, "unsupervis": 1, "resolut": 1, "coref": 1, "mine": 1, "corefer": 1, "pair": 1, "bean": 1, "larg": 1, "riloff": 1, "np": 1, "predic": 1, "approach": 1, "method": 1, "present": 1, "recent": 1}, "vector_2": [1, 0.07684404449798059, 1, 0, 0, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each another.1 2.1.2 Syntactic Seeding BABAR also uses syntactic heuristics to identify anaphors and antecedents that can be easily resolved.", "Ex: Mr. Bush disclosed the policy by reading it"], "label": "Prov", "citing": "P06-1005", "vector": [4, 0, 0, 0.06839855680567693], "context": ["", "Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)", ""], "marker": "Bean and Riloff, 2004", "vector_1": {"corpu": 1, "supervis": 1, "use": 1, "featur": 1, "set": 1, "specif": 1, "pattern": 1, "coarser": 1, "mine": 1, "kind": 1, "research": 1, "smaller": 1, "exist": 1, "learn": 2, "manuallydefin": 1, "corefer": 1, "data": 1, "sinc": 1}, "vector_2": [2, 0.46251583958354736, 4, 2, 0, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning."], "label": "Prov", "citing": "P06-1005", "vector": [4, 0, 0, 0.0734718358370645], "context": ["", "Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents. They give the example Mr. Bush disclosed the policy by reading it. Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.", ""], "marker": "2004", "vector_1": {"easilyresolv": 1, "semant": 1, "identifi": 2, "give": 1, "certain": 1, "knowledg": 1, "compat": 2, "use": 1, "polici": 2, "readobjpolici": 1, "corefer": 1, "read": 1, "bush": 1, "call": 1, "includ": 1, "contextualrol": 1, "anaphor": 1, "extend": 1, "disclos": 1, "bean": 1, "part": 1, "case": 1, "anteced": 1, "bootstrap": 1, "exampl": 1, "riloff": 1, "mr": 1, "model": 2}, "vector_2": [2, 0.6854686804342615, 1, 0, 0, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.", "The focus of our work is on the use of contextual role knowledge for coreference resolution.", "Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.", "We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain.", "Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found.", "One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves."], "label": "Prov", "citing": "P07-1067", "vector": [40, 0, 31, 0.5941943013634092], "context": ["", "Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. murder of <NP>, killed <patient>. From the case- frames, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes.", ""], "marker": "2004", "vector_1": {"set": 1, "unannot": 1, "appli": 1, "casefram": 4, "knowledg": 2, "repres": 1, "murder": 1, "kill": 1, "frame": 1, "ie": 1, "deriv": 1, "use": 1, "fill": 1, "extract": 1, "system": 1, "corefer": 1, "compon": 1, "substitut": 1, "call": 1, "text": 1, "np": 1, "role": 2, "anaphor": 1, "babar": 1, "type": 1, "eg": 1, "express": 1, "patient": 1, "syntact": 1, "gener": 1, "resolut": 2, "differ": 1, "cooccur": 1, "bean": 1, "candid": 1, "present": 1, "case": 1, "anteced": 1, "whether": 2, "contextu": 2, "exampl": 1, "riloff": 1, "posit": 1, "linguist": 1}, "vector_2": [3, 0.20856863975458412, 1, 0, 1, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["A contextual role represents the role that a noun phrase plays in an event or relationship."], "label": "Prov", "citing": "P07-1068", "vector": [5, 0, 1, 0.31980107453341566], "context": ["", "(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).", ""], "marker": "2004", "vector_1": {"play": 1, "wikipedia": 1, "bean": 1, "contextu": 1, "see": 1, "role": 1, "riloff": 1, "np": 1}, "vector_2": [3, 0.04533640401381806, 3, 0, 5, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["The focus of our work is on the use of contextual role knowledge for coreference resolution."], "label": "Prov", "citing": "P09-1074", "vector": [1, 0, 0, 0.0944911182523068], "context": ["", "Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).", ""], "marker": "2004", "vector_1": {"riloff": 1, "cardi": 1, "determin": 1, "success": 1, "eg": 1, "modul": 1, "system": 1, "corefer": 1, "bean": 1, "incorpor": 1, "anaphor": 1, "ng": 1, "final": 1, "sever": 1}, "vector_2": [5, 0.607782853127196, 2, 0, 3, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning."], "label": "Prov", "citing": "P11-1082", "vector": [7, 0, 2, 0.2649064714130087], "context": ["", "However, the use of related verbs is similar in spirit to Bean and Riloffs (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).", ""], "marker": "2004", "vector_1": {"use": 3, "knowledg": 1, "ponzetto": 1, "pattern": 1, "howev": 1, "relat": 1, "semant": 1, "contextu": 1, "discuss": 1, "also": 1, "bean": 1, "verb": 1, "role": 2, "riloff": 1, "strube": 1, "induc": 1, "similar": 1, "spirit": 1}, "vector_2": [7, 0.48848085518131135, 2, 0, 45, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes."], "label": "Prov", "citing": "P13-1121", "vector": [3, 0, 0, 0.14142135623730948], "context": ["", "Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)", ""], "marker": "2004", "vector_1": {"va": 1, "semant": 1, "task": 1, "depend": 1, "nlp": 1, "casefram": 2, "resolut": 1, "well": 1, "use": 1, "summar": 1, "approximationsth": 1, "role": 1, "consid": 1, "rieti": 1, "corefer": 1, "relev": 1, "ground": 1}, "vector_2": [9, 0.34208378672470074, 1, 0, 0, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.", "We evaluated BABAR on two domains: terrorism and natural disasters."], "label": "Prov", "citing": "P13-2015", "vector": [16, 0, 9, 0.511207720338155], "context": ["", "In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves.", ""], "marker": "Bean and Riloff, 2004", "vector_1": {"compat": 1, "context": 1, "use": 2, "resolut": 1, "acquir": 1, "natur": 1, "knowledg": 1, "domain": 1, "contextu": 1, "corefer": 1, "inform": 1, "np": 2, "role": 1, "lexic": 1, "disast": 1, "terror": 1, "surround": 1, "babar": 2, "match": 1, "addit": 1}, "vector_2": [9, 0.34818455684160454, 1, 1, 5, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning."], "label": "Prov", "citing": "W05-0612", "vector": [6, 0, 1, 0.30656966974248284], "context": ["", "There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).", ""], "marker": "Bean and Riloff, 2004", "vector_1": {"use": 2, "inform": 1, "extract": 1, "unsupervis": 1, "resolut": 1, "number": 1, "contextu": 1, "also": 1, "gender": 1, "approach": 1, "method": 1, "anaphora": 1, "roleknowledg": 1}, "vector_2": [1, 0.16214136528525888, 2, 1, 0, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.", "These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible."], "label": "Prov", "citing": "W06-0106", "vector": [9, 0, 5, 0.343203236491822], "context": ["", "Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.", ""], "marker": "2004", "vector_1": {"compat": 1, "use": 1, "identifi": 1, "determin": 1, "would": 1, "pattern": 1, "contextu": 1, "clue": 1, "inform": 1, "bean": 1, "riloff": 1, "np": 1, "extract": 1}, "vector_2": [2, 0.8935582822085889, 1, 0, 0, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["The focus of our work is on the use of contextual role knowledge for coreference resolution."], "label": "Prov", "citing": "W06-0206", "vector": [4, 0, 1, 0.1889822365046136], "context": ["", "It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)", ""], "marker": "Bean and Riloff, 2004", "vector_1": {"shown": 1, "task": 1, "name": 1, "perform": 1, "chunk": 1, "resolut": 1, "semant": 1, "corefer": 1, "tag": 1, "promis": 1, "mani": 1, "improv": 1, "extract": 1, "class": 1}, "vector_2": [2, 0.04985300815701213, 4, 1, 1, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphors caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions.", "If so, the CF Network reports that the anaphor and candidate may be coreferent."], "label": "Prov", "citing": "W10-3909", "vector": [6, 0, 0, 0.0], "context": ["", "Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 6068, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task.", ""], "marker": "2004", "vector_1": {"nlp": 1, "challeng": 1, "knowledg": 1, "second": 1, "explor": 1, "explos": 1, "68": 1, "anaphora": 1, "use": 1, "acquir": 1, "receiv": 1, "workshop": 1, "method": 1, "ponzetto": 1, "august": 1, "dagan": 1, "itai": 1, "resolut": 1, "beij": 1, "bean": 1, "attent": 1, "increas": 1, "recent": 1, "task": 1, "proceed": 1, "work": 1, "su": 1, "nlpix": 1, "inform": 1, "yang": 1, "era": 1, "riloff": 1, "strube": 1, "page": 1}, "vector_2": [6, 0.1085029045079322, 4, 0, 4, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["The focus of our work is on the use of contextual role knowledge for coreference resolution."], "label": "Prov", "citing": "W10-3909", "vector": [7, 0, 3, 0.46915743162841816], "context": ["", "Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two specific domains, terrorism and natural disasters.", ""], "marker": "2004", "vector_1": {"domain": 1, "identifi": 1, "natur": 1, "knowledg": 2, "two": 1, "substanti": 1, "heurist": 1, "articl": 1, "use": 3, "system": 1, "corefer": 2, "role": 2, "terror": 1, "got": 1, "relat": 1, "syntact": 1, "resolut": 1, "lexic": 1, "bean": 1, "train": 1, "gain": 1, "disast": 1, "data": 1, "present": 1, "specif": 1, "highconfid": 1, "contextu": 2, "riloff": 1, "learn": 1, "aid": 1}, "vector_2": [6, 0.16917484798043278, 1, 0, 4, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["Table 1: Syntactic Seeding Heuristics BABARs reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.", "The confidence level is then used as the belief value for the knowledge source.", "Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1.", "The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision.", "The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters."], "label": "Prov", "citing": "W10-3909", "vector": [12, 0, 4, 0.19696969696969696], "context": ["", "Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters.", ""], "marker": "2004", "vector_1": {"featur": 2, "identifi": 1, "natur": 1, "casefram": 1, "pair": 2, "involv": 1, "use": 3, "acquir": 1, "two": 1, "corefer": 2, "role": 1, "domainspecif": 1, "refer": 1, "resolut": 1, "mention": 1, "bean": 1, "disast": 1, "terror": 1, "handcod": 1, "network": 1, "corpora": 1, "rule": 1, "riloff": 1, "highprecis": 1, "improv": 1}, "vector_2": [6, 0.21894214763732717, 1, 0, 4, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).", "In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs."], "label": "Prov", "citing": "D10-1005", "vector": [6, 0, 1, 0.1796053020267749], "context": ["", "Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006).", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"multilingu": 1, "text": 1, "wordlevel": 1, "topic": 1, "either": 1, "model": 1, "document": 1, "parallel": 1, "requir": 1}, "vector_2": [4, 0.8395623853791184, 4, 1, 0, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs."], "label": "Prov", "citing": "D11-1084", "vector": [1, 0, 0, 0.0], "context": ["", "There are only a few studies on document-level SMT. Representative work includes Zhao et al (2006), Tam et al (2007), Carpuat (2009).", ""], "marker": "2006", "vector_1": {"smt": 1, "documentlevel": 1, "carpuat": 1, "work": 1, "al": 2, "repres": 1, "zhao": 1, "includ": 1, "tam": 1, "et": 2, "studi": 1}, "vector_2": [5, 0.1979576030521213, 3, 0, 7, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.", "In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs."], "label": "Prov", "citing": "D11-1084", "vector": [18, 0, 12, 0.46499055497527714], "context": ["", "Zhao et al (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.", ""], "marker": "2006", "vector_1": {"help": 1, "show": 1, "within": 1, "qualiti": 1, "al": 1, "topic": 1, "et": 1, "follow": 1, "constitut": 1, "perform": 1, "assum": 1, "smt": 1, "hidden": 1, "document": 1, "sentenc": 1, "mixtur": 1, "translat": 1, "pair": 3, "parallel": 1, "topicspecif": 1, "word": 2, "documentlevel": 1, "align": 1, "indirectli": 1, "inform": 1, "zhao": 1, "bilingu": 1, "improv": 2, "model": 1}, "vector_2": [5, 0.20300622472103497, 1, 0, 7, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn )."], "label": "Prov", "citing": "D13-1141", "vector": [2, 0, 0, 0.056613851707229795], "context": ["", "These methods ensure that bilingual embeddings retain els (Peirsman and Pado , 2010; Sumita, 2000), their translational equivalence while their distribu and with unsupervised algorithms such as LDA and LSA (BoydGraber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006).", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"distribu": 1, "el": 1, "lda": 1, "algorithm": 1, "equival": 1, "lsa": 1, "unsupervis": 1, "translat": 1, "bilingu": 1, "ensur": 1, "retain": 1, "embed": 1, "method": 1}, "vector_2": [7, 0.3751735078326393, 5, 1, 0, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.", "The translation lexicon p(f |e) is the key component in this generative process.", "Topic-specific translation lexicons are learned by a 3-topic BiTAM1."], "label": "Prov", "citing": "P07-1066", "vector": [11, 0, 4, 0.4365641250653993], "context": ["", "Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"lexicon": 1, "topicdepend": 1, "topic": 2, "admixtur": 1, "bitam": 2, "respect": 1, "denot": 1, "transla": 1, "includ": 1, "basic": 1, "rce": 1, "sourc": 1, "index": 1, "chines": 1, "relat": 1, "word": 3, "tion": 1, "c": 1, "e": 1, "target": 1, "consist": 1, "english": 1, "align": 1, "work": 1, "p": 1, "bilingu": 1, "model": 3, "k": 2, "propos": 1}, "vector_2": [1, 0.17197851738602063, 1, 1, 0, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.", "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT."], "label": "Prov", "citing": "P10-1115", "vector": [5, 0, 0, 0.08111071056538126], "context": ["", "Some previous work on multilingual topic models assume documents in multiple languages are aligned either at the document level, sentence level or by time stamps (Mimno et al., 2009; Zhao and Xing, 2006; Kim and Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"multilingu": 1, "previou": 1, "multipl": 1, "level": 2, "stamp": 1, "sentenc": 1, "align": 1, "work": 1, "topic": 1, "either": 1, "time": 1, "model": 1, "document": 2, "assum": 1, "languag": 1}, "vector_2": [4, 0.17681337667281619, 5, 1, 0, 0]}, {"function": "Pos", "cited": "P06-2124", "provenance": ["Topic-specific translation lexicons are learned by a 3-topic BiTAM1."], "label": "Prov", "citing": "P10-2025", "vector": [0, 1, 0, 0.0], "context": ["", "We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"topic": 1, "follow": 1, "k": 1, "adopt": 1, "set": 1}, "vector_2": [4, 0.6127854487519915, 1, 2, 2, 0]}, {"function": "Pos", "cited": "P06-2124", "provenance": ["Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA).", "Inter takes the intersection of the two directions and generates high-precision alignments;"], "label": "Prov", "citing": "P10-2025", "vector": [5, 0, 0, 0.15309310892394865], "context": ["", "The alignment results for both directions were refined with GROW heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"precis": 1, "accord": 1, "previou": 1, "recal": 1, "align": 1, "refin": 1, "direct": 1, "yield": 1, "high": 2, "result": 1, "heurist": 1, "work": 1, "grow": 1}, "vector_2": [4, 0.6194237918215614, 2, 2, 2, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V .", "The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003).", "To reduce the data sparsity problem, we introduce two remedies in our models.", "First: Laplace smoothing."], "label": "Prov", "citing": "P11-2032", "vector": [4, 0, 1, 0.12598815766974242], "context": ["", "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.", ""], "marker": "2006", "vector_1": {"em": 1, "sparsiti": 1, "use": 2, "dirichlet": 1, "xing": 1, "map": 1, "variat": 1, "solut": 1, "note": 1, "prior": 1, "zhao": 1, "estim": 1, "symmetr": 1, "suffer": 1, "data": 1, "find": 1, "paramet": 1}, "vector_2": [5, 0.13687210545065906, 1, 0, 0, 0]}, {"function": "Pos", "cited": "P06-2124", "provenance": ["Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.", "Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).", "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT."], "label": "Prov", "citing": "P12-1048", "vector": [4, 0, 0, 0.05429252897979032], "context": ["", "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"shown": 1, "appear": 1, "specif": 1, "inform": 1, "inspir": 1, "great": 1, "effect": 1, "topic": 2, "alway": 1, "translat": 2, "context": 2, "particular": 1, "studi": 1, "approach": 1, "select": 1, "recent": 1}, "vector_2": [6, 0.1186516237440417, 5, 2, 2, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.", "Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices."], "label": "Prov", "citing": "P12-1048", "vector": [7, 0, 2, 0.20464687117164013], "context": ["", "Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.", ""], "marker": "2006", "vector_1": {"captur": 1, "share": 1, "topic": 3, "admixtur": 1, "follow": 1, "assum": 1, "differ": 1, "xing": 1, "constitut": 1, "granular": 1, "topicspecif": 1, "hidden": 1, "sentenc": 1, "mixtur": 1, "translat": 1, "pair": 1, "present": 1, "formal": 1, "word": 2, "level": 1, "align": 1, "zhao": 1, "bilingu": 3, "improv": 1, "model": 1, "linguist": 1}, "vector_2": [6, 0.8427008675971331, 0, 0, 3, 0]}, {"function": "CoCo", "cited": "P06-2124", "provenance": ["Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.", "Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn )."], "label": "Prov", "citing": "P12-1048", "vector": [12, 1, 0, 0.06201736729460422], "context": ["", " In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model  HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"lda": 1, "lexicon": 1, "directli": 1, "topicdepend": 2, "appli": 1, "topic": 2, "monolingu": 1, "aspect": 1, "phrase": 1, "follow": 1, "work": 1, "extract": 1, "probabl": 2, "differ": 3, "also": 1, "rather": 1, "process": 1, "htmm": 1, "estim": 1, "take": 1, "method": 2, "assumpt": 1, "previou": 1, "use": 1, "lexic": 1, "util": 1, "train": 1, "translat": 1, "pair": 1, "topicspecif": 1, "addit": 1, "account": 1, "word": 1, "indomain": 1, "corpora": 1, "plsa": 1, "context": 1, "model": 3}, "vector_2": [6, 0.9136860875827304, 4, 2, 2, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.", "We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT."], "label": "Prov", "citing": "P12-2023", "vector": [9, 0, 2, 0.3484813916980649], "context": ["", "Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"use": 2, "learn": 1, "lsa": 1, "receiv": 1, "align": 1, "smt": 1, "topic": 2, "instanc": 1, "adapt": 1, "bitam": 1, "bilingu": 2, "model": 3}, "vector_2": [6, 0.2770396045959304, 2, 1, 0, 0]}, {"function": "Pos", "cited": "P06-2124", "provenance": ["We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT."], "label": "Prov", "citing": "P13-2122", "vector": [5, 0, 1, 0.18898223650461363], "context": ["", "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or biTAM (Zhao and Xing, 2006).", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"semant": 1, "analysi": 1, "domain": 1, "smt": 1, "latent": 1, "perform": 1, "avoid": 1, "hard": 1, "use": 2, "topic": 1, "membership": 1, "decis": 1, "bitam": 1, "need": 1, "improv": 1, "model": 1, "eg": 1}, "vector_2": [7, 0.2812275546849494, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.", "We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.", "Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE).", "As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM13 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1."], "label": "Prov", "citing": "W07-0722", "vector": [21, 1, 7, 0.34745409204160876], "context": ["", "In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"semant": 1, "captur": 1, "qualiti": 1, "topic": 3, "admixtur": 1, "coher": 1, "englishchines": 1, "baselin": 1, "latent": 1, "three": 1, "better": 1, "take": 1, "document": 1, "case": 1, "sophist": 1, "fairli": 1, "translat": 3, "bayesian": 1, "present": 1, "formal": 1, "reduc": 1, "task": 1, "word": 1, "ibm": 2, "level": 1, "provid": 1, "align": 1, "hmm": 1, "ambigu": 1, "bilingu": 1, "improv": 1, "model": 7, "order": 1, "propos": 1}, "vector_2": [1, 0.195428175599619, 1, 2, 0, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["To reduce the data sparsity problem, we introduce two remedies in our models.", "Second: interpolation smoothing.", "Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting:"], "label": "Prov", "citing": "W07-0722", "vector": [4, 0, 0, 0.14142135623730948], "context": ["", "A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"techniqu": 1, "possibl": 1, "solut": 1, "smooth": 1, "estim": 1, "interpol": 1, "sharp": 1, "implement": 1, "event": 1, "distribut": 1}, "vector_2": [1, 0.9820763702485064, 2, 2, 0, 0]}, {"function": "CoCo", "cited": "W04-0213", "provenance": ["A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees."], "label": "Prov", "citing": "P06-3008", "vector": [8, 0, 0, 0.2752409412815901], "context": ["", "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al 2003; Stede 2004).", ""], "marker": "Stede 2004", "vector_1": {"corpu": 2, "purpos": 1, "engin": 1, "stede": 1, "compar": 1, "german": 1, "commentari": 1, "chines": 1, "al": 1, "construct": 1, "carlson": 1, "investig": 1, "wsjrst": 1, "english": 1, "et": 1, "linguist": 1, "treebank": 1, "languag": 1, "potsdam": 1}, "vector_2": [2, 0.13981716607414932, 0, 0, 3, 0]}, {"function": "Neut", "cited": "W04-0213", "provenance": ["We follow the guidelines developed in the TIGER project (Brants et al 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures."], "label": "Prov", "citing": "P08-2062", "vector": [4, 0, 0, 0.03580574370197164], "context": ["", "Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser.", ""], "marker": "(Stede, 2004)", "vector_1": {"process": 1, "parser": 1, "automat": 1, "summaris": 1, "extract": 1, "highli": 1, "taboada": 1, "describ": 1, "due": 1, "alway": 1, "complet": 1, "applic": 2, "discours": 3, "sourc": 1, "genuin": 1, "cannot": 1, "relev": 1, "outlin": 1, "structur": 1, "ambigu": 1, "inform": 2, "emerg": 1, "limit": 1, "either": 1, "mann": 1}, "vector_2": [4, 0.059050339164584076, 2, 2, 0, 0]}, {"function": "Pos", "cited": "W04-0213", "provenance": ["That is, we can use the discourse parser on PCC texts, emulating for instance a co-reference oracle that adds the information from our co-reference annotations."], "label": "Prov", "citing": "P08-2062", "vector": [3, 0, 0, 0.07537783614444091], "context": ["", "Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.", ""], "marker": "2004", "vector_1": {"discours": 1, "like": 1, "stede": 1, "tree": 1, "annot": 1, "structur": 1, "one": 1, "binari": 1, "follow": 1, "model": 1, "scheme": 1}, "vector_2": [4, 0.17015351660121386, 1, 0, 12, 0]}, {"function": "Weak", "cited": "W04-0213", "provenance": ["The Potsdam Commentary Corpus"], "label": "Prov", "citing": "P13-1048", "vector": [2, 0, 0, 0.15430334996209194], "context": ["", "This clearly demonstrates the potential of TSP SW for datasets with even more leaky boundaries e.g., the Dutch (Vliet and Redeker, 2011) and the German Potsdam (Stede, 2004) corpora.", ""], "marker": "Stede, 2004", "vector_1": {"even": 1, "corpora": 1, "german": 1, "boundari": 1, "leaki": 1, "sw": 1, "clearli": 1, "dataset": 1, "demonstr": 1, "potenti": 1, "dutch": 1, "eg": 1, "tsp": 1, "potsdam": 1}, "vector_2": [9, 0.9231684044583345, 2, 1, 0, 0]}, {"function": "Neut", "cited": "W04-0213", "provenance": ["A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees."], "label": "Prov", "citing": "W07-1525", "vector": [6, 0, 4, 0.3333333333333333], "context": ["", "The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)", ""], "marker": "Stede, 2004", "vector_1": {"origin": 1, "corpu": 1, "author": 1, "german": 1, "guidelin": 1, "newspap": 1, "annot": 2, "draft": 1, "potsdam": 1, "commentari": 2, "pcc": 1}, "vector_2": [3, 0.7427729065359727, 1, 1, 0, 0]}, {"function": "Neut", "cited": "W04-0213", "provenance": ["All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988)."], "label": "Prov", "citing": "W12-3205", "vector": [6, 0, 1, 0.21677749238102995], "context": ["", "For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).", ""], "marker": "Stede, 2004", "vector_1": {"ldc": 1, "discours": 2, "corpora": 1, "portuges": 1, "german": 1, "text": 1, "treebank": 1, "relat": 1, "well": 1, "annot": 2, "consortium": 1, "avail": 1, "framework": 1, "spanish": 1, "similarli": 1, "english": 1, "rst": 2, "linguist": 1, "data": 1}, "vector_2": [8, 0.5087649269866613, 4, 1, 3, 0]}, {"function": "Pos", "cited": "W04-0213", "provenance": ["Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers."], "label": "Prov", "citing": "W13-2708", "vector": [1, 0, 0, 0.0], "context": ["", "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.", ""], "marker": "Stede, 2004", "vector_1": {"corpu": 1, "use": 1, "kupietz": 1, "text": 1, "object": 1, "lemma": 1, "figur": 1, "verb": 1, "speech": 1, "opinion": 1, "dereko": 1, "extract": 1, "indirect": 1, "first": 1}, "vector_2": [9, 0.7254536204849924, 1, 1, 1, 0]}, {"function": "Neut", "cited": "W04-0213", "provenance": ["A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure."], "label": "Prov", "citing": "W13-3306", "vector": [3, 0, 0, 0.10878565864408424], "context": ["", "For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)", ""], "marker": "Stede, 2004", "vector_1": {"discours": 1, "resourc": 1, "especi": 1, "german": 1, "relat": 1, "dc": 1, "french": 1, "annot": 1, "avail": 1, "languag": 1, "english": 1, "becom": 1, "sever": 1}, "vector_2": [9, 0.2384224792883595, 4, 1, 0, 0]}, {"function": "Pos", "cited": "W04-0213", "provenance": ["The Potsdam Commentary Corpus"], "label": "Prov", "citing": "W13-4002", "vector": [4, 0, 3, 0.4082482904638631], "context": ["", "We focus on two theories: RST, which offers the model for the annotations of the RST treebank Carlson, Marcu, and Okurowski 2002 and the Potsdam commentary corpus Stede 2004", ""], "marker": "Stede 2004", "vector_1": {"corpu": 1, "stede": 1, "offer": 1, "carlson": 1, "treebank": 1, "focu": 1, "two": 1, "commentari": 1, "theori": 1, "annot": 1, "rst": 2, "marcu": 1, "model": 1, "okurowski": 1, "potsdam": 1}, "vector_2": [9, 0.14307733063603303, 0, 0, 0, 0]}, {"function": "Neut", "cited": "W08-2222", "provenance": ["Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it."], "label": "Prov", "citing": "Q13-1015", "vector": [5, 0, 1, 0.18181818181818182], "context": ["", "This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).", ""], "marker": "Bos, 2008", "vector_1": {"determinist": 1, "form": 1, "straightforward": 1, "map": 1, "parser": 1, "system": 1, "boxer": 1, "rel": 1, "logic": 1, "output": 1, "mean": 1}, "vector_2": [5, 0.13673769687721402, 1, 2, 0, 0]}, {"function": "Pos", "cited": "W08-2222", "provenance": ["Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it."], "label": "Prov", "citing": "S12-1040", "vector": [3, 0, 1, 0.0657951694959769], "context": ["", "In this paper we present and evaluate a system that transforms texts into logical formulas  using the C&C tools and Boxer (Bos, 2008)  in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).", ""], "marker": "Bos, 2008", "vector_1": {"negat": 1, "use": 1, "task": 1, "recognis": 1, "evalu": 1, "cc": 1, "text": 2, "tool": 1, "english": 1, "share": 1, "transform": 1, "system": 1, "boxer": 1, "paper": 1, "context": 1, "logic": 1, "formula": 1, "present": 1}, "vector_2": [4, 0.05783860888466241, 2, 1, 2, 0]}, {"function": "Neut", "cited": "W08-2222", "provenance": ["Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.", "We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992)"], "label": "Prov", "citing": "W11-2408", "vector": [9, 0, 1, 0.058848988633649964], "context": ["", "Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.", ""], "marker": "2008", "vector_1": {"comput": 1, "directli": 1, "text": 1, "eg": 1, "qualiti": 1, "us": 1, "issu": 1, "rather": 1, "sententi": 1, "also": 1, "formul": 1, "statement": 1, "tens": 2, "deriv": 2, "form": 2, "gener": 1, "bo": 1, "affect": 1, "modifi": 1, "present": 1, "reshap": 1, "drop": 1, "rule": 1, "precis": 1, "allow": 1, "logic": 2}, "vector_2": [3, 0.9549596459255403, 1, 0, 2, 0]}, {"function": "Pos", "cited": "W08-2222", "provenance": ["It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic."], "label": "Prov", "citing": "W13-3209", "vector": [2, 0, 0, 0.07106690545187015], "context": ["", "This line of research converts logical representations obtained from syntactic parses using Bos Boxer (Bos, 2008)", ""], "marker": "Bos, 2008", "vector_1": {"represent": 1, "convert": 1, "bo": 1, "syntact": 1, "use": 1, "obtain": 1, "research": 1, "boxer": 1, "pars": 1, "logic": 1, "line": 1}, "vector_2": [5, 0.22210192002309803, 1, 1, 0, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["\\Ve treat context-sensitive spelling correction as a task of word disambiguation.", "\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers."], "label": "Prov", "citing": "A00-2019", "vector": [7, 0, 4, 0.23836564731139803], "context": ["", "Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're.", ""], "marker": "1995", "vector_1": {"result": 1, "among": 1, "use": 1, "detect": 1, "wsd": 1, "gold": 1, "show": 1, "could": 1, "spell": 1, "list": 1, "classifi": 1, "common": 1, "set": 1, "theyr": 1, "decis": 1, "error": 1, "bayesian": 1, "confus": 1, "method": 1, "adapt": 1}, "vector_2": [5, 0.11368621143442023, 1, 0, 1, 1]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["Table 1 shows the performance of the baseline method for 18 confusion sets.", "The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.", "and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]."], "label": "Prov", "citing": "E06-1030", "vector": [8, 1, 3, 0.2326210525996177], "context": ["", "The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.", ""], "marker": "1995", "vector_1": {"corpu": 1, "brown": 1, "use": 1, "set": 1, "word": 1, "gold": 1, "memorybas": 1, "section": 1, "wsj": 1, "learner": 1, "test": 1, "penn": 1, "confus": 1, "treebank": 1}, "vector_2": [11, 0.6580881096414778, 1, 0, 0, 1]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k.", ", c_ 1, c1, ... , cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.", "Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word."], "label": "Prov", "citing": "J98-1006", "vector": [20, 0, 12, 0.28573838340637214], "context": ["", "For each si, the probability is computed with Bayes' rule: As Golding (1995) points out, the term p(c_kf .. .,Ck I si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then", ""], "marker": "1995", "vector_1": {"ck": 1, "comput": 1, "often": 1, "gold": 1, "point": 1, "independ": 1, "done": 1, "assum": 1, "probabl": 1, "cue": 1, "estim": 1, "occurr": 1, "difficult": 1, "spars": 1, "data": 1, "term": 1, "rule": 1, "bay": 1, "si": 2, "pckf": 1, "problem": 1, "other": 1}, "vector_2": [3, 0.2752857735692459, 1, 0, 2, 1]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.", "The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction.", "A method for doing this, based on Bayesian classifiers, was presented.", "It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists."], "label": "Prov", "citing": "N03-2035", "vector": [23, 0, 12, 0.5443996397334733], "context": ["", "Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one. The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", ""], "marker": "3", "vector_1": {"spell": 1, "account": 1, "task": 1, "evid": 1, "gold": 1, "superior": 1, "appli": 1, "contextsentit": 1, "hybrid": 1, "list": 1, "one": 1, "avail": 1, "report": 1, "strongest": 1, "correct": 1, "take": 1, "bayesian": 1, "instead": 1, "decis": 1, "method": 2, "propos": 1}, "vector_2": [8, 0.2997847147470398, 1, 0, 0, 1]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers."], "label": "Prov", "citing": "N03-2035", "vector": [4, 0, 0, 0.19999999999999996], "context": ["", "Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.", ""], "marker": "3", "vector_1": {"strength": 1, "techniqu": 1, "list": 1, "hybrid": 1, "classifi": 1, "ngram": 1, "combin": 1, "decis": 1, "bayesian": 1, "approach": 1}, "vector_2": [8, 0.6637064944384643, 2, 0, 0, 1]}, {"function": "Error", "cited": "W95-0104", "provenance": ["A method is presented for doing this, based on Bayesian classifiers."], "label": "Prov", "citing": "N03-2035", "vector": [1, 0, 0, 0.0], "context": ["", "In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.", ""], "marker": "3", "vector_1": {"text": 1, "winnow": 1, "trigram": 1, "homograph": 1, "result": 1, "cdsa": 1, "differ": 1, "group": 1, "compar": 1, "bayseian": 1, "section": 1, "hybrid": 1, "three": 1, "classifi": 1, "experi": 1, "type": 1, "po": 1, "accord": 1, "depend": 1, "data": 1, "approach": 1, "ambigu": 1, "cisa": 1}, "vector_2": [8, 0.9146932185145318, 1, 0, 0, 1]}, {"function": "Pos", "cited": "W95-0104", "provenance": ["A method is presented for doing this, based on Bayesian classifiers.", "\\Ve then apply each of the two component methods mentioned above context words and collocations."], "label": "Prov", "citing": "N04-1016", "vector": [8, 0, 3, 0.23570226039551584], "context": ["", "These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations.", ""], "marker": "Golding, 1995", "vector_1": {"semant": 1, "featur": 1, "weight": 1, "lsa": 1, "er": 1, "despit": 1, "differ": 1, "colloc": 1, "latent": 1, "two": 1, "classifi": 1, "includ": 1, "varieti": 1, "approach": 1, "decis": 1, "analysi": 1, "use": 1, "mixtur": 1, "transformationbas": 1, "bayesian": 1, "multipl": 1, "word": 1, "augment": 1, "algorithm": 1, "type": 1, "list": 1, "context": 1, "learn": 1, "model": 1, "updat": 1}, "vector_2": [9, 0.38152107343689107, 7, 6, 8, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["Table 1 shows the performance of the baseline method for 18 confusion sets."], "label": "Prov", "citing": "N04-1016", "vector": [4, 1, 1, 0.21320071635561041], "context": ["", "All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).", ""], "marker": "1995", "vector_1": {"subset": 1, "origin": 1, "use": 1, "full": 1, "gold": 1, "gather": 1, "set": 2, "either": 1, "confus": 1, "method": 1}, "vector_2": [9, 0.39811257725070986, 1, 0, 3, 0]}, {"function": "Pos", "cited": "W95-0104", "provenance": ["The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction.", "The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera."], "label": "Prov", "citing": "N04-1016", "vector": [11, 1, 3, 0.06988328561118086], "context": ["", "Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71 Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24 Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.", ""], "marker": "1995", "vector_1": {"corpu": 1, "gold": 3, "martin": 1, "cucerzan": 2, "brill": 1, "88": 1, "tabl": 2, "80": 1, "simpl": 1, "87": 1, "best": 2, "baselin": 2, "web": 1, "00": 1, "roth": 1, "perform": 3, "test": 2, "874": 1, "979": 1, "accuraci": 1, "0": 1, "84": 1, "7": 3, "bnc": 5, "devis": 1, "method": 2, "brown": 1, "894": 1, "w": 23, "898": 1, "spell": 3, "use": 2, "77": 1, "98": 1, "schabe": 1, "train": 2, "89477": 1, "90": 1, "data": 1, "94": 1, "count": 3, "comparison": 1, "jone": 1, "f": 21, "mangu": 1, "unsupervis": 1, "840": 1, "literatur": 1, "alta": 2, "correct": 3, "sensit": 2, "74": 1, "context": 2, "yarowski": 2, "altavista": 3, "model": 3}, "vector_2": [9, 0.42497633762039977, 6, 0, 3, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.", "and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]."], "label": "Prov", "citing": "N04-1016", "vector": [10, 4, 3, 0.5006261743217588], "context": ["", "Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.", ""], "marker": "1995", "vector_1": {"corpu": 2, "brown": 1, "use": 1, "word": 1, "gold": 1, "show": 1, "wall": 1, "train": 1, "m": 1, "except": 1, "entir": 1, "street": 1, "4": 1, "tabl": 1, "test": 1, "journal": 1}, "vector_2": [9, 0.46517454484716886, 2, 0, 3, 0]}, {"function": "CoCo", "cited": "W95-0104", "provenance": ["A method is presented for doing this, based on Bayesian classifiers."], "label": "Prov", "citing": "N04-1016", "vector": [3, 0, 0, 0.0], "context": ["", "A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).", ""], "marker": "1995", "vector_1": {"weightupd": 1, "comparison": 1, "task": 1, "multipl": 1, "algorithm": 1, "gold": 1, "show": 1, "outperform": 1, "winnow": 1, "literatur": 1, "accuraci": 1, "achiev": 1, "altavista": 1, "jone": 1, "martin": 1, "model": 1, "highest": 1, "class": 1, "best": 1}, "vector_2": [9, 0.472885696787484, 3, 0, 3, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["Thus if C = {desert, dessert}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be."], "label": "Prov", "citing": "N10-1019", "vector": [6, 0, 1, 0.03575992699260758], "context": ["", "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).", ""], "marker": "1995", "vector_1": {"tradit": 1, "context": 1, "major": 1, "appropri": 1, "datadriven": 1, "techniqu": 1, "gold": 2, "whether": 1, "spell": 1, "use": 2, "contextu": 1, "continu": 1, "roth": 1, "determin": 1, "correct": 1, "word": 1, "establish": 1, "method": 1, "classif": 1}, "vector_2": [15, 0.0557319345710539, 2, 0, 4, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["A method is presented for doing this, based on Bayesian classifiers.", "The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;.", "An ambiguous target word is then classified by finding all collocations that match its context."], "label": "Prov", "citing": "P01-1005", "vector": [13, 0, 4, 0.23128442344214897], "context": ["", "The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.", ""], "marker": "Golding, 1995", "vector_1": {"semant": 1, "set": 5, "weight": 1, "replac": 1, "eg": 1, "system": 1, "see": 1, "marker": 2, "follow": 1, "everywher": 1, "differenti": 1, "latent": 1, "techniqu": 1, "transform": 1, "classifi": 1, "member": 2, "must": 1, "formul": 1, "includ": 1, "varieti": 1, "test": 1, "totwotoo": 1, "approach": 1, "updat": 1, "analysi": 1, "decid": 1, "choos": 1, "iplic": 1, "given": 1, "base": 1, "bayesian": 1, "mult": 1, "recent": 1, "grammar": 1, "specif": 1, "algorithm": 1, "occurr": 1, "list": 1, "learn": 1, "problem": 1, "confus": 3, "decis": 1}, "vector_2": [6, 0.16019693239916682, 8, 1, 1, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["A method for doing this, based on Bayesian classifiers, was presented.", "It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists."], "label": "Prov", "citing": "P96-1010", "vector": [10, 0, 3, 0.31622776601683794], "context": ["", "Feature-based approaches, such as Bayesian clas sifiers (Gale, Church, and Yarowsky, 1993), deci sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of suc cess for the problem of context-sensitive spelling correction.", ""], "marker": "Golding, 1995", "vector_1": {"contextsensit": 1, "suc": 1, "deci": 1, "featurebas": 1, "hybrid": 1, "spell": 1, "list": 1, "cla": 1, "correct": 1, "sifier": 1, "degre": 1, "sion": 1, "bayesian": 2, "cess": 1, "approach": 1, "problem": 1, "vari": 1}, "vector_2": [1, 0.1149759761791974, 3, 4, 1, 0]}, {"function": "Pos", "cited": "W95-0104", "provenance": ["Trigrams are at their worst when the words in the confusion set have the same part of speech.", "In such cases, the Bayesian hybrid method is clearly better."], "label": "Prov", "citing": "P96-1010", "vector": [10, 0, 8, 0.5384615384615385], "context": ["", "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.", ""], "marker": "Golding, 1995", "vector_1": {"altern": 1, "case": 1, "word": 1, "hybrid": 1, "bay": 1, "part": 1, "speech": 1, "consid": 1, "bayesian": 1, "method": 2}, "vector_2": [1, 0.18555863842457873, 1, 4, 1, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers."], "label": "Prov", "citing": "P96-1010", "vector": [6, 0, 2, 0.3450327796711771], "context": ["", "A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).", ""], "marker": "Golding, 1995", "vector_1": {"weightupd": 1, "winnow": 1, "multipl": 1, "algorithm": 1, "featurebas": 1, "hybrid": 1, "list": 1, "number": 1, "classifi": 1, "base": 1, "decis": 1, "includ": 1, "bayesian": 2, "recent": 1, "method": 2, "propos": 1}, "vector_2": [1, 0.37707247749881573, 4, 4, 1, 0]}, {"function": "Pos", "cited": "W95-0104", "provenance": ["A method is presented for doing this, based on Bayesian classifiers."], "label": "Prov", "citing": "P96-1010", "vector": [3, 0, 0, 0.4082482904638631], "context": ["", "We adopt the Bayesian hybrid method This method has been described elsewhere (Golding, 1995)", ""], "marker": "Golding, 1995", "vector_1": {"adopt": 1, "describ": 1, "elsewher": 1, "hybrid": 1, "bayesian": 1, "method": 2}, "vector_2": [1, 0.38637747851390675, 1, 4, 1, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["A method is presented for doing this, based on Bayesian classifiers.", "The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction."], "label": "Prov", "citing": "W00-0701", "vector": [7, 0, 1, 0.13006649542861798], "context": ["", "This general scheme has been used to de rive classifiers for a variety of natural lan guage applications including speech applica tions (Rab89), pos tagging (Kup92; Sch95), word-sense ambiguation (GCY93) and context sensitive spelling correction (Gol95).", ""], "marker": "Gol95", "vector_1": {"rive": 1, "guag": 1, "rab": 1, "sch": 1, "natur": 1, "tag": 1, "wordsens": 1, "gol": 1, "use": 1, "gci": 1, "classifi": 1, "kup": 1, "speech": 1, "includ": 1, "varieti": 1, "scheme": 1, "po": 1, "applic": 1, "lan": 1, "spell": 1, "gener": 1, "de": 1, "ambigu": 1, "sensit": 1, "applica": 1, "context": 1, "correct": 1, "tion": 1}, "vector_2": [3, 0.18235675275386468, 0, 0, 1, 1]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence."], "label": "Prov", "citing": "W00-0701", "vector": [6, 0, 0, 0.047836487323493986], "context": ["", "MBL, by using long and very specialized conjunctions (DBZ99) and decision lists, due to their functional form - a linear function with exponentially decreasing weights - at the cost of predicting with a single feature, rather than a combination (Gol95).", ""], "marker": "Gol95", "vector_1": {"mbl": 1, "featur": 1, "weight": 1, "predict": 1, "cost": 1, "decreas": 1, "special": 1, "gol": 1, "use": 1, "exponenti": 1, "rather": 1, "due": 1, "long": 1, "decis": 1, "singl": 1, "function": 2, "linear": 1, "form": 1, "conjunct": 1, "list": 1, "dbz": 1, "combin": 1}, "vector_2": [3, 0.793205591039526, 0, 0, 1, 1]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction."], "label": "Prov", "citing": "W04-3238", "vector": [5, 0, 1, 0.20180183819889375], "context": ["", "A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).", ""], "marker": "Golding, 1995", "vector_1": {"cssc": 1, "differ": 1, "spell": 1, "number": 1, "sensit": 1, "eg": 1, "work": 1, "resolv": 1, "focus": 1, "bodi": 1, "framework": 1, "substitut": 1, "limit": 1, "context": 1, "error": 1, "cognit": 1, "correct": 1}, "vector_2": [9, 0.08663939365109567, 3, 1, 1, 0]}, {"function": "Pos", "cited": "W95-0104", "provenance": ["Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!)"], "label": "Prov", "citing": "W06-1624", "vector": [2, 1, 0, 0.19245008972987526], "context": ["", "We use the metric described in (Yarowsky, 1994; Golding, 1995).", ""], "marker": "Golding, 1995", "vector_1": {"use": 1, "metric": 1, "describ": 1}, "vector_2": [11, 0.34764899670560045, 2, 1, 0, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["\\Ve treat context-sensitive spelling correction as a task of word disambiguation."], "label": "Prov", "citing": "W06-3604", "vector": [5, 0, 1, 0.17149858514250882], "context": ["", "More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;", ""], "marker": "Golding, 1995", "vector_1": {"machin": 1, "applic": 1, "investig": 1, "string": 1, "variou": 1, "precursor": 1, "gener": 1, "work": 1, "task": 1, "mention": 1, "paper": 1, "disambigu": 1, "learn": 1, "algorithm": 1, "confus": 1, "discuss": 1}, "vector_2": [11, 0.9321216483714234, 2, 1, 5, 0]}, {"function": "CoCo", "cited": "W95-0104", "provenance": ["The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction."], "label": "Prov", "citing": "W09-1007", "vector": [3, 0, 1, 0.22645540682891918], "context": ["", "This approach is similar to approaches used in accent restoration (Yarowsky, 1994; Golding, 1995;", ""], "marker": "Golding, 1995", "vector_1": {"similar": 1, "use": 1, "restor": 1, "approach": 2, "accent": 1}, "vector_2": [14, 0.14492645689815678, 2, 1, 0, 0]}, {"function": "Weak", "cited": "W95-0104", "provenance": ["The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction.", "\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers."], "label": "Prov", "citing": "W12-0304", "vector": [10, 0, 4, 0.25286086871208685], "context": ["", "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.", ""], "marker": "Golding, 1995", "vector_1": {"bayesian": 1, "applic": 1, "cannot": 1, "detect": 1, "grammar": 1, "appli": 1, "howev": 1, "spell": 1, "list": 1, "classifi": 1, "also": 1, "model": 1, "decis": 1, "error": 1, "report": 1, "studi": 1, "check": 1}, "vector_2": [17, 0.23290339176129057, 3, 1, 1, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction.", "\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers."], "label": "Prov", "citing": "W96-0108", "vector": [6, 0, 1, 0.18650096164806276], "context": ["", "Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.", ""], "marker": "1995", "vector_1": {"gold": 2, "appli": 1, "hybrid": 1, "correct": 1, "trigram": 1, "schabe": 1, "realword": 1, "po": 1, "error": 1, "bayesian": 2, "combin": 1, "purpos": 1, "method": 2}, "vector_2": [1, 0.15549841772151898, 2, 0, 5, 1]}, {"function": "Pos", "cited": "W95-0104", "provenance": ["The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction."], "label": "Prov", "citing": "W98-1234", "vector": [4, 0, 1, 0.1307440900921227], "context": ["", "Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).", ""], "marker": "3", "vector_1": {"use": 1, "work": 1, "develop": 1, "power": 1, "gold": 2, "modul": 1, "spell": 1, "brill": 2, "schabe": 1, "marcu": 1, "correct": 1, "basi": 1}, "vector_2": [3, 0.08180064308681673, 2, 0, 4, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I.", "Among all possible target strings, we will choose the string with the highest probability"], "label": "Prov", "citing": "E06-1004", "vector": [4, 0, 0, 0.08199200616907877], "context": ["", " Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", ""], "marker": "Tillman, 2000", "vector_1": {"given": 1, "e": 2, "f": 2, "sentenc": 1, "p": 1, "comput": 1, "pair": 1, "condit": 1, "model": 1, "probabl": 1, "paramet": 1}, "vector_2": [6, 0.19961015454587647, 4, 1, 1, 0]}, {"function": "Error", "cited": "C00-2123", "provenance": ["For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word."], "label": "Prov", "citing": "J04-2003", "vector": [10, 1, 3, 0.3], "context": ["", "Many existing systems for statistical machine translation 1 1 (Garca-Varea and Casacuberta 2001; Germann et al 2001; Nieen et al 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"brown": 1, "casacuberta": 1, "al": 2, "sourc": 2, "exist": 1, "et": 2, "mercer": 1, "och": 1, "describ": 1, "nieen": 1, "system": 1, "ney": 1, "machin": 1, "200": 2, "germann": 1, "string": 1, "tillmann": 1, "999": 1, "998": 1, "garcavarea": 1, "translat": 1, "implement": 1, "present": 1, "pietra": 2, "word": 3, "target": 2, "align": 1, "correspond": 1, "statist": 1, "posit": 2, "mani": 1, "model": 1, "assign": 1, "della": 2}, "vector_2": [4, 0.08172688817850109, 1, 0, 1, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["(1), Pr(eI 1) is the language model, which is a trigram language model in this case."], "label": "Prov", "citing": "J04-2003", "vector": [1, 0, 0, 0.0], "context": ["", "Some recent publications deal with the automatic detection of multiword phrases (Och and Weber 1998; Tillmann and Ney 2000).", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"och": 1, "detect": 1, "deal": 1, "tillmann": 1, "automat": 1, "multiword": 1, "ney": 1, "phrase": 1, "public": 1, "weber": 1, "recent": 1}, "vector_2": [4, 0.6284187413219672, 0, 0, 1, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy."], "label": "Prov", "citing": "J04-4002", "vector": [2, 0, 0, 0.08606629658238703], "context": ["", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"word": 1, "prune": 1, "tillmann": 1, "call": 1, "ney": 1, "highli": 1, "observ": 1, "select": 1, "probabl": 1}, "vector_2": [4, 0.5547631456236114, 0, 0, 1, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["A dynamic programming recursion similar to the one in Eq.", "2 is evaluated.", "We apply a beam search concept as in speech recognition."], "label": "Prov", "citing": "N03-1010", "vector": [6, 0, 3, 0.2891574659831202], "context": ["", "Och et al report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"program": 1, "och": 1, "search": 2, "word": 1, "version": 1, "algorithm": 1, "optim": 1, "variant": 1, "al": 1, "restrict": 1, "beam": 1, "rate": 1, "base": 1, "decod": 1, "error": 1, "report": 1, "et": 1, "combin": 1, "dynam": 1}, "vector_2": [3, 0.06758057556344807, 1, 2, 2, 0]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["We apply a beam search concept as in speech recognition."], "label": "Prov", "citing": "P03-1039", "vector": [3, 0, 1, 0.08084520834544434], "context": ["", "The decoding algorithm employed for this chunk + weight  j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"translationmodel": 1, "weight": 1, "chunk": 1, "consum": 1, "tive": 1, "languag": 1, "probabl": 1, "decod": 1, "input": 1, "respec": 1, "gener": 1, "arbitrari": 1, "beam": 1, "base": 2, "translat": 2, "e": 1, "plm": 1, "ptmje": 1, "present": 1, "search": 1, "word": 1, "algorithm": 2, "f": 2, "frequenc": 1, "align": 1, "j": 7, "lefttoright": 1, "employ": 1, "reqea": 2, "statist": 2, "output": 1, "model": 1, "order": 2}, "vector_2": [3, 0.6173074181438007, 1, 2, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["The type of alignment we have considered so far requires the same length for source and target sentence, i.e.", "I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either  = 0 or  = 1 new target words."], "label": "Prov", "citing": "P03-1039", "vector": [6, 0, 0, 0.050251890762960605], "context": ["", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", ""], "marker": "Tillman and Ney, 2000", "vector_1": {"insert": 1, "lexicon": 1, "string": 1, "possibl": 1, "sequenc": 1, "gener": 1, "estim": 1, "output": 1, "model": 1, "invert": 1, "chunk": 1}, "vector_2": [3, 0.6275721142422659, 1, 2, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI .", "Using the inverted alignments in the maximum approximation"], "label": "Prov", "citing": "W01-0505", "vector": [2, 0, 1, 0.0], "context": ["", "They were usually incorporated in the EM algorithm (Brown et al., 1993; Kupiec, 1993; Tillmann and Ney, 2000; Och et al., 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"em": 1, "algorithm": 1, "incorpor": 1, "usual": 1}, "vector_2": [1, 0.08832526127963293, 4, 1, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["This measure has the advantage of being completely automatic."], "label": "Prov", "citing": "W01-1404", "vector": [1, 0, 0, 0.0], "context": ["", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"concentr": 1, "closer": 1, "cannot": 1, "assign": 1, "grammar": 1, "machineri": 1, "extend": 1, "framework": 1, "chosen": 1, "comfort": 1, "two": 1, "yet": 1, "finitest": 2, "contextfre": 2, "other": 1, "either": 1, "transduct": 1, "studi": 2, "model": 1}, "vector_2": [1, 0.030689329556185082, 6, 1, 0, 0]}, {"function": "CoCo", "cited": "C00-2123", "provenance": ["For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is sucient to consider only the best 50 words."], "label": "Prov", "citing": "W01-1407", "vector": [2, 0, 0, 0.11605177063713189], "context": ["", "We used a translation system called single- word based approach described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"use": 1, "word": 1, "describ": 1, "compar": 1, "system": 1, "base": 1, "call": 1, "translat": 1, "approach": 2, "singl": 1}, "vector_2": [1, 0.7675015446033934, 2, 1, 4, 0]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["We apply a beam search concept as in speech recognition."], "label": "Prov", "citing": "W01-1408", "vector": [5, 0, 1, 0.3553345272593508], "context": ["", "Search algorithms We evaluate the following two search algorithms:  beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", ""], "marker": "Tillmann, 2001; Tillmann and Ney, 2000", "vector_1": {"search": 4, "breadthfirst": 1, "algorithm": 4, "evalu": 1, "space": 1, "two": 1, "beam": 1, "manner": 1, "explor": 1, "bs": 1, "follow": 1}, "vector_2": [0, 0.3456131436314363, 2, 3, 4, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP)."], "label": "Prov", "citing": "W02-1020", "vector": [5, 0, 0, 0.1767766952966369], "context": ["", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J for in p(v1, w2 , wm1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"heurist": 1, "even": 1, "use": 1, "pv": 1, "npcomplet": 1, "dynamicprogram": 1, "umh": 1, "channel": 1, "omj": 1, "wm": 1, "noisi": 1, "polynomi": 1, "fastest": 1, "model": 1, "search": 1, "faster": 1, "j": 1, "mt": 1, "stanc": 1, "statist": 1, "w": 1, "v": 1, "problem": 1}, "vector_2": [2, 0.35199759008924203, 4, 2, 1, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["Comparable corpora refer to texts that are not direct translation but are about the same topic.", "For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora."], "label": "Prov", "citing": "D10-1042", "vector": [12, 0, 3, 0.26885304420581435], "context": ["", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"scarciti": 2, "articl": 1, "eg": 2, "share": 1, "high": 2, "explor": 1, "suffer": 1, "event": 1, "languag": 1, "use": 1, "render": 1, "compar": 1, "wikipedia": 1, "two": 1, "accuraci": 1, "content": 1, "document": 1, "resourc": 1, "news": 1, "parallel": 3, "vast": 1, "describ": 1, "allevi": 1, "entri": 1, "corpora": 3, "person": 1, "bilingu": 1, "similar": 1, "sentencealign": 1}, "vector_2": [6, 0.3061437908496732, 4, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information."], "label": "Prov", "citing": "N09-1048", "vector": [5, 0, 1, 0.19446111706564934], "context": ["", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"multilingu": 1, "featur": 1, "co": 1, "compar": 1, "frequenc": 1, "eg": 1, "corpora": 1, "wikipedia": 1, "wherein": 1, "employ": 1, "popularli": 1, "context": 1, "occurr": 1, "parallel": 1}, "vector_2": [5, 0.07349773333762016, 4, 2, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information."], "label": "Prov", "citing": "N09-1048", "vector": [12, 0, 2, 0.37047928681747416], "context": ["", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", ""], "marker": "2004", "vector_1": {"agenc": 1, "differ": 1, "document": 1, "context": 1, "chines": 1, "mine": 1, "shao": 1, "inform": 1, "period": 1, "translat": 1, "transliter": 1, "english": 1, "new": 1, "news": 2, "ng": 1, "combin": 1, "method": 1, "present": 1}, "vector_2": [5, 0.22062180497058162, 1, 0, 4, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm."], "label": "Prov", "citing": "P13-1059", "vector": [3, 0, 0, 0.3333333333333333], "context": ["", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "pair": 1, "recent": 1}, "vector_2": [9, 0.9222239923530349, 9, 1, 0, 0]}, {"function": "CoCo", "cited": "C04-1089", "provenance": ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information."], "label": "Prov", "citing": "P13-1107", "vector": [10, 0, 5, 0.17712297710801905], "context": ["", "Other similar research lines are the TACKBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"el": 1, "predict": 1, "knowledg": 1, "mine": 1, "entiti": 2, "web": 1, "compar": 1, "research": 1, "document": 1, "tackbp": 1, "appropri": 1, "link": 3, "translat": 1, "pair": 1, "news": 1, "line": 1, "kb": 1, "task": 1, "name": 2, "entri": 1, "corpora": 1, "base": 1, "problem": 1, "similar": 1}, "vector_2": [9, 0.9328285077951002, 14, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information."], "label": "Prov", "citing": "P13-2036", "vector": [1, 0, 0, 0.08574929257125441], "context": ["", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"approach": 1, "combin": 1, "studi": 1, "similar": 1, "holist": 1, "recent": 1}, "vector_2": [9, 0.07383335499805017, 3, 2, 0, 0]}, {"function": "Pos", "cited": "C04-1089", "provenance": ["Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2)."], "label": "Prov", "citing": "P13-2036", "vector": [6, 0, 2, 0.2279211529192759], "context": ["", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "return": 1, "independ": 1, "rank": 2, "candid": 1, "cx": 1, "translat": 1, "ph": 1, "highest": 1, "averag": 1, "result": 1}, "vector_2": [9, 0.08280254777070063, 1, 2, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information."], "label": "Prov", "citing": "W11-2206", "vector": [5, 0, 3, 0.420084025208403], "context": ["", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "word": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "transliter": 1, "new": 1, "rescor": 1, "recent": 1}, "vector_2": [7, 0.850206577425219, 8, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations."], "label": "Prov", "citing": "W13-2501", "vector": [7, 0, 1, 0.1767766952966369], "context": ["", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"tradit": 1, "lexicon": 1, "target": 1, "compar": 1, "corpora": 1, "vector": 1, "avail": 1, "extens": 1, "approch": 1, "sourc": 1, "translat": 2, "bilingu": 1, "presuppos": 1, "extract": 1, "languag": 1}, "vector_2": [9, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus."], "label": "Prov", "citing": "W13-2512", "vector": [4, 0, 0, 0.0], "context": ["", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"term": 1, "name": 1, "variou": 1, "hybrid": 1, "across": 1, "exploit": 1, "translat": 1, "way": 1, "method": 1, "languag": 1, "entiti": 1}, "vector_2": [9, 0.31465281574630943, 3, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus."], "label": "Prov", "citing": "D12-1094", "vector": [1, 0, 0, 0.0], "context": ["", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", ""], "marker": "Sekine, 2005", "vector_1": {"datadriven": 1, "distribut": 1, "extend": 1, "idea": 1, "discoveri": 1, "paraphras": 1, "phrase": 1, "similar": 1, "method": 1}, "vector_2": [7, 0.21108071485183955, 4, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances.", "Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01]."], "label": "Prov", "citing": "J10-3003", "vector": [9, 0, 1, 0.14744195615489714], "context": ["", "As a later renement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for specic concepts represented by keywords.", ""], "marker": "2005", "vector_1": {"concept": 1, "repres": 1, "paraphras": 1, "entiti": 1, "use": 1, "renement": 1, "make": 1, "sekin": 1, "distribut": 1, "lexic": 1, "pair": 1, "specic": 1, "attempt": 1, "name": 1, "keyword": 1, "later": 1, "list": 1, "order": 1, "fulli": 1, "phrasal": 1, "similar": 2, "produc": 1}, "vector_2": [5, 0.5265055837563452, 1, 0, 3, 1]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs"], "label": "Prov", "citing": "J12-1003", "vector": [2, 0, 0, 0.0], "context": ["", "This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al 2010).", ""], "marker": "Sekine 2005", "vector_1": {"led": 1, "entail": 1, "dagan": 1, "etzioni": 1, "lin": 1, "activ": 1, "al": 1, "rule": 1, "research": 1, "acquisit": 1, "et": 1, "pantel": 1, "sekin": 1, "szpektor": 1, "schoenmack": 1, "predic": 1, "broadscal": 1, "yate": 1}, "vector_2": [7, 0.028151667106536465, 0, 0, 4, 1]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["For each pair we also record the context, i.e.", "the phrase between the two NEs (Step1).", "Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair."], "label": "Prov", "citing": "P06-2094", "vector": [7, 0, 2, 0.19851666679418606], "context": ["", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", ""], "marker": "Sekine 05", "vector_1": {"corpu": 1, "unannot": 1, "name": 1, "two": 1, "method": 1, "also": 1, "instanc": 1, "larg": 1, "paraphras": 1, "context": 1, "sekin": 1, "entiti": 1, "find": 1, "propos": 1}, "vector_2": [6, 0.36944602833457607, 0, 0, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus."], "label": "Prov", "citing": "P07-1058", "vector": [1, 0, 0, 0.0], "context": ["", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most dont.", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 1, "algorithm": 1, "constraint": 1, "contextu": 1, "acquisit": 1, "add": 1, "exist": 1, "rule": 1, "learn": 1, "dont": 1}, "vector_2": [2, 0.2252401115356811, 1, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus."], "label": "Prov", "citing": "P07-1058", "vector": [2, 0, 0, 0.0668153104781061], "context": ["", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"rang": 1, "distribut": 1, "suggest": 1, "share": 1, "rule": 1, "find": 1, "acquisit": 1, "automat": 1, "context": 1, "year": 1, "mani": 1, "similar": 1, "method": 1, "recent": 1}, "vector_2": [2, 0.2768081517436056, 6, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.", "In order to create an IE system for a new domain, one has to spend a long time to create the knowledge."], "label": "Prov", "citing": "P07-1058", "vector": [3, 0, 0, 0.0], "context": ["", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"judgment": 1, "algorithm": 1, "evalu": 1, "promin": 1, "qualiti": 1, "rule": 2, "inde": 1, "acquisit": 1, "human": 1, "learn": 1, "approach": 1}, "vector_2": [2, 0.32276498330407244, 6, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs"], "label": "Prov", "citing": "P08-1078", "vector": [1, 0, 0, 0.0], "context": ["", "Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 2, "algorithm": 1, "paraphras": 1, "view": 1, "automat": 1, "bidirect": 1, "rule": 2, "learn": 1, "recent": 1, "sever": 1, "propos": 1}, "vector_2": [3, 0.05281840591618735, 5, 1, 1, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs"], "label": "Prov", "citing": "P11-1062", "vector": [0, 0, 0, 0.0], "context": ["", "Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).", ""], "marker": "Sekine, 2005", "vector_1": {"last": 1, "consider": 1, "receiv": 1, "thu": 1, "knowledg": 1, "acquisit": 1, "attent": 1, "decad": 1}, "vector_2": [6, 0.0529608938547486, 4, 1, 0, 0]}, {"function": "Pos", "cited": "I05-5011", "provenance": ["Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases."], "label": "Prov", "citing": "P11-1109", "vector": [10, 0, 4, 0.6092717958449424], "context": ["", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", ""], "marker": "2005", "vector_1": {"claim": 1, "differ": 1, "varieti": 1, "discov": 1, "requir": 1, "sekin": 1, "wider": 1, "agre": 1, "method": 1, "sever": 1, "paraphras": 1}, "vector_2": [6, 0.8026571887958026, 1, 0, 0, 0]}, {"function": "Pos", "cited": "I05-5011", "provenance": ["Cluster phrases based on Links We now have a set of phrases which share a keyword.", "At this step, we will try to link those sets, and put them into a single cluster."], "label": "Prov", "citing": "P13-1131", "vector": [5, 0, 0, 0.0468292905790847], "context": ["", "Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", ""], "marker": "2005", "vector_1": {"corpu": 1, "differ": 1, "scale": 1, "word": 1, "richer": 1, "collect": 1, "share": 1, "number": 1, "templat": 1, "predic": 3, "cluster": 1, "verb": 1, "statist": 1, "sekin": 1, "co": 1, "follow": 1, "main": 1, "occurr": 1, "order": 1, "per": 1}, "vector_2": [8, 0.6507067289773216, 1, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexi cal rule-based approaches, and approaches that combine lexical information with sta tistical information."], "label": "Prov", "citing": "J04-1004", "vector": [4, 0, 0, 0.21110016546037455], "context": ["", "In Chinese text segmentation there are three basic approaches (Sproat et al 1996): pure heuristic, pure statistical, and a hybrid of the two.", ""], "marker": "Sproat et al. 1996", "vector_1": {"sproat": 1, "hybrid": 1, "text": 1, "chines": 1, "heurist": 1, "segment": 1, "three": 1, "al": 1, "statist": 1, "pure": 2, "basic": 1, "et": 1, "approach": 1, "two": 1}, "vector_2": [8, 0.1107739280993428, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The most popular approach to dealing with seg mentation ambiguities is the maximum matching method, possibly augmented with further heuristics."], "label": "Prov", "citing": "J04-1004", "vector": [2, 0, 1, 0.17320508075688776], "context": ["", "There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al 2000; Dai, Loh, and Khoo 1999; Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"use": 1, "backward": 1, "matchingteahan": 1, "sproat": 1, "loh": 1, "maximum": 2, "al": 2, "dai": 1, "sever": 1, "khoo": 1, "forward": 1, "et": 2, "match": 1, "segment": 1, "method": 1, "commonli": 1}, "vector_2": [8, 0.24448457542866717, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult."], "label": "Prov", "citing": "J04-1004", "vector": [9, 0, 1, 0.04415107856883479], "context": ["", "In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"decid": 1, "word": 2, "sproat": 1, "evalu": 1, "perform": 1, "hard": 1, "al": 1, "accept": 1, "standard": 1, "whether": 1, "meaning": 1, "et": 1, "extract": 1, "method": 1, "commonli": 1, "addit": 1}, "vector_2": [8, 0.4771656768380094, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text."], "label": "Prov", "citing": "J04-1004", "vector": [10, 0, 2, 0.21380899352993948], "context": ["", "As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al 1996), it is very difficult to compare two methods.", ""], "marker": "Sproat et al. 1996", "vector_1": {"even": 1, "differ": 2, "task": 1, "word": 1, "sproat": 1, "compar": 1, "al": 1, "text": 1, "corpora": 1, "judg": 1, "system": 2, "face": 1, "two": 1, "human": 1, "test": 1, "et": 1, "segment": 1, "method": 1, "difficult": 1}, "vector_2": [8, 0.7435816282836557, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In this paper we have argued that Chinese word segmentation can be modeled ef fectively using weighted finite-state transducers."], "label": "Prov", "citing": "J05-4005", "vector": [4, 0, 2, 0.25000000000000006], "context": ["", "A previous work along this line is Sproat et al (1996), which is based on weighted finite-state transducers (FSTs).", ""], "marker": "1996", "vector_1": {"previou": 1, "sproat": 1, "weight": 1, "work": 1, "al": 1, "finitest": 1, "base": 1, "fst": 1, "et": 1, "along": 1, "transduc": 1, "line": 1}, "vector_2": [9, 0.10361947177250078, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75"], "label": "Prov", "citing": "J05-4005", "vector": [6, 0, 3, 0.3508232077228117], "context": ["", "As shown in Sproat et al (1996), the rate of agreement between two human judges is less than 80%.", ""], "marker": "1996", "vector_1": {"shown": 1, "sproat": 1, "less": 1, "judg": 1, "agreement": 1, "al": 1, "rate": 1, "two": 1, "human": 1, "et": 1}, "vector_2": [9, 0.1413108949203278, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75"], "label": "Prov", "citing": "J05-4005", "vector": [2, 0, 1, 0.18490006540840973], "context": ["", "Similarly, Sproat et al (1996) also uses multiple human judges.", ""], "marker": "1996", "vector_1": {"use": 1, "multipl": 1, "sproat": 1, "judg": 1, "al": 1, "also": 1, "human": 1, "similarli": 1, "et": 1}, "vector_2": [9, 0.14510741599284502, 1, 0, 6, 1]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration."], "label": "Prov", "citing": "J05-4005", "vector": [3, 0, 0, 0.0], "context": ["", "The Chinese person-name model is a modified version of that described in Sproat et al (1996).", ""], "marker": "1996", "vector_1": {"personnam": 1, "describ": 1, "chines": 1, "al": 1, "version": 1, "et": 1, "model": 1, "modifi": 1, "sproat": 1}, "vector_2": [9, 0.5261010823735558, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75"], "label": "Prov", "citing": "J11-1005", "vector": [4, 0, 1, 0.2307692307692308], "context": ["", "Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"among": 1, "shown": 1, "word": 1, "sproat": 1, "regard": 1, "agreement": 1, "al": 1, "nativ": 1, "speaker": 1, "et": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [15, 0.15350465979117084, 0, 0, 2, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide."], "label": "Prov", "citing": "J97-4004", "vector": [9, 0, 0, 0.08861036804732014], "context": ["", "Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al 1994; Chen and Liu 1992; Chiang et al 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al 1992; Li et al 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al 1996;", ""], "marker": "Sproat et al. 1996", "vector_1": {"shih": 1, "consider": 1, "shen": 1, "guo": 1, "eg": 1, "al": 5, "signific": 1, "blank": 1, "chen": 2, "et": 5, "tan": 1, "li": 1, "xu": 1, "xia": 1, "jie": 2, "equival": 1, "space": 1, "jin": 2, "lua": 2, "zhang": 1, "research": 1, "liu": 4, "written": 2, "liang": 2, "lai": 1, "sproat": 2, "advanc": 1, "sentenc": 1, "chines": 2, "palmer": 1, "hannan": 1, "nie": 1, "gan": 2, "fan": 1, "huang": 2, "sun": 1, "effort": 1, "sinc": 1, "made": 1, "word": 1, "delimit": 1, "tsai": 1, "explicit": 1, "focu": 1, "token": 1, "english": 1, "chiang": 1, "problem": 1, "bai": 1}, "vector_2": [1, 0.028624192059095107, 2, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The most popular approach to dealing with seg mentation ambiguities is the maximum matching method, possibly augmented with further heuristics."], "label": "Prov", "citing": "J97-4004", "vector": [2, 0, 0, 0.035267280792929914], "context": ["", "The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realiza tions of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al 1996;", ""], "marker": "Sproat et al. 1996", "vector_1": {"yeh": 1, "wang": 2, "guo": 1, "eg": 1, "al": 1, "procedur": 1, "correspond": 1, "et": 1, "chen": 1, "follow": 1, "sproat": 1, "jie": 2, "jin": 1, "section": 1, "definit": 1, "three": 1, "construct": 1, "liu": 3, "liang": 2, "lee": 1, "hannan": 1, "restat": 1, "wu": 1, "realiza": 1, "kit": 1, "a": 1, "wide": 1, "b": 1, "essenti": 1, "mo": 1, "descript": 1, "maximum": 1, "su": 2, "turn": 1, "token": 3, "nie": 1, "webster": 1, "tion": 1, "principl": 1}, "vector_2": [1, 0.739750003445377, 1, 0, 11, 1]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["More formally, we start by representing the dictionary D as a Weighted Finite State Trans ducer (WFST) (Pereira, Riley, and Sproat 1994)."], "label": "Prov", "citing": "J97-4004", "vector": [4, 0, 0, 0.16012815380508716], "context": ["", "The weighted finite-state transducer model developed by Sproat et al (1996) is another excellent representative example.", ""], "marker": "1996", "vector_1": {"develop": 1, "weight": 1, "transduc": 1, "anoth": 1, "excel": 1, "al": 1, "repres": 1, "finitest": 1, "exampl": 1, "et": 1, "model": 1, "sproat": 1}, "vector_2": [1, 0.9175728008158652, 1, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The most popular approach to dealing with seg mentation ambiguities is the maximum matching method, possibly augmented with further heuristics."], "label": "Prov", "citing": "J97-4004", "vector": [3, 0, 0, 0.06622661785325219], "context": ["", "While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are ap parently employed neither in Sproat et al (1996) nor in Ma (1996).", ""], "marker": "1996", "vector_1": {"search": 1, "neither": 1, "parent": 1, "evalu": 1, "ap": 1, "may": 1, "gener": 1, "knowledg": 1, "al": 1, "employ": 1, "framework": 1, "et": 1, "incorpor": 1, "heurist": 1, "fulli": 1, "path": 1, "total": 1, "imposs": 1, "sproat": 1}, "vector_2": [1, 0.9342208624467689, 2, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["More formally, we start by representing the dictionary D as a Weighted Finite State Trans ducer (WFST) (Pereira, Riley, and Sproat 1994)."], "label": "Prov", "citing": "N10-1068", "vector": [3, 1, 0, 0.06726727939963126], "context": ["", "Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: WFSTs provide a uniform knowledge represen tation.", ""], "marker": "Sproat et al., 1996", "vector_1": {"knowledg": 1, "transduc": 1, "weight": 1, "captur": 1, "provid": 1, "natur": 1, "offer": 1, "uniform": 1, "finitest": 1, "benefit": 1, "sever": 1, "wfst": 1, "mani": 1, "model": 1, "represen": 1, "languag": 1, "tation": 1}, "vector_2": [14, 0.03388409961685824, 6, 1, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["More formally, we start by representing the dictionary D as a Weighted Finite State Trans ducer (WFST) (Pereira, Riley, and Sproat 1994)."], "label": "Prov", "citing": "P03-1035", "vector": [2, 0, 0, 0.16724840200141816], "context": ["", "One example of such approaches is Sproat et al (1996), which is based on weighted finite-state transducers (FSTs).", ""], "marker": "1996", "vector_1": {"sproat": 1, "weight": 1, "al": 1, "one": 1, "finitest": 1, "exampl": 1, "fst": 1, "base": 1, "et": 1, "transduc": 1, "approach": 1}, "vector_2": [7, 0.15690494321756118, 1, 0, 1, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.", "However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework."], "label": "Prov", "citing": "P03-1035", "vector": [9, 0, 1, 0.05557700143767284], "context": ["", "Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al", ""], "marker": "1996", "vector_1": {"compil": 1, "set": 1, "al": 1, "number": 1, "one": 1, "et": 1, "sproat": 1, "entiti": 2, "given": 1, "two": 1, "input": 1, "type": 2, "string": 2, "gener": 1, "use": 1, "effect": 1, "step": 1, "candid": 2, "search": 1, "name": 2, "constraint": 1, "charact": 1, "limit": 1, "principl": 1, "first": 1}, "vector_2": [7, 0.4974671390259246, 0, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name."], "label": "Prov", "citing": "P03-1035", "vector": [15, 0, 14, 0.7654655446197433], "context": ["", "5.2.4 Transliterations of foreign names As described in Sproat et al (1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.", ""], "marker": "1996", "vector_1": {"whose": 1, "use": 1, "charact": 1, "sproat": 1, "string": 1, "chines": 1, "al": 1, "name": 2, "mimic": 1, "sourc": 1, "languag": 1, "transliter": 2, "usual": 1, "pronunci": 2, "et": 1, "foreign": 1, "sequenti": 1, "fn": 1, "describ": 1}, "vector_2": [7, 0.6243419527861471, 1, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993)."], "label": "Prov", "citing": "P06-1126", "vector": [7, 0, 2, 0.34668762264076824], "context": ["", "Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).", ""], "marker": "Sproat et al., 1996", "vector_1": {"task": 1, "word": 1, "attent": 1, "process": 1, "chines": 2, "literatur": 1, "initi": 1, "receiv": 1, "lot": 1, "mani": 1, "segment": 1, "languag": 1, "stage": 1}, "vector_2": [10, 0.04702157134375618, 4, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate."], "label": "Prov", "citing": "P07-1016", "vector": [6, 0, 1, 0.061721339984836754], "context": ["", "As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.", ""], "marker": "Sproat et al., 1996", "vector_1": {"subset": 1, "corpu": 1, "use": 1, "adopt": 1, "name": 1, "eg": 1, "overwhelmingli": 1, "chines": 3, "elsewher": 1, "tend": 1, "ec": 1, "discuss": 1, "common": 1, "transliter": 1, "english": 1, "charact": 3, "hundr": 1, "thousand": 1, "sever": 1}, "vector_2": [11, 0.3490667382838727, 1, 33, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["The most popular approach to dealing with seg mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.", "This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin ning) of the sentence is reached."], "label": "Prov", "citing": "P12-1111", "vector": [4, 0, 0, 0.13801311186847084], "context": ["", "In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"earli": 1, "rulebas": 1, "word": 1, "work": 1, "maximum": 1, "one": 2, "base": 1, "heurist": 1, "forward": 1, "model": 1, "find": 1, "match": 1}, "vector_2": [16, 0.40864119391052334, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Previous Work.", "There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993)."], "label": "Prov", "citing": "P97-1041", "vector": [5, 0, 0, 0.3442651863295481], "context": ["", "For a discussion of recent Chinese segmentation work, see Sproat et al {1996).", ""], "marker": "1996", "vector_1": {"sproat": 1, "chines": 1, "work": 1, "al": 1, "see": 1, "et": 1, "segment": 1, "discuss": 1, "recent": 1}, "vector_2": [1, 0.05553015649727448, 0, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["A Stochastic Finite-State Word-Segmentation Algorithm for Chinese"], "label": "Prov", "citing": "P97-1041", "vector": [1, 0, 0, 0.0], "context": ["", "It is rule-based, but relies on 2 See, for example, Sproat et al (1996)", ""], "marker": "1996", "vector_1": {"sproat": 1, "rulebas": 1, "see": 1, "al": 1, "reli": 1, "exampl": 1, "et": 1}, "vector_2": [1, 0.20601371549147177, 1, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.", "However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework."], "label": "Prov", "citing": "P98-1076", "vector": [7, 0, 2, 0.0377157143202357], "context": ["", "The actual implementation of the weighted finite state transducer by Sproat et al (1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.", ""], "marker": "1996", "vector_1": {"use": 1, "sourc": 1, "actual": 1, "weight": 1, "transduc": 1, "evid": 1, "practic": 1, "hypothesi": 1, "finit": 1, "per": 1, "al": 1, "state": 1, "alreadi": 1, "token": 1, "taken": 1, "et": 1, "implement": 1, "one": 1, "sproat": 1}, "vector_2": [2, 0.8384123022422608, 1, 0, 3, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation."], "label": "Prov", "citing": "P98-1076", "vector": [3, 0, 0, 0.05913123959890826], "context": ["", "utilizing local and sentential constraints, what Sproat et al ( 1996) implemented was simply a token unigram scoring function.", ""], "marker": "1996", "vector_1": {"function": 1, "unigram": 1, "simpli": 1, "sproat": 1, "constraint": 1, "sententi": 1, "al": 1, "util": 1, "token": 1, "score": 1, "et": 1, "implement": 1, "local": 1}, "vector_2": [2, 0.8644579525783986, 0, 0, 3, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994)."], "label": "Prov", "citing": "W00-0803", "vector": [3, 0, 0, 0.046524210519923545], "context": ["", "Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).", ""], "marker": "Sproat et al., 1996", "vector_1": {"analysi": 1, "issu": 1, "elsewher": 1, "matsuiit": 1, "japanes": 1, "chines": 1, "relat": 1, "morpholog": 1, "intens": 1, "rutd": 1, "mani": 1, "segment": 1, "other": 1, "address": 1}, "vector_2": [4, 0.16910883433210902, 2, 5, 0, 0]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["The major problem for our seg menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).", "However, there will remain a large number of words that are not readily adduced to any produc tive pattern and that would simply have to be added to the dictionary."], "label": "Prov", "citing": "W00-1207", "vector": [8, 1, 0, 0.13693063937629152], "context": ["", "Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.", ""], "marker": "Sproat et al 1996", "vector_1": {"identifi": 1, "often": 1, "text": 1, "eg": 1, "al": 4, "fail": 1, "et": 4, "data": 1, "lee": 1, "likelihood": 1, "appear": 1, "segment": 1, "lin": 1, "lua": 1, "low": 1, "pure": 1, "method": 1, "marcken": 1, "spars": 1, "sproat": 1, "de": 1, "train": 1, "huang": 1, "tung": 1, "word": 3, "etc": 1, "statist": 1, "chiang": 1, "extrem": 1, "problem": 1}, "vector_2": [4, 0.09915192126478903, 2, 0, 0, 0]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["The most popular approach to dealing with seg mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.", "This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin ning) of the sentence is reached."], "label": "Prov", "citing": "W02-1117", "vector": [10, 0, 2, 0.13617619485048052], "context": ["", "For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al 1996; Gu and Mao 1994; Li et al 1991; Wang et al 1991b; Wang et al 1990].", ""], "marker": "Sproat et al. 1996", "vector_1": {"gu": 1, "wang": 2, "al": 4, "one": 1, "minimum": 1, "et": 4, "ambigu": 1, "find": 1, "sproat": 1, "use": 1, "li": 1, "forward": 1, "method": 2, "match": 3, "string": 3, "obtain": 1, "mao": 1, "b": 1, "word": 2, "possibl": 1, "guo": 1, "maximum": 2, "charact": 1, "exampl": 1, "problem": 1, "backward": 1, "resolv": 1}, "vector_2": [6, 0.15557105840143512, 0, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In this paper we present a stochastic finite-state model for segmenting Chinese text into words Word frequencies are estimated by a re-estimation procedure that involves apply ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.", "newspaper material, but also including kungfu fiction, Buddhist tracts, and scientific material.", "This larger corpus was kindly provided to us by United Informatics Inc.,"], "label": "Prov", "citing": "W02-1808", "vector": [4, 0, 0, 0.039283710065919304], "context": ["", "Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)", ""], "marker": "1996", "vector_1": {"involv": 1, "el": 1, "sproat": 1, "show": 1, "tsai": 1, "corpora": 1, "al": 3, "one": 1, "finitest": 1, "train": 1, "fan": 1, "statist": 1, "mostli": 1, "et": 3, "chiang": 1, "largescal": 1, "chang": 1, "approach": 1, "languag": 1, "mod": 1}, "vector_2": [6, 0.1929284750337382, 4, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In this paper we present a stochastic finite-state model for segmenting Chinese text into words"], "label": "Prov", "citing": "W03-1728", "vector": [5, 0, 0, 0.22360679774997896], "context": ["", "This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).", ""], "marker": "Sproat et al., 1996", "vector_1": {"sound": 1, "identifi": 1, "drawn": 1, "may": 1, "chines": 2, "commun": 1, "research": 1, "bodi": 1, "process": 1, "enough": 1, "larg": 1, "nontrivi": 1, "word": 1, "problem": 1, "simpl": 1, "languag": 1, "realiti": 1}, "vector_2": [7, 0.05625115080095747, 5, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["A Stochastic Finite-State Word-Segmentation Algorithm for Chinese"], "label": "Prov", "citing": "W04-3236", "vector": [1, 0, 0, 0.14907119849998599], "context": ["", "Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "process": 1, "chines": 1, "previou": 1, "focus": 1, "research": 1, "much": 1, "segment": 1, "languag": 1}, "vector_2": [8, 0.9186552724927443, 3, 1, 2, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation."], "label": "Prov", "citing": "W05-0709", "vector": [8, 0, 0, 0.18181818181818182], "context": ["", "In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 1, "charact": 1, "word": 1, "segment": 1, "chines": 1, "upon": 2, "stem": 1, "also": 1, "ngram": 1, "base": 2, "dictionari": 1, "model": 2, "experi": 1, "similar": 1, "addit": 1}, "vector_2": [9, 0.3646044293460508, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate."], "label": "Prov", "citing": "W06-1630", "vector": [10, 0, 2, 0.07607257743127308], "context": ["", "The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"handdevelop": 1, "suffix": 1, "cc": 1, "affix": 1, "foreign": 1, "extract": 1, "use": 3, "end": 1, "three": 1, "hindi": 1, "way": 1, "simpl": 1, "chines": 1, "contrast": 1, "gener": 1, "given": 1, "stem": 1, "candid": 2, "c": 10, "word": 3, "name": 1, "possibl": 2, "list": 3, "charact": 1, "exampl": 1, "singl": 1, "frequent": 1}, "vector_2": [10, 0.6469784085883157, 1, 1, 0, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement"], "label": "Prov", "citing": "W12-1011", "vector": [3, 0, 2, 0.0], "context": ["", "Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"even": 1, "word": 1, "boundari": 1, "modern": 1, "inde": 1, "nativ": 1, "speaker": 1, "time": 1, "agre": 1, "chines": 1}, "vector_2": [16, 0.19000328839197633, 1, 2, 1, 0]}, {"function": "CoCo", "cited": "J96-3004", "provenance": ["The average agreement among the human judges is .76"], "label": "Prov", "citing": "W12-1011", "vector": [3, 0, 0, 0.27386127875258304], "context": ["", "No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "compar": 2, "classic": 1, "chines": 2, "modern": 1, "rate": 2, "agreement": 1, "past": 1, "favor": 1, "figur": 1, "human": 1, "report": 1, "attempt": 1, "inter": 1, "segment": 1, "eg": 1, "averag": 1}, "vector_2": [16, 0.8573495560670832, 1, 2, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols."], "label": "Prov", "citing": "W12-2303", "vector": [8, 0, 2, 0.06356417261637282], "context": ["", "An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al (1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.", ""], "marker": "1996", "vector_1": {"corpu": 1, "effici": 1, "less": 1, "al": 1, "et": 1, "dynam": 1, "probabl": 1, "accur": 1, "program": 1, "build": 1, "lattic": 1, "approach": 1, "sproat": 1, "util": 1, "extens": 1, "search": 1, "word": 3, "frequenc": 1, "inform": 1, "combin": 1, "statist": 1, "model": 1}, "vector_2": [16, 0.07488645175584357, 2, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In this paper we have argued that Chinese word segmentation can be modeled ef fectively using weighted finite-state transducers.", "This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration."], "label": "Prov", "citing": "W12-2303", "vector": [8, 0, 1, 0.20412414523193148], "context": ["", "There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field. For example, the Sproat et al (1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.", ""], "marker": "1996", "vector_1": {"al": 1, "et": 1, "use": 1, "techniqu": 1, "recognit": 1, "pattern": 1, "system": 1, "field": 1, "method": 1, "machin": 1, "recogn": 1, "sproat": 1, "chines": 1, "rise": 1, "oov": 2, "transliter": 1, "strong": 1, "name": 1, "success": 1, "literatur": 1, "finitest": 1, "person": 1, "exampl": 1, "learn": 1, "mani": 1, "propos": 1}, "vector_2": [16, 0.8705365385325505, 1, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.", "It also incorporates the Good-Turing method (Baayen 1989; Church and Gale 1991) in estimating the likelihoods of previously unseen con structions, including morphological derivatives and personal names.", "We will evaluate various specific aspects of the segmentation, as well as the overall segmentation per formance."], "label": "Prov", "citing": "W97-0120", "vector": [11, 0, 2, 0.17349447958987207], "context": ["", "One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.", ""], "marker": "Sproat et al., 1996", "vector_1": {"major": 1, "unseen": 1, "process": 1, "one": 1, "morpholog": 1, "treatment": 1, "product": 1, "format": 1, "chines": 1, "lexic": 1, "transliter": 1, "noun": 1, "segment": 1, "word": 3, "name": 1, "unsupervis": 1, "rule": 1, "foreign": 1, "person": 1, "plur": 1, "problem": 1, "wrote": 1}, "vector_2": [1, 0.12200906871294036, 1, 3, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["The most popular approach to dealing with seg mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.", "This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin ning) of the sentence is reached."], "label": "Prov", "citing": "W97-0120", "vector": [4, 0, 1, 0.11952286093343936], "context": ["", "We used a simple greedy algorithm described in [Sproat et al., 1996].", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 1, "simpl": 1, "algorithm": 1, "greedi": 1, "describ": 1}, "vector_2": [1, 0.313986745727241, 1, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The morphological analysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.", "each word in the lexicon whether or not each string is actually an instance of the word in question."], "label": "Prov", "citing": "W97-0120", "vector": [6, 0, 1, 0.2777777777777778], "context": ["", "[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.", ""], "marker": "Sproat et al., 1996", "vector_1": {"corpu": 1, "set": 1, "word": 1, "anoth": 1, "initi": 1, "without": 1, "also": 1, "estim": 1, "frequenc": 1, "segment": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.33038018835019184, 1, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The most popular approach to dealing with seg mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.", "This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin ning) of the sentence is reached."], "label": "Prov", "citing": "W97-0120", "vector": [14, 0, 4, 0.22062437582319247], "context": ["", "The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it \"maximum matching\", we call this method \"longest match\" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi! :.", ""], "marker": "Sproat et al., 1996", "vector_1": {"text": 1, "substr": 2, "appear": 1, "japanes": 1, "review": 1, "alway": 1, "call": 2, "method": 3, "match": 3, "accord": 1, "liter": 1, "string": 1, "chines": 1, "wl": 1, "train": 1, "longest": 2, "although": 1, "segment": 1, "name": 1, "word": 3, "like": 1, "frequenc": 1, "maximum": 1, "hi": 1, "w": 3, "problem": 1, "translat": 1}, "vector_2": [1, 0.3979769794209976, 2, 3, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.", "selected; and that recall is defined to be the number of correct hits divided by the number of items that should have been selected.", "The performance was 80.99% recall and 61.83% precision."], "label": "Prov", "citing": "W97-0120", "vector": [5, 0, 2, 0.20327890704543541], "context": ["", "Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"term": 1, "recal": 1, "express": 1, "accuraci": 1, "precis": 1, "bracket": 1, "done": 1, "pars": 1, "word": 1, "segment": 1, "partial": 1}, "vector_2": [1, 0.553993721660272, 2, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In this paper we present a stochastic finite-state model for segmenting Chinese text into words"], "label": "Prov", "citing": "W97-0316", "vector": [5, 0, 0, 0.06415002990995841], "context": ["", "Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin. Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).", ""], "marker": "Sproat et al., 1996", "vector_1": {"process": 1, "automat": 1, "correctli": 1, "research": 1, "call": 1, "taken": 1, "import": 1, "therefor": 1, "method": 2, "analysi": 1, "begin": 1, "sentenc": 1, "isol": 1, "step": 1, "segment": 1, "word": 2, "practic": 1, "resolv": 1, "necessari": 1, "mani": 1, "problem": 1, "first": 1, "propos": 1}, "vector_2": [1, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.", "Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach."], "label": "Prov", "citing": "D13-1031", "vector": [5, 0, 0, 0.11846977555181847], "context": ["", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al (2006).", ""], "marker": "2006", "vector_1": {"zhang": 1, "al": 1, "system": 1, "rule": 2, "repres": 1, "base": 1, "combin": 1, "et": 1, "crf": 2, "model": 2, "present": 1}, "vector_2": [7, 0.8087412708890664, 1, 0, 7, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging."], "label": "Prov", "citing": "N09-1007", "vector": [5, 0, 1, 0.17407765595569785], "context": ["", "Z06-a and Z06-b represents the pure sub- word CRF model and the condence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);", ""], "marker": "Zhang et al., 2006", "vector_1": {"word": 1, "sub": 1, "rulebas": 1, "za": 1, "zb": 1, "repres": 1, "crf": 2, "pure": 1, "respect": 1, "combin": 1, "model": 2, "condencebas": 1}, "vector_2": [3, 0.7283127539547911, 1, 2, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation"], "label": "Prov", "citing": "P06-2123", "vector": [2, 0, 1, 0.13363062095621217], "context": ["", "Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).", ""], "marker": "Zhang et al., 2006", "vector_1": {"descript": 1, "detail": 1, "paper": 1, "tag": 1, "crf": 1, "subword": 1, "found": 1}, "vector_2": [0, 0.8364829603858622, 1, 1, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters."], "label": "Prov", "citing": "P07-1106", "vector": [1, 0, 0, 0.0], "context": ["", "One existing method that is based on sub-word information, Zhang et al (2006), combines a C R F and a rule-based model.", ""], "marker": "2006", "vector_1": {"c": 1, "rulebas": 1, "zhang": 1, "f": 1, "al": 1, "one": 1, "inform": 1, "base": 1, "exist": 1, "subword": 1, "et": 1, "model": 1, "r": 1, "combin": 1, "method": 1}, "vector_2": [1, 0.5682316977582693, 1, 0, 2, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters."], "label": "Prov", "citing": "P07-1106", "vector": [5, 0, 0, 0.055555555555555566], "context": ["", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al (2006) for comparison.", ""], "marker": "2006", "vector_1": {"comparison": 1, "chose": 1, "zhang": 1, "emerson": 1, "well": 1, "three": 1, "least": 1, "one": 1, "al": 1, "achiev": 1, "score": 1, "subwordbas": 1, "et": 1, "test": 1, "close": 1, "model": 2, "best": 1}, "vector_2": [1, 0.9070387208026336, 2, 0, 2, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging."], "label": "Prov", "citing": "P12-1027", "vector": [7, 0, 1, 0.21885688981825285], "context": ["", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al (2006).", ""], "marker": "2006", "vector_1": {"al": 1, "correspond": 1, "second": 1, "et": 1, "best": 2, "segment": 1, "system": 1, "combin": 1, "confid": 1, "zhang": 1, "chines": 1, "base": 1, "repres": 2, "data": 1, "present": 1, "word": 1, "rulebas": 1, "intern": 1, "bakeoff": 1, "crf": 2, "model": 1, "rulesystem": 1}, "vector_2": [6, 0.9252285547558841, 1, 0, 2, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.", "We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections."], "label": "Prov", "citing": "W08-0335", "vector": [11, 0, 3, 0.36842105263157887], "context": ["", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 2, "word": 1, "describ": 1, "segment": 1, "tool": 1, "work": 1, "second": 1, "accuraci": 1, "bakeoff": 1, "achiev": 1, "part": 1, "data": 1, "report": 1, "sighan": 1, "highest": 1, "approach": 1}, "vector_2": [2, 0.2661164309467527, 1, 2, 1, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.", "Tri- gram LMs were generated using the SRI LM toolkit for disambiguation."], "label": "Prov", "citing": "W08-0335", "vector": [8, 0, 0, 0.21693045781865616], "context": ["", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the dict-hybrid. (Zhang et al., 2006) We used the dict-hybrid to segment the SMT training corpus and test data.", ""], "marker": "Zhang et al., 2006", "vector_1": {"corpu": 1, "dicthybrid": 2, "use": 1, "lexicon": 1, "resourc": 1, "like": 1, "dictionarybas": 1, "segment": 1, "lm": 1, "data": 1, "note": 1, "test": 1, "train": 1, "build": 1, "need": 1, "smt": 1, "cw": 1}, "vector_2": [2, 0.6487840057694619, 1, 2, 1, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.", "We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation."], "label": "Prov", "citing": "W10-4135", "vector": [5, 0, 1, 0.16979054399120358], "context": ["", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", ""], "marker": "Zhang et al., 2006", "vector_1": {"word": 1, "varietybas": 1, "system": 1, "subwordbas": 1, "accessor": 1, "recognit": 1, "tag": 1, "combin": 1, "base": 1, "new": 1, "purpos": 1, "method": 2}, "vector_2": [4, 0.10493077483604567, 2, 3, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters."], "label": "Prov", "citing": "W10-4135", "vector": [4, 0, 0, 0.0], "context": ["", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", ""], "marker": "Zhang et al., 2006", "vector_1": {"one": 1, "confidentwordlist": 1, "use": 1, "featur": 2, "construct": 1, "confidentword": 1, "g": 1, "instr": 2, "list": 3, "dure": 1, "f": 1, "descript": 1, "subwordlist": 1, "templat": 1, "crfbase": 1, "subword": 3, "str": 2, "tabl": 1, "similar": 1, "segment": 1}, "vector_2": [4, 0.22273500121447656, 1, 3, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters."], "label": "Prov", "citing": "W10-4135", "vector": [5, 0, 0, 0.1924500897298753], "context": ["", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)", ""], "marker": "Zhang et al., 2006", "vector_1": {"word": 1, "chines": 1, "detail": 1, "subwordbas": 1, "see": 1, "segment": 1}, "vector_2": [4, 0.2786009230021861, 1, 3, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems."], "label": "Prov", "citing": "E09-1045", "vector": [4, 1, 1, 0.06428243465332249], "context": ["", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005) and (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"use": 1, "set": 1, "wsd": 1, "classbas": 1, "sensegroup": 1, "focus": 1, "classifi": 1, "research": 1, "learn": 1, "predefin": 1, "contrast": 1}, "vector_2": [4, 0.1255747791855503, 5, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns."], "label": "Prov", "citing": "N06-1017", "vector": [7, 0, 1, 0.2849014411490949], "context": ["", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", ""], "marker": "Curran, 2005", "vector_1": {"detect": 1, "word": 1, "closest": 1, "unknown": 2, "howev": 1, "logic": 1, "next": 1, "step": 1, "determin": 1, "complementari": 1, "known": 1, "sens": 2, "problem": 1, "approach": 1, "view": 1}, "vector_2": [1, 0.15268411114108443, 3, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor."], "label": "Prov", "citing": "N06-1017", "vector": [0, 0, 0, 0.0], "context": ["", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", ""], "marker": "Curran, 2005", "vector_1": {"possibl": 1, "associ": 1, "approxim": 1, "cluster": 1, "item": 1, "exist": 1, "includ": 1, "sens": 2, "similar": 1}, "vector_2": [1, 0.9910593857181395, 3, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag."], "label": "Prov", "citing": "N07-1024", "vector": [5, 0, 1, 0.16984155512168939], "context": ["", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)", ""], "marker": "Curran 2005", "vector_1": {"use": 2, "sourc": 1, "word": 1, "wsd": 1, "unknown": 1, "eg": 1, "roark": 1, "semant": 1, "contextu": 1, "research": 1, "curran": 1, "inform": 2, "aramita": 1, "lexicon": 1, "classifi": 1, "charniak": 1, "ci": 1, "primari": 1, "languag": 1, "acquir": 1}, "vector_2": [2, 0.3811786144284273, 0, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETs hierarchical structure to create many annotated training instances from the synset glosses."], "label": "Prov", "citing": "P12-2050", "vector": [7, 1, 1, 0.06819943394704735], "context": ["", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", ""], "marker": "Curran, 2005", "vector_1": {"semant": 1, "lexicon": 1, "appli": 1, "automat": 1, "ap": 1, "tag": 1, "liev": 1, "abil": 1, "need": 1, "summar": 1, "wordnet": 1, "depend": 1, "avail": 1, "figur": 2, "languag": 2, "sentenc": 1, "ought": 1, "verbsin": 1, "sst": 1, "chines": 1, "focuson": 1, "arab": 1, "centli": 1, "pli": 1, "supersens": 2, "task": 1, "noun": 2, "work": 1, "well": 1, "wordnetsmap": 1, "emerg": 1, "english": 2, "principl": 1, "italian": 1}, "vector_2": [7, 0.15643180349062702, 8, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems."], "label": "Prov", "citing": "S07-1032", "vector": [5, 2, 0, 0.09534625892455921], "context": ["", "Thus, some research has been focused on deriving different sense groupings to overcome the fine grained distinctions of WN (Hearst and Schu tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"learn": 1, "differ": 1, "set": 1, "group": 1, "deriv": 1, "classbas": 1, "distinct": 1, "wn": 1, "thu": 1, "use": 1, "focus": 1, "classifi": 1, "research": 1, "grain": 1, "overcom": 1, "sensegroup": 1, "sens": 1, "fine": 1, "predefin": 1, "wsd": 1}, "vector_2": [2, 0.22009821115398107, 9, 1, 0, 0]}, {"function": "Pos", "cited": "P05-1004", "provenance": ["Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts."], "label": "Prov", "citing": "S12-1011", "vector": [3, 0, 0, 0.0944911182523068], "context": ["", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", ""], "marker": "Curran, 2005", "vector_1": {"represent": 1, "use": 1, "word": 1, "distribut": 1, "captur": 1, "individu": 1, "mean": 1}, "vector_2": [7, 0.0735049557639859, 3, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETs hierarchical structure to create many annotated training instances from the synset glosses."], "label": "Prov", "citing": "S12-1011", "vector": [4, 1, 1, 0.15811388300841897], "context": ["", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a models ability to cluster words by their semantics.", ""], "marker": "Curran, 2005", "vector_1": {"supersens": 1, "semant": 1, "word": 1, "evalu": 1, "cluster": 1, "tag": 1, "abil": 1, "model": 1}, "vector_2": [7, 0.43856848267145615, 2, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems."], "label": "Prov", "citing": "S12-1023", "vector": [7, 0, 1, 0.12087344460380704], "context": ["", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", ""], "marker": "Curran, 2005", "vector_1": {"inde": 1, "use": 2, "concept": 1, "word": 1, "wsd": 2, "classbas": 2, "might": 1, "previou": 1, "well": 1, "work": 1, "meta": 1, "singl": 1, "cam": 1, "sens": 2, "beyond": 1, "ie": 1, "notion": 1, "analog": 1}, "vector_2": [7, 0.908493870402802, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETs hierarchical structure to create many annotated training instances from the synset glosses."], "label": "Prov", "citing": "W06-1670", "vector": [3, 1, 0, 0.10846522890932808], "context": ["", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", ""], "marker": "Curran, 2005", "vector_1": {"supersens": 1, "classif": 1, "previou": 1, "noun": 1, "level": 1, "predict": 1, "thu": 1, "work": 1, "rather": 1, "focus": 1, "lexic": 1, "aim": 1, "acquisit": 1, "tag": 1, "exclus": 1, "word": 1, "type": 1}, "vector_2": [1, 0.39584544808425404, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance."], "label": "Prov", "citing": "D07-1076", "vector": [10, 0, 2, 0.2857502857504286], "context": ["", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information", ""], "marker": "Zhou et al., 2005", "vector_1": {"among": 1, "semant": 1, "featur": 1, "certain": 1, "knowledg": 1, "vari": 1, "entiti": 1, "depend": 1, "featurebas": 1, "achiev": 1, "larg": 1, "divers": 1, "syntact": 1, "relat": 1, "lexic": 1, "pars": 1, "success": 1, "tree": 2, "method": 1, "employ": 1, "inform": 2, "amount": 1, "kambhatla": 1, "linguist": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Weak", "cited": "P05-1053", "provenance": ["Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins parser used in our system achieves the state-of-the-art performance.", "Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment."], "label": "Prov", "citing": "D07-1076", "vector": [7, 0, 1, 0.07484811885651198], "context": ["", "How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", ""], "marker": "Zhou et al 2005", "vector_1": {"zhou": 1, "captur": 1, "perform": 1, "extract": 1, "relat": 1, "tree": 1, "effect": 1, "al": 1, "inform": 1, "critic": 1, "pars": 1, "improv": 1, "struc": 1, "et": 1, "ture": 1, "ever": 1, "difficult": 1}, "vector_2": [2, 0.5, 0, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Moreover, we only apply the simple linear kernel, although other kernels can peform better.", "The reason why we choose SVMs for this purpose is that SVMs represent the state-ofthe-art in the machine learning research community, and there are good implementations of the algorithm available."], "label": "Prov", "citing": "D07-1076", "vector": [7, 0, 1, 0.14457873299156004], "context": ["", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)", ""], "marker": "Zhou et al 2005", "vector_1": {"kernel": 4, "c": 1, "via": 1, "composit": 2, "describ": 1, "zhou": 1, "zhang": 1, "appli": 1, "linear": 1, "convolut": 1, "al": 2, "tree": 1, "polynomi": 1, "state": 1, "paper": 1, "ontextsensit": 1, "integr": 1, "theart": 1, "interpol": 1, "et": 2, "propos": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree."], "label": "Prov", "citing": "D07-1076", "vector": [11, 0, 8, 0.5459486832355505], "context": ["", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", ""], "marker": "20 05", "vector_1": {"semant": 1, "set": 1, "chunk": 1, "al": 1, "featur": 1, "phrase": 1, "et": 1, "ing": 1, "ie": 1, "entiti": 1, "use": 1, "depend": 1, "overlap": 1, "type": 1, "flat": 1, "zhou": 1, "mention": 1, "base": 1, "pars": 1, "word": 1, "level": 1, "tree": 2, "inform": 1}, "vector_2": [8, 0.5, 0, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types."], "label": "Prov", "citing": "D07-1076", "vector": [4, 0, 1, 0.31622776601683794], "context": ["", " dependency kernel Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"kernel": 1, "depend": 1, "al": 1, "zhou": 1, "et": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree."], "label": "Prov", "citing": "D12-1074", "vector": [3, 0, 0, 0.09100315103865803], "context": ["", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "point": 1, "proven": 1, "lightweight": 1, "leverag": 1, "absolut": 1, "use": 1, "rather": 1, "reli": 1, "score": 1, "higher": 1, "exhaust": 1, "sourc": 1, "syntact": 1, "gener": 1, "serv": 1, "comparison": 1, "attempt": 1, "possibl": 1, "inform": 1, "model": 2, "toward": 1}, "vector_2": [7, 0.4621767713659606, 1, 1, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Prov", "citing": "E06-2012", "vector": [3, 0, 0, 0.13245323570650439], "context": ["", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", ""], "marker": "Zelenko et al, 2003, Zhou et al, 2005", "vector_1": {"classif": 1, "work": 1, "trainabl": 1, "relat": 1, "svm": 1, "chiefli": 1, "begun": 1, "address": 1, "mean": 1, "extract": 1, "event": 1, "recent": 1}, "vector_2": [3, 0.5524416730890492, 2, 1, 0, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance."], "label": "Prov", "citing": "E12-1020", "vector": [5, 0, 0, 0.18257418583505533], "context": ["", "For the choice of features, we use the full set of features from Zhou et al (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", ""], "marker": "2005", "vector_1": {"choic": 1, "use": 1, "featur": 2, "zhou": 1, "perform": 1, "al": 1, "set": 1, "full": 1, "stateoftheart": 1, "report": 1, "et": 1, "sinc": 1}, "vector_2": [7, 0.21029569892473118, 2, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information."], "label": "Prov", "citing": "E12-1020", "vector": [7, 0, 1, 0.07905694150420949], "context": ["", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "weight": 1, "show": 1, "predict": 1, "repres": 1, "extract": 1, "probabl": 1, "use": 2, "space": 1, "perform": 1, "includ": 1, "correct": 1, "competit": 1, "relat": 1, "one": 1, "associ": 1, "sinc": 1, "untag": 1, "erron": 1, "maxent": 1, "exampl": 3, "stateoftheart": 1, "learn": 1, "output": 1, "model": 1}, "vector_2": [7, 0.3843010752688172, 2, 5, 4, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Prov", "citing": "E12-1020", "vector": [1, 0, 0, 0.0], "context": ["", "We use SVM as our learning algorithm with the full feature set from Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"use": 1, "svm": 1, "set": 1, "zhou": 1, "algorithm": 1, "al": 1, "full": 1, "learn": 1, "et": 1, "featur": 1}, "vector_2": [7, 0.7999731182795699, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance."], "label": "Prov", "citing": "N06-1037", "vector": [3, 0, 1, 0.19069251784911848], "context": ["", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", ""], "marker": "Zhou et al., 2005", "vector_1": {"hierarch": 1, "featur": 1, "syntact": 1, "less": 1, "perform": 1, "howev": 1, "structur": 1, "report": 1, "improv": 1, "contribut": 1}, "vector_2": [1, 0.05714082148836878, 2, 21, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Prov", "citing": "N06-1037", "vector": [4, 0, 2, 0.21764287503300347], "context": ["", "Zhou et al (2005) explore various features in relation extraction using SVM.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "variou": 1, "relat": 1, "al": 1, "svm": 1, "explor": 1, "et": 1, "extract": 1}, "vector_2": [1, 0.14598696163300204, 1, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information."], "label": "Prov", "citing": "N06-1037", "vector": [4, 1, 0, 0.10846522890932807], "context": ["", "The features used in Kambhatla (2004) and Zhou et al (2005) have to be selected and carefully calibrated manually.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "manual": 1, "al": 1, "calibr": 1, "kambhatla": 1, "et": 1, "select": 1, "care": 1}, "vector_2": [1, 0.158490969327776, 2, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance."], "label": "Prov", "citing": "N06-1037", "vector": [4, 0, 2, 0.24174688920761409], "context": ["", "Besides, Zhou et al (2005) introduce additional chunking features to enhance the parse tree features.", ""], "marker": "2005", "vector_1": {"featur": 2, "zhou": 1, "chunk": 1, "tree": 1, "al": 1, "enhanc": 1, "besid": 1, "pars": 1, "et": 1, "introduc": 1, "addit": 1}, "vector_2": [1, 0.1660076235260589, 1, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information."], "label": "Prov", "citing": "N06-1037", "vector": [4, 1, 0, 0.10846522890932807], "context": ["", "we call the features used in Zhou et al (2005) and Kambhatla (2004) flat feature set.", ""], "marker": "2005", "vector_1": {"flat": 1, "use": 1, "featur": 2, "zhou": 1, "kambhatla": 1, "al": 1, "set": 1, "call": 1, "et": 1}, "vector_2": [1, 0.6290833956752521, 2, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number."], "label": "Prov", "citing": "N06-1037", "vector": [13, 0, 10, 0.7590721152765897], "context": ["", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", ""], "marker": "Zhou et al., 2005", "vector_1": {"poor": 1, "number": 1, "carri": 1, "relat": 2, "explicit": 1, "due": 1, "agreement": 1, "limit": 1, "annot": 1, "experi": 1, "interannot": 1, "implicit": 1}, "vector_2": [1, 0.6369562894089986, 1, 21, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree."], "label": "Prov", "citing": "N07-1015", "vector": [4, 1, 0, 0.09759000729485333], "context": ["", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", ""], "marker": "Zhou et al., 2005", "vector_1": {"analysi": 1, "differ": 1, "set": 1, "depend": 1, "full": 1, "level": 1, "text": 1, "tag": 1, "obtain": 1, "util": 1, "featur": 1, "pars": 2, "partofspeech": 1, "first": 1, "po": 1, "select": 1, "care": 1}, "vector_2": [2, 0.06748259977808413, 3, 2, 9, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information."], "label": "Prov", "citing": "N07-1015", "vector": [3, 0, 0, 0.0], "context": ["", "Zhao and Grishman (2005) and Zhou et al (2005) explored a large set of features that are potentially useful for relation extraction.", ""], "marker": "2005", "vector_1": {"use": 1, "set": 1, "potenti": 1, "zhou": 1, "grishman": 1, "al": 1, "zhao": 1, "featur": 1, "larg": 1, "explor": 1, "et": 1, "relat": 1, "extract": 1}, "vector_2": [2, 0.1637133922867422, 2, 0, 2, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": [" Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase."], "label": "Prov", "citing": "N07-1015", "vector": [5, 0, 2, 0.2637521893583148], "context": ["", "Entity Attributes: Previous studies have shown that entity types and entity mention types of arg 1 and arg 2 are very useful (Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006b).", ""], "marker": "Zhou et al., 2005", "vector_1": {"shown": 1, "previou": 1, "use": 1, "mention": 1, "arg": 2, "studi": 1, "entiti": 3, "type": 2, "attribut": 1}, "vector_2": [2, 0.4683097407619112, 3, 2, 9, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include:  WM1: bag-of-words in M1  HM1: head word of M1 3 In ACE, each mention has a head annotation and an.", "extent annotation."], "label": "Prov", "citing": "N07-1015", "vector": [4, 0, 0, 0.1126872339638022], "context": ["", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", ""], "marker": "2005", "vector_1": {"bagofword": 1, "featur": 1, "zhou": 1, "grishman": 1, "al": 1, "also": 1, "zhao": 1, "explor": 1, "et": 1}, "vector_2": [2, 0.5063716754648465, 2, 0, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree."], "label": "Prov", "citing": "N07-1015", "vector": [3, 0, 0, 0.16514456476895406], "context": ["", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", ""], "marker": "2005", "vector_1": {"featur": 1, "depend": 2, "zhou": 1, "relat": 1, "al": 1, "grishman": 1, "zhao": 1, "mooney": 1, "explor": 1, "et": 1, "path": 1, "bunescu": 1}, "vector_2": [2, 0.5275209307017249, 3, 0, 2, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance."], "label": "Prov", "citing": "N09-3012", "vector": [6, 0, 0, 0.337099931231621], "context": ["", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", ""], "marker": "Zhou et al., 2005", "vector_1": {"concentr": 1, "use": 1, "featur": 2, "syntact": 1, "captur": 1, "perform": 1, "relat": 1, "better": 1, "achiev": 1, "paper": 1, "necessari": 1, "although": 1, "proper": 1, "combin": 1, "extract": 1, "order": 1, "best": 1}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Exploring Various Knowledge in Relation Extraction"], "label": "Prov", "citing": "N13-1093", "vector": [0, 0, 0, 0.0], "context": ["", "The former is Zhou et al (2005), which uses 51 different features.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "differ": 1, "al": 1, "et": 1, "former": 1}, "vector_2": [8, 0.43778286566210534, 1, 0, 0, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Moreover, we only apply the simple linear kernel, although other kernels can peform better."], "label": "Prov", "citing": "N13-1093", "vector": [1, 0, 0, 0.23210354127426377], "context": ["", "These experiments are done using Zhou et al (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", ""], "marker": "2005", "vector_1": {"kernel": 4, "use": 1, "tpwf": 1, "zhou": 1, "f": 1, "kh": 2, "differ": 1, "al": 1, "ybrid": 1, "version": 1, "done": 1, "sl": 1, "et": 1, "experi": 1, "propos": 1}, "vector_2": [8, 0.7083222405622703, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998)."], "label": "Prov", "citing": "N13-1093", "vector": [3, 0, 0, 0.0], "context": ["", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"zhou": 1, "fold": 1, "perform": 1, "cross": 1, "classifi": 1, "al": 1, "also": 1, "valid": 1, "combin": 1, "et": 1, "experi": 1, "stage": 1}, "vector_2": [8, 0.7383525903838987, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Exploring Various Knowledge in Relation Extraction"], "label": "Prov", "citing": "N13-1095", "vector": [2, 0, 1, 0.4472135954999579], "context": ["", "Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a).", ""], "marker": "Zhou et al., 2005", "vector_1": {"problem": 1, "relat": 1, "extract": 1, "wellstudi": 1}, "vector_2": [8, 0.05677339901477833, 4, 1, 0, 0]}, {"function": "Weak", "cited": "P05-1053", "provenance": ["ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype Founder under the type ROLE."], "label": "Prov", "citing": "P06-1016", "vector": [2, 0, 0, 0.08333333333333334], "context": ["", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", ""], "marker": "Zhou et al 2005", "vector_1": {"major": 1, "zhou": 1, "spars": 1, "data": 1, "challeng": 1, "relat": 1, "al": 1, "due": 1, "one": 1, "et": 1, "problem": 1, "extract": 1}, "vector_2": [1, 0.07524341580207503, 0, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).", "ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype Founder under the type ROLE."], "label": "Prov", "citing": "P06-1016", "vector": [6, 0, 1, 0.034752402342845795], "context": ["", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", ""], "marker": "Zhou et al 2005", "vector_1": {"among": 1, "entropi": 1, "deal": 1, "miller": 1, "al": 2, "et": 2, "extract": 1, "differ": 1, "uneven": 1, "variou": 1, "support": 1, "grisman": 1, "much": 1, "appli": 1, "approach": 1, "strategi": 1, "machin": 2, "spars": 1, "distribut": 1, "zhou": 1, "gener": 1, "relat": 2, "inher": 1, "data": 1, "problem": 1, "task": 1, "explicit": 1, "maximum": 1, "caus": 1, "zhao": 1, "vector": 1, "kambhatla": 1, "learn": 2, "model": 1, "propos": 1}, "vector_2": [1, 0.105219473264166, 0, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system."], "label": "Prov", "citing": "P06-1016", "vector": [5, 0, 0, 0.03500700210070025], "context": ["", "With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"process": 1, "natur": 1, "within": 1, "al": 4, "mooney": 2, "et": 4, "attract": 1, "languag": 1, "commun": 1, "yih": 1, "research": 1, "start": 1, "grisman": 1, "includ": 1, "miller": 1, "bunescu": 2, "machin": 1, "zhou": 1, "zhang": 1, "sorensen": 1, "zelenko": 1, "culotta": 1, "increas": 1, "task": 1, "ace": 1, "work": 1, "roth": 1, "zhao": 1, "kambhatla": 1, "learn": 1, "popular": 1, "typic": 1}, "vector_2": [1, 0.1599361532322426, 10, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features."], "label": "Prov", "citing": "P06-1016", "vector": [8, 1, 5, 0.28306925853614895], "context": ["", "Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", ""], "marker": "", "vector_1": {"corpu": 1, "semant": 1, "featur": 1, "al": 1, "explor": 1, "respect": 1, "et": 1, "systemat": 1, "support": 1, "type": 1, "divers": 1, "machin": 1, "syntact": 1, "zhou": 1, "relat": 2, "lexic": 1, "measur": 1, "ace": 1, "f": 1, "achiev": 1, "vector": 1, "rdc": 1, "subtyp": 1}, "vector_2": [8, 0.243926576217079, 1, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.", "The semantic relation is determined between two mentions.", "In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g.", "M1-Parent- Of-M2 vs. M2-Parent-Of-M1."], "label": "Prov", "citing": "P06-1016", "vector": [12, 0, 11, 0.42817441928883765], "context": ["", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", ""], "marker": "2005", "vector_1": {"involv": 1, "explicitli": 1, "zhou": 1, "relat": 1, "explicit": 1, "al": 1, "argument": 1, "mention": 1, "two": 1, "et": 1, "model": 2, "order": 1}, "vector_2": [1, 0.633583399840383, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information."], "label": "Prov", "citing": "P06-1104", "vector": [4, 0, 0, 0.08770580193070293], "context": ["", "Many techniques on relation extraction, such as rule-based (MUC, 19871998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature.", ""], "marker": "Zhou et al., 2005", "vector_1": {"rulebas": 1, "techniqu": 1, "featurebas": 1, "relat": 1, "literatur": 1, "kambhatla": 1, "kernelbas": 1, "mani": 1, "extract": 1, "propos": 1}, "vector_2": [1, 0.14889867841409693, 6, 2, 0, 1]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information."], "label": "Prov", "citing": "P06-1104", "vector": [9, 0, 4, 0.35805743701971643], "context": ["", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "task": 1, "syntact": 1, "featurebas": 1, "lexic": 1, "divers": 1, "employ": 1, "amount": 1, "featur": 2, "larg": 1, "linguist": 1, "method": 1}, "vector_2": [1, 0.15784479837343274, 3, 2, 0, 1]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998)."], "label": "Prov", "citing": "P06-2012", "vector": [1, 1, 0, 0.0], "context": ["", "Many methods have been proposed to deal with this task, including supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "task": 1, "algorithm": 3, "deal": 1, "semisupervis": 1, "unsupervis": 1, "includ": 1, "learn": 3, "mani": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.037396121883656507, 9, 19, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance."], "label": "Prov", "citing": "P09-1113", "vector": [9, 0, 1, 0.15694120514358612], "context": ["", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 2, "use": 1, "featur": 2, "syntact": 2, "ace": 1, "perform": 2, "data": 1, "unsupervis": 1, "least": 1, "whether": 1, "know": 1, "clean": 1, "known": 1, "improv": 2, "handlabel": 1, "ie": 2, "distantli": 1}, "vector_2": [4, 0.20506768938276462, 2, 4, 3, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Prov", "citing": "P09-1113", "vector": [5, 0, 1, 0.10039117221153819], "context": ["", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al (2005), and work in the ACE paradigm such as Zhou et al (2005) and Zhou et al (2007).", ""], "marker": "2005", "vector_1": {"al": 3, "exploit": 1, "et": 3, "deriv": 1, "use": 1, "depend": 1, "lin": 1, "snow": 1, "includ": 1, "input": 1, "approach": 1, "syntact": 2, "zhou": 2, "sentenc": 1, "pars": 1, "recent": 1, "ace": 1, "work": 2, "pantel": 1, "deeper": 1, "inform": 1, "paradigm": 1}, "vector_2": [4, 0.30835545940276005, 4, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Prov", "citing": "P09-1114", "vector": [5, 0, 1, 0.22496063533292376], "context": ["", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", ""], "marker": "2005", "vector_1": {"variou": 1, "supervis": 1, "work": 1, "domin": 1, "zhou": 1, "featurebas": 1, "relat": 2, "al": 1, "grishman": 1, "zhao": 1, "featur": 2, "combin": 1, "kernelbas": 1, "learn": 1, "et": 1, "studi": 1, "extract": 2, "method": 1, "recent": 1}, "vector_2": [4, 0.14393990661162617, 2, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance."], "label": "Prov", "citing": "P09-1114", "vector": [5, 0, 1, 0.08333333333333333], "context": ["", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "shown": 1, "work": 1, "engin": 1, "coupl": 1, "kernel": 1, "intellig": 1, "relat": 1, "solut": 1, "provid": 1, "featur": 1, "design": 1, "stateoftheart": 1, "learn": 1, "problem": 1, "extract": 1, "recent": 1}, "vector_2": [4, 0.04172024091493537, 4, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information."], "label": "Prov", "citing": "P11-1053", "vector": [7, 0, 3, 0.21483446221182986], "context": ["", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "explicitli": 1, "syntact": 1, "gener": 1, "learn": 1, "lexic": 1, "discrimin": 1, "featur": 2, "statist": 1, "base": 1, "varieti": 1, "extract": 1, "method": 1, "either": 1}, "vector_2": [6, 0.040273082351396594, 6, 5, 4, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "The reason why we choose SVMs for this purpose is that SVMs represent the state-ofthe-art in the machine learning research community, and there are good implementations of the algorithm available."], "label": "Prov", "citing": "P11-1053", "vector": [4, 0, 0, 0.04564354645876384], "context": ["", "We first adopted the full feature set from Zhou et al (2005), a state-of-the-art feature based relation extraction system.", ""], "marker": "2005", "vector_1": {"set": 1, "zhou": 1, "adopt": 1, "al": 1, "system": 1, "featur": 2, "full": 1, "stateoftheart": 1, "base": 1, "et": 1, "relat": 1, "extract": 1, "first": 1}, "vector_2": [6, 0.335313618012932, 1, 0, 1, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": ["Exploring Various Knowledge in Relation Extraction"], "label": "Prov", "citing": "P11-1053", "vector": [3, 0, 1, 0.24806946917841693], "context": ["", "In addition, we cherry-picked the following features which were not included in Zhou et al (2005) but were shown to be quite effective for relation extraction.", ""], "marker": "2005", "vector_1": {"quit": 1, "shown": 1, "featur": 1, "zhou": 1, "cherrypick": 1, "relat": 1, "al": 1, "effect": 1, "includ": 1, "et": 1, "follow": 1, "extract": 1, "addit": 1}, "vector_2": [6, 0.3487379787967309, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system."], "label": "Prov", "citing": "P11-1053", "vector": [3, 0, 1, 0.2182178902359924], "context": ["", "Zhou et al (2005) tested their system on the ACE 2003 data;.", ""], "marker": "2005", "vector_1": {"zhou": 1, "ace": 1, "al": 1, "system": 1, "test": 1, "et": 1, "data": 1}, "vector_2": [6, 0.744969967505826, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g.", "M1-Parent- Of-M2 vs. M2-Parent-Of-M1."], "label": "Prov", "citing": "P11-1056", "vector": [2, 0, 0, 0.0], "context": ["", "However, most approaches to RE have assumed that the relations arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", ""], "marker": "Zhou et al., 2005", "vector_1": {"given": 1, "partial": 1, "offer": 1, "approach": 1, "howev": 1, "relat": 1, "solut": 1, "argument": 1, "input": 1, "problem": 1, "therefor": 1, "assum": 1}, "vector_2": [6, 0.03773584905660377, 4, 3, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In this paper, we only measure the performance of relation extraction on true mentions with true chaining of coreference (i.e.", "as annotated by the corpus annotators) in the ACE corpus."], "label": "Prov", "citing": "P11-1056", "vector": [4, 0, 0, 0.14142135623730948], "context": ["", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"given": 1, "evalu": 1, "ace": 1, "mention": 1, "data": 1, "prior": 1, "alreadi": 1, "input": 1, "assum": 1, "preannot": 1}, "vector_2": [6, 0.2830516247379455, 3, 3, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998)."], "label": "Prov", "citing": "W06-1634", "vector": [2, 0, 0, 0.0], "context": ["", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "allevi": 1, "techniqu": 1, "craft": 1, "manual": 1, "base": 1, "expect": 1, "learn": 1, "problem": 1, "ie": 1}, "vector_2": [1, 0.05796430931923331, 3, 4, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking."], "label": "Prov", "citing": "W06-1634", "vector": [3, 0, 1, 0.11396057645963795], "context": ["", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"use": 1, "shallow": 1, "techniqu": 1, "previou": 1, "pars": 1, "mani": 1, "approach": 1}, "vector_2": [1, 0.10300727032385988, 3, 4, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information."], "label": "Prov", "citing": "W11-1101", "vector": [2, 0, 0, 0.0], "context": ["", "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "previou": 1, "research": 1, "erd": 1, "explor": 1, "varieti": 1}, "vector_2": [6, 0.9046301633045148, 4, 3, 4, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Moreover, we only apply the simple linear kernel, although other kernels can peform better."], "label": "Prov", "citing": "W11-1101", "vector": [1, 0, 0, 0.08333333333333333], "context": ["", "Researchers have used supervised and semi-supervised approaches (Hasegawa et al., 2004; Mintz et al., 2009; Jiang, 2009), and explored rich features (Kambhatla, 2004), kernel design (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008) and inference algorithms (Chan and Roth, 2011), to detect predefined relations between NEs.", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "use": 1, "featur": 1, "predefin": 1, "rich": 1, "algorithm": 1, "semisupervis": 1, "kernel": 1, "relat": 1, "ne": 1, "research": 1, "detect": 1, "design": 1, "explor": 1, "approach": 1, "infer": 1}, "vector_2": [6, 0.05106628242074928, 9, 3, 4, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Entities can be of five types: persons, organizations, locations, facilities and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g.", "countries, states, cities, etc.", ")."], "label": "Prov", "citing": "W11-1815", "vector": [3, 0, 0, 0.06286946134619316], "context": ["", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"task": 1, "locat": 1, "bb": 1, "challeng": 1, "geograph": 1, "ureaplasma": 1, "biolog": 1, "exampl": 1, "mycoplasma": 1, "parvum": 1, "pathogen": 1}, "vector_2": [6, 0.041832963784183295, 2, 7, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to gether subsets of existing classes with over lapping members."], "label": "Prov", "citing": "A00-2034", "vector": [1, 0, 0, 0.0], "context": ["", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al.,  1998).", ""], "marker": "Dang et al, 1998", "vector_1": {"levin": 1, "classif": 1, "research": 1, "extend": 1, "nlp": 1}, "vector_2": [2, 0.13932515571859835, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to gether subsets of existing classes with over lapping members."], "label": "Prov", "citing": "A00-2034", "vector": [4, 0, 1, 0.32071349029490925], "context": ["", "Dang et al (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", ""], "marker": "1998", "vector_1": {"origin": 1, "ad": 1, "remov": 1, "al": 1, "overlap": 1, "new": 1, "dang": 1, "et": 1, "scheme": 1, "modifi": 1, "class": 2}, "vector_2": [2, 0.15648286140089418, 1, 0, 1, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Investigating regular sense extensions based on intersective Levin classes"], "label": "Prov", "citing": "E99-1007", "vector": [0, 0, 0, 0.0], "context": ["", "Manual classifica tion of large numbers of verbs is a difficult and resource intensive task (Levin, 1993; Miller et a!., 1990; Dang eta!., 1998).", ""], "marker": "Dang eta!., 1998", "vector_1": {"task": 1, "resourc": 1, "eta": 1, "miller": 1, "manual": 1, "number": 1, "dang": 1, "verb": 1, "larg": 1, "classifica": 1, "et": 1, "tion": 1, "intens": 1, "difficult": 1}, "vector_2": [1, 0.08618366888705935, 1, 0, 4, 1]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["2.1 Ambiguities in Levin classes.", "We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to gether subsets of existing classes with over lapping members."], "label": "Prov", "citing": "J01-3003", "vector": [7, 0, 1, 0.040422604172722164], "context": ["", "We think that many cases of ambigu ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al (1998).", ""], "marker": "1998", "vector_1": {"case": 1, "classif": 1, "set": 1, "et": 1, "entri": 1, "notion": 1, "al": 1, "lexic": 1, "ambigu": 1, "verb": 1, "intersect": 1, "address": 1, "dang": 1, "mani": 1, "ou": 1, "think": 1, "introduc": 1}, "vector_2": [3, 0.9466904497016394, 1, 0, 20, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman tic components."], "label": "Prov", "citing": "J04-1003", "vector": [6, 0, 1, 0.19200614429492777], "context": ["", "Palmer (2000) and Dang et al (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", ""], "marker": "1998", "vector_1": {"use": 1, "develop": 1, "syntact": 1, "frame": 1, "palmer": 1, "al": 1, "argu": 1, "verb": 2, "dang": 1, "et": 1, "principl": 1, "class": 1, "classif": 1}, "vector_2": [6, 0.10310412026726058, 2, 0, 27, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Investigating regular sense extensions based on intersective Levin classes"], "label": "Prov", "citing": "J04-1003", "vector": [2, 0, 0, 0.07715167498104596], "context": ["", "Levins (1993) seminal study on diathesis alternations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al 1998; Dorr and Jones 1996)", ""], "marker": "Dang et al. 1998", "vector_1": {"altern": 1, "semant": 1, "creation": 1, "jone": 1, "dorr": 2, "influenc": 1, "levin": 1, "work": 1, "semin": 1, "al": 1, "verb": 1, "dictionari": 1, "dang": 1, "et": 1, "studi": 1, "recent": 1, "class": 1, "diathesi": 1}, "vector_2": [6, 0.8078090200445435, 1, 0, 27, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Investigating regular sense extensions based on intersective Levin classes"], "label": "Prov", "citing": "N01-1012", "vector": [0, 0, 0, 0.0], "context": ["", "Besides the obvious influence of WordNet, this work is very much related to Palmer&apos;s VerbNet project (Dang et al., 1998), and has benefited from (Levin, 1993) and (Pritchett, 1992).", ""], "marker": "Dang et al., 1998", "vector_1": {"palmeraposs": 1, "relat": 1, "obviou": 1, "influenc": 1, "work": 1, "project": 1, "benefit": 1, "much": 1, "besid": 1, "wordnet": 1, "verbnet": 1}, "vector_2": [3, 0.9138599001780042, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["The ad junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise.", "The carry verbs that are not in the intersective class (carry, drag, haul, heft, hoist, lug, tote, tow) are more \"pure\" examples of the carry class and always imply the achievement of causation of motion.", "Investigating regular sense extensions based on intersective Levin classes"], "label": "Prov", "citing": "N09-1057", "vector": [11, 0, 3, 0.10741723110591493], "context": ["", "participants  causation, change of state, and others  are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "lexicon": 1, "central": 1, "causat": 1, "well": 1, "eg": 1, "work": 1, "particip": 1, "theoret": 1, "state": 1, "other": 1, "lexic": 1, "comput": 1, "chang": 1, "approach": 1}, "vector_2": [11, 0.1746642877265471, 4, 1, 1, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["The fundamental assumption is that the syntactic frames are a direct reflection of the un derlying semantics."], "label": "Prov", "citing": "P03-1009", "vector": [4, 0, 0, 0.07856742013183861], "context": ["", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", ""], "marker": "Dang et al., 1998", "vector_1": {"classif": 1, "linguist": 2, "comput": 1, "consider": 1, "captur": 1, "eg": 1, "relat": 1, "semant": 1, "syntax": 1, "aim": 1, "verb": 1, "interest": 1, "close": 1, "research": 1, "attract": 1}, "vector_2": [5, 0.041832593772518675, 6, 2, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Investigating regular sense extensions based on intersective Levin classes"], "label": "Prov", "citing": "P04-1046", "vector": [0, 0, 0, 0.0], "context": ["", "In computational linguistics, the VerbNet project (Dang et al., 1998) and Framenet (www.icsi.berkeley.edu/ framen,a) (Fillmore et al., 2003) bear relation to this work.", ""], "marker": "Dang et al., 1998", "vector_1": {"work": 1, "comput": 1, "wwwicsiberkeleyedu": 1, "relat": 1, "bear": 1, "framena": 1, "project": 1, "framenet": 1, "linguist": 1, "verbnet": 1}, "vector_2": [6, 0.9609006250797295, 2, 1, 0, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["Investigating regular sense extensions based on intersective Levin classes"], "label": "Prov", "citing": "P06-1117", "vector": [2, 0, 1, 0.18257418583505533], "context": ["", "A very good candidate seems to be the Intersective Levin class (ILC) (Dang et al., 1998) that can be found as well in other predicate resources like PB and VerbNet (VN) (Kipper et al., 2000).", ""], "marker": "Dang et al., 1998", "vector_1": {"good": 1, "resourc": 1, "ilc": 1, "levin": 1, "well": 1, "vn": 1, "predic": 1, "pb": 1, "candid": 1, "intersect": 1, "found": 1, "seem": 1, "verbnet": 1, "class": 1, "like": 1}, "vector_2": [8, 0.09937578027465668, 2, 2, 3, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to gether subsets of existing classes with over lapping members."], "label": "Prov", "citing": "P06-1117", "vector": [8, 0, 0, 0.28284271247461906], "context": ["", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levins classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "lexicon": 1, "version": 1, "insid": 1, "constraint": 1, "levin": 2, "classif": 1, "vn": 1, "base": 1, "role": 1, "construct": 1, "intersect": 1, "refin": 1, "ensur": 1, "call": 1, "class": 1, "ilc": 1}, "vector_2": [8, 0.3675405742821473, 1, 2, 3, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Investigating regular sense extensions based on intersective Levin classes"], "label": "Prov", "citing": "P13-3009", "vector": [2, 0, 0, 0.16222142113076252], "context": ["", "For English verbs, the entire verb list (3750) enlisted by Levin (Levin, 1993) including extensions (Dang et. al, 1998; Kipper et. al, 2006; Korhonen and Briscoe, 2004) was classified according to the new classification.", ""], "marker": "Dang et. al, 1998", "vector_1": {"accord": 1, "kipper": 1, "entir": 1, "levin": 1, "list": 1, "classif": 1, "classifi": 1, "enlist": 1, "extens": 1, "verb": 2, "includ": 1, "english": 1, "dang": 1, "et": 2, "new": 1}, "vector_2": [15, 0.7350045113961791, 4, 11, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["First, since our experi ment was based on a translation from English to Portuguese"], "label": "Prov", "citing": "P99-1051", "vector": [1, 0, 0, 0.0], "context": ["", "Levin's study on diathesis alternations has influenced recent work on word sense disam biguation (Dorr and Jones, 1996), machine transla tion (Dang et al., 1998), and automatic lexical ac quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"altern": 1, "machin": 1, "transla": 1, "word": 1, "influenc": 1, "levin": 1, "work": 1, "quisit": 1, "ac": 1, "automat": 1, "biguat": 1, "lexic": 1, "disam": 1, "sens": 1, "studi": 1, "diathesi": 1, "tion": 1, "recent": 1}, "vector_2": [1, 0.05723406681108197, 4, 1, 0, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["Intersective Levin sets partition these classes according to more co herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb."], "label": "Prov", "citing": "W00-0202", "vector": [8, 0, 1, 0.03108349360801046], "context": ["", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "properti": 1, "parent": 1, "creat": 1, "share": 1, "cap": 1, "associatedwith": 1, "idea": 1, "repres": 1, "motion": 1, "action": 1, "exploit": 1, "verb": 5, "contact": 1, "allow": 1, "lattic": 1, "hierarchi": 1, "close": 1, "ture": 1, "similar": 1, "common": 1}, "vector_2": [2, 0.3324968632371393, 1, 1, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We present a refinement of Levin classes, intersec tive sets, which are a more fine-grained clas sification and have more coherent sets of syn tactic frames and associated semantic compo nents."], "label": "Prov", "citing": "W03-0910", "vector": [7, 0, 1, 0.19518001458970666], "context": ["", "This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class", ""], "marker": "Dang et al 1998", "vector_1": {"use": 1, "reorgan": 1, "levin": 1, "semant": 1, "within": 1, "account": 1, "al": 1, "differ": 1, "facilit": 1, "refin": 1, "dang": 1, "et": 1, "inter": 1, "class": 3, "sectiv": 1, "syntact": 1}, "vector_2": [5, 0.17697124217314686, 0, 0, 3, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["Investigating regular sense extensions based on intersective Levin classes"], "label": "Prov", "citing": "W06-2611", "vector": [3, 0, 2, 0.294174202707276], "context": ["", "A very good candidate seems to be the Intersective Levin classes (Dang et al., 1998) that can be found as well in other predicate resources like PropBank and VerbNet (Kipper et al., 2000).", ""], "marker": "Dang et al., 1998", "vector_1": {"propbank": 1, "good": 1, "resourc": 1, "like": 1, "levin": 1, "well": 1, "predic": 1, "candid": 1, "intersect": 1, "found": 1, "seem": 1, "class": 1, "verbnet": 1}, "vector_2": [8, 0.10161655554157756, 2, 2, 3, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses."], "label": "Prov", "citing": "W06-2611", "vector": [7, 0, 2, 0.2253744679276044], "context": ["", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "inter": 1, "version": 1, "insid": 1, "constraint": 1, "levin": 2, "classif": 1, "construct": 1, "base": 1, "role": 1, "refin": 1, "ensur": 1, "call": 1, "con": 1, "lexi": 1, "class": 1, "sectiv": 1, "verbnet": 1}, "vector_2": [8, 0.3574663479682979, 1, 2, 3, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["Current approaches to English classifica tion, Levin classes and WordNet, have limita tions in their applicability that impede their utility as general classification schemes."], "label": "Prov", "citing": "W99-0503", "vector": [3, 0, 0, 0.0], "context": ["", "In explormg these quest1ons, we focus on verb clas Sificatwn for several reasons Verbs are very Impor tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998)", ""], "marker": "Dang et al , 1998", "vector_1": {"among": 1, "major": 1, "mtensv": 1, "knowledg": 2, "number": 1, "crucal": 1, "ap": 1, "et": 1, "yet": 1, "languag": 2, "990": 1, "use": 1, "hnowledg": 1, "cla": 1, "role": 1, "sever": 1, "explormg": 1, "machm": 1, "sificatwn": 1, "play": 1, "sourc": 1, "resourc": 1, "dfficult": 1, "relat0nshp": 1, "gener": 1, "pear": 1, "lexic": 1, "reason": 1, "verb": 5, "translat": 1, "engneermg": 1, "mjler": 1, "impor": 1, "class": 1, "clcbsrficatwn": 1, "task": 2, "classficatwn": 1, "document": 1, "acqusrt0n": 1, "manual": 1, "focu": 1, "queston": 1, "orgamzatwn": 1, "s": 2, "larg": 1, "mani": 1, "tant": 1, "support": 1}, "vector_2": [1, 0.07620917931012638, 4, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["We have described a robust, knowledge-poor ap proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Evaluation shows a success rate of 89.7% for the genre of tech nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.", "We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate)."], "label": "Prov", "citing": "A00-1020", "vector": [2, 0, 0, 0.08283153665625823], "context": ["", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", ""], "marker": "Mitkov, 1998", "vector_1": {"curaci": 1, "ac": 1, "show": 1, "nevertheless": 1, "cf": 1, "knowledgepoor": 1, "perform": 1, "result": 1, "amaz": 1, "method": 1, "recent": 1}, "vector_2": [2, 0.13047924193878246, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.", "Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante cedent."], "label": "Prov", "citing": "D09-1101", "vector": [5, 0, 2, 0.08058229640253803], "context": ["", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", ""], "marker": "1998", "vector_1": {"set": 1, "pronoun": 1, "appli": 1, "eg": 1, "rank": 1, "one": 1, "incompat": 1, "use": 1, "heuristicbas": 1, "salienc": 1, "factor": 1, "candid": 1, "grammat": 1, "anteced": 1, "mitkov": 1, "like": 1, "constraint": 1, "resolv": 1, "filter": 1, "remain": 1, "mani": 1, "first": 1}, "vector_2": [11, 0.19919637070641608, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["3.1 Evaluation A.", "Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).", "The resolution of anaphors was carried out with a suc cess rate of 95.8%.", "The evaluation indicated 83.6% success rate."], "label": "Prov", "citing": "E99-1031", "vector": [9, 0, 1, 0.25332019855244947], "context": ["", "A huge variety of techniques are described in the literature, many of them achieving high suc cess rates on their own evaluation texts (cf. Hobbs 1986; Strube 1998; Mitkov 1998).", ""], "marker": "Mitkov 1998", "vector_1": {"huge": 1, "suc": 1, "describ": 1, "rate": 1, "text": 1, "strube": 1, "literatur": 1, "cf": 1, "high": 1, "achiev": 1, "hobb": 1, "evalu": 1, "varieti": 1, "mani": 1, "cess": 1, "mitkov": 1, "techniqu": 1}, "vector_2": [1, 0.06462135922330096, 0, 0, 0, 1]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante cedent.", "The antecedent indicators have been identi fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms."], "label": "Prov", "citing": "E99-1031", "vector": [5, 0, 0, 0.13055824196677338], "context": ["", "We implemented meta-modules to in terface to the genetic algorithm driver and to combine different salience factors into an over all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", ""], "marker": "Mitkov, 1998", "vector_1": {"differ": 1, "algorithm": 1, "metamodul": 1, "genet": 1, "factor": 1, "driver": 1, "score": 1, "combin": 1, "terfac": 1, "salienc": 1, "implement": 1, "similar": 1}, "vector_2": [1, 0.8894757281553398, 2, 1, 0, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["3.1 Evaluation A.", "Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).", "The evaluation carried out was manual to ensure that no added error was gen erated (e.g.", "due to possible wrong sentence/clause detection or POS tagging).", "Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3).", "The evaluation indicated 83.6% success rate."], "label": "Prov", "citing": "J01-4003", "vector": [8, 0, 1, 0.08838834764831845], "context": ["", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", ""], "marker": "Mitkov 1998", "vector_1": {"corpu": 1, "hahn": 1, "mitkov": 1, "evalu": 1, "carri": 1, "eg": 1, "handtest": 1, "past": 1, "small": 1, "drawback": 1, "done": 1, "strube": 2, "walker": 1, "mani": 1, "corpora": 1}, "vector_2": [3, 0.01938847828287942, 0, 0, 6, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\")."], "label": "Prov", "citing": "J01-4005", "vector": [2, 0, 0, 0.0], "context": ["", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", ""], "marker": "1998", "vector_1": {"analysi": 1, "employ": 1, "semant": 1, "shallow": 1, "mitkov": 1, "constraint": 1, "resolut": 1, "prefer": 1, "see": 1, "exampl": 1, "current": 1, "consequ": 1, "reli": 1, "inform": 1, "heurist": 1, "morphosyntact": 1, "mainli": 1, "method": 1, "anaphora": 1}, "vector_2": [3, 0.7085780338199378, 1, 0, 3, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric."], "label": "Prov", "citing": "J01-4006", "vector": [9, 2, 6, 0.25717224993681986], "context": ["", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples:  \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", ""], "marker": "Mitkov 1998", "vector_1": {"ck": 1, "often": 1, "ron": 1, "bien": 1, "exophor": 1, "incomplet": 1, "82": 1, "oclo": 1, "raini": 1, "even": 1, "use": 1, "pronoun": 1, "author": 1, "detail": 1, "ng": 1, "tim": 1, "envi": 1, "exampl": 1, "nonanaphor": 1, "men": 1, "report": 1, "mitkov": 1, "e": 1, "exa": 1, "provid": 2, "descript": 1, "ath": 1, "hot": 1, "r": 1, "exclus": 1, "mple": 1, "confus": 1, "page": 1}, "vector_2": [3, 0.23485197659989362, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979)."], "label": "Prov", "citing": "J02-1001", "vector": [10, 3, 1, 0.3247595264191645], "context": ["", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", ""], "marker": "1998", "vector_1": {"antecedentsof": 1, "eg": 1, "prefer": 1, "socal": 1, "wada": 1, "filter": 1, "develop": 1, "divid": 1, "leass": 1, "late": 1, "rich": 1, "factor": 1, "test": 1, "anaphor": 2, "approach": 1, "common": 1, "brown": 1, "viabil": 1, "resolut": 1, "carbonel": 1, "wisdom": 1, "extens": 1, "sinc": 1, "lappin": 1, "mitkov": 1, "practic": 1, "luperfoy": 1, "asher": 1, "s": 1, "integr": 1, "determin": 1}, "vector_2": [4, 0.01786528661788908, 0, 0, 2, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["We have described a robust, knowledge-poor ap proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Evaluation shows a success rate of 89.7% for the genre of tech nical manuals and at least in this genre, the approach appears to be more successful than other similar methods."], "label": "Prov", "citing": "J05-3004", "vector": [5, 1, 1, 0.037139067635410375], "context": ["", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", ""], "marker": "1998", "vector_1": {"mueller": 1, "work": 1, "mitkov": 1, "often": 1, "pronoun": 1, "boguraev": 1, "exampl": 1, "resolut": 1, "focus": 1, "accuraci": 2, "person": 1, "achiev": 1, "good": 1, "rapp": 1, "kennedi": 1, "report": 1, "respect": 1, "strube": 1, "pronomin": 1, "fmeasur": 1, "anaphora": 2}, "vector_2": [7, 0.016565817898864556, 3, 0, 4, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.", "One of the disadvantages of developing a knowledge based system, however, is that it is a very labour intensive and time-consuming task.", "We have described a robust, knowledge-poor ap proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Evaluation shows a success rate of 89.7% for the genre of tech nical manuals and at least in this genre, the approach appears to be more successful than other similar methods."], "label": "Prov", "citing": "N01-1008", "vector": [10, 0, 1, 0.12141073279465801], "context": ["", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", ""], "marker": "Mitkov 1998", "vector_1": {"kameyama": 1, "exten": 1, "consum": 1, "cf": 1, "acquisit": 1, "sive": 1, "empir": 1, "accuraci": 1, "resultsshow": 1, "per": 1, "corefer": 1, "certain": 1, "amaz": 1, "method": 1, "discours": 1, "knowledg": 1, "form": 2, "ofcorefer": 1, "kennedi": 1, "neverthless": 1, "cult": 1, "necessaryfor": 1, "knowledgepoor": 1, "recent": 1, "linguist": 1, "mitkov": 1, "boguraev": 1, "errorpron": 1, "resolv": 1, "time": 1, "diffi": 1}, "vector_2": [3, 0.13446346280447663, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["We have described a robust, knowledge-poor ap proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger."], "label": "Prov", "citing": "N01-1008", "vector": [3, 0, 0, 0.13608276348795434], "context": ["", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", ""], "marker": "Mitkov 1998", "vector_1": {"mitkov": 1, "com": 1, "perform": 1, "cock": 1, "gener": 1, "resolut": 1, "ponent": 1, "tail": 1, "rule": 1, "corefer": 1, "filter": 1, "method": 1, "unlik": 1, "autotagcoftef": 1, "data": 1, "massivetrain": 1, "baldwin": 1, "knowledgepoor": 1}, "vector_2": [3, 0.9638742593811718, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["With a view to avoiding complex syntactic, seman tic and discourse analysis (which is vital for real world applications), we developed a robust, knowl edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "Evaluation shows a success rate of 89.7% for the genre of tech nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.", "We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate)."], "label": "Prov", "citing": "N04-1004", "vector": [8, 0, 3, 0.22880215766121476], "context": ["", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"discours": 1, "analysi": 1, "semant": 1, "mitkov": 1, "syntact": 1, "show": 1, "appli": 1, "genr": 1, "complex": 1, "saliencebas": 1, "without": 1, "approach": 1, "across": 1}, "vector_2": [6, 0.16447685438016, 1, 1, 1, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["We have described a robust, knowledge-poor ap proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger."], "label": "Prov", "citing": "P01-1006", "vector": [4, 0, 1, 0.2680281337094487], "context": ["", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevs parser- free version of Lappin and Leass RAP (Kennedy and Boguraev, 1996), Baldwins pronoun resolution method (Baldwin, 1997) and Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998b).", ""], "marker": "Mitkov, 1998b", "vector_1": {"evalu": 1, "parser": 1, "rap": 1, "baldwin": 1, "select": 1, "pronoun": 2, "compar": 1, "leass": 1, "three": 1, "version": 1, "approach": 2, "method": 1, "resolut": 2, "free": 1, "extens": 1, "knowledgepoor": 1, "lappin": 1, "mitkov": 1, "boguraev": 1, "literatur": 1, "cite": 1, "kennedi": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["This paper pres ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent."], "label": "Prov", "citing": "P01-1006", "vector": [13, 0, 1, 0.3046358979224712], "context": ["", "Mitkovs approach Mitkovs approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", ""], "marker": "Mitkov, 1998b", "vector_1": {"boost": 1, "set": 1, "mitkov": 2, "text": 1, "resolut": 1, "indic": 1, "technic": 1, "candid": 1, "base": 1, "anteced": 1, "appli": 1, "imped": 1, "robust": 1, "approach": 2, "method": 1, "anaphora": 1}, "vector_2": [3, 0.5, 1, 2, 2, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["Robust pronoun resolution with limited knowledge"], "label": "Prov", "citing": "P01-1006", "vector": [1, 0, 0, 0.06324555320336758], "context": ["", "pronouns, referential distance, average number of candidates for antecedent per pronoun and types of anaphors.7 As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996), Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison.", ""], "marker": "1998b", "vector_1": {"origin": 1, "distanc": 1, "pronoun": 2, "number": 1, "per": 1, "tabl": 1, "baldwin": 1, "ground": 1, "result": 2, "differ": 3, "referenti": 1, "publish": 1, "resort": 1, "test": 1, "anaphor": 1, "type": 1, "match": 1, "tool": 1, "preprocess": 1, "candid": 1, "report": 1, "data": 1, "averag": 1, "comparison": 1, "anteced": 1, "mitkov": 1, "algorithm": 1, "boguraev": 1, "provid": 1, "thu": 1, "manual": 1, "employ": 1, "degre": 1, "common": 1, "kennedi": 1, "intervent": 1, "reliabl": 1, "expect": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["We have described a robust, knowledge-poor ap proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger."], "label": "Prov", "citing": "P01-1006", "vector": [2, 0, 0, 0.11547005383792516], "context": ["", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common knowledge-poor philosophy: Kennedy and Boguraevs (1996) parser-free algorithm, Baldwins (1997) CogNiac and Mitkovs (1998b) knowledge-poor approach.", ""], "marker": "1998b", "vector_1": {"cogniac": 1, "mitkov": 1, "algorithm": 1, "evalu": 1, "boguraev": 1, "baldwin": 1, "configur": 1, "share": 1, "three": 1, "knowledgepoor": 2, "paper": 1, "environ": 1, "common": 1, "incorpor": 1, "particular": 1, "new": 1, "kennedi": 1, "philosophi": 1, "approach": 2, "discuss": 1, "parserfre": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The antecedent indicators have been identi fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms."], "label": "Prov", "citing": "P04-1017", "vector": [7, 0, 0, 0.09274777915203367], "context": ["", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "learningbas": 1, "chain": 1, "frequenc": 1, "resolut": 1, "variant": 1, "system": 1, "tfidf": 1, "length": 1, "candid": 1, "salienc": 1, "factor": 1, "refer": 1, "occurr": 1, "coreferenti": 1}, "vector_2": [6, 0.9360183533010451, 4, 1, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["With a view to avoiding complex syntactic, seman tic and discourse analysis (which is vital for real world applications), we developed a robust, knowl edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\")."], "label": "Prov", "citing": "P07-1068", "vector": [5, 0, 0, 0.036037498507822355], "context": ["", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", ""], "marker": "1998", "vector_1": {"knowledgelean": 1, "sourc": 1, "mitkov": 1, "process": 1, "eg": 1, "knowledg": 1, "resolv": 1, "corefer": 1, "employ": 1, "cue": 1, "morphosyntact": 1, "resolut": 1, "approach": 1, "tetreault": 1}, "vector_2": [9, 0.027767724954762298, 2, 0, 5, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Evaluation shows a success rate of 89.7% for the genre of tech nical manuals and at least in this genre, the approach appears to be more successful than other similar methods."], "label": "Prov", "citing": "P10-2049", "vector": [2, 0, 0, 0.0], "context": ["", "The algorithm with the next-to-highest results in (Char- niak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit.", ""], "marker": "Mitkov, 1998", "vector_1": {"nexttohighest": 1, "mar": 1, "algorithm": 1, "toolkit": 1, "guitar": 1, "result": 1}, "vector_2": [12, 0.44919241402725907, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["With a view to avoiding complex syntactic, seman tic and discourse analysis (which is vital for real world applications), we developed a robust, knowl edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\")."], "label": "Prov", "citing": "S10-1019", "vector": [7, 0, 0, 0.030151134457776358], "context": ["", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", ""], "marker": "Mitkov, 1998", "vector_1": {"concentr": 1, "machin": 1, "major": 1, "made": 1, "last": 1, "rulebas": 1, "eg": 1, "resolut": 1, "embrac": 1, "system": 1, "corefer": 1, "field": 1, "cf": 2, "learn": 1, "progress": 1, "decad": 1, "method": 1}, "vector_2": [12, 0.050087806367870503, 3, 1, 1, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["We have described a robust, knowledge-poor ap proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.", "Evaluation shows a success rate of 89.7% for the genre of tech nical manuals and at least in this genre, the approach appears to be more successful than other similar methods."], "label": "Prov", "citing": "W01-0704", "vector": [6, 1, 1, 0.0450377349111045], "context": ["", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998)", ""], "marker": "Mitkov, 1998", "vector_1": {"high": 1, "use": 1, "sourc": 1, "success": 1, "cor": 1, "knowledg": 1, "rate": 1, "lexic": 1, "morpholog": 1, "syntacticinform": 1, "detect": 1, "limit": 1, "anteced": 1, "english": 1, "report": 1, "rect": 1, "propos": 1}, "vector_2": [3, 0.7005733005733006, 1, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["We have described a robust, knowledge-poor ap proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger."], "label": "Prov", "citing": "W01-0717", "vector": [3, 0, 1, 0.08247860988423225], "context": ["", "There are many recent approaches to this prob lem, e. g. syntax-based approaches (Lappin and Leass, 1994), cooccurrence-based approaches(Dagan and Itai, 1990), machine-learning approaches (Connolly et al., 1994; Aone and Ben nett, 1996; Soon et al., 1999), uncertainty reasoning approaches (Mitkov, 1995; Mitkov, 1997), and robust knowledge-poor approaches (Kennedy and Boguarev, 1996; Baldwin, 1997; Mitkov, 1998b; Mitkov, 1999).", ""], "marker": "Mitkov, 1998b", "vector_1": {"syntaxbas": 1, "cooccurrencebas": 1, "e": 1, "g": 1, "machinelearn": 1, "lem": 1, "knowledgepoor": 1, "reason": 1, "robust": 1, "mani": 1, "uncertainti": 1, "approach": 6, "prob": 1, "recent": 1}, "vector_2": [3, 0.869720700737857, 11, 6, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["We have described a robust, knowledge-poor ap proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger."], "label": "Prov", "citing": "W04-0707", "vector": [3, 0, 1, 0.12909944487358055], "context": ["", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovs algorithm for pronoun resolution (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"poesio": 1, "mitkov": 1, "algorithm": 2, "g": 1, "descript": 1, "resolut": 1, "definit": 1, "resolv": 1, "vieira": 1, "r": 1, "u": 1, "includ": 1, "generalpurpos": 1, "anaphor": 1, "implement": 1, "pronoun": 1, "ta": 1}, "vector_2": [6, 0.7486840068122, 2, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Robust pronoun resolution with limited knowledge"], "label": "Prov", "citing": "W04-0711", "vector": [0, 0, 0, 0.0], "context": ["", "(PMID:9701290) Table 1: A protein domain-referring phrase example ments and lexical features, in addressing problems in the biomedical domain (cf. Mitkov et al (1998)).", ""], "marker": "1998", "vector_1": {"domain": 1, "featur": 1, "mitkov": 1, "ment": 1, "address": 1, "al": 1, "problem": 1, "lexic": 1, "cf": 1, "et": 1, "domainref": 1, "exampl": 1, "biomed": 1, "tabl": 1, "phrase": 1, "pmid": 1, "protein": 1}, "vector_2": [6, 0.12350511761537081, 1, 0, 0, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["We have described a robust, knowledge-poor ap proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger."], "label": "Prov", "citing": "W04-0714", "vector": [4, 0, 1, 0.12909944487358055], "context": ["", "Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as part- of-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrndez et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003).", ""], "marker": "Mitkov, 1998", "vector_1": {"nlp": 1, "parser": 1, "shallow": 1, "po": 1, "inexpens": 1, "reliabl": 2, "resolut": 1, "tool": 1, "procedur": 1, "cheaper": 1, "reli": 1, "part": 1, "tagger": 1, "fast": 1, "ofspeech": 1, "method": 1, "anaphora": 1}, "vector_2": [6, 0.12737480575467441, 5, 1, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["These scores have been determined experimentally on an empirical basis and are constantly being up dated.", "Top symptoms like \"lexical reiteration\" as sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\".", "We should point out that the antecedent indicators are preferences and not absolute factors."], "label": "Prov", "citing": "W04-2310", "vector": [3, 0, 0, 0.0], "context": ["", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", ""], "marker": "Mitkov, 1998", "vector_1": {"differ": 1, "depend": 1, "weight": 1, "relationship": 1, "system": 1, "programm": 1, "anaphoranteced": 1, "assign": 1}, "vector_2": [6, 0.8749348958333333, 2, 2, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.", "It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary."], "label": "Prov", "citing": "W04-2310", "vector": [9, 0, 0, 0.03580574370197165], "context": ["", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"corpu": 1, "use": 1, "often": 1, "perform": 1, "resolut": 1, "well": 1, "limit": 1, "tune": 1, "heurist": 1, "domainspecif": 1, "fine": 1, "anaphora": 1}, "vector_2": [6, 0.12135416666666667, 1, 2, 1, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["With a view to avoiding complex syntactic, seman tic and discourse analysis (which is vital for real world applications), we developed a robust, knowl edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\")."], "label": "Prov", "citing": "W06-2302", "vector": [13, 0, 2, 0.12060453783110543], "context": ["", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic it occurrences, and assigns animacy.", ""], "marker": "1998", "vector_1": {"poor": 1, "tri": 1, "use": 1, "knowledg": 1, "mitkov": 1, "pleonast": 1, "algorithm": 1, "occurr": 1, "make": 1, "resolut": 1, "chunk": 1, "assign": 1, "r": 1, "animaci": 1, "present": 1, "np": 1, "individu": 1, "approach": 1, "po": 1, "anaphora": 1}, "vector_2": [8, 0.22151446213844653, 0, 0, 1, 1]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["With a view to avoiding complex syntactic, seman tic and discourse analysis (which is vital for real world applications), we developed a robust, knowl edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.", "It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\")."], "label": "Prov", "citing": "W09-2411", "vector": [3, 0, 0, 0.0], "context": ["", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", ""], "marker": "Mitkov, 1998", "vector_1": {"tradit": 1, "machin": 1, "corpora": 1, "techniqu": 1, "allow": 1, "could": 1, "knowledg": 1, "annot": 1, "rule": 1, "autom": 1, "acquisit": 1, "base": 1, "limit": 1, "overcom": 1, "learn": 1, "approach": 1}, "vector_2": [11, 0.07112765748995788, 1, 1, 0, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["This paper pres ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent."], "label": "Prov", "citing": "W99-0104", "vector": [6, 0, 0, 0.19802950859533489], "context": ["", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "set": 1, "anteced": 1, "pronomin": 1, "eitper": 1, "resolut": 1, "knowledgepoor": 1, "candid": 1, "assign": 1, "score": 1, "combin": 1, "promot": 1, "heurist": 1, "gener": 1, "order": 1, "approach": 1, "method": 1}, "vector_2": [1, 0.18110604466269145, 1, 1, 0, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["The antecedent indicators have been identi fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms."], "label": "Prov", "citing": "W99-0104", "vector": [7, 0, 1, 0.10998533626601499], "context": ["", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "resalv": 1, "set": 1, "cal": 1, "algorithm": 2, "cogniac": 1, "eg": 1, "six": 1, "wherea": 1, "l": 1, "rule": 1, "corefer": 1, "base": 1, "limit": 1, "prefer": 1, "heurist": 1, "immedi": 1, "definit": 1, "reiter": 1, "present": 1, "refer": 1}, "vector_2": [1, 0.1865427335437858, 2, 1, 0, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers."], "label": "Prov", "citing": "D09-1138", "vector": [3, 0, 1, 0.1781741612749496], "context": ["", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"supervis": 1, "classif": 1, "investig": 1, "automat": 1, "extens": 1, "verb": 1, "method": 1}, "vector_2": [6, 0.10682836397342808, 6, 2, 31, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003)."], "label": "Prov", "citing": "D09-1138", "vector": [4, 1, 1, 0.31980107453341566], "context": ["", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"supervis": 1, "classif": 1, "work": 1, "inherit": 1, "verb": 1, "approach": 1, "spirit": 1, "present": 1}, "vector_2": [6, 0.2115453936865956, 6, 2, 31, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size."], "label": "Prov", "citing": "N13-1118", "vector": [2, 0, 0, 0.0], "context": ["", "Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"hierarch": 1, "wide": 1, "use": 1, "word": 1, "agg": 1, "cluster": 2, "method": 1}, "vector_2": [10, 0.24975210708973725, 4, 1, 0, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery."], "label": "Prov", "citing": "W06-2910", "vector": [5, 0, 0, 0.2864459496157732], "context": ["", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"altern": 1, "corpu": 1, "classif": 2, "eg": 1, "appli": 1, "induc": 1, "manual": 1, "automat": 1, "class": 1, "cluster": 1, "verb": 1, "data": 1, "method": 1, "resourceintens": 1}, "vector_2": [3, 0.04776887871853547, 6, 3, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Levins classes form a hierarchy of verb groupings with shared meaning and syntax.", "Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research."], "label": "Prov", "citing": "W06-2910", "vector": [7, 0, 1, 0.20385887657505022], "context": ["", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"classif": 1, "featur": 1, "clear": 1, "syntaxsemant": 1, "largerscal": 1, "verb": 1, "interfac": 1, "salient": 1, "model": 1, "similar": 1, "class": 1}, "vector_2": [3, 0.07576516018306637, 3, 3, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size."], "label": "Prov", "citing": "W06-2910", "vector": [6, 0, 2, 0.23145502494313788], "context": ["", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003)", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"measur": 1, "evalu": 1, "appli": 1, "cf": 1, "accuraci": 1, "cluster": 3, "calcul": 1, "result": 1, "similar": 1}, "vector_2": [3, 0.6832451372997712, 2, 3, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003)."], "label": "Prov", "citing": "E09-1072", "vector": [6, 1, 1, 0.21938172723813917], "context": ["", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"corpu": 1, "classif": 1, "set": 1, "noun": 1, "distribut": 1, "work": 1, "classifi": 1, "lemma": 1, "verb": 1, "common": 1, "base": 1, "larger": 1, "morphosyntact": 1, "follow": 1, "line": 1, "consider": 1, "strategi": 1}, "vector_2": [6, 0.3843511927063819, 2, 1, 1, 0]}, {"function": "Neut", "cited": "C90-2039", "provenance": ["itowever, the problem with his method is that a unitication result graph consists only of newly created structures.", "This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.", "Copying sharable parts is called redundant copying."], "label": "Prov", "citing": "P99-1061", "vector": [2, 0, 1, 0.11895773785772164], "context": ["", "While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.", ""], "marker": "Kogure, 1990", "vector_1": {"kogur": 1, "suffer": 1, "simpl": 1, "copi": 1, "unif": 1, "tomabechi": 1, "destruct": 1, "call": 1, "improv": 1, "redund": 1, "still": 1, "approach": 1}, "vector_2": [9, 0.24058643602995464, 1, 1, 0, 0]}, {"function": "Neut", "cited": "C90-2039", "provenance": ["Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them."], "label": "Prov", "citing": "P91-1041", "vector": [4, 0, 0, 0.053300179088902604], "context": ["", "2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).", ""], "marker": "Kogure, 1990", "vector_1": {"analysi": 1, "develop": 1, "sometim": 1, "japanes": 1, "spoken": 1, "devot": 1, "time": 1, "percent": 1, "hpsgbase": 1, "elaps": 1, "system": 1, "atr": 1, "graph": 1, "unif": 1, "in": 1, "largescal": 1}, "vector_2": [1, 0.05819360293081584, 1, 2, 0, 0]}, {"function": "Neut", "cited": "C90-2039", "provenance": ["For example, a spoken Present.", "Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification."], "label": "Prov", "citing": "P91-1041", "vector": [2, 0, 0, 0.0], "context": ["", "That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).", ""], "marker": "Kogure, 1990", "vector_1": {"reduc": 1, "unless": 1, "scuctureshar": 1, "copi": 1, "sharedforest": 1, "excess": 1, "new": 1, "scheme": 1, "introduc": 1, "unchang": 1}, "vector_2": [1, 0.9484007327039594, 1, 2, 0, 0]}, {"function": "Neut", "cited": "C90-2039", "provenance": ["Copying sharable parts is called redundant copying.", "A better method would nfinimize the copying of sharable varts."], "label": "Prov", "citing": "P91-1042", "vector": [8, 0, 5, 0.5819143739626463], "context": ["", "A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].", ""], "marker": "Kogure 90", "vector_1": {"kogur": 1, "elimin": 1, "would": 1, "avoid": 1, "copi": 1, "better": 1, "call": 1, "redund": 1, "method": 1}, "vector_2": [4, 0.2552090887116268, 0, 0, 0, 0]}, {"function": "Pos", "cited": "C90-2039", "provenance": ["Copying sharable parts is called redundant copying.", "5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.", "With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig."], "label": "Prov", "citing": "P91-1042", "vector": [15, 0, 7, 0.40540540540540543], "context": ["", "As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding \"redundant copying\" is to do copying lazily. Copying of nodes will be delayed until a destructive change is about to take place. Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying. Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.", ""], "marker": "Kogure 90", "vector_1": {"featur": 1, "idea": 1, "procedur": 1, "arc": 1, "exist": 1, "connect": 1, "need": 1, "revis": 1, "use": 1, "depend": 1, "lazili": 1, "godden": 1, "avoid": 3, "delay": 1, "take": 1, "similarli": 1, "immedi": 1, "approach": 1, "node": 3, "copynod": 1, "sequenc": 1, "copi": 7, "notic": 1, "key": 1, "path": 1, "redund": 2, "case": 1, "kogur": 3, "destruct": 1, "inform": 1, "maintain": 1, "place": 1, "chang": 1, "order": 1}, "vector_2": [4, 0.8839255499153976, 0, 0, 0, 0]}, {"function": "Neut", "cited": "C90-2039", "provenance": ["A better method would nfinimize the copying of sharable varts."], "label": "Prov", "citing": "W97-1503", "vector": [3, 0, 0, 0.0890870806374748], "context": ["", "PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990);", ""], "marker": "Kogure, 1990", "vector_1": {"control": 1, "among": 1, "differ": 1, "algorithm": 1, "minim": 1, "nondeterminist": 1, "copi": 1, "unif": 1, "also": 1, "amount": 1, "design": 1, "schemata": 1, "choos": 1, "need": 1, "pars": 1, "care": 1, "pm": 1}, "vector_2": [7, 0.6524621212121212, 2, 1, 0, 0]}, {"function": "Neut", "cited": "C94-2154", "provenance": ["By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness"], "label": "Prov", "citing": "E95-1024", "vector": [6, 0, 2, 0.19069251784911848], "context": ["", "This test- grammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994) in the Troll system (Gerdemann and King, 1994).", ""], "marker": "Gerdemann and  King, 1994", "vector_1": {"analysi": 1, "topic": 1, "grammar": 1, "german": 1, "system": 1, "vp": 1, "base": 1, "test": 1, "partial": 1, "implement": 1, "troll": 1}, "vector_2": [1, 0.7581501137225171, 2, 1, 9, 0]}, {"function": "Neut", "cited": "E03-1020", "provenance": ["Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.", "To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000)."], "label": "Prov", "citing": "S13-2038", "vector": [11, 0, 2, 0.22473328748774737], "context": ["", "Dorow and Widdows (2003) use the BNC to build a cooccurrencegraph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph.", ""], "marker": "2003", "vector_1": {"use": 1, "perform": 1, "noun": 1, "cluster": 1, "dorow": 1, "frequenc": 1, "cooccurrencegraph": 1, "bnc": 1, "base": 1, "build": 1, "threshold": 1, "graph": 1, "widdow": 1, "cooccurr": 1, "markov": 1}, "vector_2": [10, 0.17544923800136478, 1, 0, 0, 0]}, {"function": "Neut", "cited": "E03-1020", "provenance": ["We then determined the WordNet synsets which most adequately characterized the sense clusters."], "label": "Prov", "citing": "W08-2207", "vector": [0, 0, 0, 0.0], "context": ["", "highly accurateTopicSignaturesfor all monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).", ""], "marker": "DorowandWiddows,2003", "vector_1": {"monosem": 1, "wordnetforinstanceusinginfomap": 1, "wordsin": 1, "accuratetopicsignaturesfor": 1, "highli": 1}, "vector_2": [5, 0.3039868885167635, 1, 1, 0, 0]}, {"function": "Neut", "cited": "E03-1020", "provenance": ["This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies."], "label": "Prov", "citing": "P04-1080", "vector": [1, 0, 0, 0.11547005383792514], "context": ["", "there are other related efforts on word sense discrimination (Dorow and Widdows, 2003; Fukumoto and Suzuki, 1999; Pedersen and Bruce, 1997).", ""], "marker": "Dorow and Widdows, 2003", "vector_1": {"word": 1, "relat": 1, "effort": 1, "discrimin": 1, "sens": 1}, "vector_2": [1, 0.894656910120116, 3, 2, 2, 0]}, {"function": "Neut", "cited": "E03-1020", "provenance": ["Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1."], "label": "Prov", "citing": "P04-1080", "vector": [23, 0, 20, 0.39670776294437177], "context": ["", "The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word. Their algorithm required a threshold as input, which controlled the number of senses.", ""], "marker": "Dorow and Widdows, 2003", "vector_1": {"control": 1, "edg": 1, "number": 2, "repres": 1, "cluster": 1, "threshold": 1, "denot": 1, "use": 1, "graph": 2, "two": 1, "sens": 2, "local": 1, "node": 2, "around": 1, "relationship": 1, "time": 1, "given": 1, "cooccur": 1, "input": 1, "noun": 2, "requir": 1, "word": 4, "target": 3, "algorithm": 2, "iter": 1, "neighbor": 1, "learn": 1, "similar": 1}, "vector_2": [1, 0.9493649040452851, 1, 2, 2, 0]}, {"function": "Neut", "cited": "E03-1020", "provenance": ["To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000)."], "label": "Prov", "citing": "N07-3010", "vector": [4, 0, 0, 0.0], "context": ["", "The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).", ""], "marker": "2003", "vector_1": {"via": 1, "word": 2, "edg": 2, "dorow": 1, "graph": 1, "methodolog": 1, "adopt": 1, "vertex": 1, "focu": 2, "obtain": 1, "connect": 1, "widdow": 1, "neighborhood": 1, "vertic": 1}, "vector_2": [4, 0.7858675780939512, 1, 0, 3, 0]}, {"function": "Neut", "cited": "E03-1020", "provenance": ["The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the \"best\" cluster, it suffices to build a relatively small graph in 1."], "label": "Prov", "citing": "W11-2214", "vector": [5, 0, 0, 0.0], "context": ["", "This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).", ""], "marker": "Dorow and Widdows, 2003", "vector_1": {"medicin": 1, "predefin": 1, "inventori": 2, "process": 1, "unsupervis": 1, "usag": 1, "number": 1, "discoveri": 1, "reflect": 1, "may": 1, "present": 1, "sens": 4, "law": 1, "produc": 1, "corpusdriven": 1, "addit": 1}, "vector_2": [8, 0.040237046041635006, 1, 2, 0, 0]}, {"function": "Pos", "cited": "E03-1020", "provenance": ["This gives rise to an automatic, unsupervised word sense disambiguation algorithm which is trained on the data to be disambiguated."], "label": "Prov", "citing": "W11-2214", "vector": [2, 0, 0, 0.07312724241271307], "context": ["", "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.", ""], "marker": "2003", "vector_1": {"use": 1, "featur": 1, "word": 2, "last": 1, "less": 1, "dorow": 1, "sentenc": 1, "lin": 1, "relat": 1, "pantel": 1, "length": 1, "context": 1, "path": 1, "widdow": 1, "follow": 1, "depend": 1}, "vector_2": [8, 0.3476371372131895, 2, 0, 1, 0]}, {"function": "Pos", "cited": "E03-1020", "provenance": ["To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).", "The algorithm is based on a graph model representing words and relationships between them.", "Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word."], "label": "Prov", "citing": "W06-3812", "vector": [14, 0, 1, 0.2475368857441686], "context": ["", "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL.", ""], "marker": "Dorow and Widdows, 2003", "vector_1": {"mcl": 1, "word": 2, "target": 1, "subgraph": 1, "dorow": 1, "similar": 1, "induc": 1, "construct": 2, "cluster": 1, "without": 1, "take": 1, "w": 3, "graph": 2, "widdow": 1, "neighborhood": 1, "approach": 1, "present": 1}, "vector_2": [3, 0.8466216746449031, 1, 1, 0, 0]}, {"function": "CoCo", "cited": "H05-1115", "provenance": ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method."], "label": "Prov", "citing": "P09-1083", "vector": [4, 0, 0, 0.13245323570650439], "context": ["", "Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).", ""], "marker": "Otterbacher et al., 2005", "vector_1": {"topicsensit": 1, "consid": 1, "random": 1, "question": 2, "lexrank": 1, "walk": 1, "topic": 2, "incorpor": 1, "markov": 1, "similar": 1, "model": 1, "introduc": 1, "first": 1}, "vector_2": [4, 0.27820978641040445, 1, 4, 1, 0]}, {"function": "Neut", "cited": "H05-1115", "provenance": ["In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula:"], "label": "Prov", "citing": "P09-1083", "vector": [0, 0, 0, 0.0], "context": ["", "Its weight twij is calculated by tf  idf (Otterbacher et al., 2005).", ""], "marker": "Otterbacher et al., 2005", "vector_1": {"tf": 1, "idf": 1, "calcul": 1, "twij": 1, "weight": 1}, "vector_2": [4, 0.4843627966993786, 1, 4, 1, 0]}, {"function": "Neut", "cited": "H05-1115", "provenance": ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method."], "label": "Prov", "citing": "P09-1083", "vector": [0, 0, 0, 0.0], "context": ["", "includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).", ""], "marker": "Otterbacher et al., 2005", "vector_1": {"qa": 1, "text": 1, "includingfactbas": 1, "summar": 1}, "vector_2": [4, 0.22306360148052565, 4, 4, 1, 0]}, {"function": "Neut", "cited": "H05-1115", "provenance": ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method."], "label": "Prov", "citing": "D08-1032", "vector": [4, 0, 0, 0.040824829046386304], "context": ["", "A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences. Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.", ""], "marker": "Otterbacher et al., 2005", "vector_1": {"intersent": 1, "set": 1, "documentclust": 1, "random": 1, "rank": 1, "walk": 1, "topic": 1, "aresent": 1, "graph": 1, "question": 1, "system": 1, "topicsensitivelexrank": 1, "node": 2, "accord": 1, "sentenc": 4, "relat": 1, "term": 1, "lexrank": 1, "link": 1, "repres": 1, "thenth": 1, "inducedbi": 1, "descript": 1, "defin": 1, "model": 1, "similar": 3, "propos": 1}, "vector_2": [3, 0.14474269612931928, 1, 2, 2, 0]}, {"function": "Neut", "cited": "H05-1115", "provenance": ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.", "The output of our system, a ranked list of sentences relevant to the users question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences.", "Alternatively, the sentences canbe returned to the user as a question-focused summary.", "To apply LexRank, a similarity graph is producedfor the sentences in an input document set.", "In thegraph, each node represents a sentence.", "There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.", "The degree of a given node isan indication of how much information the respective sentence has in common with other sentences."], "label": "Prov", "citing": "D08-1032", "vector": [8, 0, 4, 0.1979898987322333], "context": ["", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).", ""], "marker": "Otterbacher et al., 2005", "vector_1": {"topicsensit": 1, "appli": 1, "lexrank": 2, "version": 1, "context": 1, "ispropos": 1, "queryfocus": 1}, "vector_2": [3, 0.48192199317811063, 1, 2, 2, 0]}, {"function": "Pos", "cited": "H05-1115", "provenance": ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method."], "label": "Prov", "citing": "P10-2055", "vector": [5, 0, 0, 0.12038585308576923], "context": ["", "Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.", ""], "marker": "Otterbacher et al., 2005", "vector_1": {"topicsensit": 1, "semant": 1, "convent": 1, "evalu": 1, "captur": 1, "relat": 1, "abl": 1, "two": 1, "lexrank": 1, "one": 1, "binari": 1, "reli": 1, "base": 1, "exist": 1, "call": 1, "pagerank": 1, "titlesensit": 1, "approach": 2, "afterward": 1, "network": 1}, "vector_2": [5, 0.8456326228902303, 1, 1, 0, 0]}, {"function": "Neut", "cited": "H89-2014", "provenance": ["The work described here also makes use of a hidden Markov model."], "label": "Prov", "citing": "W93-0111", "vector": [4, 0, 1, 0.2357022603955158], "context": ["", "This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989", ""], "marker": "Kupiec, 1989", "vector_1": {"model": 1, "comput": 1, "approach": 2, "iter": 1, "markov": 1, "hidden": 1, "similar": 1, "spirit": 1}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "H89-2014", "provenance": ["A model containing all of the refinements described, was tested using a magazine article containing 146 sentences (3,822 words).", "A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary."], "label": "Prov", "citing": "A92-1018", "vector": [6, 0, 0, 0.1543033499620919], "context": ["", "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.", ""], "marker": "Kupiec, 1989a", "vector_1": {"use": 2, "depend": 1, "network": 1, "augment": 1, "rather": 1, "secondord": 1, "context": 1, "basic": 1, "model": 1, "uniformli": 1, "order": 1, "select": 1, "first": 1}, "vector_2": [3, 0.15127542783338715, 1, 20, 0, 0]}, {"function": "Neut", "cited": "H89-2014", "provenance": ["An alternative to uniformly increasing the order of the conditioning is to extend it selectively.", "Mixed higher- order context can be modeled by introducing explicit state sequences.", "In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency."], "label": "Prov", "citing": "A92-1018", "vector": [2, 0, 0, 0.0], "context": ["", "adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].", ""], "marker": "Kupiec, 1989a", "vector_1": {"ten": 1, "process": 1, "thousand": 2, "token": 1, "train": 1, "adequ": 1, "hundr": 1, "requir": 1}, "vector_2": [3, 0.4838230545689377, 1, 20, 0, 0]}, {"function": "Neut", "cited": "H89-2014", "provenance": ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text."], "label": "Prov", "citing": "J93-2006", "vector": [6, 0, 2, 0.22613350843332272], "context": ["", "We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)", ""], "marker": "Kupiec 1989", "vector_1": {"kupiec": 1, "word": 1, "jelinek": 1, "text": 1, "section": 1, "well": 1, "effect": 1, "known": 1, "part": 1, "speech": 1, "church": 1, "report": 1, "model": 1, "experi": 1, "deros": 1, "assign": 1}, "vector_2": [4, 0.07920047860835416, 0, 0, 5, 1]}, {"function": "Neut", "cited": "J00-3003", "provenance": ["The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986).", "We conducted preliminary experiments to assess how neural networks compare to decision trees for the type of data studied here.", "Table 9 Combined utterance classification accuracies (chance = 35%).", "Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) for individual and combined models for two subtasks, using uniform priors (chance = 50%)."], "label": "Prov", "citing": "W13-4047", "vector": [10, 0, 2, 0.12309149097933272], "context": ["", "(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types. They also explore the performance with decision trees and neural networks and report their highest accuracy at 65% on the Switchboard corpus.", ""], "marker": "Stolcke et al., 2000", "vector_1": {"accuraci": 1, "corpu": 1, "use": 1, "network": 1, "also": 1, "perform": 1, "sequenc": 2, "neural": 1, "tree": 1, "dialogu": 2, "correspond": 1, "hmm": 1, "switchboard": 1, "decis": 1, "explor": 1, "act": 1, "report": 1, "model": 1, "highest": 1, "type": 1, "observ": 1}, "vector_2": [13, 0.14098302267600618, 1, 1, 1, 0]}, {"function": "Neut", "cited": "J00-3003", "provenance": ["<.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods.", "For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity."], "label": "Prov", "citing": "W10-1012", "vector": [9, 0, 0, 0.18741784341058307], "context": ["", "There have also been Dialogue Acts modeling approaches for automatic tagging and recognition of conversational speech (Stolcke et al., 2000) and related work in corpus linguistics where machine learning techniques have been used to find conversational patterns in spoken transcripts of dialogue corpus (Shawar and Atwell, 2005).", ""], "marker": "Stolcke et al., 2000", "vector_1": {"corpu": 2, "spoken": 1, "automat": 1, "tag": 1, "find": 1, "dialogu": 2, "techniqu": 1, "also": 1, "pattern": 1, "recognit": 1, "speech": 1, "approach": 1, "machin": 1, "relat": 1, "use": 1, "transcript": 1, "convers": 2, "work": 1, "act": 1, "learn": 1, "model": 1, "linguist": 1}, "vector_2": [10, 0.8512082221873529, 2, 2, 0, 0]}, {"function": "Neut", "cited": "J00-3003", "provenance": ["A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking.", "These are usually referred to in the conversation analysis literature as \"continuers\" and have been studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970)."], "label": "Prov", "citing": "W13-4011", "vector": [9, 0, 0, 0.12792042981336624], "context": ["", "Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000).", ""], "marker": "Stolcke et al., 2000", "vector_1": {"one": 1, "among": 1, "short": 1, "convers": 3, "performedthrough": 1, "data": 1, "particip": 1, "yeah": 1, "mh": 1, "ofth": 1, "speaker": 1, "mostli": 1, "utter": 2, "main": 1, "okaynot": 1, "produc": 1, "frequent": 1, "feedback": 1}, "vector_2": [13, 0.06296583850931677, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J00-3003", "provenance": ["While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs).", "Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria."], "label": "Prov", "citing": "N13-1099", "vector": [4, 0, 0, 0.0], "context": ["", "Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).", ""], "marker": "Stolcke et al.,2000", "vector_1": {"nlp": 1, "searl": 1, "annot": 1, "see": 1, "tag": 1, "done": 1, "use": 1, "commun": 1, "interact": 1, "inspiredbi": 1, "speech": 1, "initi": 1, "da": 1, "understand": 1, "theori": 1, "work": 1, "onspoken": 1, "exampl": 1, "dialog": 2, "act": 2, "model": 1, "austin": 1}, "vector_2": [13, 0.1530393594225781, 3, 1, 0, 0]}, {"function": "Neut", "cited": "J00-3003", "provenance": ["For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7).", "The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986).", "The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui)."], "label": "Prov", "citing": "N06-2021", "vector": [7, 0, 1, 0.05627418551130967], "context": ["", "The HMM has been widely used in many tagging problems. Stolcke et al (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.", ""], "marker": "Stolcke et al., 2000", "vector_1": {"wide": 1, "use": 3, "act": 2, "al": 1, "stolck": 1, "hmm": 1, "tag": 1, "utter": 1, "dialog": 2, "et": 1, "mani": 1, "problem": 1, "observ": 1, "classif": 1}, "vector_2": [6, 0.3195242472994714, 1, 1, 1, 0]}, {"function": "Pos", "cited": "J98-2005", "provenance": ["We will show that in both cases the estimated probability is tight."], "label": "Prov", "citing": "N06-1043", "vector": [1, 0, 0, 0.0], "context": ["", "Theline of our argument below follows a proof providedin (Chi and Geman, 1998) for the maximum like lihood estimator based on finite tree distributions.", ""], "marker": "Chi and Geman, 1998", "vector_1": {"like": 1, "tree": 1, "argument": 1, "maximum": 1, "providedin": 1, "estim": 1, "base": 1, "lihood": 1, "follow": 1, "distribut": 1, "thelin": 1, "finit": 1, "proof": 1}, "vector_2": [8, 0.44711210674978086, 1, 3, 2, 0]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t.", "H <B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, \"relative frequency,\" estimator."], "label": "Prov", "citing": "N06-1043", "vector": [2, 0, 0, 0.12598815766974242], "context": ["", "Such maximization provides the estimator (see for instance (Chi and Geman, 1998)) pG(A? a) =f(A? a, T ) f(A, T ).", ""], "marker": "Chi and Geman, 1998", "vector_1": {"pga": 1, "provid": 1, "maxim": 1, "fa": 2, "see": 1, "estim": 1, "instanc": 1}, "vector_2": [8, 0.7317619557806565, 1, 3, 2, 0]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?", "We will show that in both cases the estimated probability is tight."], "label": "Prov", "citing": "N06-1043", "vector": [2, 0, 0, 0.0], "context": ["", "This resulthas been firstly shown in (Chaudhuri et al., 1983) and later, with a different proof technique, in (Chi and Geman, 1998).", ""], "marker": "Chi and Geman, 1998", "vector_1": {"differ": 1, "techniqu": 1, "firstli": 1, "later": 1, "shown": 1, "resultha": 1, "proof": 1}, "vector_2": [8, 0.7658030583422616, 2, 3, 2, 0]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["We will show that in both cases the estimated probability is tight."], "label": "Prov", "citing": "J01-2004", "vector": [5, 0, 1, 0.28284271247461895], "context": ["", "Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.", ""], "marker": "1998", "vector_1": {"prove": 1, "chi": 1, "treebank": 1, "frequenc": 1, "tight": 1, "estim": 2, "rel": 1, "geman": 1, "pcfg": 1}, "vector_2": [3, 0.16483198526994147, 1, 0, 5, 1]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t.", "H <B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, \"relative frequency,\" estimator."], "label": "Prov", "citing": "N03-1027", "vector": [4, 0, 1, 0.1307440900921227], "context": ["", "Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.", ""], "marker": "1998", "vector_1": {"corpu": 1, "use": 1, "prove": 1, "chi": 1, "frequenc": 1, "rule": 1, "met": 1, "estim": 2, "rel": 1, "geman": 1, "condit": 1, "probabl": 1}, "vector_2": [5, 0.30095084947213874, 1, 0, 0, 1]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["We will show that in both cases the estimated probability is tight."], "label": "Prov", "citing": "J98-4004", "vector": [2, 0, 0, 0.12909944487358055], "context": ["", "PCFGs estimated from tree banks using the relative frequency estimator always satisfy (Chi and Geman 1998).", ""], "marker": "Chi and Geman 1998", "vector_1": {"use": 1, "geman": 1, "frequenc": 1, "tree": 1, "satisfi": 1, "alway": 1, "estim": 2, "pcfg": 1, "rel": 1, "chi": 1, "bank": 1}, "vector_2": [0, 0.14387257341961174, 0, 0, 3, 1]}, {"function": "Error", "cited": "J98-2005", "provenance": ["We will show that in both cases the estimated probability is tight."], "label": "Prov", "citing": "P01-1017", "vector": [6, 0, 0, 0.3442651863295481], "context": ["", "When a PCFG probability distribution is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (summing to one) probability distribution over strings [5], thus making them appropriate for language models.", ""], "marker": "5", "vector_1": {"case": 1, "thu": 1, "appropri": 1, "distribut": 2, "string": 1, "dene": 1, "treebank": 1, "model": 1, "make": 1, "one": 1, "tight": 1, "train": 1, "estim": 1, "penn": 1, "languag": 1, "data": 1, "sum": 1, "pcfg": 2, "probabl": 2}, "vector_2": [3, 0.47285295026258883, 1, 0, 0, 0]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["Dempster, Laird, and Rubin [1977] put the idea into a much more general setting and coined the Chi and Geman Probabilistic Context-Free Grammars term EM for Expectation-Maximization."], "label": "Prov", "citing": "N06-1044", "vector": [6, 0, 2, 0.1517165212272521], "context": ["", "In later work by (Sanchez and Benedi, 1997)and (Chi and Geman, 1998), the result was in dependently extended to expectation maximization,which is an unsupervised method exploited to estimate probabilistic context-free grammars by find ing local maxima of the likelihood of a sample of unannotated sentences.", ""], "marker": "Chi and Geman, 1998", "vector_1": {"work": 1, "unannot": 1, "expect": 1, "exploit": 1, "sampl": 1, "result": 1, "ing": 1, "find": 1, "maximizationwhich": 1, "depend": 1, "estim": 1, "local": 1, "method": 1, "extend": 1, "sentenc": 1, "probabilist": 1, "likelihood": 1, "grammar": 1, "contextfre": 1, "unsupervis": 1, "later": 1, "maxima": 1}, "vector_2": [8, 0.0965979381443299, 2, 6, 1, 0]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t.", "H <B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, \"relative frequency,\" estimator."], "label": "Prov", "citing": "N06-1044", "vector": [2, 0, 0, 0.0], "context": ["", "the proof in (Chi and Ge man, 1998) is based on a simpler counting argument.Both these proofs assume restrictions on the un derlying context-free grammars.", ""], "marker": "Chi and Geman, 1998", "vector_1": {"count": 1, "argumentboth": 1, "grammar": 1, "contextfre": 1, "derli": 1, "un": 1, "restrict": 1, "base": 1, "proof": 2, "assum": 1, "simpler": 1}, "vector_2": [8, 0.11628865979381443, 1, 6, 1, 0]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["Estimation of Probabilistic Context-Free Grammars"], "label": "Prov", "citing": "N06-1044", "vector": [1, 0, 0, 0.0], "context": ["", "More specifically, in (Chi and Geman, 1998) empty rules and unaryrules are not allowed, thus excluding infinite ambi guity, that is, the possibility that some string in the input sample has an infinite number of derivations inthe grammar.", ""], "marker": "Chi and Geman, 1998", "vector_1": {"infinit": 2, "guiti": 1, "deriv": 1, "inth": 1, "string": 1, "specif": 1, "possibl": 1, "thu": 1, "number": 1, "rule": 1, "exclud": 1, "grammar": 1, "unaryrul": 1, "sampl": 1, "empti": 1, "allow": 1, "input": 1, "ambi": 1}, "vector_2": [8, 0.125, 1, 6, 1, 0]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["Dempster, Laird, and Rubin [1977] put the idea into a much more general setting and coined the Chi and Geman Probabilistic Context-Free Grammars term EM for Expectation-Maximization."], "label": "Prov", "citing": "N06-1044", "vector": [3, 0, 0, 0.0], "context": ["", "d?D pG(d)f(d,D), (6) subject to the properness conditions?a pG(A ?a) = 1 for eachA ? N . The maximization problemabove has a unique solution, provided by the estima tor (see for instance (Chi and Geman, 1998)) pG(A? a) =f(A? a,D) f(A,D).", ""], "marker": "Chi and Geman, 1998", "vector_1": {"pgdfdd": 1, "tor": 1, "estima": 1, "pga": 2, "conditionsa": 1, "dd": 1, "solut": 1, "maxim": 1, "n": 1, "problemabov": 1, "provid": 1, "see": 1, "instanc": 1, "fa": 1, "uniqu": 1, "proper": 1, "ad": 1, "eacha": 1, "fad": 1, "subject": 1}, "vector_2": [8, 0.3681958762886598, 1, 6, 1, 0]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["Letting ~y denote {w Efk Y(w) = Y}, the likelihood of the corpus becomes n H E H P(A--'~oL)f(A~;~)\" i=1 ~OE~y(~i) (A---~o~)ER And the maximum-likelihood equation becomes + p(B fl) Ei=l EwEfly(wi,I-I(A--.", ")cR p(A -~ a)f(A-~\";~) = 0 fT(B ~ /3) = ~iL1 Ep~f(B ~ fl;w)lw E ~y(~,)] (4) ,~s,,, ~Ei=IEpV(\" a;w)lw E ~Y(o~,)] E(B_~,E B ~ where E~ is expectation under fi and where \"]w E~-~y(wi)\" means \"conditioned on 0.2E ~-~Y(wi)'\" There is no hope for a closed form solution, but (4) does suggest an iteration scheme, which, as it turns out, \"climbs\" the likelihood surface (though there are no guarantees about approaching a global maximum): Let P0 be an arbitrary assignment respecting (1)."], "label": "Prov", "citing": "N06-1044", "vector": [4, 0, 0, 0.044946657497549475], "context": ["", "The above maximization prob lem provides a system of |R| nonlinear equations(see (Chi and Geman, 1998)) pG(A? a) =?w?C f(w, C)  EpG(d |w) f(A? a, d)?", ""], "marker": "Chi and Geman, 1998", "vector_1": {"maxim": 1, "c": 1, "nonlinear": 1, "wc": 1, "pga": 1, "provid": 1, "equationsse": 1, "fw": 1, "lem": 1, "epgd": 1, "system": 1, "fa": 1, "r": 1, "w": 1, "prob": 1}, "vector_2": [8, 0.4068041237113402, 1, 6, 1, 0]}, {"function": "Weak", "cited": "J98-2005", "provenance": ["If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?", "We will show that in both cases the estimated probability is tight."], "label": "Prov", "citing": "N06-1044", "vector": [3, 0, 0, 0.0], "context": ["", "This solves a problem that was left open in the literature (Chi and Geman,1998)", ""], "marker": "Chi and Geman, 1998", "vector_1": {"solv": 1, "problem": 1, "open": 1, "literatur": 1, "left": 1}, "vector_2": [8, 0.7924226804123712, 1, 6, 1, 0]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["We will show that in both cases the estimated probability is tight."], "label": "Prov", "citing": "J99-1004", "vector": [5, 0, 0, 0.07559289460184544], "context": ["", "A suffi cient condition for proper assignment is established by Chi and Geman (1998), who prove that production probabilities estimated by the maximum-likelihood (ML) esti mation procedure (or relative frequency estimation procedure, as it is called in com putational linguistics) always impose proper PCFG distributions.", ""], "marker": "1998", "vector_1": {"esti": 1, "procedur": 2, "proper": 2, "establish": 1, "pcfg": 1, "probabl": 1, "prove": 1, "chi": 1, "cient": 1, "alway": 1, "mation": 1, "estim": 2, "call": 1, "maximumlikelihood": 1, "rel": 1, "condit": 1, "product": 1, "distribut": 1, "geman": 1, "impos": 1, "putat": 1, "frequenc": 1, "suffi": 1, "ml": 1, "linguist": 1, "com": 1, "assign": 1}, "vector_2": [1, 0.06704752688972422, 1, 0, 4, 1]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?", "We will show that in both cases the estimated probability is tight."], "label": "Prov", "citing": "J99-1004", "vector": [3, 0, 0, 0.055048188256318034], "context": ["", "Chi and Geman (1998) proved the properness of PCFG distributions imposed by esti mated production probabilities", ""], "marker": "1998", "vector_1": {"product": 1, "distribut": 1, "prove": 1, "chi": 1, "esti": 1, "mate": 1, "geman": 1, "proper": 1, "impos": 1, "pcfg": 1, "probabl": 1}, "vector_2": [1, 0.21150768543674867, 1, 0, 4, 1]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one."], "label": "Prov", "citing": "J99-1004", "vector": [0, 0, 0, 0.0], "context": ["", "This simple estimator, as shown by Chi and Geman (1998), assigns proper production prob abilities for PCFGs.", ""], "marker": "1998", "vector_1": {"shown": 1, "chi": 1, "product": 1, "prob": 1, "estim": 1, "abil": 1, "geman": 1, "proper": 1, "simpl": 1, "assign": 1, "pcfg": 1}, "vector_2": [1, 0.3780192787226916, 1, 0, 4, 1]}, {"function": "CoCo", "cited": "J98-2005", "provenance": ["Estimation of Probabilistic Context-Free Grammars"], "label": "Prov", "citing": "J99-1004", "vector": [0, 0, 0, 0.0], "context": ["", "Proof The proof is almost identical to the one given by Chi and Ceman (1998).", ""], "marker": "1998", "vector_1": {"given": 1, "ident": 1, "ceman": 1, "almost": 1, "chi": 1, "one": 1, "proof": 2}, "vector_2": [1, 0.4220104953664074, 1, 0, 4, 1]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["We show here that estimated production probabilities always yield proper distributions."], "label": "Prov", "citing": "J99-1004", "vector": [2, 0, 0, 0.2886751345948129], "context": ["", " impose proper probability distributions on D (Chi and Geman 1998).", ""], "marker": "Chi and Geman, 1998", "vector_1": {"distribut": 1, "chi": 1, "geman": 1, "proper": 1, "impos": 1, "probabl": 1}, "vector_2": [1, 0.39190144776508246, 0, 4, 0, 0]}, {"function": "CoCo", "cited": "N01-1011", "provenance": ["Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems.", "Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coecient.", "While the accuracy of this approach was as good as any previously published results, the learned models were complex and dicult to interpret, in e?ect acting as very accurate black boxes."], "label": "Prov", "citing": "W02-0812", "vector": [19, 1, 11, 0.2726376271173233], "context": ["", "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bi-grams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", ""], "marker": "Pedersen, 2001a", "vector_1": {"featur": 1, "bigram": 3, "evalu": 1, "lack": 1, "rank": 1, "repres": 1, "interpret": 1, "loglikelihood": 1, "quit": 1, "develop": 1, "frustrat": 1, "top": 1, "except": 1, "accuraci": 2, "exampl": 1, "decis": 4, "approach": 1, "singl": 1, "accord": 1, "overal": 1, "led": 1, "sensev": 1, "earlier": 2, "wherea": 1, "intuit": 1, "base": 1, "ratio": 1, "data": 2, "present": 2, "b": 1, "work": 1, "tree": 4, "bag": 2, "achiev": 2, "train": 1, "learn": 1, "model": 1, "similar": 1}, "vector_2": [1, 0.8821852731591449, 1, 1, 3, 1]}, {"function": "Neut", "cited": "N01-1011", "provenance": ["The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.", "The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated.", "We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests."], "label": "Prov", "citing": "W04-0813", "vector": [5, 0, 1, 0.14433756729740643], "context": ["", "We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001).", ""], "marker": "Pedersen, 2001", "vector_1": {"softwar": 1, "salient": 1, "bigram": 1, "describ": 1, "obtain": 1, "also": 1, "context": 1, "method": 1}, "vector_2": [3, 0.3132577238951897, 1, 1, 0, 0]}, {"function": "Neut", "cited": "N01-1011", "provenance": ["This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation."], "label": "Prov", "citing": "W02-1011", "vector": [6, 0, 2, 0.29814239699997197], "context": ["", "in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation.", ""], "marker": "2001", "vector_1": {"featur": 1, "bigram": 1, "alon": 1, "pedersen": 1, "eectiv": 1, "disambigu": 1, "word": 1, "sens": 1, "found": 1, "fact": 1}, "vector_2": [1, 0.7501811031770672, 1, 0, 2, 0]}, {"function": "Neut", "cited": "N01-1011", "provenance": ["The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.", "Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen.", "A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic."], "label": "Prov", "citing": "W08-0611", "vector": [2, 0, 0, 0.16129556770910236], "context": ["", " Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001).", ""], "marker": "2001", "vector_1": {"salient": 2, "bigram": 2, "describ": 1, "abstract": 1, "within": 1, "pedersen": 1, "high": 1, "score": 1, "loglikelihood": 1}, "vector_2": [7, 0.5387339530765826, 1, 0, 6, 0]}, {"function": "Neut", "cited": "P98-1081", "provenance": ["However, it appears more useful to give more weight to taggers which have proved their quality.", "This can be general quality, e.g.", "each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g.", "each tagger votes its precision on the suggested tag (Tag- Precision).", "The information about each tagger's quality is derived from an inspection of its results on Tune."], "label": "Prov", "citing": "W02-1004", "vector": [8, 0, 2, 0.024708310555370042], "context": ["", "An alternative method for estimating the parameters Akis to approximate them with the performance of the kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving more weight to classi.ers that have a smaller classi.cation error (the method will be referred to as PB).", ""], "marker": "van Halteren et al., 1998", "vector_1": {"altern": 1, "smaller": 1, "weight": 1, "give": 1, "perform": 1, "approxim": 1, "classic": 1, "performancebas": 1, "akxdpckiscorrectlxd": 1, "pb": 1, "classier": 2, "aki": 1, "estim": 1, "kth": 1, "refer": 1, "error": 1, "combin": 1, "therefor": 1, "method": 2, "paramet": 1}, "vector_2": [4, 0.5034951518860935, 2, 4, 6, 0]}, {"function": "Neut", "cited": "P98-1081", "provenance": ["A next step is to examine them in pairs.", "We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.", "When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%)."], "label": "Prov", "citing": "W02-1004", "vector": [13, 1, 2, 0.11866111313364179], "context": ["", "Van Halteren et al (1998) introduce a modi.ed version of voting called TagPair. Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er joutputs s2, P(sls i(xd)=ss j(xd)=s2), is computed on development data, and the posterior probability is estimated as N P(slx,d)e(s,sAk(x,d))+(s,sA j(x,d)) (7) k.. j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)). Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation. In the experiments presented in van Halteren et al (1998), this method was the best performer among the presented methods.", ""], "marker": "1998", "vector_1": {"among": 1, "comput": 1, "classic": 2, "al": 2, "pair": 1, "vote": 3, "et": 2, "k": 1, "probabl": 2, "given": 2, "van": 2, "perform": 1, "tagpair": 1, "sand": 1, "best": 1, "version": 1, "call": 1, "ioutput": 1, "condit": 1, "experi": 1, "method": 2, "introduc": 1, "jxd": 2, "everi": 1, "psl": 1, "halteren": 2, "joint": 1, "posterior": 1, "sens": 2, "scjxfdargmaxtptlscxfdfscjxfd": 1, "develop": 1, "data": 1, "present": 2, "word": 1, "like": 1, "classier": 4, "joutput": 1, "j": 1, "n": 1, "s": 1, "pslxdessakxdssa": 1, "estim": 1, "modi": 1, "model": 1, "ixdss": 1}, "vector_2": [4, 0.5852849273588249, 2, 0, 0, 0]}, {"function": "Neut", "cited": "P98-1081", "provenance": ["5 The most straightforward selection method is an n-way vote."], "label": "Prov", "citing": "W02-1004", "vector": [1, 0, 0, 0.0], "context": ["", "A very common technique for combination in such a case is by voting (Brill and Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).", ""], "marker": "van Halteren et al., 1998", "vector_1": {"case": 1, "vote": 1, "combin": 1, "common": 1, "techniqu": 1}, "vector_2": [4, 0.5497535676319943, 3, 4, 6, 0]}, {"function": "Pos", "cited": "P98-1081", "provenance": ["The most democratic option is to give each tagger one vote (Majority).", "This can be general quality, e.g.", "each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g.", "each tagger votes its precision on the suggested tag (Tag- Precision).", "A next step is to examine them in pairs.", "When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e.", "not just the ones suggested by the component taggers."], "label": "Prov", "citing": "E99-1025", "vector": [10, 0, 2, 0.15161960871578065], "context": ["", "We consider three voting strategies suggested by van Halteren et al (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair'wise voting.", ""], "marker": "1998", "vector_1": {"van": 1, "depend": 1, "weight": 2, "suggest": 1, "overal": 2, "three": 1, "classifi": 2, "al": 1, "equal": 2, "pairwis": 1, "consid": 1, "halteren": 1, "vote": 4, "et": 1, "accuraci": 2, "strategi": 1}, "vector_2": [1, 0.5699926900584795, 1, 0, 0, 1]}, {"function": "Neut", "cited": "P98-1081", "provenance": ["Pairwise Voting"], "label": "Prov", "citing": "W03-1728", "vector": [2, 0, 1, 0.9999999999999998], "context": ["", "pairwise voting (van Halteren et al., 1998)", ""], "marker": "van Halteren et al., 1998", "vector_1": {"vote": 1, "pairwis": 1}, "vector_2": [5, 0.5706131467501381, 1, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1081", "provenance": ["The most democratic option is to give each tagger one vote (Majority)."], "label": "Prov", "citing": "P06-2060", "vector": [2, 0, 0, 0.1781741612749496], "context": ["", "Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.", ""], "marker": "1998", "vector_1": {"major": 1, "compar": 1, "halteren": 1, "speech": 1, "tag": 1, "number": 1, "al": 1, "part": 1, "combin": 1, "includ": 1, "vote": 2, "et": 1, "scheme": 1, "method": 2}, "vector_2": [8, 0.2544967273102098, 1, 0, 3, 0]}, {"function": "Pos", "cited": "P98-1081", "provenance": ["Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems."], "label": "Prov", "citing": "A00-1024", "vector": [6, 0, 1, 0.14638501094227999], "context": ["", "Thirdly, this approach is compatible with in corporating multiple components of the same type to improve performance (cf. (van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).", ""], "marker": "van Halteren et al., 1998", "vector_1": {"compat": 1, "thirdli": 1, "multipl": 1, "found": 1, "perform": 2, "corpor": 1, "speech": 1, "cf": 1, "tagger": 1, "compon": 1, "part": 1, "combin": 1, "result": 1, "type": 1, "increas": 1, "improv": 1, "approach": 1, "sever": 1}, "vector_2": [2, 0.1692766324258315, 1, 0, 2, 0]}, {"function": "Neut", "cited": "X96-1048", "provenance": ["In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure."], "label": "Prov", "citing": "A97-1028", "vector": [4, 0, 0, 0.08753762190648172], "context": ["", "Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).", ""], "marker": "Sundheim, 1995b", "vector_1": {"whose": 1, "task": 1, "name": 1, "evalu": 2, "began": 1, "object": 1, "confer": 1, "messag": 1, "standard": 1, "part": 1, "understand": 1, "muc": 1, "entiti": 1, "ie": 1, "recent": 1}, "vector_2": [2, 0.049378000872981234, 1, 2, 0, 0]}, {"function": "Neut", "cited": "X96-1048", "provenance": [" Scenario Template (ST) --Drawing evidence from anywhere in the text, extract prespecified event information, and relate the event information to the particular organization and person entities involved in the event."], "label": "Prov", "citing": "E12-2021", "vector": [5, 0, 1, 0.22271770159368698], "context": ["", "nary associations of annotations are also supported, allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2).", ""], "marker": "Sundheim, 1996", "vector_1": {"task": 1, "bionlp": 1, "target": 1, "ace": 1, "inform": 1, "extract": 1, "support": 1, "annot": 2, "structur": 1, "also": 1, "figur": 1, "nari": 1, "allow": 1, "ie": 1, "associ": 1, "event": 1, "muc": 1}, "vector_2": [16, 0.30725872111229413, 3, 1, 0, 0]}, {"function": "Pos", "cited": "X96-1048", "provenance": [" Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases."], "label": "Prov", "citing": "W97-1307", "vector": [5, 0, 1, 0.20412414523193148], "context": ["", "Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod. uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems' ability to recog nize coreference among noun phrases (Sund heim, 1995).", ""], "marker": "Sund heim, 1995", "vector_1": {"among": 1, "evalu": 1, "fastu": 1, "one": 1, "uce": 1, "abil": 1, "phrase": 1, "top": 1, "system": 2, "corefer": 2, "score": 1, "muc": 2, "prod": 1, "recal": 1, "nize": 1, "recog": 1, "task": 1, "noun": 1, "algorithm": 1, "algorit": 1, "precis": 1, "hmthi": 1, "implement": 1, "first": 1}, "vector_2": [2, 0.19633162711338606, 2, 0, 0, 1]}, {"function": "Pos", "cited": "X96-1048", "provenance": ["When the outputs are scored in \"key-to-response\" mode, as though one annotator's output represented the \"key\" and the other the \"response,\" the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%.", "Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA \"fast\" configuration 95.66, SRA \"fastest\" configuration 92.61, SRA \"nonames\" configuration 94.92, SRI 94.0, Sterling Software 92.74.."], "label": "Prov", "citing": "W99-0612", "vector": [7, 0, 1, 0.08642716314028859], "context": ["", "It is not clear what resources are required to adapt systems to new languages.\" It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).", ""], "marker": "Sundheim 1995", "vector_1": {"sundheim": 1, "task": 1, "resourc": 1, "perform": 1, "clear": 1, "system": 1, "mention": 1, "fmeasur": 1, "adapt": 1, "human": 1, "import": 1, "new": 1, "languag": 1, "requir": 1}, "vector_2": [4, 0.6561126673691244, 0, 0, 2, 1]}, {"function": "Neut", "cited": "X96-1048", "provenance": ["Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc.."], "label": "Prov", "citing": "E99-1001", "vector": [24, 0, 21, 0.5201564866102993], "context": ["", "In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that \"common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks\" (Sundheim 1995: 16).", ""], "marker": "1995", "vector_1": {"competit": 1, "remark": 1, "locat": 1, "name": 4, "organ": 1, "mucsundheim": 1, "handl": 1, "peopl": 1, "list": 1, "drawback": 1, "articl": 1, "1": 1, "recognit": 1, "part": 1, "lookup": 1, "common": 1, "although": 1, "recours": 1, "first": 1, "sundheim": 1, "entiti": 1}, "vector_2": [4, 0.07860775588048316, 1, 0, 0, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["An error count of 0:0 is assigned to a perfect translation, and an error count of 1:0 is assigned to a semantically and syntactically wrong translation.", "4.3 Translation Experiments."], "label": "Non-Prov", "citing": "E06-1004", "vector": [2, 0, 0, 0.0], "context": ["", " Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", ""], "marker": "Tillman, 2000", "vector_1": {"given": 1, "e": 2, "f": 2, "sentenc": 1, "p": 1, "comput": 1, "pair": 1, "condit": 1, "model": 1, "probabl": 1, "paramet": 1}, "vector_2": [6, 0.19961015454587647, 4, 1, 1, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an ecient search algorithm.", "A search restriction especially useful for the translation direction from German to English is presented."], "label": "Non-Prov", "citing": "E06-1004", "vector": [3, 0, 0, 0.0], "context": ["", " Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", ""], "marker": "Tillman, 2000", "vector_1": {"given": 1, "e": 2, "f": 2, "sentenc": 1, "p": 1, "comput": 1, "pair": 1, "condit": 1, "model": 1, "probabl": 1, "paramet": 1}, "vector_2": [6, 0.19961015454587647, 4, 1, 1, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["The resulting algorithm has a complexity of O(n!).", "However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp."], "label": "Non-Prov", "citing": "E06-1004", "vector": [3, 0, 0, 0.0], "context": ["", " Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).", ""], "marker": "Tillman, 2000", "vector_1": {"given": 1, "e": 2, "f": 2, "sentenc": 1, "p": 1, "comput": 1, "pair": 1, "condit": 1, "model": 1, "probabl": 1, "paramet": 1}, "vector_2": [6, 0.19961015454587647, 4, 1, 1, 0]}, {"function": "Error", "cited": "C00-2123", "provenance": ["This algorithm can be applied to statistical machine translation."], "label": "Non-Prov", "citing": "J04-2003", "vector": [4, 0, 2, 0.1732050807568877], "context": ["", "Many existing systems for statistical machine translation 1 1 (Garca-Varea and Casacuberta 2001; Germann et al 2001; Nieen et al 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"brown": 1, "casacuberta": 1, "al": 2, "sourc": 2, "exist": 1, "et": 2, "mercer": 1, "och": 1, "describ": 1, "nieen": 1, "system": 1, "ney": 1, "machin": 1, "200": 2, "germann": 1, "string": 1, "tillmann": 1, "999": 1, "998": 1, "garcavarea": 1, "translat": 1, "implement": 1, "present": 1, "pietra": 2, "word": 3, "target": 2, "align": 1, "correspond": 1, "statist": 1, "posit": 2, "mani": 1, "model": 1, "assign": 1, "della": 2}, "vector_2": [4, 0.08172688817850109, 1, 0, 1, 1]}, {"function": "Error", "cited": "C00-2123", "provenance": ["The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0."], "label": "Non-Prov", "citing": "J04-2003", "vector": [2, 0, 1, 0.0], "context": ["", "Many existing systems for statistical machine translation 1 1 (Garca-Varea and Casacuberta 2001; Germann et al 2001; Nieen et al 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"brown": 1, "casacuberta": 1, "al": 2, "sourc": 2, "exist": 1, "et": 2, "mercer": 1, "och": 1, "describ": 1, "nieen": 1, "system": 1, "ney": 1, "machin": 1, "200": 2, "germann": 1, "string": 1, "tillmann": 1, "999": 1, "998": 1, "garcavarea": 1, "translat": 1, "implement": 1, "present": 1, "pietra": 2, "word": 3, "target": 2, "align": 1, "correspond": 1, "statist": 1, "posit": 2, "mani": 1, "model": 1, "assign": 1, "della": 2}, "vector_2": [4, 0.08172688817850109, 1, 0, 1, 1]}, {"function": "Error", "cited": "C00-2123", "provenance": ["(1), Pr(eI 1) is the language model, which is a trigram language model in this case."], "label": "Non-Prov", "citing": "J04-2003", "vector": [3, 1, 0, 0.0], "context": ["", "Many existing systems for statistical machine translation 1 1 (Garca-Varea and Casacuberta 2001; Germann et al 2001; Nieen et al 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"brown": 1, "casacuberta": 1, "al": 2, "sourc": 2, "exist": 1, "et": 2, "mercer": 1, "och": 1, "describ": 1, "nieen": 1, "system": 1, "ney": 1, "machin": 1, "200": 2, "germann": 1, "string": 1, "tillmann": 1, "999": 1, "998": 1, "garcavarea": 1, "translat": 1, "implement": 1, "present": 1, "pietra": 2, "word": 3, "target": 2, "align": 1, "correspond": 1, "statist": 1, "posit": 2, "mani": 1, "model": 1, "assign": 1, "della": 2}, "vector_2": [4, 0.08172688817850109, 1, 0, 1, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["For  = 0, no new target word is generated, while an additional source sentence position is covered."], "label": "Non-Prov", "citing": "J04-2003", "vector": [0, 0, 0, 0.0], "context": ["", "Some recent publications deal with the automatic detection of multiword phrases (Och and Weber 1998; Tillmann and Ney 2000).", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"och": 1, "detect": 1, "deal": 1, "tillmann": 1, "automat": 1, "multiword": 1, "ney": 1, "phrase": 1, "public": 1, "weber": 1, "recent": 1}, "vector_2": [4, 0.6284187413219672, 0, 0, 1, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["Table 3: Training and test conditions for the Verbmobil task (*number of words without punctuation marks)."], "label": "Non-Prov", "citing": "J04-2003", "vector": [3, 0, 0, 0.0], "context": ["", "Some recent publications deal with the automatic detection of multiword phrases (Och and Weber 1998; Tillmann and Ney 2000).", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"och": 1, "detect": 1, "deal": 1, "tillmann": 1, "automat": 1, "multiword": 1, "ney": 1, "phrase": 1, "public": 1, "weber": 1, "recent": 1}, "vector_2": [4, 0.6284187413219672, 0, 0, 1, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup."], "label": "Non-Prov", "citing": "J04-2003", "vector": [2, 0, 0, 0.0], "context": ["", "Some recent publications deal with the automatic detection of multiword phrases (Och and Weber 1998; Tillmann and Ney 2000).", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"och": 1, "detect": 1, "deal": 1, "tillmann": 1, "automat": 1, "multiword": 1, "ney": 1, "phrase": 1, "public": 1, "weber": 1, "recent": 1}, "vector_2": [4, 0.6284187413219672, 0, 0, 1, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["Subsets of partial hypotheses with coverage sets C of increasing cardinality c are processed."], "label": "Non-Prov", "citing": "J04-4002", "vector": [1, 0, 0, 0.0], "context": ["", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"word": 1, "prune": 1, "tillmann": 1, "call": 1, "ney": 1, "highli": 1, "observ": 1, "select": 1, "probabl": 1}, "vector_2": [4, 0.5547631456236114, 0, 0, 1, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["For a trigram language model, the partial hypotheses are of the form (e0; e; C; j)."], "label": "Non-Prov", "citing": "J04-4002", "vector": [1, 0, 0, 0.0], "context": ["", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"word": 1, "prune": 1, "tillmann": 1, "call": 1, "ney": 1, "highli": 1, "observ": 1, "select": 1, "probabl": 1}, "vector_2": [4, 0.5547631456236114, 0, 0, 1, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["The resulting algorithm has a complexity of O(n!)."], "label": "Non-Prov", "citing": "J04-4002", "vector": [1, 0, 0, 0.0], "context": ["", "We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).", ""], "marker": "Tillmann and Ney 2000", "vector_1": {"word": 1, "prune": 1, "tillmann": 1, "call": 1, "ney": 1, "highli": 1, "observ": 1, "select": 1, "probabl": 1}, "vector_2": [4, 0.5547631456236114, 0, 0, 1, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.", "For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.", "This algorithm can be applied to statistical machine translation."], "label": "Non-Prov", "citing": "N03-1010", "vector": [6, 0, 1, 0.06359630007488934], "context": ["", "Och et al report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"program": 1, "och": 1, "search": 2, "word": 1, "version": 1, "algorithm": 1, "optim": 1, "variant": 1, "al": 1, "restrict": 1, "beam": 1, "rate": 1, "base": 1, "decod": 1, "error": 1, "report": 1, "et": 1, "combin": 1, "dynam": 1}, "vector_2": [3, 0.06758057556344807, 1, 2, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["The computing time is low, since no reordering is carried out.", "The quasi-monotone search performs best in terms of both error rates mWER and SSER.", "Additionally, it works about 3 times as fast as the IBM style search."], "label": "Non-Prov", "citing": "N03-1010", "vector": [6, 0, 1, 0.2502172968684897], "context": ["", "Och et al report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"program": 1, "och": 1, "search": 2, "word": 1, "version": 1, "algorithm": 1, "optim": 1, "variant": 1, "al": 1, "restrict": 1, "beam": 1, "rate": 1, "base": 1, "decod": 1, "error": 1, "report": 1, "et": 1, "combin": 1, "dynam": 1}, "vector_2": [3, 0.06758057556344807, 1, 2, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["Two subjects are each given a calendar and they are asked to schedule a meeting.", "The translation direction is from German to English.", "A summary of the corpus used in the experiments is given in Table 3."], "label": "Non-Prov", "citing": "N03-1010", "vector": [4, 0, 1, 0.0], "context": ["", "Och et al report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"program": 1, "och": 1, "search": 2, "word": 1, "version": 1, "algorithm": 1, "optim": 1, "variant": 1, "al": 1, "restrict": 1, "beam": 1, "rate": 1, "base": 1, "decod": 1, "error": 1, "report": 1, "et": 1, "combin": 1, "dynam": 1}, "vector_2": [3, 0.06758057556344807, 1, 2, 2, 0]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["Translation errors are reported in terms of multireference word error rate (mWER) and subjective sentence error rate (SSER)."], "label": "Non-Prov", "citing": "P03-1039", "vector": [5, 0, 0, 0.0700140042014005], "context": ["", "The decoding algorithm employed for this chunk + weight  j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"translationmodel": 1, "weight": 1, "chunk": 1, "consum": 1, "tive": 1, "languag": 1, "probabl": 1, "decod": 1, "input": 1, "respec": 1, "gener": 1, "arbitrari": 1, "beam": 1, "base": 2, "translat": 2, "e": 1, "plm": 1, "ptmje": 1, "present": 1, "search": 1, "word": 1, "algorithm": 2, "f": 2, "frequenc": 1, "align": 1, "j": 7, "lefttoright": 1, "employ": 1, "reqea": 2, "statist": 2, "output": 1, "model": 1, "order": 2}, "vector_2": [3, 0.6173074181438007, 1, 2, 2, 0]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["10."], "label": "Non-Prov", "citing": "P03-1039", "vector": [0, 0, 0, 0.0], "context": ["", "The decoding algorithm employed for this chunk + weight  j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"translationmodel": 1, "weight": 1, "chunk": 1, "consum": 1, "tive": 1, "languag": 1, "probabl": 1, "decod": 1, "input": 1, "respec": 1, "gener": 1, "arbitrari": 1, "beam": 1, "base": 2, "translat": 2, "e": 1, "plm": 1, "ptmje": 1, "present": 1, "search": 1, "word": 1, "algorithm": 2, "f": 2, "frequenc": 1, "align": 1, "j": 7, "lefttoright": 1, "employ": 1, "reqea": 2, "statist": 2, "output": 1, "model": 1, "order": 2}, "vector_2": [3, 0.6173074181438007, 1, 2, 2, 0]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["the number of permutations carried out for the word reordering is given."], "label": "Non-Prov", "citing": "P03-1039", "vector": [4, 0, 1, 0.04042260417272217], "context": ["", "The decoding algorithm employed for this chunk + weight  j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"translationmodel": 1, "weight": 1, "chunk": 1, "consum": 1, "tive": 1, "languag": 1, "probabl": 1, "decod": 1, "input": 1, "respec": 1, "gener": 1, "arbitrari": 1, "beam": 1, "base": 2, "translat": 2, "e": 1, "plm": 1, "ptmje": 1, "present": 1, "search": 1, "word": 1, "algorithm": 2, "f": 2, "frequenc": 1, "align": 1, "j": 7, "lefttoright": 1, "employ": 1, "reqea": 2, "statist": 2, "output": 1, "model": 1, "order": 2}, "vector_2": [3, 0.6173074181438007, 1, 2, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.", "Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited."], "label": "Non-Prov", "citing": "P03-1039", "vector": [4, 0, 0, 0.0], "context": ["", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", ""], "marker": "Tillman and Ney, 2000", "vector_1": {"insert": 1, "lexicon": 1, "string": 1, "possibl": 1, "sequenc": 1, "gener": 1, "estim": 1, "output": 1, "model": 1, "invert": 1, "chunk": 1}, "vector_2": [3, 0.6275721142422659, 1, 2, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated.", "The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions."], "label": "Non-Prov", "citing": "P03-1039", "vector": [3, 0, 0, 0.0], "context": ["", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", ""], "marker": "Tillman and Ney, 2000", "vector_1": {"insert": 1, "lexicon": 1, "string": 1, "possibl": 1, "sequenc": 1, "gener": 1, "estim": 1, "output": 1, "model": 1, "invert": 1, "chunk": 1}, "vector_2": [3, 0.6275721142422659, 1, 2, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1.", "A straightforward way to find the shortest tour is by trying all possible permutations of the n cities."], "label": "Non-Prov", "citing": "P03-1039", "vector": [6, 0, 0, 0.034815531191139566], "context": ["", "The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).", ""], "marker": "Tillman and Ney, 2000", "vector_1": {"insert": 1, "lexicon": 1, "string": 1, "possibl": 1, "sequenc": 1, "gener": 1, "estim": 1, "output": 1, "model": 1, "invert": 1, "chunk": 1}, "vector_2": [3, 0.6275721142422659, 1, 2, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["Table 4 shows translation results for the three approaches.", "The computing time is given in terms of CPU time per sentence (on a 450MHz PentiumIIIPC)."], "label": "Non-Prov", "citing": "W01-0505", "vector": [2, 0, 0, 0.0], "context": ["", "They were usually incorporated in the EM algorithm (Brown et al., 1993; Kupiec, 1993; Tillmann and Ney, 2000; Och et al., 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"em": 1, "algorithm": 1, "incorpor": 1, "usual": 1}, "vector_2": [1, 0.08832526127963293, 4, 1, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj1; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).", "When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect."], "label": "Non-Prov", "citing": "W01-0505", "vector": [2, 1, 0, 0.0], "context": ["", "They were usually incorporated in the EM algorithm (Brown et al., 1993; Kupiec, 1993; Tillmann and Ney, 2000; Och et al., 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"em": 1, "algorithm": 1, "incorpor": 1, "usual": 1}, "vector_2": [1, 0.08832526127963293, 4, 1, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["f;g denotes the empty set, where no source sentence position is covered.", "The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max ;e00 np(jjj0; J) p() p(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j)."], "label": "Non-Prov", "citing": "W01-0505", "vector": [2, 0, 1, 0.0], "context": ["", "They were usually incorporated in the EM algorithm (Brown et al., 1993; Kupiec, 1993; Tillmann and Ney, 2000; Och et al., 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"em": 1, "algorithm": 1, "incorpor": 1, "usual": 1}, "vector_2": [1, 0.08832526127963293, 4, 1, 2, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["In this case, we have no finite-state restrictions for the search space."], "label": "Non-Prov", "citing": "W01-1404", "vector": [2, 0, 0, 0.16903085094570328], "context": ["", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"concentr": 1, "closer": 1, "cannot": 1, "assign": 1, "grammar": 1, "machineri": 1, "extend": 1, "framework": 1, "chosen": 1, "comfort": 1, "two": 1, "yet": 1, "finitest": 2, "contextfre": 2, "other": 1, "either": 1, "transduct": 1, "studi": 2, "model": 1}, "vector_2": [1, 0.030689329556185082, 6, 1, 0, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy."], "label": "Non-Prov", "citing": "W01-1404", "vector": [1, 0, 0, 0.0], "context": ["", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"concentr": 1, "closer": 1, "cannot": 1, "assign": 1, "grammar": 1, "machineri": 1, "extend": 1, "framework": 1, "chosen": 1, "comfort": 1, "two": 1, "yet": 1, "finitest": 2, "contextfre": 2, "other": 1, "either": 1, "transduct": 1, "studi": 2, "model": 1}, "vector_2": [1, 0.030689329556185082, 6, 1, 0, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["The algorithm works due to the fact that not all permutations of cities have to be considered explicitly."], "label": "Non-Prov", "citing": "W01-1404", "vector": [4, 0, 0, 0.0], "context": ["", "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"concentr": 1, "closer": 1, "cannot": 1, "assign": 1, "grammar": 1, "machineri": 1, "extend": 1, "framework": 1, "chosen": 1, "comfort": 1, "two": 1, "yet": 1, "finitest": 2, "contextfre": 2, "other": 1, "either": 1, "transduct": 1, "studi": 2, "model": 1}, "vector_2": [1, 0.030689329556185082, 6, 1, 0, 0]}, {"function": "CoCo", "cited": "C00-2123", "provenance": ["A straightforward way to find the shortest tour is by trying all possible permutations of the n cities."], "label": "Non-Prov", "citing": "W01-1407", "vector": [2, 0, 0, 0.0], "context": ["", "We used a translation system called single- word based approach described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"use": 1, "word": 1, "describ": 1, "compar": 1, "system": 1, "base": 1, "call": 1, "translat": 1, "approach": 2, "singl": 1}, "vector_2": [1, 0.7675015446033934, 2, 1, 4, 0]}, {"function": "CoCo", "cited": "C00-2123", "provenance": ["During the search process, a partial hypothesis is extended by choosing a source sentence position, which has not been aligned with a target sentence position yet."], "label": "Non-Prov", "citing": "W01-1407", "vector": [1, 0, 0, 0.0], "context": ["", "We used a translation system called single- word based approach described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"use": 1, "word": 1, "describ": 1, "compar": 1, "system": 1, "base": 1, "call": 1, "translat": 1, "approach": 2, "singl": 1}, "vector_2": [1, 0.7675015446033934, 2, 1, 4, 0]}, {"function": "CoCo", "cited": "C00-2123", "provenance": ["Skipped (K): The translation of up to one word may be postponed . Verb (V): The translation of up to two words may be anticipated."], "label": "Non-Prov", "citing": "W01-1407", "vector": [3, 0, 0, 0.21320071635561047], "context": ["", "We used a translation system called single- word based approach described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"use": 1, "word": 1, "describ": 1, "compar": 1, "system": 1, "base": 1, "call": 1, "translat": 1, "approach": 2, "singl": 1}, "vector_2": [1, 0.7675015446033934, 2, 1, 4, 0]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["The details are given in (Tillmann, 2000)."], "label": "Non-Prov", "citing": "W01-1408", "vector": [2, 1, 0, 0.0], "context": ["", "Search algorithms We evaluate the following two search algorithms:  beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", ""], "marker": "Tillmann, 2001; Tillmann and Ney, 2000", "vector_1": {"search": 4, "breadthfirst": 1, "algorithm": 4, "evalu": 1, "space": 1, "two": 1, "beam": 1, "manner": 1, "explor": 1, "bs": 1, "follow": 1}, "vector_2": [0, 0.3456131436314363, 2, 3, 4, 0]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000)."], "label": "Non-Prov", "citing": "W01-1408", "vector": [2, 1, 0, 0.0], "context": ["", "Search algorithms We evaluate the following two search algorithms:  beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", ""], "marker": "Tillmann, 2001; Tillmann and Ney, 2000", "vector_1": {"search": 4, "breadthfirst": 1, "algorithm": 4, "evalu": 1, "space": 1, "two": 1, "beam": 1, "manner": 1, "explor": 1, "bs": 1, "follow": 1}, "vector_2": [0, 0.3456131436314363, 2, 3, 4, 0]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["3) A tight coupling with the speech recognizer output."], "label": "Non-Prov", "citing": "W01-1408", "vector": [2, 0, 0, 0.0], "context": ["", "Search algorithms We evaluate the following two search algorithms:  beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.", ""], "marker": "Tillmann, 2001; Tillmann and Ney, 2000", "vector_1": {"search": 4, "breadthfirst": 1, "algorithm": 4, "evalu": 1, "space": 1, "two": 1, "beam": 1, "manner": 1, "explor": 1, "bs": 1, "follow": 1}, "vector_2": [0, 0.3456131436314363, 2, 3, 4, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["Figure 2: Order in which source positions are visited for the example given in Fig.1."], "label": "Non-Prov", "citing": "W02-1020", "vector": [4, 2, 0, 0.0], "context": ["", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J for in p(v1, w2 , wm1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"heurist": 1, "even": 1, "use": 1, "pv": 1, "npcomplet": 1, "dynamicprogram": 1, "umh": 1, "channel": 1, "omj": 1, "wm": 1, "noisi": 1, "polynomi": 1, "fastest": 1, "model": 1, "search": 1, "faster": 1, "j": 1, "mt": 1, "stanc": 1, "statist": 1, "w": 1, "v": 1, "problem": 1}, "vector_2": [2, 0.35199759008924203, 4, 2, 1, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["The approach recursively evaluates a quantity Q(C; j), where C is the set of already visited cities and sj is the last visited city."], "label": "Non-Prov", "citing": "W02-1020", "vector": [4, 0, 0, 0.04950737714883372], "context": ["", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J for in p(v1, w2 , wm1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"heurist": 1, "even": 1, "use": 1, "pv": 1, "npcomplet": 1, "dynamicprogram": 1, "umh": 1, "channel": 1, "omj": 1, "wm": 1, "noisi": 1, "polynomi": 1, "fastest": 1, "model": 1, "search": 1, "faster": 1, "j": 1, "mt": 1, "stanc": 1, "statist": 1, "w": 1, "v": 1, "problem": 1}, "vector_2": [2, 0.35199759008924203, 4, 2, 1, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2."], "label": "Non-Prov", "citing": "W02-1020", "vector": [6, 0, 0, 0.11572751247156893], "context": ["", "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J for in p(v1, w2 , wm1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"heurist": 1, "even": 1, "use": 1, "pv": 1, "npcomplet": 1, "dynamicprogram": 1, "umh": 1, "channel": 1, "omj": 1, "wm": 1, "noisi": 1, "polynomi": 1, "fastest": 1, "model": 1, "search": 1, "faster": 1, "j": 1, "mt": 1, "stanc": 1, "statist": 1, "w": 1, "v": 1, "problem": 1}, "vector_2": [2, 0.35199759008924203, 4, 2, 1, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["That is, Chinese is the source language and English is the target language.", "We achieved encouraging results."], "label": "Non-Prov", "citing": "D10-1042", "vector": [1, 0, 0, 0.0], "context": ["", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"scarciti": 2, "articl": 1, "eg": 2, "share": 1, "high": 2, "explor": 1, "suffer": 1, "event": 1, "languag": 1, "use": 1, "render": 1, "compar": 1, "wikipedia": 1, "two": 1, "accuraci": 1, "content": 1, "document": 1, "resourc": 1, "news": 1, "parallel": 3, "vast": 1, "describ": 1, "allevi": 1, "entri": 1, "corpora": 3, "person": 1, "bilingu": 1, "similar": 1, "sentencealign": 1}, "vector_2": [6, 0.3061437908496732, 4, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["in the English target corpus.", "In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document."], "label": "Non-Prov", "citing": "D10-1042", "vector": [5, 0, 0, 0.03774256780481986], "context": ["", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"scarciti": 2, "articl": 1, "eg": 2, "share": 1, "high": 2, "explor": 1, "suffer": 1, "event": 1, "languag": 1, "use": 1, "render": 1, "compar": 1, "wikipedia": 1, "two": 1, "accuraci": 1, "content": 1, "document": 1, "resourc": 1, "news": 1, "parallel": 3, "vast": 1, "describ": 1, "allevi": 1, "entri": 1, "corpora": 3, "person": 1, "bilingu": 1, "similar": 1, "sentencealign": 1}, "vector_2": [6, 0.3061437908496732, 4, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["rank is the context rank, Trans.", "Rank is the transliteration rank."], "label": "Non-Prov", "citing": "D10-1042", "vector": [1, 0, 0, 0.0], "context": ["", "Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity. To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"scarciti": 2, "articl": 1, "eg": 2, "share": 1, "high": 2, "explor": 1, "suffer": 1, "event": 1, "languag": 1, "use": 1, "render": 1, "compar": 1, "wikipedia": 1, "two": 1, "accuraci": 1, "content": 1, "document": 1, "resourc": 1, "news": 1, "parallel": 3, "vast": 1, "describ": 1, "allevi": 1, "entri": 1, "corpora": 3, "person": 1, "bilingu": 1, "similar": 1, "sentencealign": 1}, "vector_2": [6, 0.3061437908496732, 4, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations."], "label": "Non-Prov", "citing": "N09-1048", "vector": [3, 0, 0, 0.0], "context": ["", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"multilingu": 1, "featur": 1, "co": 1, "compar": 1, "frequenc": 1, "eg": 1, "corpora": 1, "wikipedia": 1, "wherein": 1, "employ": 1, "popularli": 1, "context": 1, "occurr": 1, "parallel": 1}, "vector_2": [5, 0.07349773333762016, 4, 2, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm."], "label": "Non-Prov", "citing": "N09-1048", "vector": [2, 1, 0, 0.0], "context": ["", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"multilingu": 1, "featur": 1, "co": 1, "compar": 1, "frequenc": 1, "eg": 1, "corpora": 1, "wikipedia": 1, "wherein": 1, "employ": 1, "popularli": 1, "context": 1, "occurr": 1, "parallel": 1}, "vector_2": [5, 0.07349773333762016, 4, 2, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["For transliteration, we estimate P(e | c) as follows: P(e | c) = P(e | pinyin) =  P(e, a | pinyin) a For the Chinese corpus, we used the Linguistic Data Consortium (LDC) Chinese Gigaword Corpus from Jan 1995 to Dec 1995."], "label": "Non-Prov", "citing": "N09-1048", "vector": [2, 0, 0, 0.0], "context": ["", "The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"multilingu": 1, "featur": 1, "co": 1, "compar": 1, "frequenc": 1, "eg": 1, "corpora": 1, "wikipedia": 1, "wherein": 1, "employ": 1, "popularli": 1, "context": 1, "occurr": 1, "parallel": 1}, "vector_2": [5, 0.07349773333762016, 4, 2, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["#e is the total number of English translation candidates in the period."], "label": "Non-Prov", "citing": "N09-1048", "vector": [4, 0, 0, 0.1649572197684645], "context": ["", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", ""], "marker": "2004", "vector_1": {"agenc": 1, "differ": 1, "document": 1, "context": 1, "chines": 1, "mine": 1, "shao": 1, "inform": 1, "period": 1, "translat": 1, "transliter": 1, "english": 1, "new": 1, "news": 2, "ng": 1, "combin": 1, "method": 1, "present": 1}, "vector_2": [5, 0.22062180497058162, 1, 0, 4, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["If no words appear within the top M positions in both lists, then no translation is output."], "label": "Non-Prov", "citing": "N09-1048", "vector": [2, 0, 0, 0.0], "context": ["", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", ""], "marker": "2004", "vector_1": {"agenc": 1, "differ": 1, "document": 1, "context": 1, "chines": 1, "mine": 1, "shao": 1, "inform": 1, "period": 1, "translat": 1, "transliter": 1, "english": 1, "new": 1, "news": 2, "ng": 1, "combin": 1, "method": 1, "present": 1}, "vector_2": [5, 0.22062180497058162, 1, 0, 4, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["8 of the 43 words are translated to English multi-word phrases (denoted as phrase in Table 3)."], "label": "Non-Prov", "citing": "N09-1048", "vector": [4, 0, 1, 0.07715167498104596], "context": ["", "Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.", ""], "marker": "2004", "vector_1": {"agenc": 1, "differ": 1, "document": 1, "context": 1, "chines": 1, "mine": 1, "shao": 1, "inform": 1, "period": 1, "translat": 1, "transliter": 1, "english": 1, "new": 1, "news": 2, "ng": 1, "combin": 1, "method": 1, "present": 1}, "vector_2": [5, 0.22062180497058162, 1, 0, 4, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["If an English word e is the translation of a Chinese word c , they will have similar contexts."], "label": "Non-Prov", "citing": "P13-1059", "vector": [1, 0, 0, 0.10050378152592121], "context": ["", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "pair": 1, "recent": 1}, "vector_2": [9, 0.9222239923530349, 9, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003)."], "label": "Non-Prov", "citing": "P13-1059", "vector": [3, 0, 1, 0.2222222222222222], "context": ["", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "pair": 1, "recent": 1}, "vector_2": [9, 0.9222239923530349, 9, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["insuff means the correct translation appears less than 10 times in the English part of the comparable corpus."], "label": "Non-Prov", "citing": "P13-1059", "vector": [2, 0, 0, 0.20100756305184242], "context": ["", "Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "pair": 1, "recent": 1}, "vector_2": [9, 0.9222239923530349, 9, 1, 0, 0]}, {"function": "CoCo", "cited": "C04-1089", "provenance": ["The window size of English context was 100 words.After all the counts were collected, we esti mated P(C (c) | C (e)) as described in Section 3, 5.2 Preprocessing."], "label": "Non-Prov", "citing": "P13-1107", "vector": [3, 0, 0, 0.0], "context": ["", "Other similar research lines are the TACKBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"el": 1, "predict": 1, "knowledg": 1, "mine": 1, "entiti": 2, "web": 1, "compar": 1, "research": 1, "document": 1, "tackbp": 1, "appropri": 1, "link": 3, "translat": 1, "pair": 1, "news": 1, "line": 1, "kb": 1, "task": 1, "name": 2, "entri": 1, "corpora": 1, "base": 1, "problem": 1, "similar": 1}, "vector_2": [9, 0.9328285077951002, 14, 1, 0, 0]}, {"function": "CoCo", "cited": "C04-1089", "provenance": ["We also divided the English corpus into 12 periods, each containing text from a half-month period."], "label": "Non-Prov", "citing": "P13-1107", "vector": [3, 0, 0, 0.0], "context": ["", "Other similar research lines are the TACKBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"el": 1, "predict": 1, "knowledg": 1, "mine": 1, "entiti": 2, "web": 1, "compar": 1, "research": 1, "document": 1, "tackbp": 1, "appropri": 1, "link": 3, "translat": 1, "pair": 1, "news": 1, "line": 1, "kb": 1, "task": 1, "name": 2, "entri": 1, "corpora": 1, "base": 1, "problem": 1, "similar": 1}, "vector_2": [9, 0.9328285077951002, 14, 1, 0, 0]}, {"function": "CoCo", "cited": "C04-1089", "provenance": ["They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination."], "label": "Non-Prov", "citing": "P13-1107", "vector": [2, 0, 0, 0.0], "context": ["", "Other similar research lines are the TACKBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"el": 1, "predict": 1, "knowledg": 1, "mine": 1, "entiti": 2, "web": 1, "compar": 1, "research": 1, "document": 1, "tackbp": 1, "appropri": 1, "link": 3, "translat": 1, "pair": 1, "news": 1, "line": 1, "kb": 1, "task": 1, "name": 2, "entri": 1, "corpora": 1, "base": 1, "problem": 1, "similar": 1}, "vector_2": [9, 0.9328285077951002, 14, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["#Cor is the number of correct English translations output."], "label": "Non-Prov", "citing": "P13-2036", "vector": [0, 0, 0, 0.0], "context": ["", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"approach": 1, "combin": 1, "studi": 1, "similar": 1, "holist": 1, "recent": 1}, "vector_2": [9, 0.07383335499805017, 3, 2, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["The corpus of the period Jul to Dec 1995 was used to come up with new Chinese words c for translation into English."], "label": "Non-Prov", "citing": "P13-2036", "vector": [0, 0, 0, 0.0], "context": ["", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"approach": 1, "combin": 1, "studi": 1, "similar": 1, "holist": 1, "recent": 1}, "vector_2": [9, 0.07383335499805017, 3, 2, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["We also investigated the effect of varying M . The results are shown in Table 2."], "label": "Non-Prov", "citing": "P13-2036", "vector": [0, 0, 0, 0.0], "context": ["", "Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"approach": 1, "combin": 1, "studi": 1, "similar": 1, "holist": 1, "recent": 1}, "vector_2": [9, 0.07383335499805017, 3, 2, 0, 0]}, {"function": "Pos", "cited": "C04-1089", "provenance": ["insuff means the correct translation appears less than 10 times in the English part of the comparable corpus."], "label": "Non-Prov", "citing": "P13-2036", "vector": [2, 0, 0, 0.08058229640253803], "context": ["", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "return": 1, "independ": 1, "rank": 2, "candid": 1, "cx": 1, "translat": 1, "ph": 1, "highest": 1, "averag": 1, "result": 1}, "vector_2": [9, 0.08280254777070063, 1, 2, 0, 0]}, {"function": "Pos", "cited": "C04-1089", "provenance": ["Then we determined the new Chinese words in each half-month period p. By new Chinese words, we refer to those words that appeared in this period p but not from Jan to Jun 1995 or any other periods that preceded p. Among all these new words, we selected those occurring at least 5 times."], "label": "Non-Prov", "citing": "P13-2036", "vector": [1, 0, 0, 0.0], "context": ["", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "return": 1, "independ": 1, "rank": 2, "candid": 1, "cx": 1, "translat": 1, "ph": 1, "highest": 1, "averag": 1, "result": 1}, "vector_2": [9, 0.08280254777070063, 1, 2, 0, 0]}, {"function": "Pos", "cited": "C04-1089", "provenance": ["We thank Jia Li for implementing the EM algorithm to train transliteration probabilities."], "label": "Non-Prov", "citing": "P13-2036", "vector": [1, 0, 0, 0.0], "context": ["", "(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "return": 1, "independ": 1, "rank": 2, "candid": 1, "cx": 1, "translat": 1, "ph": 1, "highest": 1, "averag": 1, "result": 1}, "vector_2": [9, 0.08280254777070063, 1, 2, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["So"], "label": "Non-Prov", "citing": "W11-2206", "vector": [0, 0, 0, 0.0], "context": ["", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "word": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "transliter": 1, "new": 1, "rescor": 1, "recent": 1}, "vector_2": [7, 0.850206577425219, 8, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["comm means the correct translation is a word appearing in the dictionary we used or is a stop word."], "label": "Non-Prov", "citing": "W11-2206", "vector": [3, 0, 0, 0.25000000000000006], "context": ["", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "word": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "transliter": 1, "new": 1, "rescor": 1, "recent": 1}, "vector_2": [7, 0.850206577425219, 8, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["We use the Pml (tc | Tc (C (e))) = dT (C (e )) (tc ) dT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c tTc (C ( e )) lables to English letter sequences."], "label": "Non-Prov", "citing": "W11-2206", "vector": [1, 0, 0, 0.0], "context": ["", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "word": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "transliter": 1, "new": 1, "rescor": 1, "recent": 1}, "vector_2": [7, 0.850206577425219, 8, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["We fully implemented our method and tested it on ChineseEnglish comparable corpora."], "label": "Non-Prov", "citing": "W13-2501", "vector": [3, 0, 1, 0.1889822365046136], "context": ["", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"tradit": 1, "lexicon": 1, "target": 1, "compar": 1, "corpora": 1, "vector": 1, "avail": 1, "extens": 1, "approch": 1, "sourc": 1, "translat": 2, "bilingu": 1, "presuppos": 1, "extract": 1, "languag": 1}, "vector_2": [9, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language."], "label": "Non-Prov", "citing": "W13-2501", "vector": [7, 2, 0, 0.16666666666666666], "context": ["", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"tradit": 1, "lexicon": 1, "target": 1, "compar": 1, "corpora": 1, "vector": 1, "avail": 1, "extens": 1, "approch": 1, "sourc": 1, "translat": 2, "bilingu": 1, "presuppos": 1, "extract": 1, "languag": 1}, "vector_2": [9, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["New words such as names, technical terms, etc appear frequently."], "label": "Non-Prov", "citing": "W13-2501", "vector": [0, 0, 0, 0.0], "context": ["", "The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.", ""], "marker": "Shao and Ng, 2004", "vector_1": {"tradit": 1, "lexicon": 1, "target": 1, "compar": 1, "corpora": 1, "vector": 1, "avail": 1, "extens": 1, "approch": 1, "sourc": 1, "translat": 2, "bilingu": 1, "presuppos": 1, "extract": 1, "languag": 1}, "vector_2": [9, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["NA means the word cannot be transliterated."], "label": "Non-Prov", "citing": "W13-2512", "vector": [1, 0, 0, 0.0], "context": ["", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"term": 1, "name": 1, "variou": 1, "hybrid": 1, "across": 1, "exploit": 1, "translat": 1, "way": 1, "method": 1, "languag": 1, "entiti": 1}, "vector_2": [9, 0.31465281574630943, 3, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["We fully implemented our method and tested it on ChineseEnglish comparable corpora."], "label": "Non-Prov", "citing": "W13-2512", "vector": [0, 0, 0, 0.0], "context": ["", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"term": 1, "name": 1, "variou": 1, "hybrid": 1, "across": 1, "exploit": 1, "translat": 1, "way": 1, "method": 1, "languag": 1, "entiti": 1}, "vector_2": [9, 0.31465281574630943, 3, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["We view the document D as a multinomial distribution of terms and assume that words in both lists and finde1 , e2 ,..., ek that ap query Q is generated by this model: pear in top M positions in both lists."], "label": "Non-Prov", "citing": "W13-2512", "vector": [3, 0, 0, 0.0], "context": ["", "Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"term": 1, "name": 1, "variou": 1, "hybrid": 1, "across": 1, "exploit": 1, "translat": 1, "way": 1, "method": 1, "languag": 1, "entiti": 1}, "vector_2": [9, 0.31465281574630943, 3, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["We have checked if there are similar verbs in other major domains, but this was the only one."], "label": "Non-Prov", "citing": "D12-1094", "vector": [1, 0, 0, 0.0], "context": ["", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", ""], "marker": "Sekine, 2005", "vector_1": {"datadriven": 1, "distribut": 1, "extend": 1, "idea": 1, "discoveri": 1, "paraphras": 1, "phrase": 1, "similar": 1, "method": 1}, "vector_2": [7, 0.21108071485183955, 4, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["In the CC-domain, there are 32 sets of phrases which contain more than 2 phrases."], "label": "Non-Prov", "citing": "D12-1094", "vector": [3, 0, 0, 0.2519763153394848], "context": ["", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", ""], "marker": "Sekine, 2005", "vector_1": {"datadriven": 1, "distribut": 1, "extend": 1, "idea": 1, "discoveri": 1, "paraphras": 1, "phrase": 1, "similar": 1, "method": 1}, "vector_2": [7, 0.21108071485183955, 4, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Because of this threshold, very few NE instance pairs could be used and hence the variety of phrases was also limited."], "label": "Non-Prov", "citing": "D12-1094", "vector": [3, 0, 0, 0.10050378152592121], "context": ["", "Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.", ""], "marker": "Sekine, 2005", "vector_1": {"datadriven": 1, "distribut": 1, "extend": 1, "idea": 1, "discoveri": 1, "paraphras": 1, "phrase": 1, "similar": 1, "method": 1}, "vector_2": [7, 0.21108071485183955, 4, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Although this is not a precise criterion, most cases we evaluated were relatively clear-cut.", "In general, different modalities (planned to buy, agreed to buy, bought) were considered to express the same relationship within an extraction setting."], "label": "Non-Prov", "citing": "J10-3003", "vector": [3, 0, 0, 0.0], "context": ["", "As a later renement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for specic concepts represented by keywords.", ""], "marker": "2005", "vector_1": {"concept": 1, "repres": 1, "paraphras": 1, "entiti": 1, "use": 1, "renement": 1, "make": 1, "sekin": 1, "distribut": 1, "lexic": 1, "pair": 1, "specic": 1, "attempt": 1, "name": 1, "keyword": 1, "later": 1, "list": 1, "order": 1, "fulli": 1, "phrasal": 1, "similar": 2, "produc": 1}, "vector_2": [5, 0.5265055837563452, 1, 0, 3, 1]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["All the sentences have been analyzed by our chunker and NE tag- ger.", "The procedure using the tagged sentences to discover paraphrases takes about one hour on a 2GHz Pentium 4 PC with 1GB of memory."], "label": "Non-Prov", "citing": "J10-3003", "vector": [6, 0, 0, 0.08891084489487741], "context": ["", "As a later renement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for specic concepts represented by keywords.", ""], "marker": "2005", "vector_1": {"concept": 1, "repres": 1, "paraphras": 1, "entiti": 1, "use": 1, "renement": 1, "make": 1, "sekin": 1, "distribut": 1, "lexic": 1, "pair": 1, "specic": 1, "attempt": 1, "name": 1, "keyword": 1, "later": 1, "list": 1, "order": 1, "fulli": 1, "phrasal": 1, "similar": 2, "produc": 1}, "vector_2": [5, 0.5265055837563452, 1, 0, 3, 1]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["The results, along with the total number of phrases, are shown in Table 1.", "D o m ai n # of ph ras es t o t a l p h r a s e s ac cu ra cy C C 7 o r m o r e 1 0 5 8 7 . 6 % 6 o r l e s s 1 0 6 6 7 . 0 % P C 7 o r m o r e 3 5 9 9 9 . 2 % 6 o r l e s s 2 5 5 6 5 . 1 % Table 1."], "label": "Non-Prov", "citing": "J10-3003", "vector": [3, 0, 0, 0.0], "context": ["", "As a later renement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for specic concepts represented by keywords.", ""], "marker": "2005", "vector_1": {"concept": 1, "repres": 1, "paraphras": 1, "entiti": 1, "use": 1, "renement": 1, "make": 1, "sekin": 1, "distribut": 1, "lexic": 1, "pair": 1, "specic": 1, "attempt": 1, "name": 1, "keyword": 1, "later": 1, "list": 1, "order": 1, "fulli": 1, "phrasal": 1, "similar": 2, "produc": 1}, "vector_2": [5, 0.5265055837563452, 1, 0, 3, 1]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["We evaluated the results based on two metrics."], "label": "Non-Prov", "citing": "J12-1003", "vector": [1, 0, 0, 0.0], "context": ["", "This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al 2010).", ""], "marker": "Sekine 2005", "vector_1": {"led": 1, "entail": 1, "dagan": 1, "etzioni": 1, "lin": 1, "activ": 1, "al": 1, "rule": 1, "research": 1, "acquisit": 1, "et": 1, "pantel": 1, "sekin": 1, "szpektor": 1, "schoenmack": 1, "predic": 1, "broadscal": 1, "yate": 1}, "vector_2": [7, 0.028151667106536465, 0, 0, 4, 1]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["All the sentences have been analyzed by our chunker and NE tag- ger."], "label": "Non-Prov", "citing": "J12-1003", "vector": [1, 0, 0, 0.0], "context": ["", "This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al 2010).", ""], "marker": "Sekine 2005", "vector_1": {"led": 1, "entail": 1, "dagan": 1, "etzioni": 1, "lin": 1, "activ": 1, "al": 1, "rule": 1, "research": 1, "acquisit": 1, "et": 1, "pantel": 1, "sekin": 1, "szpektor": 1, "schoenmack": 1, "predic": 1, "broadscal": 1, "yate": 1}, "vector_2": [7, 0.028151667106536465, 0, 0, 4, 1]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["There have been other kinds of efforts to discover paraphrase automatically from corpora."], "label": "Non-Prov", "citing": "J12-1003", "vector": [2, 0, 0, 0.0], "context": ["", "This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al 2010).", ""], "marker": "Sekine 2005", "vector_1": {"led": 1, "entail": 1, "dagan": 1, "etzioni": 1, "lin": 1, "activ": 1, "al": 1, "rule": 1, "research": 1, "acquisit": 1, "et": 1, "pantel": 1, "sekin": 1, "szpektor": 1, "schoenmack": 1, "predic": 1, "broadscal": 1, "yate": 1}, "vector_2": [7, 0.028151667106536465, 0, 0, 4, 1]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["In general, different modalities (planned to buy, agreed to buy, bought) were considered to express the same relationship within an extraction setting.", "We did have a problem classifying some modified noun phrases where the modified phrase does not represent a qualified or restricted form of the head, like chairman and vice chairman, as these are both represented by the keyword chairman.", "In this specific case, as these two titles could fill the same column of an IE table, we regarded them as paraphrases for the evaluation."], "label": "Non-Prov", "citing": "P06-2094", "vector": [8, 0, 0, 0.07207499701564471], "context": ["", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", ""], "marker": "Sekine 05", "vector_1": {"corpu": 1, "unannot": 1, "name": 1, "two": 1, "method": 1, "also": 1, "instanc": 1, "larg": 1, "paraphras": 1, "context": 1, "sekin": 1, "entiti": 1, "find": 1, "propos": 1}, "vector_2": [6, 0.36944602833457607, 0, 0, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["For the experiments, we used four newswire corpora, the Los Angeles Times/Washington Post, The New York Times, Reuters and the Wall Street Journal, all published in 1995.", "They contain about 200M words (25M, 110M, 40M and 19M words, respectively).", "All the sentences have been analyzed by our chunker and NE tag- ger."], "label": "Non-Prov", "citing": "P06-2094", "vector": [3, 0, 0, 0.0], "context": ["", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", ""], "marker": "Sekine 05", "vector_1": {"corpu": 1, "unannot": 1, "name": 1, "two": 1, "method": 1, "also": 1, "instanc": 1, "larg": 1, "paraphras": 1, "context": 1, "sekin": 1, "entiti": 1, "find": 1, "propos": 1}, "vector_2": [6, 0.36944602833457607, 0, 0, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Figure 3 Figure 1.", "Overview of the method 2.2 Step by Step Algorithm.", "In this section, we will explain the algorithm step by step with examples."], "label": "Non-Prov", "citing": "P06-2094", "vector": [5, 0, 0, 0.049629166698546515], "context": ["", "We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).", ""], "marker": "Sekine 05", "vector_1": {"corpu": 1, "unannot": 1, "name": 1, "two": 1, "method": 1, "also": 1, "instanc": 1, "larg": 1, "paraphras": 1, "context": 1, "sekin": 1, "entiti": 1, "find": 1, "propos": 1}, "vector_2": [6, 0.36944602833457607, 0, 0, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["For example, from the sentence Mr."], "label": "Non-Prov", "citing": "P07-1058", "vector": [1, 0, 0, 0.0], "context": ["", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most dont.", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 1, "algorithm": 1, "constraint": 1, "contextu": 1, "acquisit": 1, "add": 1, "exist": 1, "rule": 1, "learn": 1, "dont": 1}, "vector_2": [2, 0.2252401115356811, 1, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Figure 2 shows examples of extracted NE pair instances and their contexts."], "label": "Non-Prov", "citing": "P07-1058", "vector": [0, 0, 0, 0.0], "context": ["", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most dont.", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 1, "algorithm": 1, "constraint": 1, "contextu": 1, "acquisit": 1, "add": 1, "exist": 1, "rule": 1, "learn": 1, "dont": 1}, "vector_2": [2, 0.2252401115356811, 1, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["We checked whether the discovered links are listed in WordNet."], "label": "Non-Prov", "citing": "P07-1058", "vector": [1, 0, 0, 0.0], "context": ["", "Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most dont.", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 1, "algorithm": 1, "constraint": 1, "contextu": 1, "acquisit": 1, "add": 1, "exist": 1, "rule": 1, "learn": 1, "dont": 1}, "vector_2": [2, 0.2252401115356811, 1, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, IBM plans to acquire Lotus."], "label": "Non-Prov", "citing": "P07-1058", "vector": [3, 0, 0, 0.0], "context": ["", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"rang": 1, "distribut": 1, "suggest": 1, "share": 1, "rule": 1, "find": 1, "acquisit": 1, "automat": 1, "context": 1, "year": 1, "mani": 1, "similar": 1, "method": 1, "recent": 1}, "vector_2": [2, 0.2768081517436056, 6, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["If the expression is longer or complicated (like A buys B and As purchase of B), it is called paraphrase, i.e. a set of phrases which express the same thing or event."], "label": "Non-Prov", "citing": "P07-1058", "vector": [1, 0, 0, 0.0], "context": ["", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"rang": 1, "distribut": 1, "suggest": 1, "share": 1, "rule": 1, "find": 1, "acquisit": 1, "automat": 1, "context": 1, "year": 1, "mani": 1, "similar": 1, "method": 1, "recent": 1}, "vector_2": [2, 0.2768081517436056, 6, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Figure 2 shows examples of extracted NE pair instances and their contexts."], "label": "Non-Prov", "citing": "P07-1058", "vector": [2, 0, 0, 0.0944911182523068], "context": ["", "Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"rang": 1, "distribut": 1, "suggest": 1, "share": 1, "rule": 1, "find": 1, "acquisit": 1, "automat": 1, "context": 1, "year": 1, "mani": 1, "similar": 1, "method": 1, "recent": 1}, "vector_2": [2, 0.2768081517436056, 6, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Keyword detection error Even if a keyword consists of a single word, there are words which are not desirable as keywords for a domain.", "As was explained in the results section, strength or add are not desirable keywords in the CC-domain."], "label": "Non-Prov", "citing": "P07-1058", "vector": [3, 0, 0, 0.0], "context": ["", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"judgment": 1, "algorithm": 1, "evalu": 1, "promin": 1, "qualiti": 1, "rule": 2, "inde": 1, "acquisit": 1, "human": 1, "learn": 1, "approach": 1}, "vector_2": [2, 0.32276498330407244, 6, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["It is natural that the larger the data in the domain, the more keywords are found.", "In the Person  Person domain, 618 keywords are found, and in the Country  Country domain, 303 keywords are found."], "label": "Non-Prov", "citing": "P07-1058", "vector": [2, 0, 0, 0.0], "context": ["", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"judgment": 1, "algorithm": 1, "evalu": 1, "promin": 1, "qualiti": 1, "rule": 2, "inde": 1, "acquisit": 1, "human": 1, "learn": 1, "approach": 1}, "vector_2": [2, 0.32276498330407244, 6, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Our clue is the NE instance pairs.", "If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases."], "label": "Non-Prov", "citing": "P07-1058", "vector": [3, 0, 0, 0.0], "context": ["", "Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"judgment": 1, "algorithm": 1, "evalu": 1, "promin": 1, "qualiti": 1, "rule": 2, "inde": 1, "acquisit": 1, "human": 1, "learn": 1, "approach": 1}, "vector_2": [2, 0.32276498330407244, 6, 3, 3, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold, 30."], "label": "Non-Prov", "citing": "P08-1078", "vector": [1, 0, 0, 0.0], "context": ["", "Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 2, "algorithm": 1, "paraphras": 1, "view": 1, "automat": 1, "bidirect": 1, "rule": 2, "learn": 1, "recent": 1, "sever": 1, "propos": 1}, "vector_2": [3, 0.05281840591618735, 5, 1, 1, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["For example, out of 905 phrases in the CC- domain, 211 phrases contain keywords found in step 2."], "label": "Non-Prov", "citing": "P08-1078", "vector": [1, 0, 0, 0.0], "context": ["", "Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 2, "algorithm": 1, "paraphras": 1, "view": 1, "automat": 1, "bidirect": 1, "rule": 2, "learn": 1, "recent": 1, "sever": 1, "propos": 1}, "vector_2": [3, 0.05281840591618735, 5, 1, 1, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["We will return to these issues in the discussion section."], "label": "Non-Prov", "citing": "P08-1078", "vector": [0, 0, 0, 0.0], "context": ["", "Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 2, "algorithm": 1, "paraphras": 1, "view": 1, "automat": 1, "bidirect": 1, "rule": 2, "learn": 1, "recent": 1, "sever": 1, "propos": 1}, "vector_2": [3, 0.05281840591618735, 5, 1, 1, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["If a phrase does not contain any keywords, the phrase is discarded."], "label": "Non-Prov", "citing": "P11-1062", "vector": [1, 0, 0, 0.0], "context": ["", "Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).", ""], "marker": "Sekine, 2005", "vector_1": {"last": 1, "consider": 1, "receiv": 1, "thu": 1, "knowledg": 1, "acquisit": 1, "attent": 1, "decad": 1}, "vector_2": [6, 0.0529608938547486, 4, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["This result suggests the benefit of using the automatic discovery method."], "label": "Non-Prov", "citing": "P11-1062", "vector": [2, 0, 0, 0.0], "context": ["", "Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).", ""], "marker": "Sekine, 2005", "vector_1": {"last": 1, "consider": 1, "receiv": 1, "thu": 1, "knowledg": 1, "acquisit": 1, "attent": 1, "decad": 1}, "vector_2": [6, 0.0529608938547486, 4, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["For each set, the phrases with bracketed frequencies are considered not paraphrases in the set."], "label": "Non-Prov", "citing": "P11-1062", "vector": [2, 0, 1, 0.0], "context": ["", "Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).", ""], "marker": "Sekine, 2005", "vector_1": {"last": 1, "consider": 1, "receiv": 1, "thu": 1, "knowledg": 1, "acquisit": 1, "attent": 1, "decad": 1}, "vector_2": [6, 0.0529608938547486, 4, 1, 0, 0]}, {"function": "Pos", "cited": "I05-5011", "provenance": ["First, we will describe their method and compare it with our method."], "label": "Non-Prov", "citing": "P11-1109", "vector": [2, 0, 0, 0.0], "context": ["", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", ""], "marker": "2005", "vector_1": {"claim": 1, "differ": 1, "varieti": 1, "discov": 1, "requir": 1, "sekin": 1, "wider": 1, "agre": 1, "method": 1, "sever": 1, "paraphras": 1}, "vector_2": [6, 0.8026571887958026, 1, 0, 0, 0]}, {"function": "Pos", "cited": "I05-5011", "provenance": ["This result suggests the benefit of using the automatic discovery method."], "label": "Non-Prov", "citing": "P11-1109", "vector": [1, 0, 0, 0.0], "context": ["", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", ""], "marker": "2005", "vector_1": {"claim": 1, "differ": 1, "varieti": 1, "discov": 1, "requir": 1, "sekin": 1, "wider": 1, "agre": 1, "method": 1, "sever": 1, "paraphras": 1}, "vector_2": [6, 0.8026571887958026, 1, 0, 0, 0]}, {"function": "Pos", "cited": "I05-5011", "provenance": ["Overview of the method 2.2 Step by Step Algorithm."], "label": "Non-Prov", "citing": "P11-1109", "vector": [1, 0, 0, 0.0], "context": ["", "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", ""], "marker": "2005", "vector_1": {"claim": 1, "differ": 1, "varieti": 1, "discov": 1, "requir": 1, "sekin": 1, "wider": 1, "agre": 1, "method": 1, "sever": 1, "paraphras": 1}, "vector_2": [6, 0.8026571887958026, 1, 0, 0, 0]}, {"function": "Pos", "cited": "I05-5011", "provenance": ["This remains as future work.", "Limitations There are several limitations in the methods."], "label": "Non-Prov", "citing": "P13-1131", "vector": [2, 0, 1, 0.0], "context": ["", "Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", ""], "marker": "2005", "vector_1": {"corpu": 1, "differ": 1, "scale": 1, "word": 1, "richer": 1, "collect": 1, "share": 1, "number": 1, "templat": 1, "predic": 3, "cluster": 1, "verb": 1, "statist": 1, "sekin": 1, "co": 1, "follow": 1, "main": 1, "occurr": 1, "order": 1, "per": 1}, "vector_2": [8, 0.6507067289773216, 1, 0, 0, 0]}, {"function": "Pos", "cited": "I05-5011", "provenance": ["Corpus Step 1 NE pair instances Step 2 Step 1.", "Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus."], "label": "Non-Prov", "citing": "P13-1131", "vector": [4, 0, 1, 0.06537204504606135], "context": ["", "Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", ""], "marker": "2005", "vector_1": {"corpu": 1, "differ": 1, "scale": 1, "word": 1, "richer": 1, "collect": 1, "share": 1, "number": 1, "templat": 1, "predic": 3, "cluster": 1, "verb": 1, "statist": 1, "sekin": 1, "co": 1, "follow": 1, "main": 1, "occurr": 1, "order": 1, "per": 1}, "vector_2": [8, 0.6507067289773216, 1, 0, 0, 0]}, {"function": "Pos", "cited": "I05-5011", "provenance": ["Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.", "For example, in Information Retrieval (IR), we have to match a users query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the users question even if the formulation of the answer in the document is different from the question."], "label": "Non-Prov", "citing": "P13-1131", "vector": [7, 0, 1, 0.03042903097250923], "context": ["", "Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.", ""], "marker": "2005", "vector_1": {"corpu": 1, "differ": 1, "scale": 1, "word": 1, "richer": 1, "collect": 1, "share": 1, "number": 1, "templat": 1, "predic": 3, "cluster": 1, "verb": 1, "statist": 1, "sekin": 1, "co": 1, "follow": 1, "main": 1, "occurr": 1, "order": 1, "per": 1}, "vector_2": [8, 0.6507067289773216, 1, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["That is, given a choice between segmenting a sequence abc into abc and ab, c, the former will always be picked so long as its cost does not exceed the summed costs of ab and c: while; it is possible for abc to be so costly as to preclude the larger grouping, this will certainly not usually be the case."], "label": "Non-Prov", "citing": "J04-1004", "vector": [4, 0, 0, 0.0], "context": ["", "In Chinese text segmentation there are three basic approaches (Sproat et al 1996): pure heuristic, pure statistical, and a hybrid of the two.", ""], "marker": "Sproat et al. 1996", "vector_1": {"sproat": 1, "hybrid": 1, "text": 1, "chines": 1, "heurist": 1, "segment": 1, "three": 1, "al": 1, "statist": 1, "pure": 2, "basic": 1, "et": 1, "approach": 1, "two": 1}, "vector_2": [8, 0.1107739280993428, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Thus, we feel fairly confident that for the examples we have considered from Gan's study a solution can be incorporated, or at least approximated, within a finite-state framework."], "label": "Non-Prov", "citing": "J04-1004", "vector": [2, 0, 0, 0.0], "context": ["", "In Chinese text segmentation there are three basic approaches (Sproat et al 1996): pure heuristic, pure statistical, and a hybrid of the two.", ""], "marker": "Sproat et al. 1996", "vector_1": {"sproat": 1, "hybrid": 1, "text": 1, "chines": 1, "heurist": 1, "segment": 1, "three": 1, "al": 1, "statist": 1, "pure": 2, "basic": 1, "et": 1, "approach": 1, "two": 1}, "vector_2": [8, 0.1107739280993428, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["\"c' 0 + 0 \"0 '  + a n t i g r e e d y x g r e e d y < > c u r r e n t m e t h o d o d i e t . o n l y  Taiwan 0 ;; 0 c CD E i5 0\"' 9 9  Mainland     -0.30.20.1 0.0 0.1 0.2 Dimension 1 (62%) Figure 7 Classical metric multidimensional scaling of distance matrix, showing the two most significant dimensions."], "label": "Non-Prov", "citing": "J04-1004", "vector": [4, 0, 1, 0.02178034209345161], "context": ["", "In Chinese text segmentation there are three basic approaches (Sproat et al 1996): pure heuristic, pure statistical, and a hybrid of the two.", ""], "marker": "Sproat et al. 1996", "vector_1": {"sproat": 1, "hybrid": 1, "text": 1, "chines": 1, "heurist": 1, "segment": 1, "three": 1, "al": 1, "statist": 1, "pure": 2, "basic": 1, "et": 1, "approach": 1, "two": 1}, "vector_2": [8, 0.1107739280993428, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way."], "label": "Non-Prov", "citing": "J04-1004", "vector": [0, 0, 0, 0.0], "context": ["", "There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al 2000; Dai, Loh, and Khoo 1999; Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"use": 1, "backward": 1, "matchingteahan": 1, "sproat": 1, "loh": 1, "maximum": 2, "al": 2, "dai": 1, "sever": 1, "khoo": 1, "forward": 1, "et": 2, "match": 1, "segment": 1, "method": 1, "commonli": 1}, "vector_2": [8, 0.24448457542866717, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system, but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper."], "label": "Non-Prov", "citing": "J04-1004", "vector": [3, 0, 1, 0.0], "context": ["", "There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al 2000; Dai, Loh, and Khoo 1999; Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"use": 1, "backward": 1, "matchingteahan": 1, "sproat": 1, "loh": 1, "maximum": 2, "al": 2, "dai": 1, "sever": 1, "khoo": 1, "forward": 1, "et": 2, "match": 1, "segment": 1, "method": 1, "commonli": 1}, "vector_2": [8, 0.24448457542866717, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Two measures that can be used to compare judgments are: 1."], "label": "Non-Prov", "citing": "J04-1004", "vector": [2, 0, 0, 0.08164965809277261], "context": ["", "There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al 2000; Dai, Loh, and Khoo 1999; Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"use": 1, "backward": 1, "matchingteahan": 1, "sproat": 1, "loh": 1, "maximum": 2, "al": 2, "dai": 1, "sever": 1, "khoo": 1, "forward": 1, "et": 2, "match": 1, "segment": 1, "method": 1, "commonli": 1}, "vector_2": [8, 0.24448457542866717, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind."], "label": "Non-Prov", "citing": "J04-1004", "vector": [9, 0, 0, 0.0], "context": ["", "In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"decid": 1, "word": 2, "sproat": 1, "evalu": 1, "perform": 1, "hard": 1, "al": 1, "accept": 1, "standard": 1, "whether": 1, "meaning": 1, "et": 1, "extract": 1, "method": 1, "commonli": 1, "addit": 1}, "vector_2": [8, 0.4771656768380094, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Note that the sets of possible classifiers for a given noun can easily be encoded on that noun by grammatical features, which can be referred to by finite-state grammatical rules."], "label": "Non-Prov", "citing": "J04-1004", "vector": [5, 0, 0, 0.0], "context": ["", "In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"decid": 1, "word": 2, "sproat": 1, "evalu": 1, "perform": 1, "hard": 1, "al": 1, "accept": 1, "standard": 1, "whether": 1, "meaning": 1, "et": 1, "extract": 1, "method": 1, "commonli": 1, "addit": 1}, "vector_2": [8, 0.4771656768380094, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Thus in an English sentence such as I'm going to show up at the ACL one would reasonably conjecture that there are eight words separated by seven spaces."], "label": "Non-Prov", "citing": "J04-1004", "vector": [4, 0, 0, 0.0], "context": ["", "In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"decid": 1, "word": 2, "sproat": 1, "evalu": 1, "perform": 1, "hard": 1, "al": 1, "accept": 1, "standard": 1, "whether": 1, "meaning": 1, "et": 1, "extract": 1, "method": 1, "commonli": 1, "addit": 1}, "vector_2": [8, 0.4771656768380094, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework."], "label": "Non-Prov", "citing": "J04-1004", "vector": [3, 0, 0, 0.0], "context": ["", "As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al 1996), it is very difficult to compare two methods.", ""], "marker": "Sproat et al. 1996", "vector_1": {"even": 1, "differ": 2, "task": 1, "word": 1, "sproat": 1, "compar": 1, "al": 1, "text": 1, "corpora": 1, "judg": 1, "system": 2, "face": 1, "two": 1, "human": 1, "test": 1, "et": 1, "segment": 1, "method": 1, "difficult": 1}, "vector_2": [8, 0.7435816282836557, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["(a) ;IE shi4 'be' => ;IE;IE shi4bu2-shi4 (be-not-be) 'is it?'"], "label": "Non-Prov", "citing": "J04-1004", "vector": [3, 0, 0, 0.0], "context": ["", "As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al 1996), it is very difficult to compare two methods.", ""], "marker": "Sproat et al. 1996", "vector_1": {"even": 1, "differ": 2, "task": 1, "word": 1, "sproat": 1, "compar": 1, "al": 1, "text": 1, "corpora": 1, "judg": 1, "system": 2, "face": 1, "two": 1, "human": 1, "test": 1, "et": 1, "segment": 1, "method": 1, "difficult": 1}, "vector_2": [8, 0.7435816282836557, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The performance was 80.99% recall and 61.83% precision."], "label": "Non-Prov", "citing": "J04-1004", "vector": [2, 0, 0, 0.0], "context": ["", "As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al 1996), it is very difficult to compare two methods.", ""], "marker": "Sproat et al. 1996", "vector_1": {"even": 1, "differ": 2, "task": 1, "word": 1, "sproat": 1, "compar": 1, "al": 1, "text": 1, "corpora": 1, "judg": 1, "system": 2, "face": 1, "two": 1, "human": 1, "test": 1, "et": 1, "segment": 1, "method": 1, "difficult": 1}, "vector_2": [8, 0.7435816282836557, 0, 0, 8, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["JA DE G O L D G R AS S SI C K NE SS DE AT H R A T 14."], "label": "Non-Prov", "citing": "J05-4005", "vector": [1, 0, 0, 0.0], "context": ["", "A previous work along this line is Sproat et al (1996), which is based on weighted finite-state transducers (FSTs).", ""], "marker": "1996", "vector_1": {"previou": 1, "sproat": 1, "weight": 1, "work": 1, "al": 1, "finitest": 1, "base": 1, "fst": 1, "et": 1, "along": 1, "transduc": 1, "line": 1}, "vector_2": [9, 0.10361947177250078, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry."], "label": "Non-Prov", "citing": "J05-4005", "vector": [0, 0, 0, 0.0], "context": ["", "A previous work along this line is Sproat et al (1996), which is based on weighted finite-state transducers (FSTs).", ""], "marker": "1996", "vector_1": {"previou": 1, "sproat": 1, "weight": 1, "work": 1, "al": 1, "finitest": 1, "base": 1, "fst": 1, "et": 1, "along": 1, "transduc": 1, "line": 1}, "vector_2": [9, 0.10361947177250078, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["For that application, at a minimum, one would want to know the phonological word boundaries."], "label": "Non-Prov", "citing": "J05-4005", "vector": [1, 0, 0, 0.0], "context": ["", "A previous work along this line is Sproat et al (1996), which is based on weighted finite-state transducers (FSTs).", ""], "marker": "1996", "vector_1": {"previou": 1, "sproat": 1, "weight": 1, "work": 1, "al": 1, "finitest": 1, "base": 1, "fst": 1, "et": 1, "along": 1, "transduc": 1, "line": 1}, "vector_2": [9, 0.10361947177250078, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Let H be the set of hanzi, p be the set of pinyin syllables with tone marks, and P be the set of grammatical part-of-speech labels."], "label": "Non-Prov", "citing": "J05-4005", "vector": [2, 0, 0, 0.0], "context": ["", "As shown in Sproat et al (1996), the rate of agreement between two human judges is less than 80%.", ""], "marker": "1996", "vector_1": {"shown": 1, "sproat": 1, "less": 1, "judg": 1, "agreement": 1, "al": 1, "rate": 1, "two": 1, "human": 1, "et": 1}, "vector_2": [9, 0.1413108949203278, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["For example, given a sequence F1G1G2, where F1 is a legal single-hanzi family name, and Plural Nouns X g 0 g \"' X X 0 T!i c\"'."], "label": "Non-Prov", "citing": "J05-4005", "vector": [1, 0, 0, 0.0], "context": ["", "As shown in Sproat et al (1996), the rate of agreement between two human judges is less than 80%.", ""], "marker": "1996", "vector_1": {"shown": 1, "sproat": 1, "less": 1, "judg": 1, "agreement": 1, "al": 1, "rate": 1, "two": 1, "human": 1, "et": 1}, "vector_2": [9, 0.1413108949203278, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["(a) ;IE shi4 'be' => ;IE;IE shi4bu2-shi4 (be-not-be) 'is it?'"], "label": "Non-Prov", "citing": "J05-4005", "vector": [1, 0, 0, 0.0], "context": ["", "As shown in Sproat et al (1996), the rate of agreement between two human judges is less than 80%.", ""], "marker": "1996", "vector_1": {"shown": 1, "sproat": 1, "less": 1, "judg": 1, "agreement": 1, "al": 1, "rate": 1, "two": 1, "human": 1, "et": 1}, "vector_2": [9, 0.1413108949203278, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The relevance of the distinction between, say, phonological words and, say, dictionary words is shown by an example like rpftl_A :;!:Hfllil zhong1hua2 ren2min2 gong4he2-guo2 (China people republic) 'People's Republic of China.'"], "label": "Non-Prov", "citing": "J05-4005", "vector": [0, 0, 0, 0.0], "context": ["", "Similarly, Sproat et al (1996) also uses multiple human judges.", ""], "marker": "1996", "vector_1": {"use": 1, "multipl": 1, "sproat": 1, "judg": 1, "al": 1, "also": 1, "human": 1, "similarli": 1, "et": 1}, "vector_2": [9, 0.14510741599284502, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["att."], "label": "Non-Prov", "citing": "J05-4005", "vector": [0, 0, 0, 0.0], "context": ["", "Similarly, Sproat et al (1996) also uses multiple human judges.", ""], "marker": "1996", "vector_1": {"use": 1, "multipl": 1, "sproat": 1, "judg": 1, "al": 1, "also": 1, "human": 1, "similarli": 1, "et": 1}, "vector_2": [9, 0.14510741599284502, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Gan's solution depends upon a fairly sophisticated language model that attempts to find valid syntactic, semantic, and lexical relations between objects of various linguistic types (hanzi, words, phrases)."], "label": "Non-Prov", "citing": "J05-4005", "vector": [0, 0, 0, 0.0], "context": ["", "Similarly, Sproat et al (1996) also uses multiple human judges.", ""], "marker": "1996", "vector_1": {"use": 1, "multipl": 1, "sproat": 1, "judg": 1, "al": 1, "also": 1, "human": 1, "similarli": 1, "et": 1}, "vector_2": [9, 0.14510741599284502, 1, 0, 6, 1]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["Consider first the examples in (2)."], "label": "Non-Prov", "citing": "J05-4005", "vector": [2, 0, 0, 0.0], "context": ["", "The Chinese person-name model is a modified version of that described in Sproat et al (1996).", ""], "marker": "1996", "vector_1": {"personnam": 1, "describ": 1, "chines": 1, "al": 1, "version": 1, "et": 1, "model": 1, "modifi": 1, "sproat": 1}, "vector_2": [9, 0.5261010823735558, 1, 0, 6, 1]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["Our system fails in (a) because of$ shenl, a rare family name; the system identifies it as a family name, whereas it should be analyzed as part of the given name."], "label": "Non-Prov", "citing": "J05-4005", "vector": [4, 0, 0, 0.0], "context": ["", "The Chinese person-name model is a modified version of that described in Sproat et al (1996).", ""], "marker": "1996", "vector_1": {"personnam": 1, "describ": 1, "chines": 1, "al": 1, "version": 1, "et": 1, "model": 1, "modifi": 1, "sproat": 1}, "vector_2": [9, 0.5261010823735558, 1, 0, 6, 1]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["The model described here thus demonstrates great potential for use in widespread applications."], "label": "Non-Prov", "citing": "J05-4005", "vector": [4, 0, 0, 0.2222222222222222], "context": ["", "The Chinese person-name model is a modified version of that described in Sproat et al (1996).", ""], "marker": "1996", "vector_1": {"personnam": 1, "describ": 1, "chines": 1, "al": 1, "version": 1, "et": 1, "model": 1, "modifi": 1, "sproat": 1}, "vector_2": [9, 0.5261010823735558, 1, 0, 6, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In any event, to date, we have not compared different methods for deriving the set of initial frequency estimates."], "label": "Non-Prov", "citing": "J11-1005", "vector": [2, 0, 0, 0.0], "context": ["", "Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"among": 1, "shown": 1, "word": 1, "sproat": 1, "regard": 1, "agreement": 1, "al": 1, "nativ": 1, "speaker": 1, "et": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [15, 0.15350465979117084, 0, 0, 2, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Other strategies could readily 6 As a reviewer has pointed out, it should be made clear that the function for computing the best path is. an instance of the Viterbi algorithm."], "label": "Non-Prov", "citing": "J11-1005", "vector": [3, 0, 0, 0.0], "context": ["", "Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"among": 1, "shown": 1, "word": 1, "sproat": 1, "regard": 1, "agreement": 1, "al": 1, "nativ": 1, "speaker": 1, "et": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [15, 0.15350465979117084, 0, 0, 2, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The Wang, Li, and Chang system fails on fragment (b) because their system lacks the word youlyoul 'soberly' and misinterpreted the thus isolated first youl as being the final hanzi of the preceding name; similarly our system failed in fragment (h) since it is missing the abbreviation i:lJI!"], "label": "Non-Prov", "citing": "J11-1005", "vector": [3, 0, 0, 0.04499212706658476], "context": ["", "Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"among": 1, "shown": 1, "word": 1, "sproat": 1, "regard": 1, "agreement": 1, "al": 1, "nativ": 1, "speaker": 1, "et": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [15, 0.15350465979117084, 0, 0, 2, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Turning now to (1), we have the similar problem that splitting.into.ma3 'horse' andlu4 'way' is more costly than retaining this as one word .ma3lu4 'road.'"], "label": "Non-Prov", "citing": "J97-4004", "vector": [6, 0, 0, 0.04638636070427133], "context": ["", "Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al 1994; Chen and Liu 1992; Chiang et al 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al 1992; Li et al 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al 1996;", ""], "marker": "Sproat et al. 1996", "vector_1": {"shih": 1, "consider": 1, "shen": 1, "guo": 1, "eg": 1, "al": 5, "signific": 1, "blank": 1, "chen": 2, "et": 5, "tan": 1, "li": 1, "xu": 1, "xia": 1, "jie": 2, "equival": 1, "space": 1, "jin": 2, "lua": 2, "zhang": 1, "research": 1, "liu": 4, "written": 2, "liang": 2, "lai": 1, "sproat": 2, "advanc": 1, "sentenc": 1, "chines": 2, "palmer": 1, "hannan": 1, "nie": 1, "gan": 2, "fan": 1, "huang": 2, "sun": 1, "effort": 1, "sinc": 1, "made": 1, "word": 1, "delimit": 1, "tsai": 1, "explicit": 1, "focu": 1, "token": 1, "english": 1, "chiang": 1, "problem": 1, "bai": 1}, "vector_2": [1, 0.028624192059095107, 2, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The model described here thus demonstrates great potential for use in widespread applications."], "label": "Non-Prov", "citing": "J97-4004", "vector": [2, 0, 0, 0.0], "context": ["", "Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al 1994; Chen and Liu 1992; Chiang et al 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al 1992; Li et al 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al 1996;", ""], "marker": "Sproat et al. 1996", "vector_1": {"shih": 1, "consider": 1, "shen": 1, "guo": 1, "eg": 1, "al": 5, "signific": 1, "blank": 1, "chen": 2, "et": 5, "tan": 1, "li": 1, "xu": 1, "xia": 1, "jie": 2, "equival": 1, "space": 1, "jin": 2, "lua": 2, "zhang": 1, "research": 1, "liu": 4, "written": 2, "liang": 2, "lai": 1, "sproat": 2, "advanc": 1, "sentenc": 1, "chines": 2, "palmer": 1, "hannan": 1, "nie": 1, "gan": 2, "fan": 1, "huang": 2, "sun": 1, "effort": 1, "sinc": 1, "made": 1, "word": 1, "delimit": 1, "tsai": 1, "explicit": 1, "focu": 1, "token": 1, "english": 1, "chiang": 1, "problem": 1, "bai": 1}, "vector_2": [1, 0.028624192059095107, 2, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["ni2ya3 and @5:2 xilya3, respectively."], "label": "Non-Prov", "citing": "J97-4004", "vector": [1, 0, 0, 0.0], "context": ["", "Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al 1994; Chen and Liu 1992; Chiang et al 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al 1992; Li et al 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al 1996;", ""], "marker": "Sproat et al. 1996", "vector_1": {"shih": 1, "consider": 1, "shen": 1, "guo": 1, "eg": 1, "al": 5, "signific": 1, "blank": 1, "chen": 2, "et": 5, "tan": 1, "li": 1, "xu": 1, "xia": 1, "jie": 2, "equival": 1, "space": 1, "jin": 2, "lua": 2, "zhang": 1, "research": 1, "liu": 4, "written": 2, "liang": 2, "lai": 1, "sproat": 2, "advanc": 1, "sentenc": 1, "chines": 2, "palmer": 1, "hannan": 1, "nie": 1, "gan": 2, "fan": 1, "huang": 2, "sun": 1, "effort": 1, "sinc": 1, "made": 1, "word": 1, "delimit": 1, "tsai": 1, "explicit": 1, "focu": 1, "token": 1, "english": 1, "chiang": 1, "problem": 1, "bai": 1}, "vector_2": [1, 0.028624192059095107, 2, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["For example, in Northern dialects (such as Beijing), a full tone (1, 2, 3, or 4) is changed to a neutral tone (0) in the final syllable of many words: Jll donglgual 'winter melon' is often pronounced donglguaO."], "label": "Non-Prov", "citing": "J97-4004", "vector": [4, 0, 0, 0.0], "context": ["", "The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realiza tions of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al 1996;", ""], "marker": "Sproat et al. 1996", "vector_1": {"yeh": 1, "wang": 2, "guo": 1, "eg": 1, "al": 1, "procedur": 1, "correspond": 1, "et": 1, "chen": 1, "follow": 1, "sproat": 1, "jie": 2, "jin": 1, "section": 1, "definit": 1, "three": 1, "construct": 1, "liu": 3, "liang": 2, "lee": 1, "hannan": 1, "restat": 1, "wu": 1, "realiza": 1, "kit": 1, "a": 1, "wide": 1, "b": 1, "essenti": 1, "mo": 1, "descript": 1, "maximum": 1, "su": 2, "turn": 1, "token": 3, "nie": 1, "webster": 1, "tion": 1, "principl": 1}, "vector_2": [1, 0.739750003445377, 1, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Evaluation of Morphological Analysis."], "label": "Non-Prov", "citing": "J97-4004", "vector": [1, 0, 0, 0.0], "context": ["", "The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realiza tions of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al 1996;", ""], "marker": "Sproat et al. 1996", "vector_1": {"yeh": 1, "wang": 2, "guo": 1, "eg": 1, "al": 1, "procedur": 1, "correspond": 1, "et": 1, "chen": 1, "follow": 1, "sproat": 1, "jie": 2, "jin": 1, "section": 1, "definit": 1, "three": 1, "construct": 1, "liu": 3, "liang": 2, "lee": 1, "hannan": 1, "restat": 1, "wu": 1, "realiza": 1, "kit": 1, "a": 1, "wide": 1, "b": 1, "essenti": 1, "mo": 1, "descript": 1, "maximum": 1, "su": 2, "turn": 1, "token": 3, "nie": 1, "webster": 1, "tion": 1, "principl": 1}, "vector_2": [1, 0.739750003445377, 1, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words."], "label": "Non-Prov", "citing": "J97-4004", "vector": [5, 0, 1, 0.0], "context": ["", "The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realiza tions of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al 1996;", ""], "marker": "Sproat et al. 1996", "vector_1": {"yeh": 1, "wang": 2, "guo": 1, "eg": 1, "al": 1, "procedur": 1, "correspond": 1, "et": 1, "chen": 1, "follow": 1, "sproat": 1, "jie": 2, "jin": 1, "section": 1, "definit": 1, "three": 1, "construct": 1, "liu": 3, "liang": 2, "lee": 1, "hannan": 1, "restat": 1, "wu": 1, "realiza": 1, "kit": 1, "a": 1, "wide": 1, "b": 1, "essenti": 1, "mo": 1, "descript": 1, "maximum": 1, "su": 2, "turn": 1, "token": 3, "nie": 1, "webster": 1, "tion": 1, "principl": 1}, "vector_2": [1, 0.739750003445377, 1, 0, 11, 1]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["com t 600 Mountain Avenue, 2c278, Murray Hill, NJ 07974, USA."], "label": "Non-Prov", "citing": "J97-4004", "vector": [0, 0, 0, 0.0], "context": ["", "The weighted finite-state transducer model developed by Sproat et al (1996) is another excellent representative example.", ""], "marker": "1996", "vector_1": {"develop": 1, "weight": 1, "transduc": 1, "anoth": 1, "excel": 1, "al": 1, "repres": 1, "finitest": 1, "exampl": 1, "et": 1, "model": 1, "sproat": 1}, "vector_2": [1, 0.9175728008158652, 1, 0, 11, 1]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["Other kinds of productive word classes, such as company names, abbreviations (termed fijsuolxie3 in Mandarin), and place names can easily be 20 Note that 7 in E 7 is normally pronounced as leO, but as part of a resultative it is liao3.."], "label": "Non-Prov", "citing": "J97-4004", "vector": [1, 0, 0, 0.0], "context": ["", "The weighted finite-state transducer model developed by Sproat et al (1996) is another excellent representative example.", ""], "marker": "1996", "vector_1": {"develop": 1, "weight": 1, "transduc": 1, "anoth": 1, "excel": 1, "al": 1, "repres": 1, "finitest": 1, "exampl": 1, "et": 1, "model": 1, "sproat": 1}, "vector_2": [1, 0.9175728008158652, 1, 0, 11, 1]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach."], "label": "Non-Prov", "citing": "J97-4004", "vector": [2, 0, 0, 0.07216878364870323], "context": ["", "The weighted finite-state transducer model developed by Sproat et al (1996) is another excellent representative example.", ""], "marker": "1996", "vector_1": {"develop": 1, "weight": 1, "transduc": 1, "anoth": 1, "excel": 1, "al": 1, "repres": 1, "finitest": 1, "exampl": 1, "et": 1, "model": 1, "sproat": 1}, "vector_2": [1, 0.9175728008158652, 1, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["But we also need an estimate of the probability for a non-occurring though possible plural form like iJJ1l.f, nan2gua1-men0 'pumpkins.'"], "label": "Non-Prov", "citing": "J97-4004", "vector": [2, 0, 0, 0.0], "context": ["", "While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are ap parently employed neither in Sproat et al (1996) nor in Ma (1996).", ""], "marker": "1996", "vector_1": {"search": 1, "neither": 1, "parent": 1, "evalu": 1, "ap": 1, "may": 1, "gener": 1, "knowledg": 1, "al": 1, "employ": 1, "framework": 1, "et": 1, "incorpor": 1, "heurist": 1, "fulli": 1, "path": 1, "total": 1, "imposs": 1, "sproat": 1}, "vector_2": [1, 0.9342208624467689, 2, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system, but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper."], "label": "Non-Prov", "citing": "J97-4004", "vector": [7, 0, 0, 0.0], "context": ["", "While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are ap parently employed neither in Sproat et al (1996) nor in Ma (1996).", ""], "marker": "1996", "vector_1": {"search": 1, "neither": 1, "parent": 1, "evalu": 1, "ap": 1, "may": 1, "gener": 1, "knowledg": 1, "al": 1, "employ": 1, "framework": 1, "et": 1, "incorpor": 1, "heurist": 1, "fulli": 1, "path": 1, "total": 1, "imposs": 1, "sproat": 1}, "vector_2": [1, 0.9342208624467689, 2, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision."], "label": "Non-Prov", "citing": "J97-4004", "vector": [3, 0, 0, 0.0], "context": ["", "While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are ap parently employed neither in Sproat et al (1996) nor in Ma (1996).", ""], "marker": "1996", "vector_1": {"search": 1, "neither": 1, "parent": 1, "evalu": 1, "ap": 1, "may": 1, "gener": 1, "knowledg": 1, "al": 1, "employ": 1, "framework": 1, "et": 1, "incorpor": 1, "heurist": 1, "fulli": 1, "path": 1, "total": 1, "imposs": 1, "sproat": 1}, "vector_2": [1, 0.9342208624467689, 2, 0, 11, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The morphological analysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up."], "label": "Non-Prov", "citing": "N10-1068", "vector": [3, 0, 1, 0.058823529411764705], "context": ["", "Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: WFSTs provide a uniform knowledge represen tation.", ""], "marker": "Sproat et al., 1996", "vector_1": {"knowledg": 1, "transduc": 1, "weight": 1, "captur": 1, "provid": 1, "natur": 1, "offer": 1, "uniform": 1, "finitest": 1, "benefit": 1, "sever": 1, "wfst": 1, "mani": 1, "model": 1, "represen": 1, "languag": 1, "tation": 1}, "vector_2": [14, 0.03388409961685824, 6, 1, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Particular relations are also consistent with particular hypotheses about the segmentation of a given sentence, and the scores for particular relations can be incremented or decremented depending upon whether the segmentations with which they are consistent are \"popular\" or not."], "label": "Non-Prov", "citing": "N10-1068", "vector": [4, 0, 1, 0.0], "context": ["", "Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: WFSTs provide a uniform knowledge represen tation.", ""], "marker": "Sproat et al., 1996", "vector_1": {"knowledg": 1, "transduc": 1, "weight": 1, "captur": 1, "provid": 1, "natur": 1, "offer": 1, "uniform": 1, "finitest": 1, "benefit": 1, "sever": 1, "wfst": 1, "mani": 1, "model": 1, "represen": 1, "languag": 1, "tation": 1}, "vector_2": [14, 0.03388409961685824, 6, 1, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["An analysis of nouns that occur in both the singular and the plural in our database reveals that there is indeed a slight but significant positive correlation-R2 = 0.20, p < 0.005; see Figure 6."], "label": "Non-Prov", "citing": "N10-1068", "vector": [1, 0, 0, 0.0], "context": ["", "Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: WFSTs provide a uniform knowledge represen tation.", ""], "marker": "Sproat et al., 1996", "vector_1": {"knowledg": 1, "transduc": 1, "weight": 1, "captur": 1, "provid": 1, "natur": 1, "offer": 1, "uniform": 1, "finitest": 1, "benefit": 1, "sever": 1, "wfst": 1, "mani": 1, "model": 1, "represen": 1, "languag": 1, "tation": 1}, "vector_2": [14, 0.03388409961685824, 6, 1, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["A totally non stochastic rule-based system such as Wang, Li, and Chang's will generally succeed in such cases, but of course runs the risk of overgeneration wherever the single-hanzi word is really intended."], "label": "Non-Prov", "citing": "P03-1035", "vector": [3, 0, 0, 0.0], "context": ["", "One example of such approaches is Sproat et al (1996), which is based on weighted finite-state transducers (FSTs).", ""], "marker": "1996", "vector_1": {"sproat": 1, "weight": 1, "al": 1, "one": 1, "finitest": 1, "exampl": 1, "fst": 1, "base": 1, "et": 1, "transduc": 1, "approach": 1}, "vector_2": [7, 0.15690494321756118, 1, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In addition to the automatic methods, AG, GR, and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words or names)."], "label": "Non-Prov", "citing": "P03-1035", "vector": [0, 0, 0, 0.0], "context": ["", "One example of such approaches is Sproat et al (1996), which is based on weighted finite-state transducers (FSTs).", ""], "marker": "1996", "vector_1": {"sproat": 1, "weight": 1, "al": 1, "one": 1, "finitest": 1, "exampl": 1, "fst": 1, "base": 1, "et": 1, "transduc": 1, "approach": 1}, "vector_2": [7, 0.15690494321756118, 1, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X: .:.S:P:l 'How do you say octopus in Japanese?' previously shown in Figure 1."], "label": "Non-Prov", "citing": "P03-1035", "vector": [1, 0, 0, 0.0], "context": ["", "One example of such approaches is Sproat et al (1996), which is based on weighted finite-state transducers (FSTs).", ""], "marker": "1996", "vector_1": {"sproat": 1, "weight": 1, "al": 1, "one": 1, "finitest": 1, "exampl": 1, "fst": 1, "base": 1, "et": 1, "transduc": 1, "approach": 1}, "vector_2": [7, 0.15690494321756118, 1, 0, 1, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["(student+plural) 'students,' which is derived by the affixation of the plural affix f, menD to the nounxue2shengl.", "2."], "label": "Non-Prov", "citing": "P03-1035", "vector": [5, 0, 0, 0.0], "context": ["", "Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al", ""], "marker": "1996", "vector_1": {"compil": 1, "set": 1, "al": 1, "number": 1, "one": 1, "et": 1, "sproat": 1, "entiti": 2, "given": 1, "two": 1, "input": 1, "type": 2, "string": 2, "gener": 1, "use": 1, "effect": 1, "step": 1, "candid": 2, "search": 1, "name": 2, "constraint": 1, "charact": 1, "limit": 1, "principl": 1, "first": 1}, "vector_2": [7, 0.4974671390259246, 0, 0, 1, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["Again, famous place names will most likely be found in the dictionary, but less well-known names, such as 1PM R; bu4lang3-shi4wei2-ke4 'Brunswick' (as in the New Jersey town name 'New Brunswick') will not generally be found.", "In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes."], "label": "Non-Prov", "citing": "P03-1035", "vector": [6, 1, 0, 0.0], "context": ["", "Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al", ""], "marker": "1996", "vector_1": {"compil": 1, "set": 1, "al": 1, "number": 1, "one": 1, "et": 1, "sproat": 1, "entiti": 2, "given": 1, "two": 1, "input": 1, "type": 2, "string": 2, "gener": 1, "use": 1, "effect": 1, "step": 1, "candid": 2, "search": 1, "name": 2, "constraint": 1, "charact": 1, "limit": 1, "principl": 1, "first": 1}, "vector_2": [7, 0.4974671390259246, 0, 0, 1, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["example, in Northern Mandarin dialects there is a morpheme -r that attaches mostly to nouns, and which is phonologically incorporated into the syllable to which it attaches: thus men2+r (door+R) 'door' is realized as mer2.", "This is orthographically represented as 7C."], "label": "Non-Prov", "citing": "P03-1035", "vector": [5, 0, 0, 0.0], "context": ["", "Because any character strings can be in principle named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by 3 Sproat et al", ""], "marker": "1996", "vector_1": {"compil": 1, "set": 1, "al": 1, "number": 1, "one": 1, "et": 1, "sproat": 1, "entiti": 2, "given": 1, "two": 1, "input": 1, "type": 2, "string": 2, "gener": 1, "use": 1, "effect": 1, "step": 1, "candid": 2, "search": 1, "name": 2, "constraint": 1, "charact": 1, "limit": 1, "principl": 1, "first": 1}, "vector_2": [7, 0.4974671390259246, 0, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["For each pair of judges, consider one judge as the standard,."], "label": "Non-Prov", "citing": "P03-1035", "vector": [3, 0, 0, 0.0], "context": ["", "5.2.4 Transliterations of foreign names As described in Sproat et al (1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.", ""], "marker": "1996", "vector_1": {"whose": 1, "use": 1, "charact": 1, "sproat": 1, "string": 1, "chines": 1, "al": 1, "name": 2, "mimic": 1, "sourc": 1, "languag": 1, "transliter": 2, "usual": 1, "pronunci": 2, "et": 1, "foreign": 1, "sequenti": 1, "fn": 1, "describ": 1}, "vector_2": [7, 0.6243419527861471, 1, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation."], "label": "Non-Prov", "citing": "P03-1035", "vector": [3, 0, 0, 0.0468292905790847], "context": ["", "5.2.4 Transliterations of foreign names As described in Sproat et al (1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.", ""], "marker": "1996", "vector_1": {"whose": 1, "use": 1, "charact": 1, "sproat": 1, "string": 1, "chines": 1, "al": 1, "name": 2, "mimic": 1, "sourc": 1, "languag": 1, "transliter": 2, "usual": 1, "pronunci": 2, "et": 1, "foreign": 1, "sequenti": 1, "fn": 1, "describ": 1}, "vector_2": [7, 0.6243419527861471, 1, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["(a) 1  . ;m t 7 leO z h e 4 pil m a 3 lu 4 sh an g4 bi ng 4 t h i s CL (assi fier) horse w ay on sic k A SP (ec t) 'This horse got sick on the way' (b) 1: . til y zhe4 tiao2 ma3lu4 hen3 shao3 this CL road very few 'Very few cars pass by this road' :$ chel jinglguo4 car pass by 2."], "label": "Non-Prov", "citing": "P03-1035", "vector": [1, 1, 0, 0.0], "context": ["", "5.2.4 Transliterations of foreign names As described in Sproat et al (1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.", ""], "marker": "1996", "vector_1": {"whose": 1, "use": 1, "charact": 1, "sproat": 1, "string": 1, "chines": 1, "al": 1, "name": 2, "mimic": 1, "sourc": 1, "languag": 1, "transliter": 2, "usual": 1, "pronunci": 2, "et": 1, "foreign": 1, "sequenti": 1, "fn": 1, "describ": 1}, "vector_2": [7, 0.6243419527861471, 1, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993)."], "label": "Non-Prov", "citing": "P06-1126", "vector": [1, 0, 0, 0.0], "context": ["", "Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).", ""], "marker": "Sproat et al., 1996", "vector_1": {"task": 1, "word": 1, "attent": 1, "process": 1, "chines": 2, "literatur": 1, "initi": 1, "receiv": 1, "lot": 1, "mani": 1, "segment": 1, "languag": 1, "stage": 1}, "vector_2": [10, 0.04702157134375618, 4, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["(a) ;IE shi4 'be' => ;IE;IE shi4bu2-shi4 (be-not-be) 'is it?'"], "label": "Non-Prov", "citing": "P06-1126", "vector": [2, 0, 0, 0.0], "context": ["", "Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).", ""], "marker": "Sproat et al., 1996", "vector_1": {"task": 1, "word": 1, "attent": 1, "process": 1, "chines": 2, "literatur": 1, "initi": 1, "receiv": 1, "lot": 1, "mani": 1, "segment": 1, "languag": 1, "stage": 1}, "vector_2": [10, 0.04702157134375618, 4, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words."], "label": "Non-Prov", "citing": "P07-1016", "vector": [5, 0, 1, 0.0], "context": ["", "As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.", ""], "marker": "Sproat et al., 1996", "vector_1": {"subset": 1, "corpu": 1, "use": 1, "adopt": 1, "name": 1, "eg": 1, "overwhelmingli": 1, "chines": 3, "elsewher": 1, "tend": 1, "ec": 1, "discuss": 1, "common": 1, "transliter": 1, "english": 1, "charact": 3, "hundr": 1, "thousand": 1, "sever": 1}, "vector_2": [11, 0.3490667382838727, 1, 33, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Affix Pron Base category N found N missed (recall) N correct (precision) t,-,7 The second issue is that rare family names can be responsible for overgeneration, especially if these names are otherwise common as single-hanzi words."], "label": "Non-Prov", "citing": "P07-1016", "vector": [7, 0, 0, 0.08696565534786725], "context": ["", "As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.", ""], "marker": "Sproat et al., 1996", "vector_1": {"subset": 1, "corpu": 1, "use": 1, "adopt": 1, "name": 1, "eg": 1, "overwhelmingli": 1, "chines": 3, "elsewher": 1, "tend": 1, "ec": 1, "discuss": 1, "common": 1, "transliter": 1, "english": 1, "charact": 3, "hundr": 1, "thousand": 1, "sever": 1}, "vector_2": [11, 0.3490667382838727, 1, 33, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["While the semantic aspect of radicals is by no means completely predictive, the semantic homogeneity of many classes is quite striking: for example 254 out of the 263 examples (97%) of the INSECT class listed by Wieger (1965, 77376) denote crawling or invertebrate animals; similarly 21 out of the 22 examples (95%) of the GHOST class (page 808) denote ghosts or spirits."], "label": "Non-Prov", "citing": "P07-1016", "vector": [4, 0, 1, 0.0], "context": ["", "As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.", ""], "marker": "Sproat et al., 1996", "vector_1": {"subset": 1, "corpu": 1, "use": 1, "adopt": 1, "name": 1, "eg": 1, "overwhelmingli": 1, "chines": 3, "elsewher": 1, "tend": 1, "ec": 1, "discuss": 1, "common": 1, "transliter": 1, "english": 1, "charact": 3, "hundr": 1, "thousand": 1, "sever": 1}, "vector_2": [11, 0.3490667382838727, 1, 33, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["These are shown, with their associated costs, as follows: ABj nc 4.0 AB C/jj 6.0 CD /vb 5.", "0 D/ nc 5.0 The minimal dictionary encoding this information is represented by the WFST in Figure 2(a)."], "label": "Non-Prov", "citing": "P12-1111", "vector": [3, 0, 0, 0.0], "context": ["", "In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"earli": 1, "rulebas": 1, "word": 1, "work": 1, "maximum": 1, "one": 2, "base": 1, "heurist": 1, "forward": 1, "model": 1, "find": 1, "match": 1}, "vector_2": [16, 0.40864119391052334, 1, 1, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["gaolgaolxing4xing4 'happily' In the particular form of A-not-A reduplication illustrated in (3a), the first syllable of the verb is copied, and the negative markerbu4 'not' is inserted between the copy and the full verb.", "In the case of adverbial reduplication illustrated in (3b) an adjective of the form AB is reduplicated as AABB."], "label": "Non-Prov", "citing": "P12-1111", "vector": [2, 0, 0, 0.0], "context": ["", "In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"earli": 1, "rulebas": 1, "word": 1, "work": 1, "maximum": 1, "one": 2, "base": 1, "heurist": 1, "forward": 1, "model": 1, "find": 1, "match": 1}, "vector_2": [16, 0.40864119391052334, 1, 1, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["While Gan's system incorporates fairly sophisticated models of various linguistic information, it has the drawback that it has only been tested with a very small lexicon (a few hundred words) and on a very small test set (thirty sentences); there is therefore serious concern as to whether the methods that he discusses are scalable.", "Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models, and therefore could be directly interfaced with the segmentation model that we have presented in this paper."], "label": "Non-Prov", "citing": "P12-1111", "vector": [5, 0, 0, 0.09999999999999999], "context": ["", "In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"earli": 1, "rulebas": 1, "word": 1, "work": 1, "maximum": 1, "one": 2, "base": 1, "heurist": 1, "forward": 1, "model": 1, "find": 1, "match": 1}, "vector_2": [16, 0.40864119391052334, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In the numerator, however, the counts of ni1s are quite irregular, in cluding several zeros (e.g., RAT, none of whose members were seen).", "However, there is a strong relationship between ni1s and the number of hanzi in the class."], "label": "Non-Prov", "citing": "P97-1041", "vector": [2, 0, 0, 0.0], "context": ["", "For a discussion of recent Chinese segmentation work, see Sproat et al {1996).", ""], "marker": "1996", "vector_1": {"sproat": 1, "chines": 1, "work": 1, "al": 1, "see": 1, "et": 1, "segment": 1, "discuss": 1, "recent": 1}, "vector_2": [1, 0.05553015649727448, 0, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.", "Several papers report the use of part-of-speech information to rank segmentations (Lin, Chiang, and Su 1993; Peng and Chang 1993; Chang and Chen 1993); typically, the probability of a segmentation is multiplied by the probability of the tagging(s) for that segmentation to yield an estimate of the total probability for the analysis."], "label": "Non-Prov", "citing": "P97-1041", "vector": [4, 0, 0, 0.0839921051131616], "context": ["", "For a discussion of recent Chinese segmentation work, see Sproat et al {1996).", ""], "marker": "1996", "vector_1": {"sproat": 1, "chines": 1, "work": 1, "al": 1, "see": 1, "et": 1, "segment": 1, "discuss": 1, "recent": 1}, "vector_2": [1, 0.05553015649727448, 0, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The most accurate characterization of Chinese writing is that it is morphosyllabic (DeFrancis 1984): each hanzi represents one morpheme lexically and semantically, and one syllable phonologi cally.", "Thus in a two-hanzi word like lflli?J zhong1guo2 (middle country) 'China' there are two syllables, and at the same time two morphemes."], "label": "Non-Prov", "citing": "P97-1041", "vector": [3, 0, 0, 0.057166195047502935], "context": ["", "For a discussion of recent Chinese segmentation work, see Sproat et al {1996).", ""], "marker": "1996", "vector_1": {"sproat": 1, "chines": 1, "work": 1, "al": 1, "see": 1, "et": 1, "segment": 1, "discuss": 1, "recent": 1}, "vector_2": [1, 0.05553015649727448, 0, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Given names are most commonly two hanzi long, occasionally one hanzi long: there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following: 1."], "label": "Non-Prov", "citing": "P97-1041", "vector": [0, 0, 0, 0.0], "context": ["", "It is rule-based, but relies on 2 See, for example, Sproat et al (1996)", ""], "marker": "1996", "vector_1": {"sproat": 1, "rulebas": 1, "see": 1, "al": 1, "reli": 1, "exampl": 1, "et": 1}, "vector_2": [1, 0.20601371549147177, 1, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["newspaper material, but also including kungfu fiction, Buddhist tracts, and scientific material."], "label": "Non-Prov", "citing": "P97-1041", "vector": [1, 0, 0, 0.0], "context": ["", "It is rule-based, but relies on 2 See, for example, Sproat et al (1996)", ""], "marker": "1996", "vector_1": {"sproat": 1, "rulebas": 1, "see": 1, "al": 1, "reli": 1, "exampl": 1, "et": 1}, "vector_2": [1, 0.20601371549147177, 1, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["f, nan2gual+men0 'pumpkins' is by no means impossible."], "label": "Non-Prov", "citing": "P97-1041", "vector": [1, 1, 0, 0.0], "context": ["", "It is rule-based, but relies on 2 See, for example, Sproat et al (1996)", ""], "marker": "1996", "vector_1": {"sproat": 1, "rulebas": 1, "see": 1, "al": 1, "reli": 1, "exampl": 1, "et": 1}, "vector_2": [1, 0.20601371549147177, 1, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["(See also Wu and Fung [1994].)", "Various segmentation approaches were then compared with human performance: 1."], "label": "Non-Prov", "citing": "P98-1076", "vector": [0, 0, 0, 0.0], "context": ["", "The actual implementation of the weighted finite state transducer by Sproat et al (1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.", ""], "marker": "1996", "vector_1": {"use": 1, "sourc": 1, "actual": 1, "weight": 1, "transduc": 1, "evid": 1, "practic": 1, "hypothesi": 1, "finit": 1, "per": 1, "al": 1, "state": 1, "alreadi": 1, "token": 1, "taken": 1, "et": 1, "implement": 1, "one": 1, "sproat": 1}, "vector_2": [2, 0.8384123022422608, 1, 0, 3, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["However, some caveats are in order in comparing this method (or any method) with other approaches to seg mentation reported in the literature.", "First of all, most previous articles report perfor mance in terms of a single percent-correct score, or else in terms of the paired measures of precision and recall."], "label": "Non-Prov", "citing": "P98-1076", "vector": [3, 0, 1, 0.0], "context": ["", "The actual implementation of the weighted finite state transducer by Sproat et al (1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.", ""], "marker": "1996", "vector_1": {"use": 1, "sourc": 1, "actual": 1, "weight": 1, "transduc": 1, "evid": 1, "practic": 1, "hypothesi": 1, "finit": 1, "per": 1, "al": 1, "state": 1, "alreadi": 1, "token": 1, "taken": 1, "et": 1, "implement": 1, "one": 1, "sproat": 1}, "vector_2": [2, 0.8384123022422608, 1, 0, 3, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["30 16.", "42 nator, the N31s can be measured well by counting, and we replace the expectation by the observation."], "label": "Non-Prov", "citing": "P98-1076", "vector": [4, 0, 1, 0.0], "context": ["", "The actual implementation of the weighted finite state transducer by Sproat et al (1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.", ""], "marker": "1996", "vector_1": {"use": 1, "sourc": 1, "actual": 1, "weight": 1, "transduc": 1, "evid": 1, "practic": 1, "hypothesi": 1, "finit": 1, "per": 1, "al": 1, "state": 1, "alreadi": 1, "token": 1, "taken": 1, "et": 1, "implement": 1, "one": 1, "sproat": 1}, "vector_2": [2, 0.8384123022422608, 1, 0, 3, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Of course, we."], "label": "Non-Prov", "citing": "P98-1076", "vector": [0, 0, 0, 0.0], "context": ["", "utilizing local and sentential constraints, what Sproat et al ( 1996) implemented was simply a token unigram scoring function.", ""], "marker": "1996", "vector_1": {"function": 1, "unigram": 1, "simpli": 1, "sproat": 1, "constraint": 1, "sententi": 1, "al": 1, "util": 1, "token": 1, "score": 1, "et": 1, "implement": 1, "local": 1}, "vector_2": [2, 0.8644579525783986, 0, 0, 3, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["3."], "label": "Non-Prov", "citing": "P98-1076", "vector": [0, 0, 0, 0.0], "context": ["", "utilizing local and sentential constraints, what Sproat et al ( 1996) implemented was simply a token unigram scoring function.", ""], "marker": "1996", "vector_1": {"function": 1, "unigram": 1, "simpli": 1, "sproat": 1, "constraint": 1, "sententi": 1, "al": 1, "util": 1, "token": 1, "score": 1, "et": 1, "implement": 1, "local": 1}, "vector_2": [2, 0.8644579525783986, 0, 0, 3, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["However, until such standards are universally adopted in evaluating Chinese segmenters, claims about performance in terms of simple measures like percent correct should be taken with a grain of salt; see, again, Wu and Fung (1994) for further arguments supporting this conclusion."], "label": "Non-Prov", "citing": "P98-1076", "vector": [2, 0, 0, 0.0], "context": ["", "utilizing local and sentential constraints, what Sproat et al ( 1996) implemented was simply a token unigram scoring function.", ""], "marker": "1996", "vector_1": {"function": 1, "unigram": 1, "simpli": 1, "sproat": 1, "constraint": 1, "sententi": 1, "al": 1, "util": 1, "token": 1, "score": 1, "et": 1, "implement": 1, "local": 1}, "vector_2": [2, 0.8644579525783986, 0, 0, 3, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["For example, suppose one is building a ITS system for Mandarin Chinese."], "label": "Non-Prov", "citing": "W00-0803", "vector": [1, 0, 0, 0.10101525445522107], "context": ["", "Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).", ""], "marker": "Sproat et al., 1996", "vector_1": {"analysi": 1, "issu": 1, "elsewher": 1, "matsuiit": 1, "japanes": 1, "chines": 1, "relat": 1, "morpholog": 1, "intens": 1, "rutd": 1, "mani": 1, "segment": 1, "other": 1, "address": 1}, "vector_2": [4, 0.16910883433210902, 2, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Finally, we model the probability of a new transliterated name as the product of PTN and PTN(hanzi;) for each hanzi; in the putative name.13 The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morpho 13 The current model is too simplistic in several respects."], "label": "Non-Prov", "citing": "W00-0803", "vector": [2, 0, 0, 0.0], "context": ["", "Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).", ""], "marker": "Sproat et al., 1996", "vector_1": {"analysi": 1, "issu": 1, "elsewher": 1, "matsuiit": 1, "japanes": 1, "chines": 1, "relat": 1, "morpholog": 1, "intens": 1, "rutd": 1, "mani": 1, "segment": 1, "other": 1, "address": 1}, "vector_2": [4, 0.16910883433210902, 2, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Finally, quite a few hanzi are homographs, meaning that they may be pronounced in several different ways, and in extreme cases apparently represent different morphemes: The prenominal modifi cation marker eg deO is presumably a different morpheme from the second morpheme of eg mu4di4, even though they are written the same way.4 The second point, which will be relevant in the discussion of personal names in Section 4.4, relates to the internal structure of hanzi."], "label": "Non-Prov", "citing": "W00-0803", "vector": [3, 0, 0, 0.0], "context": ["", "Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).", ""], "marker": "Sproat et al., 1996", "vector_1": {"analysi": 1, "issu": 1, "elsewher": 1, "matsuiit": 1, "japanes": 1, "chines": 1, "relat": 1, "morpholog": 1, "intens": 1, "rutd": 1, "mani": 1, "segment": 1, "other": 1, "address": 1}, "vector_2": [4, 0.16910883433210902, 2, 5, 0, 0]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion \"word\" has never played a role in Chinese philological tradition, and the idea that Chinese lacks any thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).", "Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view."], "label": "Non-Prov", "citing": "W00-1207", "vector": [9, 0, 1, 0.04192627457812106], "context": ["", "Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.", ""], "marker": "Sproat et al 1996", "vector_1": {"identifi": 1, "often": 1, "text": 1, "eg": 1, "al": 4, "fail": 1, "et": 4, "data": 1, "lee": 1, "likelihood": 1, "appear": 1, "segment": 1, "lin": 1, "lua": 1, "low": 1, "pure": 1, "method": 1, "marcken": 1, "spars": 1, "sproat": 1, "de": 1, "train": 1, "huang": 1, "tung": 1, "word": 3, "etc": 1, "statist": 1, "chiang": 1, "extrem": 1, "problem": 1}, "vector_2": [4, 0.09915192126478903, 2, 0, 0, 0]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["In the pinyin transliterations a dash(-) separates syllables that may be considered part of the same phonological word; spaces are used to separate plausible phonological words; and a plus sign (+) is used, where relevant, to indicate morpheme boundaries of interest.", "raphy: A ren2 'person' is a fairly uncontroversial case of a monographemic word, and rplil zhong1guo2 (middle country) 'China' a fairly uncontroversial case of a di graphernic word."], "label": "Non-Prov", "citing": "W00-1207", "vector": [8, 0, 2, 0.08136806936301492], "context": ["", "Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.", ""], "marker": "Sproat et al 1996", "vector_1": {"identifi": 1, "often": 1, "text": 1, "eg": 1, "al": 4, "fail": 1, "et": 4, "data": 1, "lee": 1, "likelihood": 1, "appear": 1, "segment": 1, "lin": 1, "lua": 1, "low": 1, "pure": 1, "method": 1, "marcken": 1, "spars": 1, "sproat": 1, "de": 1, "train": 1, "huang": 1, "tung": 1, "word": 3, "etc": 1, "statist": 1, "chiang": 1, "extrem": 1, "problem": 1}, "vector_2": [4, 0.09915192126478903, 2, 0, 0, 0]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["0 X u} \"' o; .2 X X><X X XX X X X X X X x X X X X X x X V X X X X .;t'*- XXX:OX X X X X X X 9 x X X XX XX X X X X X X X XXX:< X X>O<XX>!KXX XI<>< C X X XX :X: X X \"' X X XX >OO<X>D<XIK X X X X X X --XX: XXX X XC X XX...C:XXX X Xll< X X ><XX>IIC:liiC:oiiiiCI--8!X:liiOC!I!S8K X X X 10 100 1000 10000 log(F)_base: R\"2=0.20 (p < 0.005) X 100000 Figure 6 Plot of log frequency of base noun, against log frequency of plural nouns.", "G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of:  the probability that a word chosen randomly from a text will be a name-p(rule 1), and  the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and  the probability that the family name is the particular hanzi F1-p(rule 6), and  the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al."], "label": "Non-Prov", "citing": "W00-1207", "vector": [9, 0, 3, 0.018319948827976183], "context": ["", "Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.", ""], "marker": "Sproat et al 1996", "vector_1": {"identifi": 1, "often": 1, "text": 1, "eg": 1, "al": 4, "fail": 1, "et": 4, "data": 1, "lee": 1, "likelihood": 1, "appear": 1, "segment": 1, "lin": 1, "lua": 1, "low": 1, "pure": 1, "method": 1, "marcken": 1, "spars": 1, "sproat": 1, "de": 1, "train": 1, "huang": 1, "tung": 1, "word": 3, "etc": 1, "statist": 1, "chiang": 1, "extrem": 1, "problem": 1}, "vector_2": [4, 0.09915192126478903, 2, 0, 0, 0]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.", "As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis."], "label": "Non-Prov", "citing": "W02-1117", "vector": [6, 0, 0, 0.03140371465106639], "context": ["", "For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al 1996; Gu and Mao 1994; Li et al 1991; Wang et al 1991b; Wang et al 1990].", ""], "marker": "Sproat et al. 1996", "vector_1": {"gu": 1, "wang": 2, "al": 4, "one": 1, "minimum": 1, "et": 4, "ambigu": 1, "find": 1, "sproat": 1, "use": 1, "li": 1, "forward": 1, "method": 2, "match": 3, "string": 3, "obtain": 1, "mao": 1, "b": 1, "word": 2, "possibl": 1, "guo": 1, "maximum": 2, "charact": 1, "exampl": 1, "problem": 1, "backward": 1, "resolv": 1}, "vector_2": [6, 0.15557105840143512, 0, 0, 1, 1]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["constitute names, since we have only their segmentation, not the actual classification of the segmented words.", "Therefore in cases where the segmentation is identical between the two systems we assume that tagging is also identical."], "label": "Non-Prov", "citing": "W02-1117", "vector": [3, 0, 0, 0.024140227479263372], "context": ["", "For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al 1996; Gu and Mao 1994; Li et al 1991; Wang et al 1991b; Wang et al 1990].", ""], "marker": "Sproat et al. 1996", "vector_1": {"gu": 1, "wang": 2, "al": 4, "one": 1, "minimum": 1, "et": 4, "ambigu": 1, "find": 1, "sproat": 1, "use": 1, "li": 1, "forward": 1, "method": 2, "match": 3, "string": 3, "obtain": 1, "mao": 1, "b": 1, "word": 2, "possibl": 1, "guo": 1, "maximum": 2, "charact": 1, "exampl": 1, "problem": 1, "backward": 1, "resolv": 1}, "vector_2": [6, 0.15557105840143512, 0, 0, 1, 1]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["Precision.", "For each pair of judges consider one judge as the standard,."], "label": "Non-Prov", "citing": "W02-1117", "vector": [3, 0, 0, 0.04279604925109128], "context": ["", "For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al 1996; Gu and Mao 1994; Li et al 1991; Wang et al 1991b; Wang et al 1990].", ""], "marker": "Sproat et al. 1996", "vector_1": {"gu": 1, "wang": 2, "al": 4, "one": 1, "minimum": 1, "et": 4, "ambigu": 1, "find": 1, "sproat": 1, "use": 1, "li": 1, "forward": 1, "method": 2, "match": 3, "string": 3, "obtain": 1, "mao": 1, "b": 1, "word": 2, "possibl": 1, "guo": 1, "maximum": 2, "charact": 1, "exampl": 1, "problem": 1, "backward": 1, "resolv": 1}, "vector_2": [6, 0.15557105840143512, 0, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The performance of our system on those sentences ap peared rather better than theirs.", "On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.", "However, they list two sets, one consisting of 28 fragments and the other of 22 fragments, in which they had 0% recall and precision."], "label": "Non-Prov", "citing": "W02-1808", "vector": [3, 0, 0, 0.0], "context": ["", "Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)", ""], "marker": "1996", "vector_1": {"involv": 1, "el": 1, "sproat": 1, "show": 1, "tsai": 1, "corpora": 1, "al": 3, "one": 1, "finitest": 1, "train": 1, "fan": 1, "statist": 1, "mostli": 1, "et": 3, "chiang": 1, "largescal": 1, "chang": 1, "approach": 1, "languag": 1, "mod": 1}, "vector_2": [6, 0.1929284750337382, 4, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["For the examples given in (1) and (2) this certainly seems possible.", "Consider first the examples in (2).", "The segmenter will give both analyses :1 cai2 neng2 'just be able,' and ?]cai2neng2 'talent,' but the latter analysis is preferred since splitting these two morphemes is generally more costly than grouping them."], "label": "Non-Prov", "citing": "W02-1808", "vector": [2, 0, 0, 0.0], "context": ["", "Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)", ""], "marker": "1996", "vector_1": {"involv": 1, "el": 1, "sproat": 1, "show": 1, "tsai": 1, "corpora": 1, "al": 3, "one": 1, "finitest": 1, "train": 1, "fan": 1, "statist": 1, "mostli": 1, "et": 3, "chiang": 1, "largescal": 1, "chang": 1, "approach": 1, "languag": 1, "mod": 1}, "vector_2": [6, 0.1929284750337382, 4, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["gaolgaolxing4xing4 'happily' In the particular form of A-not-A reduplication illustrated in (3a), the first syllable of the verb is copied, and the negative markerbu4 'not' is inserted between the copy and the full verb.", "In the case of adverbial reduplication illustrated in (3b) an adjective of the form AB is reduplicated as AABB.", "The only way to handle such phenomena within the framework described here is simply to expand out the reduplicated forms beforehand, and incorporate the expanded forms into the lexical transducer."], "label": "Non-Prov", "citing": "W02-1808", "vector": [3, 0, 0, 0.0], "context": ["", "Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996)", ""], "marker": "1996", "vector_1": {"involv": 1, "el": 1, "sproat": 1, "show": 1, "tsai": 1, "corpora": 1, "al": 3, "one": 1, "finitest": 1, "train": 1, "fan": 1, "statist": 1, "mostli": 1, "et": 3, "chiang": 1, "largescal": 1, "chang": 1, "approach": 1, "languag": 1, "mod": 1}, "vector_2": [6, 0.1929284750337382, 4, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["An examination of the subjects' bracketings confirmed that these instructions were satisfactory in yielding plausible word-sized units."], "label": "Non-Prov", "citing": "W03-1728", "vector": [4, 0, 0, 0.0], "context": ["", "This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).", ""], "marker": "Sproat et al., 1996", "vector_1": {"sound": 1, "identifi": 1, "drawn": 1, "may": 1, "chines": 2, "commun": 1, "research": 1, "bodi": 1, "process": 1, "enough": 1, "larg": 1, "nontrivi": 1, "word": 1, "problem": 1, "simpl": 1, "languag": 1, "realiti": 1}, "vector_2": [7, 0.05625115080095747, 5, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Our System Wang, Li, and Chang a. 1\\!f!IP Eflltii /1\\!f!J:P $1til I b. agm: I a m: c. 5 Bf is Bf 1 d. \"*:t: w _t ff 1 \"* :t: w_tff 1 g., , Transliteration/Translation chen2zhongl-shenl qu3 'music by Chen Zhongshen ' huang2rong2 youlyoul de dao4 'Huang Rong said soberly' zhangl qun2 Zhang Qun xian4zhang3 you2qingl shang4ren2 hou4 'after the county president You Qing had assumed the position' lin2 quan2 'Lin Quan' wang2jian4 'Wang Jian' oulyang2-ke4 'Ouyang Ke' yinl qi2 bu4 ke2neng2 rong2xu3 tai2du2 er2 'because it cannot permit Taiwan Independence so' silfa3-yuan4zhang3 lin2yang2-gang3 'president of the Judicial Yuan, Lin Yanggang' lin2zhangl-hu2 jiangl zuo4 xian4chang3 jie3shuol 'Lin Zhanghu will give an ex planation live' jin4/iang3 nian2 nei4 sa3 xia4 de jinlqian2 hui4 ting2zhi3 'in two years the distributed money will stop' gaoltangl da4chi2 ye1zi0 fen3 'chicken stock, a tablespoon of coconut flakes' you2qingl ru4zhu3 xian4fu3 lwu4 'after You Qing headed the county government' Table 5 Performance on morphological analysis."], "label": "Non-Prov", "citing": "W03-1728", "vector": [5, 0, 0, 0.0], "context": ["", "This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).", ""], "marker": "Sproat et al., 1996", "vector_1": {"sound": 1, "identifi": 1, "drawn": 1, "may": 1, "chines": 2, "commun": 1, "research": 1, "bodi": 1, "process": 1, "enough": 1, "larg": 1, "nontrivi": 1, "word": 1, "problem": 1, "simpl": 1, "languag": 1, "realiti": 1}, "vector_2": [7, 0.05625115080095747, 5, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to."], "label": "Non-Prov", "citing": "W03-1728", "vector": [5, 0, 0, 0.1860521018838127], "context": ["", "This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).", ""], "marker": "Sproat et al., 1996", "vector_1": {"sound": 1, "identifi": 1, "drawn": 1, "may": 1, "chines": 2, "commun": 1, "research": 1, "bodi": 1, "process": 1, "enough": 1, "larg": 1, "nontrivi": 1, "word": 1, "problem": 1, "simpl": 1, "languag": 1, "realiti": 1}, "vector_2": [7, 0.05625115080095747, 5, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["For example, hanzi containing the INSECT radical !R tend to denote insects and other crawling animals; examples include tr wal 'frog,' feng1 'wasp,' and !Itt she2 'snake.'"], "label": "Non-Prov", "citing": "W04-3236", "vector": [0, 0, 0, 0.0], "context": ["", "Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "process": 1, "chines": 1, "previou": 1, "focus": 1, "research": 1, "much": 1, "segment": 1, "languag": 1}, "vector_2": [8, 0.9186552724927443, 3, 1, 2, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The best analysis of the corpus is taken to be the true analysis, the frequencies are re-estimated, and the algorithm is repeated until it converges."], "label": "Non-Prov", "citing": "W04-3236", "vector": [0, 0, 0, 0.0], "context": ["", "Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "process": 1, "chines": 1, "previou": 1, "focus": 1, "research": 1, "much": 1, "segment": 1, "languag": 1}, "vector_2": [8, 0.9186552724927443, 3, 1, 2, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The particular classifier used depends upon the noun."], "label": "Non-Prov", "citing": "W04-3236", "vector": [0, 0, 0, 0.0], "context": ["", "Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "process": 1, "chines": 1, "previou": 1, "focus": 1, "research": 1, "much": 1, "segment": 1, "languag": 1}, "vector_2": [8, 0.9186552724927443, 3, 1, 2, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["so that 'door' would be and in this case the hanzi 7C, does not represent a syllable."], "label": "Non-Prov", "citing": "W05-0709", "vector": [4, 0, 0, 0.0], "context": ["", "In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 1, "charact": 1, "word": 1, "segment": 1, "chines": 1, "upon": 2, "stem": 1, "also": 1, "ngram": 1, "base": 2, "dictionari": 1, "model": 2, "experi": 1, "similar": 1, "addit": 1}, "vector_2": [9, 0.3646044293460508, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Two issues distinguish the various proposals."], "label": "Non-Prov", "citing": "W05-0709", "vector": [1, 0, 0, 0.0], "context": ["", "In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 1, "charact": 1, "word": 1, "segment": 1, "chines": 1, "upon": 2, "stem": 1, "also": 1, "ngram": 1, "base": 2, "dictionari": 1, "model": 2, "experi": 1, "similar": 1, "addit": 1}, "vector_2": [9, 0.3646044293460508, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Particular instances of relations are associated with goodness scores."], "label": "Non-Prov", "citing": "W05-0709", "vector": [2, 0, 0, 0.0], "context": ["", "In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 1, "charact": 1, "word": 1, "segment": 1, "chines": 1, "upon": 2, "stem": 1, "also": 1, "ngram": 1, "base": 2, "dictionari": 1, "model": 2, "experi": 1, "similar": 1, "addit": 1}, "vector_2": [9, 0.3646044293460508, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin ning) of the sentence is reached."], "label": "Non-Prov", "citing": "W06-1630", "vector": [10, 0, 1, 0.03857583749052298], "context": ["", "The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"handdevelop": 1, "suffix": 1, "cc": 1, "affix": 1, "foreign": 1, "extract": 1, "use": 3, "end": 1, "three": 1, "hindi": 1, "way": 1, "simpl": 1, "chines": 1, "contrast": 1, "gener": 1, "given": 1, "stem": 1, "candid": 2, "c": 10, "word": 3, "name": 1, "possibl": 2, "list": 3, "charact": 1, "exampl": 1, "singl": 1, "frequent": 1}, "vector_2": [10, 0.6469784085883157, 1, 1, 0, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["There are two weaknesses in Chang et al.'s model, which we improve upon."], "label": "Non-Prov", "citing": "W06-1630", "vector": [2, 0, 0, 0.0], "context": ["", "The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"handdevelop": 1, "suffix": 1, "cc": 1, "affix": 1, "foreign": 1, "extract": 1, "use": 3, "end": 1, "three": 1, "hindi": 1, "way": 1, "simpl": 1, "chines": 1, "contrast": 1, "gener": 1, "given": 1, "stem": 1, "candid": 2, "c": 10, "word": 3, "name": 1, "possibl": 2, "list": 3, "charact": 1, "exampl": 1, "singl": 1, "frequent": 1}, "vector_2": [10, 0.6469784085883157, 1, 1, 0, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In the numerator, however, the counts of ni1s are quite irregular, in cluding several zeros (e.g., RAT, none of whose members were seen)."], "label": "Non-Prov", "citing": "W06-1630", "vector": [5, 1, 0, 0.0], "context": ["", "The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"handdevelop": 1, "suffix": 1, "cc": 1, "affix": 1, "foreign": 1, "extract": 1, "use": 3, "end": 1, "three": 1, "hindi": 1, "way": 1, "simpl": 1, "chines": 1, "contrast": 1, "gener": 1, "given": 1, "stem": 1, "candid": 2, "c": 10, "word": 3, "name": 1, "possibl": 2, "list": 3, "charact": 1, "exampl": 1, "singl": 1, "frequent": 1}, "vector_2": [10, 0.6469784085883157, 1, 1, 0, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The first concerns how to deal with ambiguities in segmentation."], "label": "Non-Prov", "citing": "W12-1011", "vector": [2, 0, 0, 0.0], "context": ["", "Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"even": 1, "word": 1, "boundari": 1, "modern": 1, "inde": 1, "nativ": 1, "speaker": 1, "time": 1, "agre": 1, "chines": 1}, "vector_2": [16, 0.19000328839197633, 1, 2, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["We of course also fail to identify, by the methods just described, given names used without their associated family name."], "label": "Non-Prov", "citing": "W12-1011", "vector": [2, 0, 0, 0.0], "context": ["", "Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"even": 1, "word": 1, "boundari": 1, "modern": 1, "inde": 1, "nativ": 1, "speaker": 1, "time": 1, "agre": 1, "chines": 1}, "vector_2": [16, 0.19000328839197633, 1, 2, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["We have not to date explored these various options."], "label": "Non-Prov", "citing": "W12-1011", "vector": [0, 0, 0, 0.0], "context": ["", "Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"even": 1, "word": 1, "boundari": 1, "modern": 1, "inde": 1, "nativ": 1, "speaker": 1, "time": 1, "agre": 1, "chines": 1}, "vector_2": [16, 0.19000328839197633, 1, 2, 1, 0]}, {"function": "CoCo", "cited": "J96-3004", "provenance": ["Figure 4 Input lattice (top) and two segmentations (bottom) of the sentence 'How do you say octopus in Japanese?'."], "label": "Non-Prov", "citing": "W12-1011", "vector": [3, 0, 0, 0.05892556509887897], "context": ["", "No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "compar": 2, "classic": 1, "chines": 2, "modern": 1, "rate": 2, "agreement": 1, "past": 1, "favor": 1, "figur": 1, "human": 1, "report": 1, "attempt": 1, "inter": 1, "segment": 1, "eg": 1, "averag": 1}, "vector_2": [16, 0.8573495560670832, 1, 2, 1, 0]}, {"function": "CoCo", "cited": "J96-3004", "provenance": ["For the examples given in (1) and (2) this certainly seems possible."], "label": "Non-Prov", "citing": "W12-1011", "vector": [3, 0, 0, 0.0], "context": ["", "No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "compar": 2, "classic": 1, "chines": 2, "modern": 1, "rate": 2, "agreement": 1, "past": 1, "favor": 1, "figur": 1, "human": 1, "report": 1, "attempt": 1, "inter": 1, "segment": 1, "eg": 1, "averag": 1}, "vector_2": [16, 0.8573495560670832, 1, 2, 1, 0]}, {"function": "CoCo", "cited": "J96-3004", "provenance": ["The simplest approach involves scoring the various analyses by costs based on word frequency, and picking the lowest cost path; variants of this approach have been described in Chang, Chen, and Chen (1991) and Chang and Chen (1993)."], "label": "Non-Prov", "citing": "W12-1011", "vector": [5, 0, 0, 0.036084391824351615], "context": ["", "No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "compar": 2, "classic": 1, "chines": 2, "modern": 1, "rate": 2, "agreement": 1, "past": 1, "favor": 1, "figur": 1, "human": 1, "report": 1, "attempt": 1, "inter": 1, "segment": 1, "eg": 1, "averag": 1}, "vector_2": [16, 0.8573495560670832, 1, 2, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["As we shall argue, the semantic class affiliation of a hanzi constitutes useful information in predicting its properties."], "label": "Non-Prov", "citing": "W12-2303", "vector": [6, 0, 0, 0.055048188256318034], "context": ["", "An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al (1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.", ""], "marker": "1996", "vector_1": {"corpu": 1, "effici": 1, "less": 1, "al": 1, "et": 1, "dynam": 1, "probabl": 1, "accur": 1, "program": 1, "build": 1, "lattic": 1, "approach": 1, "sproat": 1, "util": 1, "extens": 1, "search": 1, "word": 3, "frequenc": 1, "inform": 1, "combin": 1, "statist": 1, "model": 1}, "vector_2": [16, 0.07488645175584357, 2, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus: seemingly, our system finds as many names as their system, but with four times as many false hits."], "label": "Non-Prov", "citing": "W12-2303", "vector": [9, 0, 1, 0.223606797749979], "context": ["", "An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al (1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.", ""], "marker": "1996", "vector_1": {"corpu": 1, "effici": 1, "less": 1, "al": 1, "et": 1, "dynam": 1, "probabl": 1, "accur": 1, "program": 1, "build": 1, "lattic": 1, "approach": 1, "sproat": 1, "util": 1, "extens": 1, "search": 1, "word": 3, "frequenc": 1, "inform": 1, "combin": 1, "statist": 1, "model": 1}, "vector_2": [16, 0.07488645175584357, 2, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The use of the Good-Turing equation presumes suitable estimates of the unknown expectations it requires."], "label": "Non-Prov", "citing": "W12-2303", "vector": [2, 0, 1, 0.0], "context": ["", "An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al (1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.", ""], "marker": "1996", "vector_1": {"corpu": 1, "effici": 1, "less": 1, "al": 1, "et": 1, "dynam": 1, "probabl": 1, "accur": 1, "program": 1, "build": 1, "lattic": 1, "approach": 1, "sproat": 1, "util": 1, "extens": 1, "search": 1, "word": 3, "frequenc": 1, "inform": 1, "combin": 1, "statist": 1, "model": 1}, "vector_2": [16, 0.07488645175584357, 2, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["pronunciations of individual words; they also need to compute intonational phrase boundaries in long utterances and assign relative prominence to words in those utterances.", "It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks."], "label": "Non-Prov", "citing": "W12-2303", "vector": [4, 0, 0, 0.03163859985841663], "context": ["", "There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field. For example, the Sproat et al (1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.", ""], "marker": "1996", "vector_1": {"al": 1, "et": 1, "use": 1, "techniqu": 1, "recognit": 1, "pattern": 1, "system": 1, "field": 1, "method": 1, "machin": 1, "recogn": 1, "sproat": 1, "chines": 1, "rise": 1, "oov": 2, "transliter": 1, "strong": 1, "name": 1, "success": 1, "literatur": 1, "finitest": 1, "person": 1, "exampl": 1, "learn": 1, "mani": 1, "propos": 1}, "vector_2": [16, 0.8705365385325505, 1, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Table 4 Differences in performance between our system and Wang, Li, and Chang (1992).", "Our System Wang, Li, and Chang a. 1\\!f!IP Eflltii /1\\!f!J:P $1til I b. agm: I a m: c. 5 Bf is Bf 1 d. \"*:t: w _t ff 1 \"* :t: w_tff 1 g., , Transliteration/Translation chen2zhongl-shenl qu3 'music by Chen Zhongshen ' huang2rong2 youlyoul de dao4 'Huang Rong said soberly' zhangl qun2 Zhang Qun xian4zhang3 you2qingl shang4ren2 hou4 'after the county president You Qing had assumed the position' lin2 quan2 'Lin Quan' wang2jian4 'Wang Jian' oulyang2-ke4 'Ouyang Ke' yinl qi2 bu4 ke2neng2 rong2xu3 tai2du2 er2 'because it cannot permit Taiwan Independence so' silfa3-yuan4zhang3 lin2yang2-gang3 'president of the Judicial Yuan, Lin Yanggang' lin2zhangl-hu2 jiangl zuo4 xian4chang3 jie3shuol 'Lin Zhanghu will give an ex planation live' jin4/iang3 nian2 nei4 sa3 xia4 de jinlqian2 hui4 ting2zhi3 'in two years the distributed money will stop' gaoltangl da4chi2 ye1zi0 fen3 'chicken stock, a tablespoon of coconut flakes' you2qingl ru4zhu3 xian4fu3 lwu4 'after You Qing headed the county government' Table 5 Performance on morphological analysis."], "label": "Non-Prov", "citing": "W12-2303", "vector": [4, 0, 0, 0.028071730702217298], "context": ["", "There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field. For example, the Sproat et al (1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.", ""], "marker": "1996", "vector_1": {"al": 1, "et": 1, "use": 1, "techniqu": 1, "recognit": 1, "pattern": 1, "system": 1, "field": 1, "method": 1, "machin": 1, "recogn": 1, "sproat": 1, "chines": 1, "rise": 1, "oov": 2, "transliter": 1, "strong": 1, "name": 1, "success": 1, "literatur": 1, "finitest": 1, "person": 1, "exampl": 1, "learn": 1, "mani": 1, "propos": 1}, "vector_2": [16, 0.8705365385325505, 1, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["As noted, this sentence consists of four words, namely B X ri4wen2 'Japanese,' :, zhanglyu2 'octopus/ :&P:l zen3me0 'how,' and IDt shuol 'say.'", "As indicated in Figure 1(c), apart from this correct analysis, there is also the analysis taking B ri4 as a word (e.g., a common abbreviation for Japan), along with X: wen2zhangl 'essay/ and f!!."], "label": "Non-Prov", "citing": "W12-2303", "vector": [6, 0, 0, 0.0], "context": ["", "There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field. For example, the Sproat et al (1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.", ""], "marker": "1996", "vector_1": {"al": 1, "et": 1, "use": 1, "techniqu": 1, "recognit": 1, "pattern": 1, "system": 1, "field": 1, "method": 1, "machin": 1, "recogn": 1, "sproat": 1, "chines": 1, "rise": 1, "oov": 2, "transliter": 1, "strong": 1, "name": 1, "success": 1, "literatur": 1, "finitest": 1, "person": 1, "exampl": 1, "learn": 1, "mani": 1, "propos": 1}, "vector_2": [16, 0.8705365385325505, 1, 0, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Let H be the set of hanzi, p be the set of pinyin syllables with tone marks, and P be the set of grammatical part-of-speech labels.", "Then each arc of D maps either from an element of H to an element of p, or from E-i.e., the empty string-to an element of P. More specifically, each word is represented in the dictionary as a sequence of arcs, starting from the initial state of D and labeled with an element 5 of Hxp, which is terminated with a weighted arc labeled with an element of Ex P. The weight represents the estimated cost (negative log probability) of the word.", "Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994)."], "label": "Non-Prov", "citing": "W97-0120", "vector": [8, 0, 1, 0.0644825880219161], "context": ["", "One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.", ""], "marker": "Sproat et al., 1996", "vector_1": {"major": 1, "unseen": 1, "process": 1, "one": 1, "morpholog": 1, "treatment": 1, "product": 1, "format": 1, "chines": 1, "lexic": 1, "transliter": 1, "noun": 1, "segment": 1, "word": 3, "name": 1, "unsupervis": 1, "rule": 1, "foreign": 1, "person": 1, "plur": 1, "problem": 1, "wrote": 1}, "vector_2": [1, 0.12200906871294036, 1, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["raphy: A ren2 'person' is a fairly uncontroversial case of a monographemic word, and rplil zhong1guo2 (middle country) 'China' a fairly uncontroversial case of a di graphernic word.", "The relevance of the distinction between, say, phonological words and, say, dictionary words is shown by an example like rpftl_A :;!:Hfllil zhong1hua2 ren2min2 gong4he2-guo2 (China people republic) 'People's Republic of China.'", "Arguably this consists of about three phonological words."], "label": "Non-Prov", "citing": "W97-0120", "vector": [6, 0, 1, 0.15958626340564364], "context": ["", "One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.", ""], "marker": "Sproat et al., 1996", "vector_1": {"major": 1, "unseen": 1, "process": 1, "one": 1, "morpholog": 1, "treatment": 1, "product": 1, "format": 1, "chines": 1, "lexic": 1, "transliter": 1, "noun": 1, "segment": 1, "word": 3, "name": 1, "unsupervis": 1, "rule": 1, "foreign": 1, "person": 1, "plur": 1, "problem": 1, "wrote": 1}, "vector_2": [1, 0.12200906871294036, 1, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["computing the precision of the other's judgments relative to this standard.", "2.", "Recall."], "label": "Non-Prov", "citing": "W97-0120", "vector": [2, 0, 1, 0.0], "context": ["", "One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.", ""], "marker": "Sproat et al., 1996", "vector_1": {"major": 1, "unseen": 1, "process": 1, "one": 1, "morpholog": 1, "treatment": 1, "product": 1, "format": 1, "chines": 1, "lexic": 1, "transliter": 1, "noun": 1, "segment": 1, "word": 3, "name": 1, "unsupervis": 1, "rule": 1, "foreign": 1, "person": 1, "plur": 1, "problem": 1, "wrote": 1}, "vector_2": [1, 0.12200906871294036, 1, 3, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary.", "Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult."], "label": "Non-Prov", "citing": "W97-0120", "vector": [3, 0, 0, 0.0659380473395787], "context": ["", "We used a simple greedy algorithm described in [Sproat et al., 1996].", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 1, "simpl": 1, "algorithm": 1, "greedi": 1, "describ": 1}, "vector_2": [1, 0.313986745727241, 1, 3, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["In this example there are four \"input characters,\" A, B, C and D, and these map respectively to four \"pronunciations\" a, b, c and d. Furthermore, there are four \"words\" represented in the dictionary.", "These are shown, with their associated costs, as follows: ABj nc 4.0 AB C/jj 6.0 CD /vb 5."], "label": "Non-Prov", "citing": "W97-0120", "vector": [2, 0, 0, 0.0], "context": ["", "We used a simple greedy algorithm described in [Sproat et al., 1996].", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 1, "simpl": 1, "algorithm": 1, "greedi": 1, "describ": 1}, "vector_2": [1, 0.313986745727241, 1, 3, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["Table 3 Classes of words found by ST for the test corpus.", "Word type N % Dic tion ary entr ies 2 , 5 4 3 9 7 . 4 7 Mor pho logi call y deri ved wor ds 3 0 . 1 1 Fore ign tran slite rati ons 9 0 . 3 4 Per son al na mes 5 4 2 . 0 7 cases."], "label": "Non-Prov", "citing": "W97-0120", "vector": [0, 0, 0, 0.0], "context": ["", "We used a simple greedy algorithm described in [Sproat et al., 1996].", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 1, "simpl": 1, "algorithm": 1, "greedi": 1, "describ": 1}, "vector_2": [1, 0.313986745727241, 1, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects.", "To this end, we picked 100 sentences at random containing 4,372 total hanzi from a test corpus.14 (There were 487 marks of punctuation in the test sentences, including the sentence-final periods, meaning that the average inter-punctuation distance was about 9 hanzi.)"], "label": "Non-Prov", "citing": "W97-0120", "vector": [6, 0, 0, 0.0890870806374748], "context": ["", "[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.", ""], "marker": "Sproat et al., 1996", "vector_1": {"corpu": 1, "set": 1, "word": 1, "anoth": 1, "initi": 1, "without": 1, "also": 1, "estim": 1, "frequenc": 1, "segment": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.33038018835019184, 1, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["This suggests that the backoff model is as reasonable a model as we can use in the absence of further information about the expected cost of a plural form.", "10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to."], "label": "Non-Prov", "citing": "W97-0120", "vector": [4, 0, 0, 0.0], "context": ["", "[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.", ""], "marker": "Sproat et al., 1996", "vector_1": {"corpu": 1, "set": 1, "word": 1, "anoth": 1, "initi": 1, "without": 1, "also": 1, "estim": 1, "frequenc": 1, "segment": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.33038018835019184, 1, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["We have argued that the proposed method performs well.", "However, some caveats are in order in comparing this method (or any method) with other approaches to seg mentation reported in the literature."], "label": "Non-Prov", "citing": "W97-0120", "vector": [4, 0, 0, 0.24077170617153845], "context": ["", "[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.", ""], "marker": "Sproat et al., 1996", "vector_1": {"corpu": 1, "set": 1, "word": 1, "anoth": 1, "initi": 1, "without": 1, "also": 1, "estim": 1, "frequenc": 1, "segment": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.33038018835019184, 1, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Word type N % Dic tion ary entr ies 2 , 5 4 3 9 7 . 4 7 Mor pho logi call y deri ved wor ds 3 0 . 1 1 Fore ign tran slite rati ons 9 0 . 3 4 Per son al na mes 5 4 2 . 0 7 cases.", "Nonetheless, the results of the comparison with human judges demonstrates that there is mileage being gained by incorporating models of these types of words."], "label": "Non-Prov", "citing": "W97-0120", "vector": [6, 2, 1, 0.08032193289024989], "context": ["", "The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it \"maximum matching\", we call this method \"longest match\" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi! :.", ""], "marker": "Sproat et al., 1996", "vector_1": {"text": 1, "substr": 2, "appear": 1, "japanes": 1, "review": 1, "alway": 1, "call": 2, "method": 3, "match": 3, "accord": 1, "liter": 1, "string": 1, "chines": 1, "wl": 1, "train": 1, "longest": 2, "although": 1, "segment": 1, "name": 1, "word": 3, "like": 1, "frequenc": 1, "maximum": 1, "hi": 1, "w": 3, "problem": 1, "translat": 1}, "vector_2": [1, 0.3979769794209976, 2, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["However, they list two sets, one consisting of 28 fragments and the other of 22 fragments, in which they had 0% recall and precision.", "On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision."], "label": "Non-Prov", "citing": "W97-0120", "vector": [6, 0, 1, 0.0], "context": ["", "The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it \"maximum matching\", we call this method \"longest match\" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi! :.", ""], "marker": "Sproat et al., 1996", "vector_1": {"text": 1, "substr": 2, "appear": 1, "japanes": 1, "review": 1, "alway": 1, "call": 2, "method": 3, "match": 3, "accord": 1, "liter": 1, "string": 1, "chines": 1, "wl": 1, "train": 1, "longest": 2, "although": 1, "segment": 1, "name": 1, "word": 3, "like": 1, "frequenc": 1, "maximum": 1, "hi": 1, "w": 3, "problem": 1, "translat": 1}, "vector_2": [1, 0.3979769794209976, 2, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way.", "The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind."], "label": "Non-Prov", "citing": "W97-0120", "vector": [12, 0, 0, 0.03786412228313765], "context": ["", "The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it \"maximum matching\", we call this method \"longest match\" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi! :.", ""], "marker": "Sproat et al., 1996", "vector_1": {"text": 1, "substr": 2, "appear": 1, "japanes": 1, "review": 1, "alway": 1, "call": 2, "method": 3, "match": 3, "accord": 1, "liter": 1, "string": 1, "chines": 1, "wl": 1, "train": 1, "longest": 2, "although": 1, "segment": 1, "name": 1, "word": 3, "like": 1, "frequenc": 1, "maximum": 1, "hi": 1, "w": 3, "problem": 1, "translat": 1}, "vector_2": [1, 0.3979769794209976, 2, 3, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["In a few cases, the criteria for correctness are made more explicit.", "For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary.", "Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult."], "label": "Non-Prov", "citing": "W97-0120", "vector": [8, 0, 0, 0.1266600992762247], "context": ["", "Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"term": 1, "recal": 1, "express": 1, "accuraci": 1, "precis": 1, "bracket": 1, "done": 1, "pars": 1, "word": 1, "segment": 1, "partial": 1}, "vector_2": [1, 0.553993721660272, 2, 3, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["The major problem for our seg menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).", "We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted.", "However, there will remain a large number of words that are not readily adduced to any produc tive pattern and that would simply have to be added to the dictionary."], "label": "Non-Prov", "citing": "W97-0120", "vector": [4, 1, 0, 0.0], "context": ["", "Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"term": 1, "recal": 1, "express": 1, "accuraci": 1, "precis": 1, "bracket": 1, "done": 1, "pars": 1, "word": 1, "segment": 1, "partial": 1}, "vector_2": [1, 0.553993721660272, 2, 3, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["Clearly the percentage of productively formed words is quite small (for this particular corpus), meaning that dictionary entries are covering most of the 15 GR is .73 or 96%..", "16 As one reviewer points out, one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words.", "That is, given a choice between segmenting a sequence abc into abc and ab, c, the former will always be picked so long as its cost does not exceed the summed costs of ab and c: while; it is possible for abc to be so costly as to preclude the larger grouping, this will certainly not usually be the case."], "label": "Non-Prov", "citing": "W97-0120", "vector": [6, 0, 0, 0.034815531191139566], "context": ["", "Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"term": 1, "recal": 1, "express": 1, "accuraci": 1, "precis": 1, "bracket": 1, "done": 1, "pars": 1, "word": 1, "segment": 1, "partial": 1}, "vector_2": [1, 0.553993721660272, 2, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": [" The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words."], "label": "Non-Prov", "citing": "W97-0316", "vector": [6, 0, 0, 0.0769800358919501], "context": ["", "Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin. Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).", ""], "marker": "Sproat et al., 1996", "vector_1": {"process": 1, "automat": 1, "correctli": 1, "research": 1, "call": 1, "taken": 1, "import": 1, "therefor": 1, "method": 2, "analysi": 1, "begin": 1, "sentenc": 1, "isol": 1, "step": 1, "segment": 1, "word": 2, "practic": 1, "resolv": 1, "necessari": 1, "mani": 1, "problem": 1, "first": 1, "propos": 1}, "vector_2": [1, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994)."], "label": "Non-Prov", "citing": "W97-0316", "vector": [4, 0, 1, 0.0670025210172808], "context": ["", "Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin. Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).", ""], "marker": "Sproat et al., 1996", "vector_1": {"process": 1, "automat": 1, "correctli": 1, "research": 1, "call": 1, "taken": 1, "import": 1, "therefor": 1, "method": 2, "analysi": 1, "begin": 1, "sentenc": 1, "isol": 1, "step": 1, "segment": 1, "word": 2, "practic": 1, "resolv": 1, "necessari": 1, "mani": 1, "problem": 1, "first": 1, "propos": 1}, "vector_2": [1, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Therefore in cases where the segmentation is identical between the two systems we assume that tagging is also identical."], "label": "Non-Prov", "citing": "W97-0316", "vector": [4, 0, 1, 0.1111111111111111], "context": ["", "Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin. Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin & Chen, 1996, Ponte & Croft, 1996, Sproat et al., 1996, Sun et al., 1997).", ""], "marker": "Sproat et al., 1996", "vector_1": {"process": 1, "automat": 1, "correctli": 1, "research": 1, "call": 1, "taken": 1, "import": 1, "therefor": 1, "method": 2, "analysi": 1, "begin": 1, "sentenc": 1, "isol": 1, "step": 1, "segment": 1, "word": 2, "practic": 1, "resolv": 1, "necessari": 1, "mani": 1, "problem": 1, "first": 1, "propos": 1}, "vector_2": [1, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["We will give a detailed description of this approach in Section 2.", " Now the second author is affiliated with NTT."], "label": "Non-Prov", "citing": "D13-1031", "vector": [3, 0, 0, 0.0], "context": ["", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al (2006).", ""], "marker": "2006", "vector_1": {"zhang": 1, "al": 1, "system": 1, "rule": 2, "repres": 1, "base": 1, "combin": 1, "et": 1, "crf": 2, "model": 2, "present": 1}, "vector_2": [7, 0.8087412708890664, 1, 0, 7, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["0 means the current word; 1, 2, the first or second word to the left; 1, 2, the first or second word to the right.", "For the bigram features, we only used the previous and the current observations, t1 t0 . As to feature selection, we simply used absolute counts for each feature in the training data."], "label": "Non-Prov", "citing": "D13-1031", "vector": [2, 0, 0, 0.0], "context": ["", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al (2006).", ""], "marker": "2006", "vector_1": {"zhang": 1, "al": 1, "system": 1, "rule": 2, "repres": 1, "base": 1, "combin": 1, "et": 1, "crf": 2, "model": 2, "present": 1}, "vector_2": [7, 0.8087412708890664, 1, 0, 7, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["Since it was a closed test, some information such as Arabic and Chinese number and alphabetical letters cannot be used.", "We could yield a better results than those shown in Table 4 using such information."], "label": "Non-Prov", "citing": "D13-1031", "vector": [3, 0, 0, 0.0], "context": ["", "CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al (2006).", ""], "marker": "2006", "vector_1": {"zhang": 1, "al": 1, "system": 1, "rule": 2, "repres": 1, "base": 1, "combin": 1, "et": 1, "crf": 2, "model": 2, "present": 1}, "vector_2": [7, 0.8087412708890664, 1, 0, 7, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["We found that so far all the existing implementations were using character-based IOB tagging."], "label": "Non-Prov", "citing": "N09-1007", "vector": [1, 0, 0, 0.0], "context": ["", "Z06-a and Z06-b represents the pure sub- word CRF model and the condence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);", ""], "marker": "Zhang et al., 2006", "vector_1": {"word": 1, "sub": 1, "rulebas": 1, "za": 1, "zb": 1, "repres": 1, "crf": 2, "pure": 1, "respect": 1, "combin": 1, "model": 2, "condencebas": 1}, "vector_2": [3, 0.7283127539547911, 1, 2, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["9 4 3 0."], "label": "Non-Prov", "citing": "N09-1007", "vector": [0, 0, 0, 0.0], "context": ["", "Z06-a and Z06-b represents the pure sub- word CRF model and the condence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);", ""], "marker": "Zhang et al., 2006", "vector_1": {"word": 1, "sub": 1, "rulebas": 1, "za": 1, "zb": 1, "repres": 1, "crf": 2, "pure": 1, "respect": 1, "combin": 1, "model": 2, "condencebas": 1}, "vector_2": [3, 0.7283127539547911, 1, 2, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation."], "label": "Non-Prov", "citing": "N09-1007", "vector": [2, 0, 1, 0.0], "context": ["", "Z06-a and Z06-b represents the pure sub- word CRF model and the condence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);", ""], "marker": "Zhang et al., 2006", "vector_1": {"word": 1, "sub": 1, "rulebas": 1, "za": 1, "zb": 1, "repres": 1, "crf": 2, "pure": 1, "respect": 1, "combin": 1, "model": 2, "condencebas": 1}, "vector_2": [3, 0.7283127539547911, 1, 2, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["The subscripts are position indicators."], "label": "Non-Prov", "citing": "P06-2123", "vector": [0, 0, 0, 0.0], "context": ["", "Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).", ""], "marker": "Zhang et al., 2006", "vector_1": {"descript": 1, "detail": 1, "paper": 1, "tag": 1, "crf": 1, "subword": 1, "found": 1}, "vector_2": [0, 0.8364829603858622, 1, 1, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["The effect of the confidence measure is shown in Table 3, where we used  = 0.7 and confidence threshold t = 0.8."], "label": "Non-Prov", "citing": "P06-2123", "vector": [1, 0, 0, 0.0], "context": ["", "Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).", ""], "marker": "Zhang et al., 2006", "vector_1": {"descript": 1, "detail": 1, "paper": 1, "tag": 1, "crf": 1, "subword": 1, "found": 1}, "vector_2": [0, 0.8364829603858622, 1, 1, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["We also successfully employed the confidence measure to make a confidence-dependent word segmentation."], "label": "Non-Prov", "citing": "P06-2123", "vector": [0, 0, 0, 0.0], "context": ["", "Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).", ""], "marker": "Zhang et al., 2006", "vector_1": {"descript": 1, "detail": 1, "paper": 1, "tag": 1, "crf": 1, "subword": 1, "found": 1}, "vector_2": [0, 0.8364829603858622, 1, 1, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["We will give a detailed description of this approach in Section 2."], "label": "Non-Prov", "citing": "P07-1106", "vector": [1, 0, 0, 0.0], "context": ["", "One existing method that is based on sub-word information, Zhang et al (2006), combines a C R F and a rule-based model.", ""], "marker": "2006", "vector_1": {"c": 1, "rulebas": 1, "zhang": 1, "f": 1, "al": 1, "one": 1, "inform": 1, "base": 1, "exist": 1, "subword": 1, "et": 1, "model": 1, "r": 1, "combin": 1, "method": 1}, "vector_2": [1, 0.5682316977582693, 1, 0, 2, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["By this approach we can change R-oovs and R-ivs and find an optimal tradeoff."], "label": "Non-Prov", "citing": "P07-1106", "vector": [1, 0, 0, 0.0], "context": ["", "One existing method that is based on sub-word information, Zhang et al (2006), combines a C R F and a rule-based model.", ""], "marker": "2006", "vector_1": {"c": 1, "rulebas": 1, "zhang": 1, "f": 1, "al": 1, "one": 1, "inform": 1, "base": 1, "exist": 1, "subword": 1, "et": 1, "model": 1, "r": 1, "combin": 1, "method": 1}, "vector_2": [1, 0.5682316977582693, 1, 0, 2, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["Table 1 shows the performance of the dictionary-based segmentation."], "label": "Non-Prov", "citing": "P07-1106", "vector": [0, 0, 0, 0.0], "context": ["", "One existing method that is based on sub-word information, Zhang et al (2006), combines a C R F and a rule-based model.", ""], "marker": "2006", "vector_1": {"c": 1, "rulebas": 1, "zhang": 1, "f": 1, "al": 1, "one": 1, "inform": 1, "base": 1, "exist": 1, "subword": 1, "et": 1, "model": 1, "r": 1, "combin": 1, "method": 1}, "vector_2": [1, 0.5682316977582693, 1, 0, 2, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them."], "label": "Non-Prov", "citing": "P07-1106", "vector": [3, 0, 1, 0.0], "context": ["", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al (2006) for comparison.", ""], "marker": "2006", "vector_1": {"comparison": 1, "chose": 1, "zhang": 1, "emerson": 1, "well": 1, "three": 1, "least": 1, "one": 1, "al": 1, "achiev": 1, "score": 1, "subwordbas": 1, "et": 1, "test": 1, "close": 1, "model": 2, "best": 1}, "vector_2": [1, 0.9070387208026336, 2, 0, 2, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["2.2 Confidence-dependent word segmentation."], "label": "Non-Prov", "citing": "P07-1106", "vector": [0, 0, 0, 0.0], "context": ["", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al (2006) for comparison.", ""], "marker": "2006", "vector_1": {"comparison": 1, "chose": 1, "zhang": 1, "emerson": 1, "well": 1, "three": 1, "least": 1, "one": 1, "al": 1, "achiev": 1, "score": 1, "subwordbas": 1, "et": 1, "test": 1, "close": 1, "model": 2, "best": 1}, "vector_2": [1, 0.9070387208026336, 2, 0, 2, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["Section 5 provides the concluding remarks."], "label": "Non-Prov", "citing": "P07-1106", "vector": [1, 0, 0, 0.0], "context": ["", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al (2006) for comparison.", ""], "marker": "2006", "vector_1": {"comparison": 1, "chose": 1, "zhang": 1, "emerson": 1, "well": 1, "three": 1, "least": 1, "one": 1, "al": 1, "achiev": 1, "score": 1, "subwordbas": 1, "et": 1, "test": 1, "close": 1, "model": 2, "best": 1}, "vector_2": [1, 0.9070387208026336, 2, 0, 2, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["using the FMM, and then labeled with IOB tags by the CRFs."], "label": "Non-Prov", "citing": "P12-1027", "vector": [2, 0, 0, 0.0], "context": ["", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al (2006).", ""], "marker": "2006", "vector_1": {"al": 1, "correspond": 1, "second": 1, "et": 1, "best": 2, "segment": 1, "system": 1, "combin": 1, "confid": 1, "zhang": 1, "chines": 1, "base": 1, "repres": 2, "data": 1, "present": 1, "word": 1, "rulebas": 1, "intern": 1, "bakeoff": 1, "crf": 2, "model": 1, "rulesystem": 1}, "vector_2": [6, 0.9252285547558841, 1, 0, 2, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["2.2 Confidence-dependent word segmentation."], "label": "Non-Prov", "citing": "P12-1027", "vector": [2, 0, 1, 0.2073903389460851], "context": ["", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al (2006).", ""], "marker": "2006", "vector_1": {"al": 1, "correspond": 1, "second": 1, "et": 1, "best": 2, "segment": 1, "system": 1, "combin": 1, "confid": 1, "zhang": 1, "chines": 1, "base": 1, "repres": 2, "data": 1, "present": 1, "word": 1, "rulebas": 1, "intern": 1, "bakeoff": 1, "crf": 2, "model": 1, "rulesystem": 1}, "vector_2": [6, 0.9252285547558841, 1, 0, 2, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["We downloaded and used the package CRF++ from the site http://www.chasen.org/taku/software. According to the CRFs, the probability of an IOB tag sequence, T = t0 t1    tM , given the word sequence, W = w0 w1    wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . k and k are the model parameters corresponding to feature functions fk and gk respectively."], "label": "Non-Prov", "citing": "P12-1027", "vector": [6, 0, 0, 0.07531100588381143], "context": ["", "Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al (2006).", ""], "marker": "2006", "vector_1": {"al": 1, "correspond": 1, "second": 1, "et": 1, "best": 2, "segment": 1, "system": 1, "combin": 1, "confid": 1, "zhang": 1, "chines": 1, "base": 1, "repres": 2, "data": 1, "present": 1, "word": 1, "rulebas": 1, "intern": 1, "bakeoff": 1, "crf": 2, "model": 1, "rulesystem": 1}, "vector_2": [6, 0.9252285547558841, 1, 0, 2, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["We found a speed up both in training and test.", "The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs."], "label": "Non-Prov", "citing": "W08-0335", "vector": [5, 0, 1, 0.13245323570650439], "context": ["", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 2, "word": 1, "describ": 1, "segment": 1, "tool": 1, "work": 1, "second": 1, "accuraci": 1, "bakeoff": 1, "achiev": 1, "part": 1, "data": 1, "report": 1, "sighan": 1, "highest": 1, "approach": 1}, "vector_2": [2, 0.2661164309467527, 1, 2, 1, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["By Eq. 2 the results of IOB tagging were reevaluated.", "A confidence measure threshold, t, was defined for making a decision based on the value."], "label": "Non-Prov", "citing": "W08-0335", "vector": [4, 0, 0, 0.0], "context": ["", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 2, "word": 1, "describ": 1, "segment": 1, "tool": 1, "work": 1, "second": 1, "accuraci": 1, "bakeoff": 1, "achiev": 1, "part": 1, "data": 1, "report": 1, "sighan": 1, "highest": 1, "approach": 1}, "vector_2": [2, 0.2661164309467527, 1, 2, 1, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores.", "We achieved the highest F-scores in CITYU, PKU and MSR corpora."], "label": "Non-Prov", "citing": "W08-0335", "vector": [5, 0, 1, 0.1025978352085154], "context": ["", "Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 2, "word": 1, "describ": 1, "segment": 1, "tool": 1, "work": 1, "second": 1, "accuraci": 1, "bakeoff": 1, "achiev": 1, "part": 1, "data": 1, "report": 1, "sighan": 1, "highest": 1, "approach": 1}, "vector_2": [2, 0.2661164309467527, 1, 2, 1, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["We regard the words in the subset as the subwords for the IOB tagging.", "Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them."], "label": "Non-Prov", "citing": "W08-0335", "vector": [7, 0, 0, 0.08770580193070293], "context": ["", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the dict-hybrid. (Zhang et al., 2006) We used the dict-hybrid to segment the SMT training corpus and test data.", ""], "marker": "Zhang et al., 2006", "vector_1": {"corpu": 1, "dicthybrid": 2, "use": 1, "lexicon": 1, "resourc": 1, "like": 1, "dictionarybas": 1, "segment": 1, "lm": 1, "data": 1, "note": 1, "test": 1, "train": 1, "build": 1, "need": 1, "smt": 1, "cw": 1}, "vector_2": [2, 0.6487840057694619, 1, 2, 1, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used.", "Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001)."], "label": "Non-Prov", "citing": "W08-0335", "vector": [3, 0, 0, 0.0778498944161523], "context": ["", "Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the dict-hybrid. (Zhang et al., 2006) We used the dict-hybrid to segment the SMT training corpus and test data.", ""], "marker": "Zhang et al., 2006", "vector_1": {"corpu": 1, "dicthybrid": 2, "use": 1, "lexicon": 1, "resourc": 1, "like": 1, "dictionarybas": 1, "segment": 1, "lm": 1, "data": 1, "note": 1, "test": 1, "train": 1, "build": 1, "need": 1, "smt": 1, "cw": 1}, "vector_2": [2, 0.6487840057694619, 1, 2, 1, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["In Section 3.2 we will present the experimental segmentation results of the confidence measure approach.", "We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections."], "label": "Non-Prov", "citing": "W10-4135", "vector": [2, 0, 0, 0.0], "context": ["", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", ""], "marker": "Zhang et al., 2006", "vector_1": {"word": 1, "varietybas": 1, "system": 1, "subwordbas": 1, "accessor": 1, "recognit": 1, "tag": 1, "combin": 1, "base": 1, "new": 1, "purpos": 1, "method": 2}, "vector_2": [4, 0.10493077483604567, 2, 3, 0, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["We proved the new approach enhanced the word segmentation significantly.", "Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores."], "label": "Non-Prov", "citing": "W10-4135", "vector": [4, 0, 0, 0.12171612389003691], "context": ["", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", ""], "marker": "Zhang et al., 2006", "vector_1": {"word": 1, "varietybas": 1, "system": 1, "subwordbas": 1, "accessor": 1, "recognit": 1, "tag": 1, "combin": 1, "base": 1, "new": 1, "purpos": 1, "method": 2}, "vector_2": [4, 0.10493077483604567, 2, 3, 0, 0]}, {"function": "Pos", "cited": "N06-2049", "provenance": ["3.2 Effect of the confidence measure.", "In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation."], "label": "Non-Prov", "citing": "W10-4135", "vector": [3, 0, 0, 0.05504818825631802], "context": ["", "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).", ""], "marker": "Zhang et al., 2006", "vector_1": {"word": 1, "varietybas": 1, "system": 1, "subwordbas": 1, "accessor": 1, "recognit": 1, "tag": 1, "combin": 1, "base": 1, "new": 1, "purpos": 1, "method": 2}, "vector_2": [4, 0.10493077483604567, 2, 3, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based."], "label": "Non-Prov", "citing": "W10-4135", "vector": [4, 0, 0, 0.0], "context": ["", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", ""], "marker": "Zhang et al., 2006", "vector_1": {"one": 1, "confidentwordlist": 1, "use": 1, "featur": 2, "construct": 1, "confidentword": 1, "g": 1, "instr": 2, "list": 3, "dure": 1, "f": 1, "descript": 1, "subwordlist": 1, "templat": 1, "crfbase": 1, "subword": 3, "str": 2, "tabl": 1, "similar": 1, "segment": 1}, "vector_2": [4, 0.22273500121447656, 1, 3, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["96 4 0."], "label": "Non-Prov", "citing": "W10-4135", "vector": [0, 0, 0, 0.0], "context": ["", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", ""], "marker": "Zhang et al., 2006", "vector_1": {"one": 1, "confidentwordlist": 1, "use": 1, "featur": 2, "construct": 1, "confidentword": 1, "g": 1, "instr": 2, "list": 3, "dure": 1, "f": 1, "descript": 1, "subwordlist": 1, "templat": 1, "crfbase": 1, "subword": 3, "str": 2, "tabl": 1, "similar": 1, "segment": 1}, "vector_2": [4, 0.22273500121447656, 1, 3, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["In Section 3.2 we will present the experimental segmentation results of the confidence measure approach."], "label": "Non-Prov", "citing": "W10-4135", "vector": [2, 0, 0, 0.0], "context": ["", "Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).", ""], "marker": "Zhang et al., 2006", "vector_1": {"one": 1, "confidentwordlist": 1, "use": 1, "featur": 2, "construct": 1, "confidentword": 1, "g": 1, "instr": 2, "list": 3, "dure": 1, "f": 1, "descript": 1, "subwordlist": 1, "templat": 1, "crfbase": 1, "subword": 3, "str": 2, "tabl": 1, "similar": 1, "segment": 1}, "vector_2": [4, 0.22273500121447656, 1, 3, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches."], "label": "Non-Prov", "citing": "W10-4135", "vector": [1, 0, 0, 0.0], "context": ["", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)", ""], "marker": "Zhang et al., 2006", "vector_1": {"word": 1, "chines": 1, "detail": 1, "subwordbas": 1, "see": 1, "segment": 1}, "vector_2": [4, 0.2786009230021861, 1, 3, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["The effect of the confidence measure is shown in Table 3, where we used  = 0.7 and confidence threshold t = 0.8."], "label": "Non-Prov", "citing": "W10-4135", "vector": [3, 0, 0, 0.0], "context": ["", "See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)", ""], "marker": "Zhang et al., 2006", "vector_1": {"word": 1, "chines": 1, "detail": 1, "subwordbas": 1, "see": 1, "segment": 1}, "vector_2": [4, 0.2786009230021861, 1, 3, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Also, the same questions arise in the construction of these vectors."], "label": "Non-Prov", "citing": "E09-1045", "vector": [2, 0, 0, 0.0], "context": ["", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005) and (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"use": 1, "set": 1, "wsd": 1, "classbas": 1, "sensegroup": 1, "focus": 1, "classifi": 1, "research": 1, "learn": 1, "predefin": 1, "contrast": 1}, "vector_2": [4, 0.1255747791855503, 5, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["JACCARD and TTEST produced better quality synonyms than existing measures in the literature, so we use Curran and Moens configuration for our super- sense tagging experiments."], "label": "Non-Prov", "citing": "E09-1045", "vector": [3, 0, 0, 0.0], "context": ["", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005) and (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"use": 1, "set": 1, "wsd": 1, "classbas": 1, "sensegroup": 1, "focus": 1, "classifi": 1, "research": 1, "learn": 1, "predefin": 1, "contrast": 1}, "vector_2": [4, 0.1255747791855503, 5, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["If the VP is active, a subject (subj) relation is created; otherwise, a direct object (dobj) relation is created."], "label": "Non-Prov", "citing": "E09-1045", "vector": [0, 0, 0, 0.0], "context": ["", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005) and (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"use": 1, "set": 1, "wsd": 1, "classbas": 1, "sensegroup": 1, "focus": 1, "classifi": 1, "research": 1, "learn": 1, "predefin": 1, "contrast": 1}, "vector_2": [4, 0.1255747791855503, 5, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Ciaramita et al."], "label": "Non-Prov", "citing": "N06-1017", "vector": [0, 0, 0, 0.0], "context": ["", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", ""], "marker": "Curran, 2005", "vector_1": {"detect": 1, "word": 1, "closest": 1, "unknown": 2, "howev": 1, "logic": 1, "next": 1, "step": 1, "determin": 1, "complementari": 1, "known": 1, "sens": 2, "problem": 1, "approach": 1, "view": 1}, "vector_2": [1, 0.15268411114108443, 3, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["The columns show the number of instances of each supersense with the precision, recall and f-score measures as percentages."], "label": "Non-Prov", "citing": "N06-1017", "vector": [3, 0, 0, 0.0], "context": ["", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", ""], "marker": "Curran, 2005", "vector_1": {"detect": 1, "word": 1, "closest": 1, "unknown": 2, "howev": 1, "logic": 1, "next": 1, "step": 1, "determin": 1, "complementari": 1, "known": 1, "sens": 2, "problem": 1, "approach": 1, "view": 1}, "vector_2": [1, 0.15268411114108443, 3, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity."], "label": "Non-Prov", "citing": "N06-1017", "vector": [3, 0, 0, 0.0], "context": ["", "There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.", ""], "marker": "Curran, 2005", "vector_1": {"detect": 1, "word": 1, "closest": 1, "unknown": 2, "howev": 1, "logic": 1, "next": 1, "step": 1, "determin": 1, "complementari": 1, "known": 1, "sens": 2, "problem": 1, "approach": 1, "view": 1}, "vector_2": [1, 0.15268411114108443, 3, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["6.2 Morphological Analysis."], "label": "Non-Prov", "citing": "N06-1017", "vector": [0, 0, 0, 0.0], "context": ["", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", ""], "marker": "Curran, 2005", "vector_1": {"possibl": 1, "associ": 1, "approxim": 1, "cluster": 1, "item": 1, "exist": 1, "includ": 1, "sens": 2, "similar": 1}, "vector_2": [1, 0.9910593857181395, 3, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["The corpus consists of the British National Corpus (BNC), the Reuters Corpus Volume 1 (RCV1), and most of the Linguistic Data Consortiums news text collected since 1987: Continuous Speech Recognition III (CSRIII); North American News Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus."], "label": "Non-Prov", "citing": "N06-1017", "vector": [0, 0, 0, 0.0], "context": ["", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", ""], "marker": "Curran, 2005", "vector_1": {"possibl": 1, "associ": 1, "approxim": 1, "cluster": 1, "item": 1, "exist": 1, "includ": 1, "sens": 2, "similar": 1}, "vector_2": [1, 0.9910593857181395, 3, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["This has the advantage of producing a much smaller number of vectors to compare against."], "label": "Non-Prov", "citing": "N06-1017", "vector": [0, 0, 0, 0.0], "context": ["", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", ""], "marker": "Curran, 2005", "vector_1": {"possibl": 1, "associ": 1, "approxim": 1, "cluster": 1, "item": 1, "exist": 1, "includ": 1, "sens": 2, "similar": 1}, "vector_2": [1, 0.9910593857181395, 3, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["However, SEXTANT always attaches the PP to the previous phrase."], "label": "Non-Prov", "citing": "N07-1024", "vector": [1, 0, 0, 0.0], "context": ["", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)", ""], "marker": "Curran 2005", "vector_1": {"use": 2, "sourc": 1, "word": 1, "wsd": 1, "unknown": 1, "eg": 1, "roark": 1, "semant": 1, "contextu": 1, "research": 1, "curran": 1, "inform": 2, "aramita": 1, "lexicon": 1, "classifi": 1, "charniak": 1, "ci": 1, "primari": 1, "languag": 1, "acquir": 1}, "vector_2": [2, 0.3811786144284273, 0, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["For instance, entity is less frequent than many concepts it subsumes."], "label": "Non-Prov", "citing": "N07-1024", "vector": [2, 0, 0, 0.0], "context": ["", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)", ""], "marker": "Curran 2005", "vector_1": {"use": 2, "sourc": 1, "word": 1, "wsd": 1, "unknown": 1, "eg": 1, "roark": 1, "semant": 1, "contextu": 1, "research": 1, "curran": 1, "inform": 2, "aramita": 1, "lexicon": 1, "classifi": 1, "charniak": 1, "ci": 1, "primari": 1, "languag": 1, "acquir": 1}, "vector_2": [2, 0.3811786144284273, 0, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["They find that simple frequency counts are the most effective way of determining the parent-child ordering, achieving 83% accuracy over types of vehicle, food and occupation."], "label": "Non-Prov", "citing": "N07-1024", "vector": [3, 0, 0, 0.0], "context": ["", "While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)", ""], "marker": "Curran 2005", "vector_1": {"use": 2, "sourc": 1, "word": 1, "wsd": 1, "unknown": 1, "eg": 1, "roark": 1, "semant": 1, "contextu": 1, "research": 1, "curran": 1, "inform": 2, "aramita": 1, "lexicon": 1, "classifi": 1, "charniak": 1, "ci": 1, "primari": 1, "languag": 1, "acquir": 1}, "vector_2": [2, 0.3811786144284273, 0, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["The names and descriptions of the noun lex-files are shown in Table 1."], "label": "Non-Prov", "citing": "P12-2050", "vector": [5, 1, 1, 0.062257280636469035], "context": ["", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", ""], "marker": "Curran, 2005", "vector_1": {"semant": 1, "lexicon": 1, "appli": 1, "automat": 1, "ap": 1, "tag": 1, "liev": 1, "abil": 1, "need": 1, "summar": 1, "wordnet": 1, "depend": 1, "avail": 1, "figur": 2, "languag": 2, "sentenc": 1, "ought": 1, "verbsin": 1, "sst": 1, "chines": 1, "focuson": 1, "arab": 1, "centli": 1, "pli": 1, "supersens": 2, "task": 1, "noun": 2, "work": 1, "well": 1, "wordnetsmap": 1, "emerg": 1, "english": 2, "principl": 1, "italian": 1}, "vector_2": [7, 0.15643180349062702, 8, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Canonical attributes summarise the key contexts for each headword and are used to improve the efficiency of the similarity comparisons."], "label": "Non-Prov", "citing": "P12-2050", "vector": [5, 0, 0, 0.0], "context": ["", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", ""], "marker": "Curran, 2005", "vector_1": {"semant": 1, "lexicon": 1, "appli": 1, "automat": 1, "ap": 1, "tag": 1, "liev": 1, "abil": 1, "need": 1, "summar": 1, "wordnet": 1, "depend": 1, "avail": 1, "figur": 2, "languag": 2, "sentenc": 1, "ought": 1, "verbsin": 1, "sst": 1, "chines": 1, "focuson": 1, "arab": 1, "centli": 1, "pli": 1, "supersens": 2, "task": 1, "noun": 2, "work": 1, "well": 1, "wordnetsmap": 1, "emerg": 1, "english": 2, "principl": 1, "italian": 1}, "vector_2": [7, 0.15643180349062702, 8, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Since the Penn Treebank separates PPs and conjunctions from NPs, they are concatenated to match Grefenstettes table-based results, i.e. the SEXTANT always prefers noun attachment."], "label": "Non-Prov", "citing": "P12-2050", "vector": [4, 0, 0, 0.03594425773447948], "context": ["", "More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paa and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.", ""], "marker": "Curran, 2005", "vector_1": {"semant": 1, "lexicon": 1, "appli": 1, "automat": 1, "ap": 1, "tag": 1, "liev": 1, "abil": 1, "need": 1, "summar": 1, "wordnet": 1, "depend": 1, "avail": 1, "figur": 2, "languag": 2, "sentenc": 1, "ought": 1, "verbsin": 1, "sst": 1, "chines": 1, "focuson": 1, "arab": 1, "centli": 1, "pli": 1, "supersens": 2, "task": 1, "noun": 2, "work": 1, "well": 1, "wordnetsmap": 1, "emerg": 1, "english": 2, "principl": 1, "italian": 1}, "vector_2": [7, 0.15643180349062702, 8, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["For lexical FreeNet, Beefer- man (1998) adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus of broadcast news using mutual information."], "label": "Non-Prov", "citing": "S07-1032", "vector": [3, 1, 0, 0.04999999999999999], "context": ["", "Thus, some research has been focused on deriving different sense groupings to overcome the fine grained distinctions of WN (Hearst and Schu tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"learn": 1, "differ": 1, "set": 1, "group": 1, "deriv": 1, "classbas": 1, "distinct": 1, "wn": 1, "thu": 1, "use": 1, "focus": 1, "classifi": 1, "research": 1, "grain": 1, "overcom": 1, "sensegroup": 1, "sens": 1, "fine": 1, "predefin": 1, "wsd": 1}, "vector_2": [2, 0.22009821115398107, 9, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Technical domains, such as medicine, require separate treatment since common words often take on special meanings, and a significant proportion of their vocabulary does not overlap with everyday vocabulary."], "label": "Non-Prov", "citing": "S07-1032", "vector": [3, 0, 0, 0.0], "context": ["", "Thus, some research has been focused on deriving different sense groupings to overcome the fine grained distinctions of WN (Hearst and Schu tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"learn": 1, "differ": 1, "set": 1, "group": 1, "deriv": 1, "classbas": 1, "distinct": 1, "wn": 1, "thu": 1, "use": 1, "focus": 1, "classifi": 1, "research": 1, "grain": 1, "overcom": 1, "sensegroup": 1, "sens": 1, "fine": 1, "predefin": 1, "wsd": 1}, "vector_2": [2, 0.22009821115398107, 9, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results."], "label": "Non-Prov", "citing": "S07-1032", "vector": [2, 0, 0, 0.0], "context": ["", "Thus, some research has been focused on deriving different sense groupings to overcome the fine grained distinctions of WN (Hearst and Schu tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"learn": 1, "differ": 1, "set": 1, "group": 1, "deriv": 1, "classbas": 1, "distinct": 1, "wn": 1, "thu": 1, "use": 1, "focus": 1, "classifi": 1, "research": 1, "grain": 1, "overcom": 1, "sensegroup": 1, "sens": 1, "fine": 1, "predefin": 1, "wsd": 1}, "vector_2": [2, 0.22009821115398107, 9, 1, 0, 0]}, {"function": "Pos", "cited": "P05-1004", "provenance": ["Progressive verbs can function as nouns, verbs and adjectives and once again a nave approximation to the correct attachment is made."], "label": "Non-Prov", "citing": "S12-1011", "vector": [0, 0, 0, 0.0], "context": ["", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", ""], "marker": "Curran, 2005", "vector_1": {"represent": 1, "use": 1, "word": 1, "distribut": 1, "captur": 1, "individu": 1, "mean": 1}, "vector_2": [7, 0.0735049557639859, 3, 2, 0, 0]}, {"function": "Pos", "cited": "P05-1004", "provenance": ["If a preposition is encountered between the noun heads, a prepositional noun (nnprep) GR is created, otherwise an appositional noun (nn) GR is created."], "label": "Non-Prov", "citing": "S12-1011", "vector": [0, 0, 0, 0.0], "context": ["", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", ""], "marker": "Curran, 2005", "vector_1": {"represent": 1, "use": 1, "word": 1, "distribut": 1, "captur": 1, "individu": 1, "mean": 1}, "vector_2": [7, 0.0735049557639859, 3, 2, 0, 0]}, {"function": "Pos", "cited": "P05-1004", "provenance": ["Ciaramita et al."], "label": "Non-Prov", "citing": "S12-1011", "vector": [0, 0, 0, 0.0], "context": ["", "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).", ""], "marker": "Curran, 2005", "vector_1": {"represent": 1, "use": 1, "word": 1, "distribut": 1, "captur": 1, "individu": 1, "mean": 1}, "vector_2": [7, 0.0735049557639859, 3, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["The initial weight could then be divided by the number of supersenses to share out the weight (SHARED)."], "label": "Non-Prov", "citing": "S12-1011", "vector": [2, 0, 0, 0.0], "context": ["", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a models ability to cluster words by their semantics.", ""], "marker": "Curran, 2005", "vector_1": {"supersens": 1, "semant": 1, "word": 1, "evalu": 1, "cluster": 1, "tag": 1, "abil": 1, "model": 1}, "vector_2": [7, 0.43856848267145615, 2, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["On the previously unused WORDNET 1.7.1 test set, our accuracy is 63% using the best system on the WORDNET 1.6 test set."], "label": "Non-Prov", "citing": "S12-1011", "vector": [0, 0, 0, 0.0], "context": ["", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a models ability to cluster words by their semantics.", ""], "marker": "Curran, 2005", "vector_1": {"supersens": 1, "semant": 1, "word": 1, "evalu": 1, "cluster": 1, "tag": 1, "abil": 1, "model": 1}, "vector_2": [7, 0.43856848267145615, 2, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Schu tze (1993) and Widdows (2003)."], "label": "Non-Prov", "citing": "S12-1011", "vector": [4, 1, 1, 0.20412414523193148], "context": ["", "Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a models ability to cluster words by their semantics.", ""], "marker": "Curran, 2005", "vector_1": {"supersens": 1, "semant": 1, "word": 1, "evalu": 1, "cluster": 1, "tag": 1, "abil": 1, "model": 1}, "vector_2": [7, 0.43856848267145615, 2, 2, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["The supersenses of these synonyms are then combined to determine the supersense."], "label": "Non-Prov", "citing": "S12-1023", "vector": [3, 0, 0, 0.0], "context": ["", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", ""], "marker": "Curran, 2005", "vector_1": {"inde": 1, "use": 2, "concept": 1, "word": 1, "wsd": 2, "classbas": 2, "might": 1, "previou": 1, "well": 1, "work": 1, "meta": 1, "singl": 1, "cam": 1, "sens": 2, "beyond": 1, "ie": 1, "notion": 1, "analog": 1}, "vector_2": [7, 0.908493870402802, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Again these options have been considered below."], "label": "Non-Prov", "citing": "S12-1023", "vector": [1, 0, 0, 0.0], "context": ["", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", ""], "marker": "Curran, 2005", "vector_1": {"inde": 1, "use": 2, "concept": 1, "word": 1, "wsd": 2, "classbas": 2, "might": 1, "previou": 1, "well": 1, "work": 1, "meta": 1, "singl": 1, "cam": 1, "sens": 2, "beyond": 1, "ie": 1, "notion": 1, "analog": 1}, "vector_2": [7, 0.908493870402802, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation."], "label": "Non-Prov", "citing": "S12-1023", "vector": [8, 0, 0, 0.03571428571428571], "context": ["", "A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.", ""], "marker": "Curran, 2005", "vector_1": {"inde": 1, "use": 2, "concept": 1, "word": 1, "wsd": 2, "classbas": 2, "might": 1, "previou": 1, "well": 1, "work": 1, "meta": 1, "singl": 1, "cam": 1, "sens": 2, "beyond": 1, "ie": 1, "notion": 1, "analog": 1}, "vector_2": [7, 0.908493870402802, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["The supersenses of these synonyms are then combined to determine the supersense."], "label": "Non-Prov", "citing": "W06-1670", "vector": [2, 0, 1, 0.10846522890932808], "context": ["", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", ""], "marker": "Curran, 2005", "vector_1": {"supersens": 1, "classif": 1, "previou": 1, "noun": 1, "level": 1, "predict": 1, "thu": 1, "work": 1, "rather": 1, "focus": 1, "lexic": 1, "aim": 1, "acquisit": 1, "tag": 1, "exclus": 1, "word": 1, "type": 1}, "vector_2": [1, 0.39584544808425404, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Qc 2005 Association for Computational Linguistics L E X -FI L E D E S C R I P T I O N act acts or actions animal animals artifact man-made objects attribute attributes of people and objects body body parts cognition cognitive processes and contents communication communicative processes and contents event natural events feeling feelings and emotions food foods and drinks group groupings of people or objects location spatial position motive goals object natural objects (not man-made) person people phenomenon natural phenomena plant plants possession possession and transfer of possession process natural processes quantity quantities and units of measure relation relations between people/things/ideas shape two and three dimensional shapes state stable states of affairs substance substances time time and temporal relations Table 1: 25 noun lexicographer files in WORDNET"], "label": "Non-Prov", "citing": "W06-1670", "vector": [0, 1, 0, 0.0], "context": ["", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", ""], "marker": "Curran, 2005", "vector_1": {"supersens": 1, "classif": 1, "previou": 1, "noun": 1, "level": 1, "predict": 1, "thu": 1, "work": 1, "rather": 1, "focus": 1, "lexic": 1, "aim": 1, "acquisit": 1, "tag": 1, "exclus": 1, "word": 1, "type": 1}, "vector_2": [1, 0.39584544808425404, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger."], "label": "Non-Prov", "citing": "W06-1670", "vector": [1, 0, 0, 0.0], "context": ["", "Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.", ""], "marker": "Curran, 2005", "vector_1": {"supersens": 1, "classif": 1, "previou": 1, "noun": 1, "level": 1, "predict": 1, "thu": 1, "work": 1, "rather": 1, "focus": 1, "lexic": 1, "aim": 1, "acquisit": 1, "tag": 1, "exclus": 1, "word": 1, "type": 1}, "vector_2": [1, 0.39584544808425404, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["1 6 4 . 6 6 8 . 6Part Of 1 3 6 7 6 3 2 70.", "4 5 5 . 9 6 2 . 3 Subsid iary 2 7 1 4 2 3 37."], "label": "Non-Prov", "citing": "D07-1076", "vector": [1, 0, 0, 0.0], "context": ["", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information", ""], "marker": "Zhou et al., 2005", "vector_1": {"among": 1, "semant": 1, "featur": 1, "certain": 1, "knowledg": 1, "vari": 1, "entiti": 1, "depend": 1, "featurebas": 1, "achiev": 1, "larg": 1, "divers": 1, "syntact": 1, "relat": 1, "lexic": 1, "pars": 1, "success": 1, "tree": 2, "method": 1, "employ": 1, "inform": 2, "amount": 1, "kambhatla": 1, "linguist": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.", "It achieves 52.8 F- measure on the 24 ACE relation subtypes."], "label": "Non-Prov", "citing": "D07-1076", "vector": [7, 1, 0, 0.16675933649126753], "context": ["", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information", ""], "marker": "Zhou et al., 2005", "vector_1": {"among": 1, "semant": 1, "featur": 1, "certain": 1, "knowledg": 1, "vari": 1, "entiti": 1, "depend": 1, "featurebas": 1, "achiev": 1, "larg": 1, "divers": 1, "syntact": 1, "relat": 1, "lexic": 1, "pars": 1, "success": 1, "tree": 2, "method": 1, "employ": 1, "inform": 2, "amount": 1, "kambhatla": 1, "linguist": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["However, full parsing is always prone to long distance errors although the Collins parser used in our system represents the state-of-the-art in full parsing.", " Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype ROLE.Citizen-Of from ROLE."], "label": "Non-Prov", "citing": "D07-1076", "vector": [6, 0, 0, 0.027389551783238836], "context": ["", "Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information", ""], "marker": "Zhou et al., 2005", "vector_1": {"among": 1, "semant": 1, "featur": 1, "certain": 1, "knowledg": 1, "vari": 1, "entiti": 1, "depend": 1, "featurebas": 1, "achiev": 1, "larg": 1, "divers": 1, "syntact": 1, "relat": 1, "lexic": 1, "pars": 1, "success": 1, "tree": 2, "method": 1, "employ": 1, "inform": 2, "amount": 1, "kambhatla": 1, "linguist": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Weak", "cited": "P05-1053", "provenance": [" PTP: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree  PTPH: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path.", "4.8 Semantic Resources."], "label": "Non-Prov", "citing": "D07-1076", "vector": [3, 0, 1, 0.1194162868053064], "context": ["", "How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", ""], "marker": "Zhou et al 2005", "vector_1": {"zhou": 1, "captur": 1, "perform": 1, "extract": 1, "relat": 1, "tree": 1, "effect": 1, "al": 1, "inform": 1, "critic": 1, "pars": 1, "improv": 1, "struc": 1, "et": 1, "ture": 1, "ever": 1, "difficult": 1}, "vector_2": [2, 0.5, 0, 2, 2, 0]}, {"function": "Weak", "cited": "P05-1053", "provenance": ["8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71.", "1 5 3 . 7 6 2 . 3 Manag ement 1 6 5 1 0 6 7 2 59."], "label": "Non-Prov", "citing": "D07-1076", "vector": [0, 0, 0, 0.0], "context": ["", "How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", ""], "marker": "Zhou et al 2005", "vector_1": {"zhou": 1, "captur": 1, "perform": 1, "extract": 1, "relat": 1, "tree": 1, "effect": 1, "al": 1, "inform": 1, "critic": 1, "pars": 1, "improv": 1, "struc": 1, "et": 1, "ture": 1, "ever": 1, "difficult": 1}, "vector_2": [2, 0.5, 0, 2, 2, 0]}, {"function": "Weak", "cited": "P05-1053", "provenance": [" Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.", " The usefulness of mention level features is quite limited."], "label": "Non-Prov", "citing": "D07-1076", "vector": [2, 0, 0, 0.0], "context": ["", "How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.", ""], "marker": "Zhou et al 2005", "vector_1": {"zhou": 1, "captur": 1, "perform": 1, "extract": 1, "relat": 1, "tree": 1, "effect": 1, "al": 1, "inform": 1, "critic": 1, "pars": 1, "improv": 1, "struc": 1, "et": 1, "ture": 1, "ever": 1, "difficult": 1}, "vector_2": [2, 0.5, 0, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It consists of 97 documents (~50k words) and 1386 instances of relations.", "Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set."], "label": "Non-Prov", "citing": "D07-1076", "vector": [4, 0, 0, 0.0], "context": ["", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)", ""], "marker": "Zhou et al 2005", "vector_1": {"kernel": 4, "c": 1, "via": 1, "composit": 2, "describ": 1, "zhou": 1, "zhang": 1, "appli": 1, "linear": 1, "convolut": 1, "al": 2, "tree": 1, "polynomi": 1, "state": 1, "paper": 1, "ontextsensit": 1, "integr": 1, "theart": 1, "interpol": 1, "et": 2, "propos": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.", "Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE."], "label": "Non-Prov", "citing": "D07-1076", "vector": [10, 0, 1, 0.2360960823249428], "context": ["", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)", ""], "marker": "Zhou et al 2005", "vector_1": {"kernel": 4, "c": 1, "via": 1, "composit": 2, "describ": 1, "zhou": 1, "zhang": 1, "appli": 1, "linear": 1, "convolut": 1, "al": 2, "tree": 1, "polynomi": 1, "state": 1, "paper": 1, "ontextsensit": 1, "integr": 1, "theart": 1, "interpol": 1, "et": 2, "propos": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It only improves the F-measure by 0.8 due to the recall increase.", " Incorporating the overlap features gives some balance between precision and recall."], "label": "Non-Prov", "citing": "D07-1076", "vector": [2, 0, 0, 0.0], "context": ["", "Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)", ""], "marker": "Zhou et al 2005", "vector_1": {"kernel": 4, "c": 1, "via": 1, "composit": 2, "describ": 1, "zhou": 1, "zhang": 1, "appli": 1, "linear": 1, "convolut": 1, "al": 2, "tree": 1, "polynomi": 1, "state": 1, "paper": 1, "ontextsensit": 1, "integr": 1, "theart": 1, "interpol": 1, "et": 2, "propos": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE."], "label": "Non-Prov", "citing": "D07-1076", "vector": [2, 0, 0, 0.0], "context": ["", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", ""], "marker": "20 05", "vector_1": {"semant": 1, "set": 1, "chunk": 1, "al": 1, "featur": 1, "phrase": 1, "et": 1, "ing": 1, "ie": 1, "entiti": 1, "use": 1, "depend": 1, "overlap": 1, "type": 1, "flat": 1, "zhou": 1, "mention": 1, "base": 1, "pars": 1, "word": 1, "level": 1, "tree": 2, "inform": 1}, "vector_2": [8, 0.5, 0, 2, 2, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN:  ML12: combination of mention levels 4.4 Overlap."], "label": "Non-Prov", "citing": "D07-1076", "vector": [7, 0, 0, 0.21350420507344953], "context": ["", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", ""], "marker": "20 05", "vector_1": {"semant": 1, "set": 1, "chunk": 1, "al": 1, "featur": 1, "phrase": 1, "et": 1, "ing": 1, "ie": 1, "entiti": 1, "use": 1, "depend": 1, "overlap": 1, "type": 1, "flat": 1, "zhou": 1, "mention": 1, "base": 1, "pars": 1, "word": 1, "level": 1, "tree": 2, "inform": 1}, "vector_2": [8, 0.5, 0, 2, 2, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted."], "label": "Non-Prov", "citing": "D07-1076", "vector": [6, 1, 1, 0.1701034543599429], "context": ["", "7 Here, we use the same set of flat features (i.e. word,. entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).", ""], "marker": "20 05", "vector_1": {"semant": 1, "set": 1, "chunk": 1, "al": 1, "featur": 1, "phrase": 1, "et": 1, "ing": 1, "ie": 1, "entiti": 1, "use": 1, "depend": 1, "overlap": 1, "type": 1, "flat": 1, "zhou": 1, "mention": 1, "base": 1, "pars": 1, "word": 1, "level": 1, "tree": 2, "inform": 1}, "vector_2": [8, 0.5, 0, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["(1999)."], "label": "Non-Prov", "citing": "D07-1076", "vector": [0, 0, 0, 0.0], "context": ["", " dependency kernel Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"kernel": 1, "depend": 1, "al": 1, "zhou": 1, "et": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships."], "label": "Non-Prov", "citing": "D07-1076", "vector": [0, 0, 0, 0.0], "context": ["", " dependency kernel Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"kernel": 1, "depend": 1, "al": 1, "zhou": 1, "et": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Advances in Neural Information Processing Systems 14."], "label": "Non-Prov", "citing": "D07-1076", "vector": [0, 0, 0, 0.0], "context": ["", " dependency kernel Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"kernel": 1, "depend": 1, "al": 1, "zhou": 1, "et": 1}, "vector_2": [2, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first."], "label": "Non-Prov", "citing": "D12-1074", "vector": [4, 0, 0, 0.06286946134619316], "context": ["", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "point": 1, "proven": 1, "lightweight": 1, "leverag": 1, "absolut": 1, "use": 1, "rather": 1, "reli": 1, "score": 1, "higher": 1, "exhaust": 1, "sourc": 1, "syntact": 1, "gener": 1, "serv": 1, "comparison": 1, "attempt": 1, "possibl": 1, "inform": 1, "model": 2, "toward": 1}, "vector_2": [7, 0.4621767713659606, 1, 1, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["The training set consists of 674 annotated text documents (~300k words) and 9683 instances of relations."], "label": "Non-Prov", "citing": "D12-1074", "vector": [3, 0, 0, 0.0], "context": ["", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "point": 1, "proven": 1, "lightweight": 1, "leverag": 1, "absolut": 1, "use": 1, "rather": 1, "reli": 1, "score": 1, "higher": 1, "exhaust": 1, "sourc": 1, "syntact": 1, "gener": 1, "serv": 1, "comparison": 1, "attempt": 1, "possibl": 1, "inform": 1, "model": 2, "toward": 1}, "vector_2": [7, 0.4621767713659606, 1, 1, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted."], "label": "Non-Prov", "citing": "D12-1074", "vector": [5, 0, 0, 0.0], "context": ["", "This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "point": 1, "proven": 1, "lightweight": 1, "leverag": 1, "absolut": 1, "use": 1, "rather": 1, "reli": 1, "score": 1, "higher": 1, "exhaust": 1, "sourc": 1, "syntact": 1, "gener": 1, "serv": 1, "comparison": 1, "attempt": 1, "possibl": 1, "inform": 1, "model": 2, "toward": 1}, "vector_2": [7, 0.4621767713659606, 1, 1, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This feature concerns about the entity type of both the mentions, which can be PERSON, ORGANIZATION, FACILITY, LOCATION and GeoPolitical Entity or GPE:  ET12: combination of mention entity types 4.3 Mention Level."], "label": "Non-Prov", "citing": "E06-2012", "vector": [1, 0, 0, 0.0], "context": ["", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", ""], "marker": "Zelenko et al, 2003, Zhou et al, 2005", "vector_1": {"classif": 1, "work": 1, "trainabl": 1, "relat": 1, "svm": 1, "chiefli": 1, "begun": 1, "address": 1, "mean": 1, "extract": 1, "event": 1, "recent": 1}, "vector_2": [3, 0.5524416730890492, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["set as the last word of the mention."], "label": "Non-Prov", "citing": "E06-2012", "vector": [0, 0, 0, 0.0], "context": ["", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", ""], "marker": "Zelenko et al, 2003, Zhou et al, 2005", "vector_1": {"classif": 1, "work": 1, "trainabl": 1, "relat": 1, "svm": 1, "chiefli": 1, "begun": 1, "address": 1, "mean": 1, "extract": 1, "event": 1, "recent": 1}, "vector_2": [3, 0.5524416730890492, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This category of features concerns about the information inherent only in the full parse tree."], "label": "Non-Prov", "citing": "E06-2012", "vector": [0, 0, 0, 0.0], "context": ["", "Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).", ""], "marker": "Zelenko et al, 2003, Zhou et al, 2005", "vector_1": {"classif": 1, "work": 1, "trainabl": 1, "relat": 1, "svm": 1, "chiefli": 1, "begun": 1, "address": 1, "mean": 1, "extract": 1, "event": 1, "recent": 1}, "vector_2": [3, 0.5524416730890492, 2, 1, 0, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["(1999).", "Head-driven statistical models for natural language parsing."], "label": "Non-Prov", "citing": "E12-1020", "vector": [1, 0, 0, 0.0], "context": ["", "For the choice of features, we use the full set of features from Zhou et al (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", ""], "marker": "2005", "vector_1": {"choic": 1, "use": 1, "featur": 2, "zhou": 1, "perform": 1, "al": 1, "set": 1, "full": 1, "stateoftheart": 1, "report": 1, "et": 1, "sinc": 1}, "vector_2": [7, 0.21029569892473118, 2, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus.", "Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted."], "label": "Non-Prov", "citing": "E12-1020", "vector": [6, 0, 0, 0.11065666703449761], "context": ["", "For the choice of features, we use the full set of features from Zhou et al (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", ""], "marker": "2005", "vector_1": {"choic": 1, "use": 1, "featur": 2, "zhou": 1, "perform": 1, "al": 1, "set": 1, "full": 1, "stateoftheart": 1, "report": 1, "et": 1, "sinc": 1}, "vector_2": [7, 0.21029569892473118, 2, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Finally, Table 6 shows the distributions of errors.", "It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types."], "label": "Non-Prov", "citing": "E12-1020", "vector": [4, 0, 0, 0.0], "context": ["", "For the choice of features, we use the full set of features from Zhou et al (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).", ""], "marker": "2005", "vector_1": {"choic": 1, "use": 1, "featur": 2, "zhou": 1, "perform": 1, "al": 1, "set": 1, "full": 1, "stateoftheart": 1, "report": 1, "et": 1, "sinc": 1}, "vector_2": [7, 0.21029569892473118, 2, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["The EDT task entails the detection of entity mentions and chaining them together by identifying their coreference.", "In ACE vocabulary, entities are objects, mentions are references to them, and relations are semantic relationships between entities."], "label": "Non-Prov", "citing": "E12-1020", "vector": [4, 0, 0, 0.0], "context": ["", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "weight": 1, "show": 1, "predict": 1, "repres": 1, "extract": 1, "probabl": 1, "use": 2, "space": 1, "perform": 1, "includ": 1, "correct": 1, "competit": 1, "relat": 1, "one": 1, "associ": 1, "sinc": 1, "untag": 1, "erron": 1, "maxent": 1, "exampl": 3, "stateoftheart": 1, "learn": 1, "output": 1, "model": 1}, "vector_2": [7, 0.3843010752688172, 2, 5, 4, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion.", "In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed."], "label": "Non-Prov", "citing": "E12-1020", "vector": [9, 0, 1, 0.08703882797784893], "context": ["", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "weight": 1, "show": 1, "predict": 1, "repres": 1, "extract": 1, "probabl": 1, "use": 2, "space": 1, "perform": 1, "includ": 1, "correct": 1, "competit": 1, "relat": 1, "one": 1, "associ": 1, "sinc": 1, "untag": 1, "erron": 1, "maxent": 1, "exampl": 3, "stateoftheart": 1, "learn": 1, "output": 1, "model": 1}, "vector_2": [7, 0.3843010752688172, 2, 5, 4, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collins parser (Collins 1999) is employed for full parsing.", "Most of the chunking features concern about the head words of the phrases between the two mentions."], "label": "Non-Prov", "citing": "E12-1020", "vector": [2, 0, 0, 0.0], "context": ["", "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "weight": 1, "show": 1, "predict": 1, "repres": 1, "extract": 1, "probabl": 1, "use": 2, "space": 1, "perform": 1, "includ": 1, "correct": 1, "competit": 1, "relat": 1, "one": 1, "associ": 1, "sinc": 1, "untag": 1, "erron": 1, "maxent": 1, "exampl": 3, "stateoftheart": 1, "learn": 1, "output": 1, "model": 1}, "vector_2": [7, 0.3843010752688172, 2, 5, 4, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": [" Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase."], "label": "Non-Prov", "citing": "E12-1020", "vector": [1, 0, 0, 0.0], "context": ["", "We use SVM as our learning algorithm with the full feature set from Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"use": 1, "svm": 1, "set": 1, "zhou": 1, "algorithm": 1, "al": 1, "full": 1, "learn": 1, "et": 1, "featur": 1}, "vector_2": [7, 0.7999731182795699, 1, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment."], "label": "Non-Prov", "citing": "E12-1020", "vector": [2, 0, 0, 0.07905694150420949], "context": ["", "We use SVM as our learning algorithm with the full feature set from Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"use": 1, "svm": 1, "set": 1, "zhou": 1, "algorithm": 1, "al": 1, "full": 1, "learn": 1, "et": 1, "featur": 1}, "vector_2": [7, 0.7999731182795699, 1, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework."], "label": "Non-Prov", "citing": "E12-1020", "vector": [3, 0, 0, 0.0], "context": ["", "We use SVM as our learning algorithm with the full feature set from Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"use": 1, "svm": 1, "set": 1, "zhou": 1, "algorithm": 1, "al": 1, "full": 1, "learn": 1, "et": 1, "featur": 1}, "vector_2": [7, 0.7999731182795699, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Collins M. and Duffy N."], "label": "Non-Prov", "citing": "N06-1037", "vector": [0, 0, 0, 0.0], "context": ["", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", ""], "marker": "Zhou et al., 2005", "vector_1": {"hierarch": 1, "featur": 1, "syntact": 1, "less": 1, "perform": 1, "howev": 1, "structur": 1, "report": 1, "improv": 1, "contribut": 1}, "vector_2": [1, 0.05714082148836878, 2, 21, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query Who is the president of the United States?."], "label": "Non-Prov", "citing": "N06-1037", "vector": [2, 0, 0, 0.0], "context": ["", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", ""], "marker": "Zhou et al., 2005", "vector_1": {"hierarch": 1, "featur": 1, "syntact": 1, "less": 1, "perform": 1, "howev": 1, "structur": 1, "report": 1, "improv": 1, "contribut": 1}, "vector_2": [1, 0.05714082148836878, 2, 21, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998)."], "label": "Non-Prov", "citing": "N06-1037", "vector": [0, 0, 0, 0.0], "context": ["", "However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.", ""], "marker": "Zhou et al., 2005", "vector_1": {"hierarch": 1, "featur": 1, "syntact": 1, "less": 1, "perform": 1, "howev": 1, "structur": 1, "report": 1, "improv": 1, "contribut": 1}, "vector_2": [1, 0.05714082148836878, 2, 21, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines."], "label": "Non-Prov", "citing": "N06-1037", "vector": [3, 0, 0, 0.253546276418555], "context": ["", "Zhou et al (2005) explore various features in relation extraction using SVM.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "variou": 1, "relat": 1, "al": 1, "svm": 1, "explor": 1, "et": 1, "extract": 1}, "vector_2": [1, 0.14598696163300204, 1, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["During development, 155 of 674 documents in the training set are set aside for fine-tuning the system."], "label": "Non-Prov", "citing": "N06-1037", "vector": [1, 0, 0, 0.0], "context": ["", "Zhou et al (2005) explore various features in relation extraction using SVM.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "variou": 1, "relat": 1, "al": 1, "svm": 1, "explor": 1, "et": 1, "extract": 1}, "vector_2": [1, 0.14598696163300204, 1, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Since a pronominal mention (especially neutral pronoun such as it and its) contains little information about the sense of the mention, the co- reference chain is used to decide its sense."], "label": "Non-Prov", "citing": "N06-1037", "vector": [0, 0, 0, 0.0], "context": ["", "Zhou et al (2005) explore various features in relation extraction using SVM.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "variou": 1, "relat": 1, "al": 1, "svm": 1, "explor": 1, "et": 1, "extract": 1}, "vector_2": [1, 0.14598696163300204, 1, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It also shows that our system performs best on the subtype SOCIAL.Parent and ROLE.", "Citizen-Of."], "label": "Non-Prov", "citing": "N06-1037", "vector": [2, 0, 0, 0.0], "context": ["", "The features used in Kambhatla (2004) and Zhou et al (2005) have to be selected and carefully calibrated manually.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "manual": 1, "al": 1, "calibr": 1, "kambhatla": 1, "et": 1, "select": 1, "care": 1}, "vector_2": [1, 0.158490969327776, 2, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Moreover, we only apply the simple linear kernel, although other kernels can peform better.", "The reason why we choose SVMs for this purpose is that SVMs represent the state-ofthe-art in the machine learning research community, and there are good implementations of the algorithm available."], "label": "Non-Prov", "citing": "N06-1037", "vector": [3, 0, 0, 0.0], "context": ["", "The features used in Kambhatla (2004) and Zhou et al (2005) have to be selected and carefully calibrated manually.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "manual": 1, "al": 1, "calibr": 1, "kambhatla": 1, "et": 1, "select": 1, "care": 1}, "vector_2": [1, 0.158490969327776, 2, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Ph.D. Dissertation, University of Pennsylvania.", "Collins M. and Duffy N."], "label": "Non-Prov", "citing": "N06-1037", "vector": [1, 0, 0, 0.0], "context": ["", "The features used in Kambhatla (2004) and Zhou et al (2005) have to be selected and carefully calibrated manually.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "manual": 1, "al": 1, "calibr": 1, "kambhatla": 1, "et": 1, "select": 1, "care": 1}, "vector_2": [1, 0.158490969327776, 2, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Non-Prov", "citing": "N06-1037", "vector": [1, 0, 0, 0.0], "context": ["", "Besides, Zhou et al (2005) introduce additional chunking features to enhance the parse tree features.", ""], "marker": "2005", "vector_1": {"featur": 2, "zhou": 1, "chunk": 1, "tree": 1, "al": 1, "enhanc": 1, "besid": 1, "pars": 1, "et": 1, "introduc": 1, "addit": 1}, "vector_2": [1, 0.1660076235260589, 1, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations."], "label": "Non-Prov", "citing": "N06-1037", "vector": [5, 0, 3, 0.472455591261534], "context": ["", "Besides, Zhou et al (2005) introduce additional chunking features to enhance the parse tree features.", ""], "marker": "2005", "vector_1": {"featur": 2, "zhou": 1, "chunk": 1, "tree": 1, "al": 1, "enhanc": 1, "besid": 1, "pars": 1, "et": 1, "introduc": 1, "addit": 1}, "vector_2": [1, 0.1660076235260589, 1, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively."], "label": "Non-Prov", "citing": "N06-1037", "vector": [1, 0, 0, 0.0], "context": ["", "Besides, Zhou et al (2005) introduce additional chunking features to enhance the parse tree features.", ""], "marker": "2005", "vector_1": {"featur": 2, "zhou": 1, "chunk": 1, "tree": 1, "al": 1, "enhanc": 1, "besid": 1, "pars": 1, "et": 1, "introduc": 1, "addit": 1}, "vector_2": [1, 0.1660076235260589, 1, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Citizen-Of.", "This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list."], "label": "Non-Prov", "citing": "N06-1037", "vector": [2, 0, 0, 0.0], "context": ["", "we call the features used in Zhou et al (2005) and Kambhatla (2004) flat feature set.", ""], "marker": "2005", "vector_1": {"flat": 1, "use": 1, "featur": 2, "zhou": 1, "kambhatla": 1, "al": 1, "set": 1, "call": 1, "et": 1}, "vector_2": [1, 0.6290833956752521, 2, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Residence by distinguishing country GPEs from other GPEs.", "The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes."], "label": "Non-Prov", "citing": "N06-1037", "vector": [1, 0, 0, 0.0], "context": ["", "we call the features used in Zhou et al (2005) and Kambhatla (2004) flat feature set.", ""], "marker": "2005", "vector_1": {"flat": 1, "use": 1, "featur": 2, "zhou": 1, "kambhatla": 1, "al": 1, "set": 1, "call": 1, "et": 1}, "vector_2": [1, 0.6290833956752521, 2, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["4 6 3 . 4 7 2 . 4 Citize n-Of 3 6 2 5 8 75.", "8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71."], "label": "Non-Prov", "citing": "N06-1037", "vector": [0, 0, 0, 0.0], "context": ["", "we call the features used in Zhou et al (2005) and Kambhatla (2004) flat feature set.", ""], "marker": "2005", "vector_1": {"flat": 1, "use": 1, "featur": 2, "zhou": 1, "kambhatla": 1, "al": 1, "set": 1, "call": 1, "et": 1}, "vector_2": [1, 0.6290833956752521, 2, 0, 21, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees."], "label": "Non-Prov", "citing": "N06-1037", "vector": [1, 0, 0, 0.1556997888323046], "context": ["", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", ""], "marker": "Zhou et al., 2005", "vector_1": {"poor": 1, "number": 1, "carri": 1, "relat": 2, "explicit": 1, "due": 1, "agreement": 1, "limit": 1, "annot": 1, "experi": 1, "interannot": 1, "implicit": 1}, "vector_2": [1, 0.6369562894089986, 1, 21, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["3 5 5 . 2 4 1 . 6 Parent 2 5 1 7 0 10 0 6 8 . 0 8 1 . 0 System Table 4: Performa nce of different relation types and major subtypes in the test data R e l a t i o n D e t e c t i o n R D C o n T y p e s R D C o n S u b t y p e s P R F P R F P R F Ou rs: fea ture bas ed 8 4."], "label": "Non-Prov", "citing": "N06-1037", "vector": [4, 0, 0, 0.0], "context": ["", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", ""], "marker": "Zhou et al., 2005", "vector_1": {"poor": 1, "number": 1, "carri": 1, "relat": 2, "explicit": 1, "due": 1, "agreement": 1, "limit": 1, "annot": 1, "experi": 1, "interannot": 1, "implicit": 1}, "vector_2": [1, 0.6369562894089986, 1, 21, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Ph.D. Dissertation, University of Pennsylvania."], "label": "Non-Prov", "citing": "N06-1037", "vector": [1, 0, 0, 0.0], "context": ["", "(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.", ""], "marker": "Zhou et al., 2005", "vector_1": {"poor": 1, "number": 1, "carri": 1, "relat": 2, "explicit": 1, "due": 1, "agreement": 1, "limit": 1, "annot": 1, "experi": 1, "interannot": 1, "implicit": 1}, "vector_2": [1, 0.6369562894089986, 1, 21, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It also shows that our system performs best on the subtype SOCIAL.Parent and ROLE."], "label": "Non-Prov", "citing": "N07-1015", "vector": [2, 0, 0, 0.0], "context": ["", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", ""], "marker": "Zhou et al., 2005", "vector_1": {"analysi": 1, "differ": 1, "set": 1, "depend": 1, "full": 1, "level": 1, "text": 1, "tag": 1, "obtain": 1, "util": 1, "featur": 1, "pars": 2, "partofspeech": 1, "first": 1, "po": 1, "select": 1, "care": 1}, "vector_2": [2, 0.06748259977808413, 3, 2, 9, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Moreover, we also consider the phrase path in between."], "label": "Non-Prov", "citing": "N07-1015", "vector": [1, 0, 0, 0.0], "context": ["", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", ""], "marker": "Zhou et al., 2005", "vector_1": {"analysi": 1, "differ": 1, "set": 1, "depend": 1, "full": 1, "level": 1, "text": 1, "tag": 1, "obtain": 1, "util": 1, "featur": 1, "pars": 2, "partofspeech": 1, "first": 1, "po": 1, "select": 1, "care": 1}, "vector_2": [2, 0.06748259977808413, 3, 2, 9, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees."], "label": "Non-Prov", "citing": "N07-1015", "vector": [4, 0, 0, 0.047673129462279605], "context": ["", "The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.", ""], "marker": "Zhou et al., 2005", "vector_1": {"analysi": 1, "differ": 1, "set": 1, "depend": 1, "full": 1, "level": 1, "text": 1, "tag": 1, "obtain": 1, "util": 1, "featur": 1, "pars": 2, "partofspeech": 1, "first": 1, "po": 1, "select": 1, "care": 1}, "vector_2": [2, 0.06748259977808413, 3, 2, 9, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998)."], "label": "Non-Prov", "citing": "N07-1015", "vector": [4, 0, 0, 0.07412493166611013], "context": ["", "Zhao and Grishman (2005) and Zhou et al (2005) explored a large set of features that are potentially useful for relation extraction.", ""], "marker": "2005", "vector_1": {"use": 1, "set": 1, "potenti": 1, "zhou": 1, "grishman": 1, "al": 1, "zhao": 1, "featur": 1, "larg": 1, "explor": 1, "et": 1, "relat": 1, "extract": 1}, "vector_2": [2, 0.1637133922867422, 2, 0, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include:  WM1: bag-of-words in M1  HM1: head word of M1 3 In ACE, each mention has a head annotation and an."], "label": "Non-Prov", "citing": "N07-1015", "vector": [4, 0, 0, 0.049813548138671795], "context": ["", "Zhao and Grishman (2005) and Zhou et al (2005) explored a large set of features that are potentially useful for relation extraction.", ""], "marker": "2005", "vector_1": {"use": 1, "set": 1, "potenti": 1, "zhou": 1, "grishman": 1, "al": 1, "zhao": 1, "featur": 1, "larg": 1, "explor": 1, "et": 1, "relat": 1, "extract": 1}, "vector_2": [2, 0.1637133922867422, 2, 0, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": [" Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase."], "label": "Non-Prov", "citing": "N07-1015", "vector": [4, 0, 0, 0.17541160386140586], "context": ["", "Zhao and Grishman (2005) and Zhou et al (2005) explored a large set of features that are potentially useful for relation extraction.", ""], "marker": "2005", "vector_1": {"use": 1, "set": 1, "potenti": 1, "zhou": 1, "grishman": 1, "al": 1, "zhao": 1, "featur": 1, "larg": 1, "explor": 1, "et": 1, "relat": 1, "extract": 1}, "vector_2": [2, 0.1637133922867422, 2, 0, 2, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Table 4 separately measures the performance of different relation types and major subtypes."], "label": "Non-Prov", "citing": "N07-1015", "vector": [3, 0, 1, 0.13900960937138318], "context": ["", "Entity Attributes: Previous studies have shown that entity types and entity mention types of arg 1 and arg 2 are very useful (Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006b).", ""], "marker": "Zhou et al., 2005", "vector_1": {"shown": 1, "previou": 1, "use": 1, "mention": 1, "arg": 2, "studi": 1, "entiti": 3, "type": 2, "attribut": 1}, "vector_2": [2, 0.4683097407619112, 3, 2, 9, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion."], "label": "Non-Prov", "citing": "N07-1015", "vector": [3, 0, 0, 0.0], "context": ["", "Entity Attributes: Previous studies have shown that entity types and entity mention types of arg 1 and arg 2 are very useful (Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006b).", ""], "marker": "Zhou et al., 2005", "vector_1": {"shown": 1, "previou": 1, "use": 1, "mention": 1, "arg": 2, "studi": 1, "entiti": 3, "type": 2, "attribut": 1}, "vector_2": [2, 0.4683097407619112, 3, 2, 9, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking."], "label": "Non-Prov", "citing": "N07-1015", "vector": [4, 0, 0, 0.06286946134619316], "context": ["", "Entity Attributes: Previous studies have shown that entity types and entity mention types of arg 1 and arg 2 are very useful (Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006b).", ""], "marker": "Zhou et al., 2005", "vector_1": {"shown": 1, "previou": 1, "use": 1, "mention": 1, "arg": 2, "studi": 1, "entiti": 3, "type": 2, "attribut": 1}, "vector_2": [2, 0.4683097407619112, 3, 2, 9, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["For example, we want to determine whether a person is at a location, based on the evidence in the context.", "Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query Who is the president of the United States?."], "label": "Non-Prov", "citing": "N07-1015", "vector": [0, 0, 0, 0.0], "context": ["", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", ""], "marker": "2005", "vector_1": {"bagofword": 1, "featur": 1, "zhou": 1, "grishman": 1, "al": 1, "also": 1, "zhao": 1, "explor": 1, "et": 1}, "vector_2": [2, 0.5063716754648465, 2, 0, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.", "Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect."], "label": "Non-Prov", "citing": "N07-1015", "vector": [2, 0, 0, 0.06085806194501846], "context": ["", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", ""], "marker": "2005", "vector_1": {"bagofword": 1, "featur": 1, "zhou": 1, "grishman": 1, "al": 1, "also": 1, "zhao": 1, "explor": 1, "et": 1}, "vector_2": [2, 0.5063716754648465, 2, 0, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Two features are defined to include this information:  ET1Country: the entity type of M1 when M2 is a country name  CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other- Personal.", "This trigger word list is first gathered from WordNet by checking whether a word has the semantic class person||relative."], "label": "Non-Prov", "citing": "N07-1015", "vector": [3, 0, 0, 0.035533452725935076], "context": ["", "Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et. al. (2005).", ""], "marker": "2005", "vector_1": {"bagofword": 1, "featur": 1, "zhou": 1, "grishman": 1, "al": 1, "also": 1, "zhao": 1, "explor": 1, "et": 1}, "vector_2": [2, 0.5063716754648465, 2, 0, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["2 Joachims has just released a new version of SVMLight."], "label": "Non-Prov", "citing": "N07-1015", "vector": [0, 0, 0, 0.0], "context": ["", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", ""], "marker": "2005", "vector_1": {"featur": 1, "depend": 2, "zhou": 1, "relat": 1, "al": 1, "grishman": 1, "zhao": 1, "mooney": 1, "explor": 1, "et": 1, "path": 1, "bunescu": 1}, "vector_2": [2, 0.5275209307017249, 3, 0, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": [" ET1DW1: combination of the entity type and the dependent word for M1  H1DW1: combination of the head word and the dependent word for M1  ET2DW2: combination of the entity type and the dependent word for M2  H2DW2: combination of the head word and the dependent word for M2  ET12SameNP: combination of ET12 and whether M1 and M2 included in the same NP  ET12SamePP: combination of ET12 and whether M1 and M2 exist in the same PP  ET12SameVP: combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree."], "label": "Non-Prov", "citing": "N07-1015", "vector": [2, 0, 0, 0.04879500364742665], "context": ["", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", ""], "marker": "2005", "vector_1": {"featur": 1, "depend": 2, "zhou": 1, "relat": 1, "al": 1, "grishman": 1, "zhao": 1, "mooney": 1, "explor": 1, "et": 1, "path": 1, "bunescu": 1}, "vector_2": [2, 0.5275209307017249, 3, 0, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["However, The remaining words that do not have above four classes are manually classified."], "label": "Non-Prov", "citing": "N07-1015", "vector": [1, 0, 0, 0.0], "context": ["", "Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et. al. (2005).", ""], "marker": "2005", "vector_1": {"featur": 1, "depend": 2, "zhou": 1, "relat": 1, "al": 1, "grishman": 1, "zhao": 1, "mooney": 1, "explor": 1, "et": 1, "path": 1, "bunescu": 1}, "vector_2": [2, 0.5275209307017249, 3, 0, 2, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes."], "label": "Non-Prov", "citing": "N09-3012", "vector": [5, 0, 0, 0.0], "context": ["", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", ""], "marker": "Zhou et al., 2005", "vector_1": {"concentr": 1, "use": 1, "featur": 2, "syntact": 1, "captur": 1, "perform": 1, "relat": 1, "better": 1, "achiev": 1, "paper": 1, "necessari": 1, "although": 1, "proper": 1, "combin": 1, "extract": 1, "order": 1, "best": 1}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Non-Prov", "citing": "N09-3012", "vector": [8, 0, 2, 0.2051956704170308], "context": ["", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", ""], "marker": "Zhou et al., 2005", "vector_1": {"concentr": 1, "use": 1, "featur": 2, "syntact": 1, "captur": 1, "perform": 1, "relat": 1, "better": 1, "achiev": 1, "paper": 1, "necessari": 1, "although": 1, "proper": 1, "combin": 1, "extract": 1, "order": 1, "best": 1}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["4.5 Base Phrase Chunking."], "label": "Non-Prov", "citing": "N09-3012", "vector": [0, 0, 0, 0.0], "context": ["", "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", ""], "marker": "Zhou et al., 2005", "vector_1": {"concentr": 1, "use": 1, "featur": 2, "syntact": 1, "captur": 1, "perform": 1, "relat": 1, "better": 1, "achiev": 1, "paper": 1, "necessari": 1, "although": 1, "proper": 1, "combin": 1, "extract": 1, "order": 1, "best": 1}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["4.5 Base Phrase Chunking."], "label": "Non-Prov", "citing": "N13-1093", "vector": [0, 0, 0, 0.0], "context": ["", "The former is Zhou et al (2005), which uses 51 different features.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "differ": 1, "al": 1, "et": 1, "former": 1}, "vector_2": [8, 0.43778286566210534, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["GrandParent, Spouse and Sibling are automatically set with the same classes without change."], "label": "Non-Prov", "citing": "N13-1093", "vector": [1, 0, 0, 0.0], "context": ["", "The former is Zhou et al (2005), which uses 51 different features.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "differ": 1, "al": 1, "et": 1, "former": 1}, "vector_2": [8, 0.43778286566210534, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["8 66 .7 74 .7 77 .2 60 .7 68 .0 6 3."], "label": "Non-Prov", "citing": "N13-1093", "vector": [0, 0, 0, 0.0], "context": ["", "The former is Zhou et al (2005), which uses 51 different features.", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "zhou": 1, "differ": 1, "al": 1, "et": 1, "former": 1}, "vector_2": [8, 0.43778286566210534, 1, 0, 0, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Table 2 also measures the contributions of different features by gradually increasing the feature set."], "label": "Non-Prov", "citing": "N13-1093", "vector": [2, 0, 0, 0.055048188256318034], "context": ["", "These experiments are done using Zhou et al (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", ""], "marker": "2005", "vector_1": {"kernel": 4, "use": 1, "tpwf": 1, "zhou": 1, "f": 1, "kh": 2, "differ": 1, "al": 1, "ybrid": 1, "version": 1, "done": 1, "sl": 1, "et": 1, "experi": 1, "propos": 1}, "vector_2": [8, 0.7083222405622703, 1, 0, 0, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["explicit relations occur in text with explicit evidence suggesting the relationships."], "label": "Non-Prov", "citing": "N13-1093", "vector": [0, 0, 0, 0.0], "context": ["", "These experiments are done using Zhou et al (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", ""], "marker": "2005", "vector_1": {"kernel": 4, "use": 1, "tpwf": 1, "zhou": 1, "f": 1, "kh": 2, "differ": 1, "al": 1, "ybrid": 1, "version": 1, "done": 1, "sl": 1, "et": 1, "experi": 1, "propos": 1}, "vector_2": [8, 0.7083222405622703, 1, 0, 0, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a NONE class for the case where the two mentions are not related."], "label": "Non-Prov", "citing": "N13-1093", "vector": [2, 0, 0, 0.0], "context": ["", "These experiments are done using Zhou et al (2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.", ""], "marker": "2005", "vector_1": {"kernel": 4, "use": 1, "tpwf": 1, "zhou": 1, "f": 1, "kh": 2, "differ": 1, "al": 1, "ybrid": 1, "version": 1, "done": 1, "sl": 1, "et": 1, "experi": 1, "propos": 1}, "vector_2": [8, 0.7083222405622703, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features."], "label": "Non-Prov", "citing": "N13-1093", "vector": [2, 0, 0, 0.08333333333333334], "context": ["", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"zhou": 1, "fold": 1, "perform": 1, "cross": 1, "classifi": 1, "al": 1, "also": 1, "valid": 1, "combin": 1, "et": 1, "experi": 1, "stage": 1}, "vector_2": [8, 0.7383525903838987, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word."], "label": "Non-Prov", "citing": "N13-1093", "vector": [3, 0, 0, 0.0], "context": ["", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"zhou": 1, "fold": 1, "perform": 1, "cross": 1, "classifi": 1, "al": 1, "also": 1, "valid": 1, "combin": 1, "et": 1, "experi": 1, "stage": 1}, "vector_2": [8, 0.7383525903838987, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect."], "label": "Non-Prov", "citing": "N13-1093", "vector": [4, 0, 1, 0.0], "context": ["", "We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"zhou": 1, "fold": 1, "perform": 1, "cross": 1, "classifi": 1, "al": 1, "also": 1, "valid": 1, "combin": 1, "et": 1, "experi": 1, "stage": 1}, "vector_2": [8, 0.7383525903838987, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["3 4 6 . 4 5 7 . 1 S O CI A L 9 5 6 0 2 1 74."], "label": "Non-Prov", "citing": "N13-1095", "vector": [1, 0, 0, 0.0], "context": ["", "Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a).", ""], "marker": "Zhou et al., 2005", "vector_1": {"problem": 1, "relat": 1, "extract": 1, "wellstudi": 1}, "vector_2": [8, 0.05677339901477833, 4, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features."], "label": "Non-Prov", "citing": "N13-1095", "vector": [0, 0, 0, 0.0], "context": ["", "Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a).", ""], "marker": "Zhou et al., 2005", "vector_1": {"problem": 1, "relat": 1, "extract": 1, "wellstudi": 1}, "vector_2": [8, 0.05677339901477833, 4, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Since a pronominal mention (especially neutral pronoun such as it and its) contains little information about the sense of the mention, the co- reference chain is used to decide its sense."], "label": "Non-Prov", "citing": "N13-1095", "vector": [2, 0, 0, 0.0], "context": ["", "Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a).", ""], "marker": "Zhou et al., 2005", "vector_1": {"problem": 1, "relat": 1, "extract": 1, "wellstudi": 1}, "vector_2": [8, 0.05677339901477833, 4, 1, 0, 0]}, {"function": "Weak", "cited": "P05-1053", "provenance": ["This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list."], "label": "Non-Prov", "citing": "P06-1016", "vector": [4, 0, 1, 0.0700140042014005], "context": ["", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", ""], "marker": "Zhou et al 2005", "vector_1": {"major": 1, "zhou": 1, "spars": 1, "data": 1, "challeng": 1, "relat": 1, "al": 1, "due": 1, "one": 1, "et": 1, "problem": 1, "extract": 1}, "vector_2": [1, 0.07524341580207503, 0, 0, 1, 0]}, {"function": "Weak", "cited": "P05-1053", "provenance": [" To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively."], "label": "Non-Prov", "citing": "P06-1016", "vector": [2, 0, 0, 0.0], "context": ["", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", ""], "marker": "Zhou et al 2005", "vector_1": {"major": 1, "zhou": 1, "spars": 1, "data": 1, "challeng": 1, "relat": 1, "al": 1, "due": 1, "one": 1, "et": 1, "problem": 1, "extract": 1}, "vector_2": [1, 0.07524341580207503, 0, 0, 1, 0]}, {"function": "Weak", "cited": "P05-1053", "provenance": [" Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase."], "label": "Non-Prov", "citing": "P06-1016", "vector": [3, 0, 2, 0.09128709291752768], "context": ["", "One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).", ""], "marker": "Zhou et al 2005", "vector_1": {"major": 1, "zhou": 1, "spars": 1, "data": 1, "challeng": 1, "relat": 1, "al": 1, "due": 1, "one": 1, "et": 1, "problem": 1, "extract": 1}, "vector_2": [1, 0.07524341580207503, 0, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.", "Finally, Table 6 shows the distributions of errors."], "label": "Non-Prov", "citing": "P06-1016", "vector": [6, 1, 1, 0.1843024451936214], "context": ["", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", ""], "marker": "Zhou et al 2005", "vector_1": {"among": 1, "entropi": 1, "deal": 1, "miller": 1, "al": 2, "et": 2, "extract": 1, "differ": 1, "uneven": 1, "variou": 1, "support": 1, "grisman": 1, "much": 1, "appli": 1, "approach": 1, "strategi": 1, "machin": 2, "spars": 1, "distribut": 1, "zhou": 1, "gener": 1, "relat": 2, "inher": 1, "data": 1, "problem": 1, "task": 1, "explicit": 1, "maximum": 1, "caus": 1, "zhao": 1, "vector": 1, "kambhatla": 1, "learn": 2, "model": 1, "propos": 1}, "vector_2": [1, 0.105219473264166, 0, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Moreover, we only apply the simple linear kernel, although other kernels can peform better.", "The reason why we choose SVMs for this purpose is that SVMs represent the state-ofthe-art in the machine learning research community, and there are good implementations of the algorithm available."], "label": "Non-Prov", "citing": "P06-1016", "vector": [6, 0, 2, 0.08674723979493604], "context": ["", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", ""], "marker": "Zhou et al 2005", "vector_1": {"among": 1, "entropi": 1, "deal": 1, "miller": 1, "al": 2, "et": 2, "extract": 1, "differ": 1, "uneven": 1, "variou": 1, "support": 1, "grisman": 1, "much": 1, "appli": 1, "approach": 1, "strategi": 1, "machin": 2, "spars": 1, "distribut": 1, "zhou": 1, "gener": 1, "relat": 2, "inher": 1, "data": 1, "problem": 1, "task": 1, "explicit": 1, "maximum": 1, "caus": 1, "zhao": 1, "vector": 1, "kambhatla": 1, "learn": 2, "model": 1, "propos": 1}, "vector_2": [1, 0.105219473264166, 0, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["However, this paper only uses the binary-class version.", "For details about SVMLight, please see http://svmlight.joachims.org/"], "label": "Non-Prov", "citing": "P06-1016", "vector": [1, 0, 0, 0.0], "context": ["", "While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.", ""], "marker": "Zhou et al 2005", "vector_1": {"among": 1, "entropi": 1, "deal": 1, "miller": 1, "al": 2, "et": 2, "extract": 1, "differ": 1, "uneven": 1, "variou": 1, "support": 1, "grisman": 1, "much": 1, "appli": 1, "approach": 1, "strategi": 1, "machin": 2, "spars": 1, "distribut": 1, "zhou": 1, "gener": 1, "relat": 2, "inher": 1, "data": 1, "problem": 1, "task": 1, "explicit": 1, "maximum": 1, "caus": 1, "zhao": 1, "vector": 1, "kambhatla": 1, "learn": 2, "model": 1, "propos": 1}, "vector_2": [1, 0.105219473264166, 0, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Head-driven statistical models for natural language parsing."], "label": "Non-Prov", "citing": "P06-1016", "vector": [2, 0, 1, 0.09901475429766744], "context": ["", "With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"process": 1, "natur": 1, "within": 1, "al": 4, "mooney": 2, "et": 4, "attract": 1, "languag": 1, "commun": 1, "yih": 1, "research": 1, "start": 1, "grisman": 1, "includ": 1, "miller": 1, "bunescu": 2, "machin": 1, "zhou": 1, "zhang": 1, "sorensen": 1, "zelenko": 1, "culotta": 1, "increas": 1, "task": 1, "ace": 1, "work": 1, "roth": 1, "zhao": 1, "kambhatla": 1, "learn": 1, "popular": 1, "typic": 1}, "vector_2": [1, 0.1599361532322426, 10, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Last, effective ways need to be explored to incorporate information embedded in the full Collins M."], "label": "Non-Prov", "citing": "P06-1016", "vector": [2, 0, 0, 0.0], "context": ["", "With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"process": 1, "natur": 1, "within": 1, "al": 4, "mooney": 2, "et": 4, "attract": 1, "languag": 1, "commun": 1, "yih": 1, "research": 1, "start": 1, "grisman": 1, "includ": 1, "miller": 1, "bunescu": 2, "machin": 1, "zhou": 1, "zhang": 1, "sorensen": 1, "zelenko": 1, "culotta": 1, "increas": 1, "task": 1, "ace": 1, "work": 1, "roth": 1, "zhao": 1, "kambhatla": 1, "learn": 1, "popular": 1, "typic": 1}, "vector_2": [1, 0.1599361532322426, 10, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["However, when a preposition exists in the mention, its head word is set as the last word before the preposition."], "label": "Non-Prov", "citing": "P06-1016", "vector": [2, 0, 0, 0.0], "context": ["", "With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"process": 1, "natur": 1, "within": 1, "al": 4, "mooney": 2, "et": 4, "attract": 1, "languag": 1, "commun": 1, "yih": 1, "research": 1, "start": 1, "grisman": 1, "includ": 1, "miller": 1, "bunescu": 2, "machin": 1, "zhou": 1, "zhang": 1, "sorensen": 1, "zelenko": 1, "culotta": 1, "increas": 1, "task": 1, "ace": 1, "work": 1, "roth": 1, "zhao": 1, "kambhatla": 1, "learn": 1, "popular": 1, "typic": 1}, "vector_2": [1, 0.1599361532322426, 10, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Therefore, they are HM12+M1>M2; 4) HM12+M1<M2."], "label": "Non-Prov", "citing": "P06-1016", "vector": [0, 0, 0, 0.0], "context": ["", "Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", ""], "marker": "", "vector_1": {"corpu": 1, "semant": 1, "featur": 1, "al": 1, "explor": 1, "respect": 1, "et": 1, "systemat": 1, "support": 1, "type": 1, "divers": 1, "machin": 1, "syntact": 1, "zhou": 1, "relat": 2, "lexic": 1, "measur": 1, "ace": 1, "f": 1, "achiev": 1, "vector": 1, "rdc": 1, "subtyp": 1}, "vector_2": [8, 0.243926576217079, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set."], "label": "Non-Prov", "citing": "P06-1016", "vector": [9, 0, 3, 0.3149448894660933], "context": ["", "Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", ""], "marker": "", "vector_1": {"corpu": 1, "semant": 1, "featur": 1, "al": 1, "explor": 1, "respect": 1, "et": 1, "systemat": 1, "support": 1, "type": 1, "divers": 1, "machin": 1, "syntact": 1, "zhou": 1, "relat": 2, "lexic": 1, "measur": 1, "ace": 1, "f": 1, "achiev": 1, "vector": 1, "rdc": 1, "subtyp": 1}, "vector_2": [8, 0.243926576217079, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["We use the official ACE corpus from LDC."], "label": "Non-Prov", "citing": "P06-1016", "vector": [3, 0, 0, 0.17541160386140586], "context": ["", "Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.", ""], "marker": "", "vector_1": {"corpu": 1, "semant": 1, "featur": 1, "al": 1, "explor": 1, "respect": 1, "et": 1, "systemat": 1, "support": 1, "type": 1, "divers": 1, "machin": 1, "syntact": 1, "zhou": 1, "relat": 2, "lexic": 1, "measur": 1, "ace": 1, "f": 1, "achiev": 1, "vector": 1, "rdc": 1, "subtyp": 1}, "vector_2": [8, 0.243926576217079, 1, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["1 5 3 . 7 6 2 . 3 Manag ement 1 6 5 1 0 6 7 2 59.", "6 6 4 . 2 6 1 . 8 Memb er 2 2 4 1 0 4 3 6 74.", "3 4 6 . 4 5 7 . 1 S O CI A L 9 5 6 0 2 1 74.", "1 6 3 . 2 6 8 . 5Other Profes sional 2 9 1 6 3 2 33."], "label": "Non-Prov", "citing": "P06-1016", "vector": [0, 0, 0, 0.0], "context": ["", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", ""], "marker": "2005", "vector_1": {"involv": 1, "explicitli": 1, "zhou": 1, "relat": 1, "explicit": 1, "al": 1, "argument": 1, "mention": 1, "two": 1, "et": 1, "model": 2, "order": 1}, "vector_2": [1, 0.633583399840383, 1, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Covolution kernels for natural language.", "In Dietterich T.G., Becker S. and Ghahramani Z. editors.", "Advances in Neural Information Processing Systems 14.", "Cambridge, MA."], "label": "Non-Prov", "citing": "P06-1016", "vector": [1, 0, 0, 0.0], "context": ["", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", ""], "marker": "2005", "vector_1": {"involv": 1, "explicitli": 1, "zhou": 1, "relat": 1, "explicit": 1, "al": 1, "argument": 1, "mention": 1, "two": 1, "et": 1, "model": 2, "order": 1}, "vector_2": [1, 0.633583399840383, 1, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["The testing set is held out only for final evaluation.", "It consists of 97 documents (~50k words) and 1386 instances of relations.", "Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set.", "It shows that the"], "label": "Non-Prov", "citing": "P06-1016", "vector": [5, 0, 0, 0.08728715609439694], "context": ["", "Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.", ""], "marker": "2005", "vector_1": {"involv": 1, "explicitli": 1, "zhou": 1, "relat": 1, "explicit": 1, "al": 1, "argument": 1, "mention": 1, "two": 1, "et": 1, "model": 2, "order": 1}, "vector_2": [1, 0.633583399840383, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance."], "label": "Non-Prov", "citing": "P06-1104", "vector": [8, 0, 2, 0.2631174057921088], "context": ["", "Many techniques on relation extraction, such as rule-based (MUC, 19871998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature.", ""], "marker": "Zhou et al., 2005", "vector_1": {"rulebas": 1, "techniqu": 1, "featurebas": 1, "relat": 1, "literatur": 1, "kambhatla": 1, "kernelbas": 1, "mani": 1, "extract": 1, "propos": 1}, "vector_2": [1, 0.14889867841409693, 6, 2, 0, 1]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["9 2 2 . 9 3 6 . 4 Relative Locati on 3 5 8 1 88."], "label": "Non-Prov", "citing": "P06-1104", "vector": [1, 0, 0, 0.0], "context": ["", "Many techniques on relation extraction, such as rule-based (MUC, 19871998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature.", ""], "marker": "Zhou et al., 2005", "vector_1": {"rulebas": 1, "techniqu": 1, "featurebas": 1, "relat": 1, "literatur": 1, "kambhatla": 1, "kernelbas": 1, "mani": 1, "extract": 1, "propos": 1}, "vector_2": [1, 0.14889867841409693, 6, 2, 0, 1]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["9 2 8 . 8 4 0 . 4 N EA R 3 5 8 1 88."], "label": "Non-Prov", "citing": "P06-1104", "vector": [0, 0, 0, 0.0], "context": ["", "Many techniques on relation extraction, such as rule-based (MUC, 19871998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature.", ""], "marker": "Zhou et al., 2005", "vector_1": {"rulebas": 1, "techniqu": 1, "featurebas": 1, "relat": 1, "literatur": 1, "kambhatla": 1, "kernelbas": 1, "mani": 1, "extract": 1, "propos": 1}, "vector_2": [1, 0.14889867841409693, 6, 2, 0, 1]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Table 5 separates the performance of relation detection from overall performance on the testing set."], "label": "Non-Prov", "citing": "P06-1104", "vector": [1, 0, 0, 0.0], "context": ["", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "task": 1, "syntact": 1, "featurebas": 1, "lexic": 1, "divers": 1, "employ": 1, "amount": 1, "featur": 2, "larg": 1, "linguist": 1, "method": 1}, "vector_2": [1, 0.15784479837343274, 3, 2, 0, 1]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427434, Ann Arbor, June 2005."], "label": "Non-Prov", "citing": "P06-1104", "vector": [1, 1, 0, 0.0], "context": ["", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "task": 1, "syntact": 1, "featurebas": 1, "lexic": 1, "divers": 1, "employ": 1, "amount": 1, "featur": 2, "larg": 1, "linguist": 1, "method": 1}, "vector_2": [1, 0.15784479837343274, 3, 2, 0, 1]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["1 6 3 . 2 6 8 . 5Other Profes sional 2 9 1 6 3 2 33."], "label": "Non-Prov", "citing": "P06-1104", "vector": [0, 0, 0, 0.0], "context": ["", "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "task": 1, "syntact": 1, "featurebas": 1, "lexic": 1, "divers": 1, "employ": 1, "amount": 1, "featur": 2, "larg": 1, "linguist": 1, "method": 1}, "vector_2": [1, 0.15784479837343274, 3, 2, 0, 1]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall."], "label": "Non-Prov", "citing": "P06-2012", "vector": [1, 0, 0, 0.0], "context": ["", "Many methods have been proposed to deal with this task, including supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "task": 1, "algorithm": 3, "deal": 1, "semisupervis": 1, "unsupervis": 1, "includ": 1, "learn": 3, "mani": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.037396121883656507, 9, 19, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data."], "label": "Non-Prov", "citing": "P06-2012", "vector": [4, 0, 0, 0.04550157551932901], "context": ["", "Many methods have been proposed to deal with this task, including supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "task": 1, "algorithm": 3, "deal": 1, "semisupervis": 1, "unsupervis": 1, "includ": 1, "learn": 3, "mani": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.037396121883656507, 9, 19, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance."], "label": "Non-Prov", "citing": "P06-2012", "vector": [1, 0, 0, 0.0], "context": ["", "Many methods have been proposed to deal with this task, including supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "task": 1, "algorithm": 3, "deal": 1, "semisupervis": 1, "unsupervis": 1, "includ": 1, "learn": 3, "mani": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.037396121883656507, 9, 19, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Non-Prov", "citing": "P09-1113", "vector": [4, 0, 0, 0.15511334686589623], "context": ["", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 2, "use": 1, "featur": 2, "syntact": 2, "ace": 1, "perform": 2, "data": 1, "unsupervis": 1, "least": 1, "whether": 1, "know": 1, "clean": 1, "known": 1, "improv": 2, "handlabel": 1, "ie": 2, "distantli": 1}, "vector_2": [4, 0.20506768938276462, 2, 4, 3, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Most of the chunking features concern about the head words of the phrases between the two mentions."], "label": "Non-Prov", "citing": "P09-1113", "vector": [3, 0, 0, 0.11952286093343936], "context": ["", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 2, "use": 1, "featur": 2, "syntact": 2, "ace": 1, "perform": 2, "data": 1, "unsupervis": 1, "least": 1, "whether": 1, "know": 1, "clean": 1, "known": 1, "improv": 2, "handlabel": 1, "ie": 2, "distantli": 1}, "vector_2": [4, 0.20506768938276462, 2, 4, 3, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE."], "label": "Non-Prov", "citing": "P09-1113", "vector": [3, 0, 0, 0.045175395145262566], "context": ["", "While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 2, "use": 1, "featur": 2, "syntact": 2, "ace": 1, "perform": 2, "data": 1, "unsupervis": 1, "least": 1, "whether": 1, "know": 1, "clean": 1, "known": 1, "improv": 2, "handlabel": 1, "ie": 2, "distantli": 1}, "vector_2": [4, 0.20506768938276462, 2, 4, 3, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1."], "label": "Non-Prov", "citing": "P09-1113", "vector": [4, 0, 1, 0.0], "context": ["", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al (2005), and work in the ACE paradigm such as Zhou et al (2005) and Zhou et al (2007).", ""], "marker": "2005", "vector_1": {"al": 3, "exploit": 1, "et": 3, "deriv": 1, "use": 1, "depend": 1, "lin": 1, "snow": 1, "includ": 1, "input": 1, "approach": 1, "syntact": 2, "zhou": 2, "sentenc": 1, "pars": 1, "recent": 1, "ace": 1, "work": 2, "pantel": 1, "deeper": 1, "inform": 1, "paradigm": 1}, "vector_2": [4, 0.30835545940276005, 4, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data."], "label": "Non-Prov", "citing": "P09-1113", "vector": [4, 0, 1, 0.03183035070396152], "context": ["", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al (2005), and work in the ACE paradigm such as Zhou et al (2005) and Zhou et al (2007).", ""], "marker": "2005", "vector_1": {"al": 3, "exploit": 1, "et": 3, "deriv": 1, "use": 1, "depend": 1, "lin": 1, "snow": 1, "includ": 1, "input": 1, "approach": 1, "syntact": 2, "zhou": 2, "sentenc": 1, "pars": 1, "recent": 1, "ace": 1, "work": 2, "pantel": 1, "deeper": 1, "inform": 1, "paradigm": 1}, "vector_2": [4, 0.30835545940276005, 4, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system."], "label": "Non-Prov", "citing": "P09-1113", "vector": [4, 0, 1, 0.04210759605332595], "context": ["", "More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al (2005), and work in the ACE paradigm such as Zhou et al (2005) and Zhou et al (2007).", ""], "marker": "2005", "vector_1": {"al": 3, "exploit": 1, "et": 3, "deriv": 1, "use": 1, "depend": 1, "lin": 1, "snow": 1, "includ": 1, "input": 1, "approach": 1, "syntact": 2, "zhou": 2, "sentenc": 1, "pars": 1, "recent": 1, "ace": 1, "work": 2, "pantel": 1, "deeper": 1, "inform": 1, "paradigm": 1}, "vector_2": [4, 0.30835545940276005, 4, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Head-driven statistical models for natural language parsing."], "label": "Non-Prov", "citing": "P09-1114", "vector": [1, 0, 0, 0.0], "context": ["", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", ""], "marker": "2005", "vector_1": {"variou": 1, "supervis": 1, "work": 1, "domin": 1, "zhou": 1, "featurebas": 1, "relat": 2, "al": 1, "grishman": 1, "zhao": 1, "featur": 2, "combin": 1, "kernelbas": 1, "learn": 1, "et": 1, "studi": 1, "extract": 2, "method": 1, "recent": 1}, "vector_2": [4, 0.14393990661162617, 2, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1."], "label": "Non-Prov", "citing": "P09-1114", "vector": [2, 0, 0, 0.0], "context": ["", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", ""], "marker": "2005", "vector_1": {"variou": 1, "supervis": 1, "work": 1, "domin": 1, "zhou": 1, "featurebas": 1, "relat": 2, "al": 1, "grishman": 1, "zhao": 1, "featur": 2, "combin": 1, "kernelbas": 1, "learn": 1, "et": 1, "studi": 1, "extract": 2, "method": 1, "recent": 1}, "vector_2": [4, 0.14393990661162617, 2, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["We use the official ACE corpus from LDC."], "label": "Non-Prov", "citing": "P09-1114", "vector": [0, 0, 0, 0.0], "context": ["", "Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.", ""], "marker": "2005", "vector_1": {"variou": 1, "supervis": 1, "work": 1, "domin": 1, "zhou": 1, "featurebas": 1, "relat": 2, "al": 1, "grishman": 1, "zhao": 1, "featur": 2, "combin": 1, "kernelbas": 1, "learn": 1, "et": 1, "studi": 1, "extract": 2, "method": 1, "recent": 1}, "vector_2": [4, 0.14393990661162617, 2, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1.", "Note that only 6 of these 24 relation subtypes are symmetric: Relative- Location, Associate, Other-Relative, Other- Professional, Sibling, and Spouse."], "label": "Non-Prov", "citing": "P09-1114", "vector": [2, 0, 0, 0.0445435403187374], "context": ["", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "shown": 1, "work": 1, "engin": 1, "coupl": 1, "kernel": 1, "intellig": 1, "relat": 1, "solut": 1, "provid": 1, "featur": 1, "design": 1, "stateoftheart": 1, "learn": 1, "problem": 1, "extract": 1, "recent": 1}, "vector_2": [4, 0.04172024091493537, 4, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.", "The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus."], "label": "Non-Prov", "citing": "P09-1114", "vector": [5, 0, 1, 0.09072184232530289], "context": ["", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "shown": 1, "work": 1, "engin": 1, "coupl": 1, "kernel": 1, "intellig": 1, "relat": 1, "solut": 1, "provid": 1, "featur": 1, "design": 1, "stateoftheart": 1, "learn": 1, "problem": 1, "extract": 1, "recent": 1}, "vector_2": [4, 0.04172024091493537, 4, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.", "It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall."], "label": "Non-Prov", "citing": "P09-1114", "vector": [5, 0, 0, 0.040422604172722164], "context": ["", "Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "shown": 1, "work": 1, "engin": 1, "coupl": 1, "kernel": 1, "intellig": 1, "relat": 1, "solut": 1, "provid": 1, "featur": 1, "design": 1, "stateoftheart": 1, "learn": 1, "problem": 1, "extract": 1, "recent": 1}, "vector_2": [4, 0.04172024091493537, 4, 2, 2, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["The testing set is held out only for final evaluation."], "label": "Non-Prov", "citing": "P11-1053", "vector": [2, 0, 0, 0.0], "context": ["", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "explicitli": 1, "syntact": 1, "gener": 1, "learn": 1, "lexic": 1, "discrimin": 1, "featur": 2, "statist": 1, "base": 1, "varieti": 1, "extract": 1, "method": 1, "either": 1}, "vector_2": [6, 0.040273082351396594, 6, 5, 4, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This category of features includes:  #MB: number of other mentions in between  #WB: number of words in between  M1>M2 or M1<M2: flag indicating whether M2/M1is included in M1/M2."], "label": "Non-Prov", "citing": "P11-1053", "vector": [3, 0, 0, 0.051639777949432225], "context": ["", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "explicitli": 1, "syntact": 1, "gener": 1, "learn": 1, "lexic": 1, "discrimin": 1, "featur": 2, "statist": 1, "base": 1, "varieti": 1, "extract": 1, "method": 1, "either": 1}, "vector_2": [6, 0.040273082351396594, 6, 5, 4, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Ph.D. Dissertation, University of Pennsylvania."], "label": "Non-Prov", "citing": "P11-1053", "vector": [1, 0, 0, 0.0], "context": ["", "Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "explicitli": 1, "syntact": 1, "gener": 1, "learn": 1, "lexic": 1, "discrimin": 1, "featur": 2, "statist": 1, "base": 1, "varieti": 1, "extract": 1, "method": 1, "either": 1}, "vector_2": [6, 0.040273082351396594, 6, 5, 4, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype Founder under the type ROLE.", "It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes Based-In, Located and Residence under the type AT, which are difficult even for human experts to differentiate."], "label": "Non-Prov", "citing": "P11-1053", "vector": [3, 0, 0, 0.0], "context": ["", "We first adopted the full feature set from Zhou et al (2005), a state-of-the-art feature based relation extraction system.", ""], "marker": "2005", "vector_1": {"set": 1, "zhou": 1, "adopt": 1, "al": 1, "system": 1, "featur": 2, "full": 1, "stateoftheart": 1, "base": 1, "et": 1, "relat": 1, "extract": 1, "first": 1}, "vector_2": [6, 0.335313618012932, 1, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["8 5 1 . 9 4 3 . 8 R O LE 6 9 9 4 4 3 8 2 84.", "4 6 3 . 4 7 2 . 4 Citize n-Of 3 6 2 5 8 75."], "label": "Non-Prov", "citing": "P11-1053", "vector": [0, 0, 0, 0.0], "context": ["", "We first adopted the full feature set from Zhou et al (2005), a state-of-the-art feature based relation extraction system.", ""], "marker": "2005", "vector_1": {"set": 1, "zhou": 1, "adopt": 1, "al": 1, "system": 1, "featur": 2, "full": 1, "stateoftheart": 1, "base": 1, "et": 1, "relat": 1, "extract": 1, "first": 1}, "vector_2": [6, 0.335313618012932, 1, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["This category of features concerns about the information inherent only in the full parse tree.", " PTP: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree  PTPH: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path."], "label": "Non-Prov", "citing": "P11-1053", "vector": [2, 0, 1, 0.02795084971874737], "context": ["", "We first adopted the full feature set from Zhou et al (2005), a state-of-the-art feature based relation extraction system.", ""], "marker": "2005", "vector_1": {"set": 1, "zhou": 1, "adopt": 1, "al": 1, "system": 1, "featur": 2, "full": 1, "stateoftheart": 1, "base": 1, "et": 1, "relat": 1, "extract": 1, "first": 1}, "vector_2": [6, 0.335313618012932, 1, 0, 1, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": ["It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features."], "label": "Non-Prov", "citing": "P11-1053", "vector": [2, 0, 0, 0.08006407690254358], "context": ["", "In addition, we cherry-picked the following features which were not included in Zhou et al (2005) but were shown to be quite effective for relation extraction.", ""], "marker": "2005", "vector_1": {"quit": 1, "shown": 1, "featur": 1, "zhou": 1, "cherrypick": 1, "relat": 1, "al": 1, "effect": 1, "includ": 1, "et": 1, "follow": 1, "extract": 1, "addit": 1}, "vector_2": [6, 0.3487379787967309, 1, 0, 1, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": ["Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment."], "label": "Non-Prov", "citing": "P11-1053", "vector": [3, 0, 1, 0.0], "context": ["", "In addition, we cherry-picked the following features which were not included in Zhou et al (2005) but were shown to be quite effective for relation extraction.", ""], "marker": "2005", "vector_1": {"quit": 1, "shown": 1, "featur": 1, "zhou": 1, "cherrypick": 1, "relat": 1, "al": 1, "effect": 1, "includ": 1, "et": 1, "follow": 1, "extract": 1, "addit": 1}, "vector_2": [6, 0.3487379787967309, 1, 0, 1, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": ["It also shows that feature-based methods dramatically outperform kernel methods."], "label": "Non-Prov", "citing": "P11-1053", "vector": [0, 0, 0, 0.0], "context": ["", "In addition, we cherry-picked the following features which were not included in Zhou et al (2005) but were shown to be quite effective for relation extraction.", ""], "marker": "2005", "vector_1": {"quit": 1, "shown": 1, "featur": 1, "zhou": 1, "cherrypick": 1, "relat": 1, "al": 1, "effect": 1, "includ": 1, "et": 1, "follow": 1, "extract": 1, "addit": 1}, "vector_2": [6, 0.3487379787967309, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE."], "label": "Non-Prov", "citing": "P11-1053", "vector": [2, 0, 0, 0.10101525445522107], "context": ["", "Zhou et al (2005) tested their system on the ACE 2003 data;.", ""], "marker": "2005", "vector_1": {"zhou": 1, "ace": 1, "al": 1, "system": 1, "test": 1, "et": 1, "data": 1}, "vector_2": [6, 0.744969967505826, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes."], "label": "Non-Prov", "citing": "P11-1053", "vector": [4, 0, 2, 0.30304576336566325], "context": ["", "Zhou et al (2005) tested their system on the ACE 2003 data;.", ""], "marker": "2005", "vector_1": {"zhou": 1, "ace": 1, "al": 1, "system": 1, "test": 1, "et": 1, "data": 1}, "vector_2": [6, 0.744969967505826, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output."], "label": "Non-Prov", "citing": "P11-1053", "vector": [1, 0, 0, 0.0], "context": ["", "Zhou et al (2005) tested their system on the ACE 2003 data;.", ""], "marker": "2005", "vector_1": {"zhou": 1, "ace": 1, "al": 1, "system": 1, "test": 1, "et": 1, "data": 1}, "vector_2": [6, 0.744969967505826, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": [" PTP: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree  PTPH: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path.", "4.8 Semantic Resources."], "label": "Non-Prov", "citing": "P11-1056", "vector": [2, 0, 0, 0.0], "context": ["", "However, most approaches to RE have assumed that the relations arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", ""], "marker": "Zhou et al., 2005", "vector_1": {"given": 1, "partial": 1, "offer": 1, "approach": 1, "howev": 1, "relat": 1, "solut": 1, "argument": 1, "input": 1, "problem": 1, "therefor": 1, "assum": 1}, "vector_2": [6, 0.03773584905660377, 4, 3, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships.", "Two features are defined to include this information:  ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype."], "label": "Non-Prov", "citing": "P11-1056", "vector": [5, 0, 0, 0.0], "context": ["", "However, most approaches to RE have assumed that the relations arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", ""], "marker": "Zhou et al., 2005", "vector_1": {"given": 1, "partial": 1, "offer": 1, "approach": 1, "howev": 1, "relat": 1, "solut": 1, "argument": 1, "input": 1, "problem": 1, "therefor": 1, "assum": 1}, "vector_2": [6, 0.03773584905660377, 4, 3, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins parser used in our system achieves the state-of-the-art performance.", "Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment."], "label": "Non-Prov", "citing": "P11-1056", "vector": [4, 0, 0, 0.0445435403187374], "context": ["", "However, most approaches to RE have assumed that the relations arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.", ""], "marker": "Zhou et al., 2005", "vector_1": {"given": 1, "partial": 1, "offer": 1, "approach": 1, "howev": 1, "relat": 1, "solut": 1, "argument": 1, "input": 1, "problem": 1, "therefor": 1, "assum": 1}, "vector_2": [6, 0.03773584905660377, 4, 3, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus.", "Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted."], "label": "Non-Prov", "citing": "P11-1056", "vector": [5, 0, 0, 0.09035079029052512], "context": ["", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"given": 1, "evalu": 1, "ace": 1, "mention": 1, "data": 1, "prior": 1, "alreadi": 1, "input": 1, "assum": 1, "preannot": 1}, "vector_2": [6, 0.2830516247379455, 3, 3, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It also shows that feature-based methods dramatically outperform kernel methods.", "This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction."], "label": "Non-Prov", "citing": "P11-1056", "vector": [3, 0, 0, 0.0], "context": ["", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"given": 1, "evalu": 1, "ace": 1, "mention": 1, "data": 1, "prior": 1, "alreadi": 1, "input": 1, "assum": 1, "preannot": 1}, "vector_2": [6, 0.2830516247379455, 3, 3, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In this paper, we separate the features of base phrase chunking from those of full parsing.", "In this way, we can separately evaluate the contributions of base phrase chunking and full parsing."], "label": "Non-Prov", "citing": "P11-1056", "vector": [1, 0, 0, 0.0], "context": ["", "Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"given": 1, "evalu": 1, "ace": 1, "mention": 1, "data": 1, "prior": 1, "alreadi": 1, "input": 1, "assum": 1, "preannot": 1}, "vector_2": [6, 0.2830516247379455, 3, 3, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["However, when a preposition exists in the mention, its head word is set as the last word before the preposition."], "label": "Non-Prov", "citing": "W06-1634", "vector": [1, 0, 0, 0.0], "context": ["", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "allevi": 1, "techniqu": 1, "craft": 1, "manual": 1, "base": 1, "expect": 1, "learn": 1, "problem": 1, "ie": 1}, "vector_2": [1, 0.05796430931923331, 3, 4, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Table 2 also measures the contributions of different features by gradually increasing the feature set."], "label": "Non-Prov", "citing": "W06-1634", "vector": [0, 0, 0, 0.0], "context": ["", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "allevi": 1, "techniqu": 1, "craft": 1, "manual": 1, "base": 1, "expect": 1, "learn": 1, "problem": 1, "ie": 1}, "vector_2": [1, 0.05796430931923331, 3, 4, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In this paper, we only measure the performance of relation extraction on true mentions with true chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus."], "label": "Non-Prov", "citing": "W06-1634", "vector": [4, 0, 0, 0.07071067811865474], "context": ["", "Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "allevi": 1, "techniqu": 1, "craft": 1, "manual": 1, "base": 1, "expect": 1, "learn": 1, "problem": 1, "ie": 1}, "vector_2": [1, 0.05796430931923331, 3, 4, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations."], "label": "Non-Prov", "citing": "W06-1634", "vector": [0, 0, 0, 0.0], "context": ["", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"use": 1, "shallow": 1, "techniqu": 1, "previou": 1, "pars": 1, "mani": 1, "approach": 1}, "vector_2": [1, 0.10300727032385988, 3, 4, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["However, The remaining words that do not have above four classes are manually classified."], "label": "Non-Prov", "citing": "W06-1634", "vector": [2, 0, 0, 0.0], "context": ["", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"use": 1, "shallow": 1, "techniqu": 1, "previou": 1, "pars": 1, "mani": 1, "approach": 1}, "vector_2": [1, 0.10300727032385988, 3, 4, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include:  WM1: bag-of-words in M1  HM1: head word of M1 3 In ACE, each mention has a head annotation and an."], "label": "Non-Prov", "citing": "W06-1634", "vector": [2, 0, 0, 0.0], "context": ["", "A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"use": 1, "shallow": 1, "techniqu": 1, "previou": 1, "pars": 1, "mani": 1, "approach": 1}, "vector_2": [1, 0.10300727032385988, 3, 4, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1."], "label": "Non-Prov", "citing": "W11-1101", "vector": [1, 0, 0, 0.0], "context": ["", "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "previou": 1, "research": 1, "erd": 1, "explor": 1, "varieti": 1}, "vector_2": [6, 0.9046301633045148, 4, 3, 4, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Note that only 6 of these 24 relation subtypes are symmetric: Relative- Location, Associate, Other-Relative, Other- Professional, Sibling, and Spouse."], "label": "Non-Prov", "citing": "W11-1101", "vector": [1, 0, 0, 0.0], "context": ["", "A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "previou": 1, "research": 1, "erd": 1, "explor": 1, "varieti": 1}, "vector_2": [6, 0.9046301633045148, 4, 3, 4, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities."], "label": "Non-Prov", "citing": "W11-1101", "vector": [2, 0, 0, 0.0], "context": ["", "Researchers have used supervised and semi-supervised approaches (Hasegawa et al., 2004; Mintz et al., 2009; Jiang, 2009), and explored rich features (Kambhatla, 2004), kernel design (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008) and inference algorithms (Chan and Roth, 2011), to detect predefined relations between NEs.", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "use": 1, "featur": 1, "predefin": 1, "rich": 1, "algorithm": 1, "semisupervis": 1, "kernel": 1, "relat": 1, "ne": 1, "research": 1, "detect": 1, "design": 1, "explor": 1, "approach": 1, "infer": 1}, "vector_2": [6, 0.05106628242074928, 9, 3, 4, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree."], "label": "Non-Prov", "citing": "W11-1101", "vector": [2, 1, 0, 0.0545544725589981], "context": ["", "Researchers have used supervised and semi-supervised approaches (Hasegawa et al., 2004; Mintz et al., 2009; Jiang, 2009), and explored rich features (Kambhatla, 2004), kernel design (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008) and inference algorithms (Chan and Roth, 2011), to detect predefined relations between NEs.", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "use": 1, "featur": 1, "predefin": 1, "rich": 1, "algorithm": 1, "semisupervis": 1, "kernel": 1, "relat": 1, "ne": 1, "research": 1, "detect": 1, "design": 1, "explor": 1, "approach": 1, "infer": 1}, "vector_2": [6, 0.05106628242074928, 9, 3, 4, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["for multi-class classification."], "label": "Non-Prov", "citing": "W11-1101", "vector": [0, 0, 0, 0.0], "context": ["", "Researchers have used supervised and semi-supervised approaches (Hasegawa et al., 2004; Mintz et al., 2009; Jiang, 2009), and explored rich features (Kambhatla, 2004), kernel design (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008) and inference algorithms (Chan and Roth, 2011), to detect predefined relations between NEs.", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "use": 1, "featur": 1, "predefin": 1, "rich": 1, "algorithm": 1, "semisupervis": 1, "kernel": 1, "relat": 1, "ne": 1, "research": 1, "detect": 1, "design": 1, "explor": 1, "approach": 1, "infer": 1}, "vector_2": [6, 0.05106628242074928, 9, 3, 4, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["9 2 8 . 8 4 0 . 4 N EA R 3 5 8 1 88.", "9 2 2 . 9 3 6 . 4 Relative Locati on 3 5 8 1 88.", "9 2 2 . 9 3 6 . 4 P A R T 1 6 4 1 0 6 3 9 73."], "label": "Non-Prov", "citing": "W11-1815", "vector": [1, 0, 0, 0.0], "context": ["", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"task": 1, "locat": 1, "bb": 1, "challeng": 1, "geograph": 1, "ureaplasma": 1, "biolog": 1, "exampl": 1, "mycoplasma": 1, "parvum": 1, "pathogen": 1}, "vector_2": [6, 0.041832963784183295, 2, 7, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["For each pair of mentions3, we compute various lexical, syntactic and semantic features.", "4.1 Words.", "According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2."], "label": "Non-Prov", "citing": "W11-1815", "vector": [1, 0, 0, 0.0], "context": ["", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"task": 1, "locat": 1, "bb": 1, "challeng": 1, "geograph": 1, "ureaplasma": 1, "biolog": 1, "exampl": 1, "mycoplasma": 1, "parvum": 1, "pathogen": 1}, "vector_2": [6, 0.041832963784183295, 2, 7, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["4 In this paper, the head word of a mention is normally.", "set as the last word of the mention.", "However, when a preposition exists in the mention, its head word is set as the last word before the preposition."], "label": "Non-Prov", "citing": "W11-1815", "vector": [2, 0, 0, 0.0], "context": ["", "BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"task": 1, "locat": 1, "bb": 1, "challeng": 1, "geograph": 1, "ureaplasma": 1, "biolog": 1, "exampl": 1, "mycoplasma": 1, "parvum": 1, "pathogen": 1}, "vector_2": [6, 0.041832963784183295, 2, 7, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We see verb classes as the key to making gen eralizations about regular extensions of mean ing."], "label": "Non-Prov", "citing": "A00-2034", "vector": [0, 0, 0, 0.0], "context": ["", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al.,  1998).", ""], "marker": "Dang et al, 1998", "vector_1": {"levin": 1, "classif": 1, "research": 1, "extend": 1, "nlp": 1}, "vector_2": [2, 0.13932515571859835, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We also investigated the Portuguese translation of some intersective classes of motion verbs."], "label": "Non-Prov", "citing": "A00-2034", "vector": [0, 0, 0, 0.0], "context": ["", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al.,  1998).", ""], "marker": "Dang et al, 1998", "vector_1": {"levin": 1, "classif": 1, "research": 1, "extend": 1, "nlp": 1}, "vector_2": [2, 0.13932515571859835, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["completely (scrape, scratch), having cut into, incise as an immediate hypernym, which in turn has cut, separate with an in strument as an immediate hypernym."], "label": "Non-Prov", "citing": "A00-2034", "vector": [1, 0, 0, 0.0], "context": ["", "Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al.,  1998).", ""], "marker": "Dang et al, 1998", "vector_1": {"levin": 1, "classif": 1, "research": 1, "extend": 1, "nlp": 1}, "vector_2": [2, 0.13932515571859835, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["(split verb implies separation, no causation of accompanied motion)"], "label": "Non-Prov", "citing": "A00-2034", "vector": [0, 0, 0, 0.0], "context": ["", "Dang et al (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", ""], "marker": "1998", "vector_1": {"origin": 1, "ad": 1, "remov": 1, "al": 1, "overlap": 1, "new": 1, "dang": 1, "et": 1, "scheme": 1, "modifi": 1, "class": 2}, "vector_2": [2, 0.15648286140089418, 1, 0, 1, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993)."], "label": "Non-Prov", "citing": "A00-2034", "vector": [2, 0, 0, 0.0], "context": ["", "Dang et al (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", ""], "marker": "1998", "vector_1": {"origin": 1, "ad": 1, "remov": 1, "al": 1, "overlap": 1, "new": 1, "dang": 1, "et": 1, "scheme": 1, "modifi": 1, "class": 2}, "vector_2": [2, 0.15648286140089418, 1, 0, 1, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We also investigated the Portuguese translation of some intersective classes of motion verbs."], "label": "Non-Prov", "citing": "A00-2034", "vector": [2, 0, 0, 0.1889822365046136], "context": ["", "Dang et al (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.", ""], "marker": "1998", "vector_1": {"origin": 1, "ad": 1, "remov": 1, "al": 1, "overlap": 1, "new": 1, "dang": 1, "et": 1, "scheme": 1, "modifi": 1, "class": 2}, "vector_2": [2, 0.15648286140089418, 1, 0, 1, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["The only escape from this lim itation will be through the use of automated or semi-automated methods of lexical acquisi tion."], "label": "Non-Prov", "citing": "E99-1007", "vector": [2, 0, 0, 0.0816496580927726], "context": ["", "Manual classifica tion of large numbers of verbs is a difficult and resource intensive task (Levin, 1993; Miller et a!., 1990; Dang eta!., 1998).", ""], "marker": "Dang eta!., 1998", "vector_1": {"task": 1, "resourc": 1, "eta": 1, "miller": 1, "manual": 1, "number": 1, "dang": 1, "verb": 1, "larg": 1, "classifica": 1, "et": 1, "tion": 1, "intens": 1, "difficult": 1}, "vector_2": [1, 0.08618366888705935, 1, 0, 4, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["WordNet was de signed principally as a semantic network, and contains little syntactic information."], "label": "Non-Prov", "citing": "E99-1007", "vector": [2, 0, 0, 0.0], "context": ["", "Manual classifica tion of large numbers of verbs is a difficult and resource intensive task (Levin, 1993; Miller et a!., 1990; Dang eta!., 1998).", ""], "marker": "Dang eta!., 1998", "vector_1": {"task": 1, "resourc": 1, "eta": 1, "miller": 1, "manual": 1, "number": 1, "dang": 1, "verb": 1, "larg": 1, "classifica": 1, "et": 1, "tion": 1, "intens": 1, "difficult": 1}, "vector_2": [1, 0.08618366888705935, 1, 0, 4, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["motion Figure 3 shows intersective classes involving two classes of verbs of manner of motion (run and roll verbs) and a class of verbs of existence (me ander verbs)."], "label": "Non-Prov", "citing": "E99-1007", "vector": [4, 0, 1, 0.17213259316477406], "context": ["", "Manual classifica tion of large numbers of verbs is a difficult and resource intensive task (Levin, 1993; Miller et a!., 1990; Dang eta!., 1998).", ""], "marker": "Dang eta!., 1998", "vector_1": {"task": 1, "resourc": 1, "eta": 1, "miller": 1, "manual": 1, "number": 1, "dang": 1, "verb": 1, "larg": 1, "classifica": 1, "et": 1, "tion": 1, "intens": 1, "difficult": 1}, "vector_2": [1, 0.08618366888705935, 1, 0, 4, 1]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["Simultaneously, the verb was removed from the membership lists of those existing classes.", "3.1 Using intersective Levin classes to."], "label": "Non-Prov", "citing": "J01-3003", "vector": [4, 0, 0, 0.12964074471043288], "context": ["", "We think that many cases of ambigu ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al (1998).", ""], "marker": "1998", "vector_1": {"case": 1, "classif": 1, "set": 1, "et": 1, "entri": 1, "notion": 1, "al": 1, "lexic": 1, "ambigu": 1, "verb": 1, "intersect": 1, "address": 1, "dang": 1, "mani": 1, "ou": 1, "think": 1, "introduc": 1}, "vector_2": [3, 0.9466904497016394, 1, 0, 20, 1]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["It takes a gerund plus a reflexive, as in A porta deslizou abrindose (The door slid opening itself).", "Transitivity is also not always preserved in the translations."], "label": "Non-Prov", "citing": "J01-3003", "vector": [2, 0, 0, 0.0], "context": ["", "We think that many cases of ambigu ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al (1998).", ""], "marker": "1998", "vector_1": {"case": 1, "classif": 1, "set": 1, "et": 1, "entri": 1, "notion": 1, "al": 1, "lexic": 1, "ambigu": 1, "verb": 1, "intersect": 1, "address": 1, "dang": 1, "mani": 1, "ou": 1, "think": 1, "introduc": 1}, "vector_2": [3, 0.9466904497016394, 1, 0, 20, 1]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["Depending on the par ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compo nent Levin classes.", "1."], "label": "Non-Prov", "citing": "J01-3003", "vector": [5, 0, 2, 0.048507125007266595], "context": ["", "We think that many cases of ambigu ous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al (1998).", ""], "marker": "1998", "vector_1": {"case": 1, "classif": 1, "set": 1, "et": 1, "entri": 1, "notion": 1, "al": 1, "lexic": 1, "ambigu": 1, "verb": 1, "intersect": 1, "address": 1, "dang": 1, "mani": 1, "ou": 1, "think": 1, "introduc": 1}, "vector_2": [3, 0.9466904497016394, 1, 0, 20, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose."], "label": "Non-Prov", "citing": "J04-1003", "vector": [4, 0, 1, 0.24174688920761409], "context": ["", "Palmer (2000) and Dang et al (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", ""], "marker": "1998", "vector_1": {"use": 1, "develop": 1, "syntact": 1, "frame": 1, "palmer": 1, "al": 1, "argu": 1, "verb": 2, "dang": 1, "et": 1, "principl": 1, "class": 1, "classif": 1}, "vector_2": [6, 0.10310412026726058, 2, 0, 27, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses."], "label": "Non-Prov", "citing": "J04-1003", "vector": [5, 0, 0, 0.11396057645963795], "context": ["", "Palmer (2000) and Dang et al (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", ""], "marker": "1998", "vector_1": {"use": 1, "develop": 1, "syntact": 1, "frame": 1, "palmer": 1, "al": 1, "argu": 1, "verb": 2, "dang": 1, "et": 1, "principl": 1, "class": 1, "classif": 1}, "vector_2": [6, 0.10310412026726058, 2, 0, 27, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["However, only break verbs can also occur in the simple intransitive, The window broke, *The bread cut."], "label": "Non-Prov", "citing": "J04-1003", "vector": [1, 0, 0, 0.07715167498104597], "context": ["", "Palmer (2000) and Dang et al (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.", ""], "marker": "1998", "vector_1": {"use": 1, "develop": 1, "syntact": 1, "frame": 1, "palmer": 1, "al": 1, "argu": 1, "verb": 2, "dang": 1, "et": 1, "principl": 1, "class": 1, "classif": 1}, "vector_2": [6, 0.10310412026726058, 2, 0, 27, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["On the other hand, the scribble verbs do form an intersective class with the perfor mance verbs, since paint and write are also in both classes, in addition to draw."], "label": "Non-Prov", "citing": "J04-1003", "vector": [4, 0, 0, 0.051434449987363975], "context": ["", "Levins (1993) seminal study on diathesis alternations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al 1998; Dorr and Jones 1996)", ""], "marker": "Dang et al. 1998", "vector_1": {"altern": 1, "semant": 1, "creation": 1, "jone": 1, "dorr": 2, "influenc": 1, "levin": 1, "work": 1, "semin": 1, "al": 1, "verb": 1, "dictionari": 1, "dang": 1, "et": 1, "studi": 1, "recent": 1, "class": 1, "diathesi": 1}, "vector_2": [6, 0.8078090200445435, 1, 0, 27, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses."], "label": "Non-Prov", "citing": "J04-1003", "vector": [3, 0, 0, 0.13957263155977062], "context": ["", "Levins (1993) seminal study on diathesis alternations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al 1998; Dorr and Jones 1996)", ""], "marker": "Dang et al. 1998", "vector_1": {"altern": 1, "semant": 1, "creation": 1, "jone": 1, "dorr": 2, "influenc": 1, "levin": 1, "work": 1, "semin": 1, "al": 1, "verb": 1, "dictionari": 1, "dang": 1, "et": 1, "studi": 1, "recent": 1, "class": 1, "diathesi": 1}, "vector_2": [6, 0.8078090200445435, 1, 0, 27, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Whereas high level semantic relations (syn onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class."], "label": "Non-Prov", "citing": "J04-1003", "vector": [4, 0, 0, 0.15018785229652765], "context": ["", "Levins (1993) seminal study on diathesis alternations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al 1998; Dorr and Jones 1996)", ""], "marker": "Dang et al. 1998", "vector_1": {"altern": 1, "semant": 1, "creation": 1, "jone": 1, "dorr": 2, "influenc": 1, "levin": 1, "work": 1, "semin": 1, "al": 1, "verb": 1, "dictionari": 1, "dang": 1, "et": 1, "studi": 1, "recent": 1, "class": 1, "diathesi": 1}, "vector_2": [6, 0.8078090200445435, 1, 0, 27, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["There are still many questions that require further investigation."], "label": "Non-Prov", "citing": "N01-1012", "vector": [0, 0, 0, 0.0], "context": ["", "Besides the obvious influence of WordNet, this work is very much related to Palmer&apos;s VerbNet project (Dang et al., 1998), and has benefited from (Levin, 1993) and (Pritchett, 1992).", ""], "marker": "Dang et al., 1998", "vector_1": {"palmeraposs": 1, "relat": 1, "obviou": 1, "influenc": 1, "work": 1, "project": 1, "benefit": 1, "much": 1, "besid": 1, "wordnet": 1, "verbnet": 1}, "vector_2": [3, 0.9138599001780042, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We base these regular extensions on a fine-grained variation on Levin classes, inter sective Levin classes, as a source of semantic components associated with specific adjuncts."], "label": "Non-Prov", "citing": "N01-1012", "vector": [1, 0, 0, 0.0], "context": ["", "Besides the obvious influence of WordNet, this work is very much related to Palmer&apos;s VerbNet project (Dang et al., 1998), and has benefited from (Levin, 1993) and (Pritchett, 1992).", ""], "marker": "Dang et al., 1998", "vector_1": {"palmeraposs": 1, "relat": 1, "obviou": 1, "influenc": 1, "work": 1, "project": 1, "benefit": 1, "much": 1, "besid": 1, "wordnet": 1, "verbnet": 1}, "vector_2": [3, 0.9138599001780042, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["To explicitly list separa tion as a possible sense for all these verbs would be extravagant when this sense can be gener ated from the combination of the adjunct with the force (potential cause of change of physical state) or motion (itself a special kind of change of state, i.e., of position) semantic component of the verb."], "label": "Non-Prov", "citing": "N01-1012", "vector": [5, 0, 0, 0.0], "context": ["", "Besides the obvious influence of WordNet, this work is very much related to Palmer&apos;s VerbNet project (Dang et al., 1998), and has benefited from (Levin, 1993) and (Pritchett, 1992).", ""], "marker": "Dang et al., 1998", "vector_1": {"palmeraposs": 1, "relat": 1, "obviou": 1, "influenc": 1, "work": 1, "project": 1, "benefit": 1, "much": 1, "besid": 1, "wordnet": 1, "verbnet": 1}, "vector_2": [3, 0.9138599001780042, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["The algorithm we used is given in Figure 1.", "1. Enumerate all sets S = {c1, ...", ", Cn} of se-."], "label": "Non-Prov", "citing": "N09-1057", "vector": [3, 0, 0, 0.0], "context": ["", "participants  causation, change of state, and others  are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "lexicon": 1, "central": 1, "causat": 1, "well": 1, "eg": 1, "work": 1, "particip": 1, "theoret": 1, "state": 1, "other": 1, "lexic": 1, "comput": 1, "chang": 1, "approach": 1}, "vector_2": [11, 0.1746642877265471, 4, 1, 1, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Figure 2 depicts the intersection of split, carry and push/pull.", "Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compo nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion.", "Depending on the par ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compo nent Levin classes."], "label": "Non-Prov", "citing": "N09-1057", "vector": [10, 0, 0, 0.022140372138502378], "context": ["", "participants  causation, change of state, and others  are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "lexicon": 1, "central": 1, "causat": 1, "well": 1, "eg": 1, "work": 1, "particip": 1, "theoret": 1, "state": 1, "other": 1, "lexic": 1, "comput": 1, "chang": 1, "approach": 1}, "vector_2": [11, 0.1746642877265471, 4, 1, 1, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.", "Whereas each WordNet synset is hierarchicalized accord ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary.", "Intersective Levin sets partition these classes according to more co herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb."], "label": "Non-Prov", "citing": "N09-1057", "vector": [8, 0, 0, 0.02817180849095055], "context": ["", "participants  causation, change of state, and others  are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "lexicon": 1, "central": 1, "causat": 1, "well": 1, "eg": 1, "work": 1, "particip": 1, "theoret": 1, "state": 1, "other": 1, "lexic": 1, "comput": 1, "chang": 1, "approach": 1}, "vector_2": [11, 0.1746642877265471, 4, 1, 1, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["tion into pieces (chip, clip, cut, hack, hew, saw, slash, snip), having cut, separate with an instrument as an immediate hypernym."], "label": "Non-Prov", "citing": "P03-1009", "vector": [0, 0, 0, 0.0], "context": ["", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", ""], "marker": "Dang et al., 1998", "vector_1": {"classif": 1, "linguist": 2, "comput": 1, "consider": 1, "captur": 1, "eg": 1, "relat": 1, "semant": 1, "syntax": 1, "aim": 1, "verb": 1, "interest": 1, "close": 1, "research": 1, "attract": 1}, "vector_2": [5, 0.041832593772518675, 6, 2, 0, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["par t. y e s y e s y e s y e s y e s ye s ye s ye s y e s y e s y e s y e s y e s y e s y e s ind. acti on loc at."], "label": "Non-Prov", "citing": "P03-1009", "vector": [0, 0, 0, 0.0], "context": ["", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", ""], "marker": "Dang et al., 1998", "vector_1": {"classif": 1, "linguist": 2, "comput": 1, "consider": 1, "captur": 1, "eg": 1, "relat": 1, "semant": 1, "syntax": 1, "aim": 1, "verb": 1, "interest": 1, "close": 1, "research": 1, "attract": 1}, "vector_2": [5, 0.041832593772518675, 6, 2, 0, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["The slide class parti tions this roll/run intersection into verbs that can take the transitive alternation and verbs that cannot (drift and glide cannot be causative, because they are not typically externally con trollable)."], "label": "Non-Prov", "citing": "P03-1009", "vector": [3, 0, 0, 0.09622504486493764], "context": ["", "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).", ""], "marker": "Dang et al., 1998", "vector_1": {"classif": 1, "linguist": 2, "comput": 1, "consider": 1, "captur": 1, "eg": 1, "relat": 1, "semant": 1, "syntax": 1, "aim": 1, "verb": 1, "interest": 1, "close": 1, "research": 1, "attract": 1}, "vector_2": [5, 0.041832593772518675, 6, 2, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["However, other intersective classes, such as the split/push/carry class, are no more con sistent with WordNet than the original Levin classes."], "label": "Non-Prov", "citing": "P04-1046", "vector": [1, 0, 0, 0.0], "context": ["", "In computational linguistics, the VerbNet project (Dang et al., 1998) and Framenet (www.icsi.berkeley.edu/ framen,a) (Fillmore et al., 2003) bear relation to this work.", ""], "marker": "Dang et al., 1998", "vector_1": {"work": 1, "comput": 1, "wwwicsiberkeleyedu": 1, "relat": 1, "bear": 1, "framena": 1, "project": 1, "framenet": 1, "linguist": 1, "verbnet": 1}, "vector_2": [6, 0.9609006250797295, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["motion Figure 3 shows intersective classes involving two classes of verbs of manner of motion (run and roll verbs) and a class of verbs of existence (me ander verbs)."], "label": "Non-Prov", "citing": "P04-1046", "vector": [1, 0, 0, 0.0], "context": ["", "In computational linguistics, the VerbNet project (Dang et al., 1998) and Framenet (www.icsi.berkeley.edu/ framen,a) (Fillmore et al., 2003) bear relation to this work.", ""], "marker": "Dang et al., 1998", "vector_1": {"work": 1, "comput": 1, "wwwicsiberkeleyedu": 1, "relat": 1, "bear": 1, "framena": 1, "project": 1, "framenet": 1, "linguist": 1, "verbnet": 1}, "vector_2": [6, 0.9609006250797295, 2, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["intersective class Is such that a verb v E Is iff v E c1 n ... n en, and there is no S' = {d1, .. ,c} such that S C S' and v E ci n ... n dm (subset criterion)."], "label": "Non-Prov", "citing": "P04-1046", "vector": [1, 0, 0, 0.0], "context": ["", "In computational linguistics, the VerbNet project (Dang et al., 1998) and Framenet (www.icsi.berkeley.edu/ framen,a) (Fillmore et al., 2003) bear relation to this work.", ""], "marker": "Dang et al., 1998", "vector_1": {"work": 1, "comput": 1, "wwwicsiberkeleyedu": 1, "relat": 1, "bear": 1, "framena": 1, "project": 1, "framenet": 1, "linguist": 1, "verbnet": 1}, "vector_2": [6, 0.9609006250797295, 2, 1, 0, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["They are interpretable as verbs of splitting or separating only in particular syn tactic frames (I pulled the twig and the branch apart, I pulled the twig off {of) the branch, but not *I pulled the twig and the branch)."], "label": "Non-Prov", "citing": "P06-1117", "vector": [4, 0, 0, 0.0], "context": ["", "A very good candidate seems to be the Intersective Levin class (ILC) (Dang et al., 1998) that can be found as well in other predicate resources like PB and VerbNet (VN) (Kipper et al., 2000).", ""], "marker": "Dang et al., 1998", "vector_1": {"good": 1, "resourc": 1, "ilc": 1, "levin": 1, "well": 1, "vn": 1, "predic": 1, "pb": 1, "candid": 1, "intersect": 1, "found": 1, "seem": 1, "verbnet": 1, "class": 1, "like": 1}, "vector_2": [8, 0.09937578027465668, 2, 2, 3, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un derlying semantic components that constrain al lowable arguments."], "label": "Non-Prov", "citing": "P06-1117", "vector": [8, 0, 2, 0.11846977555181847], "context": ["", "A very good candidate seems to be the Intersective Levin class (ILC) (Dang et al., 1998) that can be found as well in other predicate resources like PB and VerbNet (VN) (Kipper et al., 2000).", ""], "marker": "Dang et al., 1998", "vector_1": {"good": 1, "resourc": 1, "ilc": 1, "levin": 1, "well": 1, "vn": 1, "predic": 1, "pb": 1, "candid": 1, "intersect": 1, "found": 1, "seem": 1, "verbnet": 1, "class": 1, "like": 1}, "vector_2": [8, 0.09937578027465668, 2, 2, 3, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["The answer to this question is the key to breaking the bottleneck of semantic representation that is currently the single great est limitation on the general application of nat ural language processing techniques."], "label": "Non-Prov", "citing": "P06-1117", "vector": [3, 0, 0, 0.0], "context": ["", "A very good candidate seems to be the Intersective Levin class (ILC) (Dang et al., 1998) that can be found as well in other predicate resources like PB and VerbNet (VN) (Kipper et al., 2000).", ""], "marker": "Dang et al., 1998", "vector_1": {"good": 1, "resourc": 1, "ilc": 1, "levin": 1, "well": 1, "vn": 1, "predic": 1, "pb": 1, "candid": 1, "intersect": 1, "found": 1, "seem": 1, "verbnet": 1, "class": 1, "like": 1}, "vector_2": [8, 0.09937578027465668, 2, 2, 3, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We selected the slide/roll/run, meander/roll and roll/run intersective classes."], "label": "Non-Prov", "citing": "P06-1117", "vector": [3, 0, 0, 0.1924500897298753], "context": ["", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levins classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "lexicon": 1, "version": 1, "insid": 1, "constraint": 1, "levin": 2, "classif": 1, "vn": 1, "base": 1, "role": 1, "construct": 1, "intersect": 1, "refin": 1, "ensur": 1, "call": 1, "class": 1, "ilc": 1}, "vector_2": [8, 0.3675405742821473, 1, 2, 3, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un derlying semantic components that constrain al lowable arguments."], "label": "Non-Prov", "citing": "P06-1117", "vector": [5, 0, 0, 0.10814761408717503], "context": ["", "This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levins classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "lexicon": 1, "version": 1, "insid": 1, "constraint": 1, "levin": 2, "classif": 1, "vn": 1, "base": 1, "role": 1, "construct": 1, "intersect": 1, "refin": 1, "ensur": 1, "call": 1, "class": 1, "ilc": 1}, "vector_2": [8, 0.3675405742821473, 1, 2, 3, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["mantic classes such that let n ... n enI e, where e is a relevance cutoff."], "label": "Non-Prov", "citing": "P13-3009", "vector": [0, 0, 0, 0.0], "context": ["", "For English verbs, the entire verb list (3750) enlisted by Levin (Levin, 1993) including extensions (Dang et. al, 1998; Kipper et. al, 2006; Korhonen and Briscoe, 2004) was classified according to the new classification.", ""], "marker": "Dang et. al, 1998", "vector_1": {"accord": 1, "kipper": 1, "entir": 1, "levin": 1, "list": 1, "classif": 1, "classifi": 1, "enlist": 1, "extens": 1, "verb": 2, "includ": 1, "english": 1, "dang": 1, "et": 2, "new": 1}, "vector_2": [15, 0.7350045113961791, 4, 11, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["However, only break verbs can also occur in the simple intransitive, The window broke, *The bread cut."], "label": "Non-Prov", "citing": "P13-3009", "vector": [2, 0, 0, 0.06622661785325219], "context": ["", "For English verbs, the entire verb list (3750) enlisted by Levin (Levin, 1993) including extensions (Dang et. al, 1998; Kipper et. al, 2006; Korhonen and Briscoe, 2004) was classified according to the new classification.", ""], "marker": "Dang et. al, 1998", "vector_1": {"accord": 1, "kipper": 1, "entir": 1, "levin": 1, "list": 1, "classif": 1, "classifi": 1, "enlist": 1, "extens": 1, "verb": 2, "includ": 1, "english": 1, "dang": 1, "et": 2, "new": 1}, "vector_2": [15, 0.7350045113961791, 4, 11, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Nora pushed the package to Pamela.."], "label": "Non-Prov", "citing": "P13-3009", "vector": [2, 0, 0, 0.0], "context": ["", "For English verbs, the entire verb list (3750) enlisted by Levin (Levin, 1993) including extensions (Dang et. al, 1998; Kipper et. al, 2006; Korhonen and Briscoe, 2004) was classified according to the new classification.", ""], "marker": "Dang et. al, 1998", "vector_1": {"accord": 1, "kipper": 1, "entir": 1, "levin": 1, "list": 1, "classif": 1, "classifi": 1, "enlist": 1, "extens": 1, "verb": 2, "includ": 1, "english": 1, "dang": 1, "et": 2, "new": 1}, "vector_2": [15, 0.7350045113961791, 4, 11, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["This filter al lowed us to reject the potential intersective class that would have resulted from combining the re move verbs with the scribble verbs, for example."], "label": "Non-Prov", "citing": "P99-1051", "vector": [0, 0, 0, 0.0], "context": ["", "Levin's study on diathesis alternations has influenced recent work on word sense disam biguation (Dorr and Jones, 1996), machine transla tion (Dang et al., 1998), and automatic lexical ac quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"altern": 1, "machin": 1, "transla": 1, "word": 1, "influenc": 1, "levin": 1, "work": 1, "quisit": 1, "ac": 1, "automat": 1, "biguat": 1, "lexic": 1, "disam": 1, "sens": 1, "studi": 1, "diathesi": 1, "tion": 1, "recent": 1}, "vector_2": [1, 0.05723406681108197, 4, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["The distribution of syntactic frames in which a verb can appear determines its class member ship."], "label": "Non-Prov", "citing": "P99-1051", "vector": [0, 0, 0, 0.0], "context": ["", "Levin's study on diathesis alternations has influenced recent work on word sense disam biguation (Dorr and Jones, 1996), machine transla tion (Dang et al., 1998), and automatic lexical ac quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"altern": 1, "machin": 1, "transla": 1, "word": 1, "influenc": 1, "levin": 1, "work": 1, "quisit": 1, "ac": 1, "automat": 1, "biguat": 1, "lexic": 1, "disam": 1, "sens": 1, "studi": 1, "diathesi": 1, "tion": 1, "recent": 1}, "vector_2": [1, 0.05723406681108197, 4, 1, 0, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["When examining in detail the intersec tive classes just described, which emphasize not only the individual classes, but also their rela tion to other classes, we see a rich semantic lat tice much like WordNet."], "label": "Non-Prov", "citing": "P99-1051", "vector": [1, 0, 0, 0.045360921162651446], "context": ["", "Levin's study on diathesis alternations has influenced recent work on word sense disam biguation (Dorr and Jones, 1996), machine transla tion (Dang et al., 1998), and automatic lexical ac quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"altern": 1, "machin": 1, "transla": 1, "word": 1, "influenc": 1, "levin": 1, "work": 1, "quisit": 1, "ac": 1, "automat": 1, "biguat": 1, "lexic": 1, "disam": 1, "sens": 1, "studi": 1, "diathesi": 1, "tion": 1, "recent": 1}, "vector_2": [1, 0.05723406681108197, 4, 1, 0, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["inv ers."], "label": "Non-Prov", "citing": "W00-0202", "vector": [0, 0, 0, 0.0], "context": ["", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "properti": 1, "parent": 1, "creat": 1, "share": 1, "cap": 1, "associatedwith": 1, "idea": 1, "repres": 1, "motion": 1, "action": 1, "exploit": 1, "verb": 5, "contact": 1, "allow": 1, "lattic": 1, "hierarchi": 1, "close": 1, "ture": 1, "similar": 1, "common": 1}, "vector_2": [2, 0.3324968632371393, 1, 1, 2, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["The list of members for each Levin verb class is not always complete, so to check if a particular verb belongs to a class it is better to check that the verb exhibits all the alternations that de fine the class."], "label": "Non-Prov", "citing": "W00-0202", "vector": [7, 0, 0, 0.0], "context": ["", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "properti": 1, "parent": 1, "creat": 1, "share": 1, "cap": 1, "associatedwith": 1, "idea": 1, "repres": 1, "motion": 1, "action": 1, "exploit": 1, "verb": 5, "contact": 1, "allow": 1, "lattic": 1, "hierarchi": 1, "close": 1, "ture": 1, "similar": 1, "common": 1}, "vector_2": [2, 0.3324968632371393, 1, 1, 2, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["completely (scrape, scratch), having cut into, incise as an immediate hypernym, which in turn has cut, separate with an in strument as an immediate hypernym."], "label": "Non-Prov", "citing": "W00-0202", "vector": [2, 0, 0, 0.0], "context": ["", "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "properti": 1, "parent": 1, "creat": 1, "share": 1, "cap": 1, "associatedwith": 1, "idea": 1, "repres": 1, "motion": 1, "action": 1, "exploit": 1, "verb": 5, "contact": 1, "allow": 1, "lattic": 1, "hierarchi": 1, "close": 1, "ture": 1, "similar": 1, "common": 1}, "vector_2": [2, 0.3324968632371393, 1, 1, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": [" Although kick is not listed as a verb of exerting force, it displays all the alternations that define this class."], "label": "Non-Prov", "citing": "W03-0910", "vector": [5, 0, 0, 0.07071067811865474], "context": ["", "This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class", ""], "marker": "Dang et al 1998", "vector_1": {"use": 1, "reorgan": 1, "levin": 1, "semant": 1, "within": 1, "account": 1, "al": 1, "differ": 1, "facilit": 1, "refin": 1, "dang": 1, "et": 1, "inter": 1, "class": 3, "sectiv": 1, "syntact": 1}, "vector_2": [5, 0.17697124217314686, 0, 0, 3, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev eral of the same properties as the corresponding verbs in English."], "label": "Non-Prov", "citing": "W03-0910", "vector": [5, 0, 0, 0.0], "context": ["", "This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class", ""], "marker": "Dang et al 1998", "vector_1": {"use": 1, "reorgan": 1, "levin": 1, "semant": 1, "within": 1, "account": 1, "al": 1, "differ": 1, "facilit": 1, "refin": 1, "dang": 1, "et": 1, "inter": 1, "class": 3, "sectiv": 1, "syntact": 1}, "vector_2": [5, 0.17697124217314686, 0, 0, 3, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose."], "label": "Non-Prov", "citing": "W03-0910", "vector": [4, 0, 0, 0.13483997249264842], "context": ["", "This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class", ""], "marker": "Dang et al 1998", "vector_1": {"use": 1, "reorgan": 1, "levin": 1, "semant": 1, "within": 1, "account": 1, "al": 1, "differ": 1, "facilit": 1, "refin": 1, "dang": 1, "et": 1, "inter": 1, "class": 3, "sectiv": 1, "syntact": 1}, "vector_2": [5, 0.17697124217314686, 0, 0, 3, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": [", Cn} of se-."], "label": "Non-Prov", "citing": "W06-2611", "vector": [0, 0, 0, 0.0], "context": ["", "A very good candidate seems to be the Intersective Levin classes (Dang et al., 1998) that can be found as well in other predicate resources like PropBank and VerbNet (Kipper et al., 2000).", ""], "marker": "Dang et al., 1998", "vector_1": {"propbank": 1, "good": 1, "resourc": 1, "like": 1, "levin": 1, "well": 1, "predic": 1, "candid": 1, "intersect": 1, "found": 1, "seem": 1, "class": 1, "verbnet": 1}, "vector_2": [8, 0.10161655554157756, 2, 2, 3, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["cor e."], "label": "Non-Prov", "citing": "W06-2611", "vector": [0, 0, 0, 0.0], "context": ["", "A very good candidate seems to be the Intersective Levin classes (Dang et al., 1998) that can be found as well in other predicate resources like PropBank and VerbNet (Kipper et al., 2000).", ""], "marker": "Dang et al., 1998", "vector_1": {"propbank": 1, "good": 1, "resourc": 1, "like": 1, "levin": 1, "well": 1, "predic": 1, "candid": 1, "intersect": 1, "found": 1, "seem": 1, "class": 1, "verbnet": 1}, "vector_2": [8, 0.10161655554157756, 2, 2, 3, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["Nora pushed the branches apart.."], "label": "Non-Prov", "citing": "W06-2611", "vector": [1, 0, 0, 0.0], "context": ["", "A very good candidate seems to be the Intersective Levin classes (Dang et al., 1998) that can be found as well in other predicate resources like PropBank and VerbNet (Kipper et al., 2000).", ""], "marker": "Dang et al., 1998", "vector_1": {"propbank": 1, "good": 1, "resourc": 1, "like": 1, "levin": 1, "well": 1, "predic": 1, "candid": 1, "intersect": 1, "found": 1, "seem": 1, "class": 1, "verbnet": 1}, "vector_2": [8, 0.10161655554157756, 2, 2, 3, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Many of the verbs participate in alternations that are direct trans lations of the English alternations."], "label": "Non-Prov", "citing": "W06-2611", "vector": [3, 0, 1, 0.0], "context": ["", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "inter": 1, "version": 1, "insid": 1, "constraint": 1, "levin": 2, "classif": 1, "construct": 1, "base": 1, "role": 1, "refin": 1, "ensur": 1, "call": 1, "con": 1, "lexi": 1, "class": 1, "sectiv": 1, "verbnet": 1}, "vector_2": [8, 0.3574663479682979, 1, 2, 3, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un derlying semantic components that constrain al lowable arguments."], "label": "Non-Prov", "citing": "W06-2611", "vector": [6, 0, 0, 0.15018785229652765], "context": ["", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "inter": 1, "version": 1, "insid": 1, "constraint": 1, "levin": 2, "classif": 1, "construct": 1, "base": 1, "role": 1, "refin": 1, "ensur": 1, "call": 1, "con": 1, "lexi": 1, "class": 1, "sectiv": 1, "verbnet": 1}, "vector_2": [8, 0.3574663479682979, 1, 2, 3, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Although the Levin classes that make up an intersective class may have conflicting alterna tions {e.g., verbs of exerting force can take the conative alternation, while carry verbs cannot), this does not invalidate the semantic regularity of the intersective class."], "label": "Non-Prov", "citing": "W06-2611", "vector": [7, 0, 3, 0.15677236033392414], "context": ["", "This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "inter": 1, "version": 1, "insid": 1, "constraint": 1, "levin": 2, "classif": 1, "construct": 1, "base": 1, "role": 1, "refin": 1, "ensur": 1, "call": 1, "con": 1, "lexi": 1, "class": 1, "sectiv": 1, "verbnet": 1}, "vector_2": [8, 0.3574663479682979, 1, 2, 3, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["(carry verb implies causation of accompa nied motion, no separation) 2."], "label": "Non-Prov", "citing": "W99-0503", "vector": [2, 0, 0, 0.0839181358296689], "context": ["", "In explormg these quest1ons, we focus on verb clas Sificatwn for several reasons Verbs are very Impor tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998)", ""], "marker": "Dang et al , 1998", "vector_1": {"among": 1, "major": 1, "mtensv": 1, "knowledg": 2, "number": 1, "crucal": 1, "ap": 1, "et": 1, "yet": 1, "languag": 2, "990": 1, "use": 1, "hnowledg": 1, "cla": 1, "role": 1, "sever": 1, "explormg": 1, "machm": 1, "sificatwn": 1, "play": 1, "sourc": 1, "resourc": 1, "dfficult": 1, "relat0nshp": 1, "gener": 1, "pear": 1, "lexic": 1, "reason": 1, "verb": 5, "translat": 1, "engneermg": 1, "mjler": 1, "impor": 1, "class": 1, "clcbsrficatwn": 1, "task": 2, "classficatwn": 1, "document": 1, "acqusrt0n": 1, "manual": 1, "focu": 1, "queston": 1, "orgamzatwn": 1, "s": 2, "larg": 1, "mani": 1, "tant": 1, "support": 1}, "vector_2": [1, 0.07620917931012638, 4, 1, 1, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components."], "label": "Non-Prov", "citing": "W99-0503", "vector": [8, 0, 1, 0.06851887098275317], "context": ["", "In explormg these quest1ons, we focus on verb clas Sificatwn for several reasons Verbs are very Impor tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998)", ""], "marker": "Dang et al , 1998", "vector_1": {"among": 1, "major": 1, "mtensv": 1, "knowledg": 2, "number": 1, "crucal": 1, "ap": 1, "et": 1, "yet": 1, "languag": 2, "990": 1, "use": 1, "hnowledg": 1, "cla": 1, "role": 1, "sever": 1, "explormg": 1, "machm": 1, "sificatwn": 1, "play": 1, "sourc": 1, "resourc": 1, "dfficult": 1, "relat0nshp": 1, "gener": 1, "pear": 1, "lexic": 1, "reason": 1, "verb": 5, "translat": 1, "engneermg": 1, "mjler": 1, "impor": 1, "class": 1, "clcbsrficatwn": 1, "task": 2, "classficatwn": 1, "document": 1, "acqusrt0n": 1, "manual": 1, "focu": 1, "queston": 1, "orgamzatwn": 1, "s": 2, "larg": 1, "mani": 1, "tant": 1, "support": 1}, "vector_2": [1, 0.07620917931012638, 4, 1, 1, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["All subsets were included which shared a minimum of three members."], "label": "Non-Prov", "citing": "W99-0503", "vector": [2, 0, 0, 0.0], "context": ["", "In explormg these quest1ons, we focus on verb clas Sificatwn for several reasons Verbs are very Impor tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs ap pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb'S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a! , 1990, Dang et al , 1998)", ""], "marker": "Dang et al , 1998", "vector_1": {"among": 1, "major": 1, "mtensv": 1, "knowledg": 2, "number": 1, "crucal": 1, "ap": 1, "et": 1, "yet": 1, "languag": 2, "990": 1, "use": 1, "hnowledg": 1, "cla": 1, "role": 1, "sever": 1, "explormg": 1, "machm": 1, "sificatwn": 1, "play": 1, "sourc": 1, "resourc": 1, "dfficult": 1, "relat0nshp": 1, "gener": 1, "pear": 1, "lexic": 1, "reason": 1, "verb": 5, "translat": 1, "engneermg": 1, "mjler": 1, "impor": 1, "class": 1, "clcbsrficatwn": 1, "task": 2, "classficatwn": 1, "document": 1, "acqusrt0n": 1, "manual": 1, "focu": 1, "queston": 1, "orgamzatwn": 1, "s": 2, "larg": 1, "mani": 1, "tant": 1, "support": 1}, "vector_2": [1, 0.07620917931012638, 4, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["To turn on the printer, press the Power buttoni and hold iti down for a moment.", "Unwrap the paperi form iti and align iti then load iti into the drawer.", "Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1)."], "label": "Non-Prov", "citing": "A00-1020", "vector": [0, 0, 0, 0.0], "context": ["", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", ""], "marker": "Mitkov, 1998", "vector_1": {"curaci": 1, "ac": 1, "show": 1, "nevertheless": 1, "cf": 1, "knowledgepoor": 1, "perform": 1, "result": 1, "amaz": 1, "method": 1, "recent": 1}, "vector_2": [2, 0.13047924193878246, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["This case and other cases suggest that it might be worthwhile reconsider ing/refining the weights for the indicator \"referential distance\".", "Similarly to the first evaluation, we found that the robust approach was not very successful on sen tences with too complicated syntax - a price we have to pay for the \"convenience\" of developing a knowl edge-poor system.", "The results from experiment 1 and experiment 2 can be summarised in the following (statistically) slightly more representative figures."], "label": "Non-Prov", "citing": "A00-1020", "vector": [3, 0, 0, 0.04767312946227962], "context": ["", "Nevertheless, recent results show that knowledge-poor methods perform with amazing ac curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).", ""], "marker": "Mitkov, 1998", "vector_1": {"curaci": 1, "ac": 1, "show": 1, "nevertheless": 1, "cf": 1, "knowledgepoor": 1, "perform": 1, "result": 1, "amaz": 1, "method": 1, "recent": 1}, "vector_2": [2, 0.13047924193878246, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["If still no choice is possible, the most recent from the remaining candi dates is selected as the antecedent.", "2.1 Antecedent indicators."], "label": "Non-Prov", "citing": "D09-1101", "vector": [2, 0, 1, 0.05913123959890826], "context": ["", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", ""], "marker": "1998", "vector_1": {"set": 1, "pronoun": 1, "appli": 1, "eg": 1, "rank": 1, "one": 1, "incompat": 1, "use": 1, "heuristicbas": 1, "salienc": 1, "factor": 1, "candid": 1, "grammat": 1, "anteced": 1, "mitkov": 1, "like": 1, "constraint": 1, "resolv": 1, "filter": 1, "remain": 1, "mani": 1, "first": 1}, "vector_2": [11, 0.19919637070641608, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["If we regard as \"discriminative power\" of each antecedent indicator the ratio \"number of successful antecedent identifications when this indicator was applied\"/\"number of applications of this indicator\" (for the non-prepositional noun phrase and definite ness being penalising indicators, this figure is calcu lated as the ratio \"number of unsuccessful antece dent identifications\"/\"number of applications\"), the immediate reference emerges as the most discrimi native indicator (100%), followed by non prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).", "The rela tively low figures for the majority of indicators should not be regarded as a surprise: firstly, we should bear in mind that in most cases a candidate was picked (or rejected) as an antecedent on the ba sis of applying a number of different indicators and secondly, that most anaphors had a relatively high number of candidates for antecedent."], "label": "Non-Prov", "citing": "D09-1101", "vector": [5, 0, 0, 0.01848684666616339], "context": ["", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", ""], "marker": "1998", "vector_1": {"set": 1, "pronoun": 1, "appli": 1, "eg": 1, "rank": 1, "one": 1, "incompat": 1, "use": 1, "heuristicbas": 1, "salienc": 1, "factor": 1, "candid": 1, "grammat": 1, "anteced": 1, "mitkov": 1, "like": 1, "constraint": 1, "resolv": 1, "filter": 1, "remain": 1, "mani": 1, "first": 1}, "vector_2": [11, 0.19919637070641608, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Given that our approach is robust and returns an tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.", "CogNIAC successfully resolved the pronouns in 75% of the cases."], "label": "Non-Prov", "citing": "D09-1101", "vector": [5, 0, 0, 0.038291979053374184], "context": ["", "Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", ""], "marker": "1998", "vector_1": {"set": 1, "pronoun": 1, "appli": 1, "eg": 1, "rank": 1, "one": 1, "incompat": 1, "use": 1, "heuristicbas": 1, "salienc": 1, "factor": 1, "candid": 1, "grammat": 1, "anteced": 1, "mitkov": 1, "like": 1, "constraint": 1, "resolv": 1, "filter": 1, "remain": 1, "mani": 1, "first": 1}, "vector_2": [11, 0.19919637070641608, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not).", "21dentification of clauses in complex sentences is do e heuristically.", "As already mentioned, each of the antecedent in dicators assigns a score with a value {-1, 0, 1, 2}.", "These scores have been determined experimentally on an empirical basis and are constantly being up dated."], "label": "Non-Prov", "citing": "E99-1031", "vector": [6, 0, 1, 0.0], "context": ["", "A huge variety of techniques are described in the literature, many of them achieving high suc cess rates on their own evaluation texts (cf. Hobbs 1986; Strube 1998; Mitkov 1998).", ""], "marker": "Mitkov 1998", "vector_1": {"huge": 1, "suc": 1, "describ": 1, "rate": 1, "text": 1, "strube": 1, "literatur": 1, "cf": 1, "high": 1, "achiev": 1, "hobb": 1, "evalu": 1, "varieti": 1, "mani": 1, "cess": 1, "mitkov": 1, "techniqu": 1}, "vector_2": [1, 0.06462135922330096, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary.", "languages An attractive feature of any NLP approach would be its language \"universality\".", "While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta tion.", "We used the robust approach as a basis for devel oping a genre-specific reference resolution approach in Polish."], "label": "Non-Prov", "citing": "E99-1031", "vector": [7, 0, 0, 0.02556549962824568], "context": ["", "A huge variety of techniques are described in the literature, many of them achieving high suc cess rates on their own evaluation texts (cf. Hobbs 1986; Strube 1998; Mitkov 1998).", ""], "marker": "Mitkov 1998", "vector_1": {"huge": 1, "suc": 1, "describ": 1, "rate": 1, "text": 1, "strube": 1, "literatur": 1, "cf": 1, "high": 1, "achiev": 1, "hobb": 1, "evalu": 1, "varieti": 1, "mani": 1, "cess": 1, "mitkov": 1, "techniqu": 1}, "vector_2": [1, 0.06462135922330096, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["These scores have been determined experimentally on an empirical basis and are constantly being up dated.", "Top symptoms like \"lexical reiteration\" as sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\".", "We should point out that the antecedent indicators are preferences and not absolute factors.", "There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent."], "label": "Non-Prov", "citing": "E99-1031", "vector": [5, 0, 0, 0.0], "context": ["", "A huge variety of techniques are described in the literature, many of them achieving high suc cess rates on their own evaluation texts (cf. Hobbs 1986; Strube 1998; Mitkov 1998).", ""], "marker": "Mitkov 1998", "vector_1": {"huge": 1, "suc": 1, "describ": 1, "rate": 1, "text": 1, "strube": 1, "literatur": 1, "cf": 1, "high": 1, "achiev": 1, "hobb": 1, "evalu": 1, "varieti": 1, "mani": 1, "cess": 1, "mitkov": 1, "techniqu": 1}, "vector_2": [1, 0.06462135922330096, 0, 0, 0, 1]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.", "While various alternatives have been proposed, making use of e.g. neural networks, a situation se mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro cessing of growing language resources."], "label": "Non-Prov", "citing": "E99-1031", "vector": [3, 0, 1, 0.0], "context": ["", "We implemented meta-modules to in terface to the genetic algorithm driver and to combine different salience factors into an over all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", ""], "marker": "Mitkov, 1998", "vector_1": {"differ": 1, "algorithm": 1, "metamodul": 1, "genet": 1, "factor": 1, "driver": 1, "score": 1, "combin": 1, "terfac": 1, "salienc": 1, "implement": 1, "similar": 1}, "vector_2": [1, 0.8894757281553398, 2, 1, 0, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["The first Base line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%.", "There fore, the 93.3% success rate (see above) demon strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences."], "label": "Non-Prov", "citing": "E99-1031", "vector": [3, 0, 1, 0.0], "context": ["", "We implemented meta-modules to in terface to the genetic algorithm driver and to combine different salience factors into an over all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", ""], "marker": "Mitkov, 1998", "vector_1": {"differ": 1, "algorithm": 1, "metamodul": 1, "genet": 1, "factor": 1, "driver": 1, "score": 1, "combin": 1, "terfac": 1, "salienc": 1, "implement": 1, "similar": 1}, "vector_2": [1, 0.8894757281553398, 2, 1, 0, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["The results from experiment 1 and experiment 2 can be summarised in the following (statistically) slightly more representative figures.", "R ob ust aQ pr oa ch B a s el i n e s u b je ct B as eli ne m os t re ce nt Su cc es s rat e (= Pr ec isi on / Re ca ll) 8 9."], "label": "Non-Prov", "citing": "E99-1031", "vector": [3, 0, 0, 0.0], "context": ["", "We implemented meta-modules to in terface to the genetic algorithm driver and to combine different salience factors into an over all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).", ""], "marker": "Mitkov, 1998", "vector_1": {"differ": 1, "algorithm": 1, "metamodul": 1, "genet": 1, "factor": 1, "driver": 1, "score": 1, "combin": 1, "terfac": 1, "salienc": 1, "implement": 1, "similar": 1}, "vector_2": [1, 0.8894757281553398, 2, 1, 0, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.", "The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.", "Syntactic parallelism, useful in discrimi nating between identical pronouns on the basis of their syntactic function, also has to be forgone.", "Lack of semantic knowledge rules out the use of verb se mantics and semantic parallelism.", "Our evaluation, however, suggests that much less is lost than might be feared.", "In fact, our evaluation shows that the re sults are comparable to syntax-based methods (Lappin & Leass I994)."], "label": "Non-Prov", "citing": "J01-4003", "vector": [7, 0, 0, 0.019849711139180458], "context": ["", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", ""], "marker": "Mitkov 1998", "vector_1": {"corpu": 1, "hahn": 1, "mitkov": 1, "evalu": 1, "carri": 1, "eg": 1, "handtest": 1, "past": 1, "small": 1, "drawback": 1, "done": 1, "strube": 2, "walker": 1, "mani": 1, "corpora": 1}, "vector_2": [3, 0.01938847828287942, 0, 0, 6, 1]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%.", "Given that our knowledge poor approach is basically an enhancement of a baseline model through a set of antecedent indica tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.", "Typically, our preference-based model proved superior to both baseline models when the antece dent was neither the most recent subject nor the most recent noun phrase matching the anaphor in gender and number.", "Example: Identify the draweq by the lit paper port LED and add paper to itj.", "The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera tion 0 + section heading 0 + collocation 0 + referen tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4).", "From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the antecedent have not only different syntactic functions but also different semantic roles."], "label": "Non-Prov", "citing": "J01-4003", "vector": [7, 0, 0, 0.0], "context": ["", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", ""], "marker": "Mitkov 1998", "vector_1": {"corpu": 1, "hahn": 1, "mitkov": 1, "evalu": 1, "carri": 1, "eg": 1, "handtest": 1, "past": 1, "small": 1, "drawback": 1, "done": 1, "strube": 2, "walker": 1, "mani": 1, "corpora": 1}, "vector_2": [3, 0.01938847828287942, 0, 0, 6, 1]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["The collocation preference here is restricted to the patterns \"noun phrase (pronoun), verb\" and \"verb, noun phrase (pronoun)\".", "Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in (Dagan & ltai 1990).", "Example: Press the keyi down and turn the volume up...", "Press iti again.", "Immediate reference In technical manuals the \"immediate reference\" clue can often be useful in identifying the antecedent.", "The heuristics used is that in constructions of the form \"...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)\", where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun \"it\" imme diately following V2 and is therefore given preference (scores 2 and 0)."], "label": "Non-Prov", "citing": "J01-4003", "vector": [4, 0, 0, 0.0], "context": ["", "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.", ""], "marker": "Mitkov 1998", "vector_1": {"corpu": 1, "hahn": 1, "mitkov": 1, "evalu": 1, "carri": 1, "eg": 1, "handtest": 1, "past": 1, "small": 1, "drawback": 1, "done": 1, "strube": 2, "walker": 1, "mani": 1, "corpora": 1}, "vector_2": [3, 0.01938847828287942, 0, 0, 6, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The rela tively low figures for the majority of indicators should not be regarded as a surprise: firstly, we should bear in mind that in most cases a candidate was picked (or rejected) as an antecedent on the ba sis of applying a number of different indicators and secondly, that most anaphors had a relatively high number of candidates for antecedent."], "label": "Non-Prov", "citing": "J01-4005", "vector": [4, 0, 0, 0.0], "context": ["", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", ""], "marker": "1998", "vector_1": {"analysi": 1, "employ": 1, "semant": 1, "shallow": 1, "mitkov": 1, "constraint": 1, "resolut": 1, "prefer": 1, "see": 1, "exampl": 1, "current": 1, "consequ": 1, "reli": 1, "inform": 1, "heurist": 1, "morphosyntact": 1, "mainli": 1, "method": 1, "anaphora": 1}, "vector_2": [3, 0.7085780338199378, 1, 0, 3, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%."], "label": "Non-Prov", "citing": "J01-4005", "vector": [0, 0, 0, 0.0], "context": ["", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", ""], "marker": "1998", "vector_1": {"analysi": 1, "employ": 1, "semant": 1, "shallow": 1, "mitkov": 1, "constraint": 1, "resolut": 1, "prefer": 1, "see": 1, "exampl": 1, "current": 1, "consequ": 1, "reli": 1, "inform": 1, "heurist": 1, "morphosyntact": 1, "mainli": 1, "method": 1, "anaphora": 1}, "vector_2": [3, 0.7085780338199378, 1, 0, 3, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent."], "label": "Non-Prov", "citing": "J01-4005", "vector": [1, 0, 0, 0.0], "context": ["", "Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).", ""], "marker": "1998", "vector_1": {"analysi": 1, "employ": 1, "semant": 1, "shallow": 1, "mitkov": 1, "constraint": 1, "resolut": 1, "prefer": 1, "see": 1, "exampl": 1, "current": 1, "consequ": 1, "reli": 1, "inform": 1, "heurist": 1, "morphosyntact": 1, "mainli": 1, "method": 1, "anaphora": 1}, "vector_2": [3, 0.7085780338199378, 1, 0, 3, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["For anaphors in simple sentences, noun phrases in the previous sen tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1)."], "label": "Non-Prov", "citing": "J01-4006", "vector": [4, 0, 0, 0.0], "context": ["", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples:  \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", ""], "marker": "Mitkov 1998", "vector_1": {"ck": 1, "often": 1, "ron": 1, "bien": 1, "exophor": 1, "incomplet": 1, "82": 1, "oclo": 1, "raini": 1, "even": 1, "use": 1, "pronoun": 1, "author": 1, "detail": 1, "ng": 1, "tim": 1, "envi": 1, "exampl": 1, "nonanaphor": 1, "men": 1, "report": 1, "mitkov": 1, "e": 1, "exa": 1, "provid": 2, "descript": 1, "ath": 1, "hot": 1, "r": 1, "exclus": 1, "mple": 1, "confus": 1, "page": 1}, "vector_2": [3, 0.23485197659989362, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Immediate reference In technical manuals the \"immediate reference\" clue can often be useful in identifying the antecedent."], "label": "Non-Prov", "citing": "J01-4006", "vector": [3, 0, 0, 0.03984095364447979], "context": ["", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples:  \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", ""], "marker": "Mitkov 1998", "vector_1": {"ck": 1, "often": 1, "ron": 1, "bien": 1, "exophor": 1, "incomplet": 1, "82": 1, "oclo": 1, "raini": 1, "even": 1, "use": 1, "pronoun": 1, "author": 1, "detail": 1, "ng": 1, "tim": 1, "envi": 1, "exampl": 1, "nonanaphor": 1, "men": 1, "report": 1, "mitkov": 1, "e": 1, "exa": 1, "provid": 2, "descript": 1, "ath": 1, "hot": 1, "r": 1, "exclus": 1, "mple": 1, "confus": 1, "page": 1}, "vector_2": [3, 0.23485197659989362, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score."], "label": "Non-Prov", "citing": "J01-4006", "vector": [2, 0, 0, 0.0], "context": ["", "Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o'clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples:  \"7 of the pronouns were non-anaphoric and 16 exophoric\" (Mitkov 1998, page 872).", ""], "marker": "Mitkov 1998", "vector_1": {"ck": 1, "often": 1, "ron": 1, "bien": 1, "exophor": 1, "incomplet": 1, "82": 1, "oclo": 1, "raini": 1, "even": 1, "use": 1, "pronoun": 1, "author": 1, "detail": 1, "ng": 1, "tim": 1, "envi": 1, "exampl": 1, "nonanaphor": 1, "men": 1, "report": 1, "mitkov": 1, "e": 1, "exa": 1, "provid": 2, "descript": 1, "ath": 1, "hot": 1, "r": 1, "exclus": 1, "mple": 1, "confus": 1, "page": 1}, "vector_2": [3, 0.23485197659989362, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent."], "label": "Non-Prov", "citing": "J02-1001", "vector": [1, 0, 0, 0.0], "context": ["", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", ""], "marker": "1998", "vector_1": {"antecedentsof": 1, "eg": 1, "prefer": 1, "socal": 1, "wada": 1, "filter": 1, "develop": 1, "divid": 1, "leass": 1, "late": 1, "rich": 1, "factor": 1, "test": 1, "anaphor": 2, "approach": 1, "common": 1, "brown": 1, "viabil": 1, "resolut": 1, "carbonel": 1, "wisdom": 1, "extens": 1, "sinc": 1, "lappin": 1, "mitkov": 1, "practic": 1, "luperfoy": 1, "asher": 1, "s": 1, "integr": 1, "determin": 1}, "vector_2": [4, 0.01786528661788908, 0, 0, 2, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The evaluation for Polish was based technical manuals available on the Internet (Internet Manual, 1994; Java Manual 1998)."], "label": "Non-Prov", "citing": "J02-1001", "vector": [2, 2, 0, 0.0], "context": ["", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", ""], "marker": "1998", "vector_1": {"antecedentsof": 1, "eg": 1, "prefer": 1, "socal": 1, "wada": 1, "filter": 1, "develop": 1, "divid": 1, "leass": 1, "late": 1, "rich": 1, "factor": 1, "test": 1, "anaphor": 2, "approach": 1, "common": 1, "brown": 1, "viabil": 1, "resolut": 1, "carbonel": 1, "wisdom": 1, "extens": 1, "sinc": 1, "lappin": 1, "mitkov": 1, "practic": 1, "luperfoy": 1, "asher": 1, "s": 1, "integr": 1, "determin": 1}, "vector_2": [4, 0.01786528661788908, 0, 0, 2, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["For in stance, in the sentence \"Insert the cassette into the VCRi making sure iti is turned on\", the indicator \"non-prepositional noun phrases\" would penalise the correct antecedent."], "label": "Non-Prov", "citing": "J02-1001", "vector": [3, 0, 1, 0.0], "context": ["", "Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.", ""], "marker": "1998", "vector_1": {"antecedentsof": 1, "eg": 1, "prefer": 1, "socal": 1, "wada": 1, "filter": 1, "develop": 1, "divid": 1, "leass": 1, "late": 1, "rich": 1, "factor": 1, "test": 1, "anaphor": 2, "approach": 1, "common": 1, "brown": 1, "viabil": 1, "resolut": 1, "carbonel": 1, "wisdom": 1, "extens": 1, "sinc": 1, "lappin": 1, "mitkov": 1, "practic": 1, "luperfoy": 1, "asher": 1, "s": 1, "integr": 1, "determin": 1}, "vector_2": [4, 0.01786528661788908, 0, 0, 2, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["In order to evaluate the effectiveness of the ap proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.", "The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%."], "label": "Non-Prov", "citing": "J05-3004", "vector": [7, 0, 1, 0.057735026918962574], "context": ["", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", ""], "marker": "1998", "vector_1": {"mueller": 1, "work": 1, "mitkov": 1, "often": 1, "pronoun": 1, "boguraev": 1, "exampl": 1, "resolut": 1, "focus": 1, "accuraci": 2, "person": 1, "achiev": 1, "good": 1, "rapp": 1, "kennedi": 1, "report": 1, "respect": 1, "strube": 1, "pronomin": 1, "fmeasur": 1, "anaphora": 2}, "vector_2": [7, 0.016565817898864556, 3, 0, 4, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["We have recently adapted the approach for Ara bic as well (Mitkov & Belguith 1998).", "Our evalua tion, based on 63 examples (anaphors) from a tech nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %)."], "label": "Non-Prov", "citing": "J05-3004", "vector": [5, 1, 1, 0.03779644730092272], "context": ["", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", ""], "marker": "1998", "vector_1": {"mueller": 1, "work": 1, "mitkov": 1, "often": 1, "pronoun": 1, "boguraev": 1, "exampl": 1, "resolut": 1, "focus": 1, "accuraci": 2, "person": 1, "achiev": 1, "good": 1, "rapp": 1, "kennedi": 1, "report": 1, "respect": 1, "strube": 1, "pronomin": 1, "fmeasur": 1, "anaphora": 2}, "vector_2": [7, 0.016565817898864556, 3, 0, 4, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Our preference-based approach showed clear su periority over both baseline models.", "The first Base line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%."], "label": "Non-Prov", "citing": "J05-3004", "vector": [2, 0, 0, 0.0], "context": ["", "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", ""], "marker": "1998", "vector_1": {"mueller": 1, "work": 1, "mitkov": 1, "often": 1, "pronoun": 1, "boguraev": 1, "exampl": 1, "resolut": 1, "focus": 1, "accuraci": 2, "person": 1, "achiev": 1, "good": 1, "rapp": 1, "kennedi": 1, "report": 1, "respect": 1, "strube": 1, "pronomin": 1, "fmeasur": 1, "anaphora": 2}, "vector_2": [7, 0.016565817898864556, 3, 0, 4, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphors than indefinite ones (definite noun phrases score 0 and indefinite ones are penalised by -1).", "We regard a noun phrase as definite if the head noun is modified by a definite article, or by demonstrative or posses sive pronouns.", "This rule is ignored if there are no definite articles, possessive or demonstrative pro nouns in the paragraph (this exception is taken into account because some English user's guides tend to omit articles).", "Givenness Noun phrases in previous sentences representing the \"given information\" (theme) 1 are deemed good candidates for antecedents and score I (candidates not representing the theme score 0)."], "label": "Non-Prov", "citing": "N01-1008", "vector": [5, 0, 0, 0.014887283354385342], "context": ["", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", ""], "marker": "Mitkov 1998", "vector_1": {"kameyama": 1, "exten": 1, "consum": 1, "cf": 1, "acquisit": 1, "sive": 1, "empir": 1, "accuraci": 1, "resultsshow": 1, "per": 1, "corefer": 1, "certain": 1, "amaz": 1, "method": 1, "discours": 1, "knowledg": 1, "form": 2, "ofcorefer": 1, "kennedi": 1, "neverthless": 1, "cult": 1, "necessaryfor": 1, "knowledgepoor": 1, "recent": 1, "linguist": 1, "mitkov": 1, "boguraev": 1, "errorpron": 1, "resolv": 1, "time": 1, "diffi": 1}, "vector_2": [3, 0.13446346280447663, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["These scores have been determined experimentally on an empirical basis and are constantly being up dated.", "Top symptoms like \"lexical reiteration\" as sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\".", "We should point out that the antecedent indicators are preferences and not absolute factors.", "There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent."], "label": "Non-Prov", "citing": "N01-1008", "vector": [6, 0, 0, 0.025785531156469834], "context": ["", "The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone. Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf. (Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).", ""], "marker": "Mitkov 1998", "vector_1": {"kameyama": 1, "exten": 1, "consum": 1, "cf": 1, "acquisit": 1, "sive": 1, "empir": 1, "accuraci": 1, "resultsshow": 1, "per": 1, "corefer": 1, "certain": 1, "amaz": 1, "method": 1, "discours": 1, "knowledg": 1, "form": 2, "ofcorefer": 1, "kennedi": 1, "neverthless": 1, "cult": 1, "necessaryfor": 1, "knowledgepoor": 1, "recent": 1, "linguist": 1, "mitkov": 1, "boguraev": 1, "errorpron": 1, "resolv": 1, "time": 1, "diffi": 1}, "vector_2": [3, 0.13446346280447663, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["With a view to avoiding complex syntactic, seman tic and discourse analysis (which is vital for real world applications), we developed a robust, knowl edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors."], "label": "Non-Prov", "citing": "N01-1008", "vector": [2, 0, 0, 0.045360921162651446], "context": ["", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", ""], "marker": "Mitkov 1998", "vector_1": {"mitkov": 1, "com": 1, "perform": 1, "cock": 1, "gener": 1, "resolut": 1, "ponent": 1, "tail": 1, "rule": 1, "corefer": 1, "filter": 1, "method": 1, "unlik": 1, "autotagcoftef": 1, "data": 1, "massivetrain": 1, "baldwin": 1, "knowledgepoor": 1}, "vector_2": [3, 0.9638742593811718, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["These scores have been determined experimentally on an empirical basis and are constantly being up dated."], "label": "Non-Prov", "citing": "N01-1008", "vector": [0, 0, 0, 0.0], "context": ["", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", ""], "marker": "Mitkov 1998", "vector_1": {"mitkov": 1, "com": 1, "perform": 1, "cock": 1, "gener": 1, "resolut": 1, "ponent": 1, "tail": 1, "rule": 1, "corefer": 1, "filter": 1, "method": 1, "unlik": 1, "autotagcoftef": 1, "data": 1, "massivetrain": 1, "baldwin": 1, "knowledgepoor": 1}, "vector_2": [3, 0.9638742593811718, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["This result is comparable with the results described in (Baldwin 1997)."], "label": "Non-Prov", "citing": "N01-1008", "vector": [1, 1, 0, 0.10540925533894598], "context": ["", "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.", ""], "marker": "Mitkov 1998", "vector_1": {"mitkov": 1, "com": 1, "perform": 1, "cock": 1, "gener": 1, "resolut": 1, "ponent": 1, "tail": 1, "rule": 1, "corefer": 1, "filter": 1, "method": 1, "unlik": 1, "autotagcoftef": 1, "data": 1, "massivetrain": 1, "baldwin": 1, "knowledgepoor": 1}, "vector_2": [3, 0.9638742593811718, 0, 0, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolv ing anaphors (with critical success rate of 86.2%).", "Similarly to the evaluation for English, we com pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.", "Our preference-based approach showed clear su periority over both baseline models."], "label": "Non-Prov", "citing": "N04-1004", "vector": [5, 0, 0, 0.10300524052492097], "context": ["", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"discours": 1, "analysi": 1, "semant": 1, "mitkov": 1, "syntact": 1, "show": 1, "appli": 1, "genr": 1, "complex": 1, "saliencebas": 1, "without": 1, "approach": 1, "across": 1}, "vector_2": [6, 0.16447685438016, 1, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation.", "In order to evaluate the effectiveness of the ap proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.", "The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%."], "label": "Non-Prov", "citing": "N04-1004", "vector": [3, 0, 0, 0.0], "context": ["", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"discours": 1, "analysi": 1, "semant": 1, "mitkov": 1, "syntact": 1, "show": 1, "appli": 1, "genr": 1, "complex": 1, "saliencebas": 1, "without": 1, "approach": 1, "across": 1}, "vector_2": [6, 0.16447685438016, 1, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["This paper pres ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Input is checked against agreement and for a number of antecedent indicators.", "Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent."], "label": "Non-Prov", "citing": "N04-1004", "vector": [3, 0, 0, 0.049029033784546004], "context": ["", "Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"discours": 1, "analysi": 1, "semant": 1, "mitkov": 1, "syntact": 1, "show": 1, "appli": 1, "genr": 1, "complex": 1, "saliencebas": 1, "without": 1, "approach": 1, "across": 1}, "vector_2": [6, 0.16447685438016, 1, 1, 1, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%."], "label": "Non-Prov", "citing": "P01-1006", "vector": [2, 0, 0, 0.0], "context": ["", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevs parser- free version of Lappin and Leass RAP (Kennedy and Boguraev, 1996), Baldwins pronoun resolution method (Baldwin, 1997) and Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998b).", ""], "marker": "Mitkov, 1998b", "vector_1": {"evalu": 1, "parser": 1, "rap": 1, "baldwin": 1, "select": 1, "pronoun": 2, "compar": 1, "leass": 1, "three": 1, "version": 1, "approach": 2, "method": 1, "resolut": 2, "free": 1, "extens": 1, "knowledgepoor": 1, "lappin": 1, "mitkov": 1, "boguraev": 1, "literatur": 1, "cite": 1, "kennedi": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["Evaluation."], "label": "Non-Prov", "citing": "P01-1006", "vector": [1, 0, 0, 0.18569533817705186], "context": ["", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevs parser- free version of Lappin and Leass RAP (Kennedy and Boguraev, 1996), Baldwins pronoun resolution method (Baldwin, 1997) and Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998b).", ""], "marker": "Mitkov, 1998b", "vector_1": {"evalu": 1, "parser": 1, "rap": 1, "baldwin": 1, "select": 1, "pronoun": 2, "compar": 1, "leass": 1, "three": 1, "version": 1, "approach": 2, "method": 1, "resolut": 2, "free": 1, "extens": 1, "knowledgepoor": 1, "lappin": 1, "mitkov": 1, "boguraev": 1, "literatur": 1, "cite": 1, "kennedi": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["The new information, or rheme, provides some information about the theme."], "label": "Non-Prov", "citing": "P01-1006", "vector": [1, 0, 0, 0.0], "context": ["", "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevs parser- free version of Lappin and Leass RAP (Kennedy and Boguraev, 1996), Baldwins pronoun resolution method (Baldwin, 1997) and Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998b).", ""], "marker": "Mitkov, 1998b", "vector_1": {"evalu": 1, "parser": 1, "rap": 1, "baldwin": 1, "select": 1, "pronoun": 2, "compar": 1, "leass": 1, "three": 1, "version": 1, "approach": 2, "method": 1, "resolut": 2, "free": 1, "extens": 1, "knowledgepoor": 1, "lappin": 1, "mitkov": 1, "boguraev": 1, "literatur": 1, "cite": 1, "kennedi": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).", "There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric."], "label": "Non-Prov", "citing": "P01-1006", "vector": [6, 0, 2, 0.11677484162422844], "context": ["", "Mitkovs approach Mitkovs approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", ""], "marker": "Mitkov, 1998b", "vector_1": {"boost": 1, "set": 1, "mitkov": 2, "text": 1, "resolut": 1, "indic": 1, "technic": 1, "candid": 1, "base": 1, "anteced": 1, "appli": 1, "imped": 1, "robust": 1, "approach": 2, "method": 1, "anaphora": 1}, "vector_2": [3, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the antecedent have not only different syntactic functions but also different semantic roles.", "Usually knowledge-based ap proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\"."], "label": "Non-Prov", "citing": "P01-1006", "vector": [5, 0, 0, 0.10241831129983783], "context": ["", "Mitkovs approach Mitkovs approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", ""], "marker": "Mitkov, 1998b", "vector_1": {"boost": 1, "set": 1, "mitkov": 2, "text": 1, "resolut": 1, "indic": 1, "technic": 1, "candid": 1, "base": 1, "anteced": 1, "appli": 1, "imped": 1, "robust": 1, "approach": 2, "method": 1, "anaphora": 1}, "vector_2": [3, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent.", "For in stance, in the sentence \"Insert the cassette into the VCRi making sure iti is turned on\", the indicator \"non-prepositional noun phrases\" would penalise the correct antecedent."], "label": "Non-Prov", "citing": "P01-1006", "vector": [7, 0, 0, 0.14625448482542613], "context": ["", "Mitkovs approach Mitkovs approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.", ""], "marker": "Mitkov, 1998b", "vector_1": {"boost": 1, "set": 1, "mitkov": 2, "text": 1, "resolut": 1, "indic": 1, "technic": 1, "candid": 1, "base": 1, "anteced": 1, "appli": 1, "imped": 1, "robust": 1, "approach": 2, "method": 1, "anaphora": 1}, "vector_2": [3, 0.5, 1, 2, 2, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979)."], "label": "Non-Prov", "citing": "P01-1006", "vector": [4, 0, 0, 0.02886751345948129], "context": ["", "pronouns, referential distance, average number of candidates for antecedent per pronoun and types of anaphors.7 As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996), Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison.", ""], "marker": "1998b", "vector_1": {"origin": 1, "distanc": 1, "pronoun": 2, "number": 1, "per": 1, "tabl": 1, "baldwin": 1, "ground": 1, "result": 2, "differ": 3, "referenti": 1, "publish": 1, "resort": 1, "test": 1, "anaphor": 1, "type": 1, "match": 1, "tool": 1, "preprocess": 1, "candid": 1, "report": 1, "data": 1, "averag": 1, "comparison": 1, "anteced": 1, "mitkov": 1, "algorithm": 1, "boguraev": 1, "provid": 1, "thu": 1, "manual": 1, "employ": 1, "degre": 1, "common": 1, "kennedi": 1, "intervent": 1, "reliabl": 1, "expect": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["Example: To print the paper, you can stand the printeri up or lay iti flat."], "label": "Non-Prov", "citing": "P01-1006", "vector": [2, 0, 0, 0.0], "context": ["", "pronouns, referential distance, average number of candidates for antecedent per pronoun and types of anaphors.7 As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996), Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison.", ""], "marker": "1998b", "vector_1": {"origin": 1, "distanc": 1, "pronoun": 2, "number": 1, "per": 1, "tabl": 1, "baldwin": 1, "ground": 1, "result": 2, "differ": 3, "referenti": 1, "publish": 1, "resort": 1, "test": 1, "anaphor": 1, "type": 1, "match": 1, "tool": 1, "preprocess": 1, "candid": 1, "report": 1, "data": 1, "averag": 1, "comparison": 1, "anteced": 1, "mitkov": 1, "algorithm": 1, "boguraev": 1, "provid": 1, "thu": 1, "manual": 1, "employ": 1, "degre": 1, "common": 1, "kennedi": 1, "intervent": 1, "reliabl": 1, "expect": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking."], "label": "Non-Prov", "citing": "P01-1006", "vector": [7, 0, 0, 0.029488391230979426], "context": ["", "pronouns, referential distance, average number of candidates for antecedent per pronoun and types of anaphors.7 As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996), Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison.", ""], "marker": "1998b", "vector_1": {"origin": 1, "distanc": 1, "pronoun": 2, "number": 1, "per": 1, "tabl": 1, "baldwin": 1, "ground": 1, "result": 2, "differ": 3, "referenti": 1, "publish": 1, "resort": 1, "test": 1, "anaphor": 1, "type": 1, "match": 1, "tool": 1, "preprocess": 1, "candid": 1, "report": 1, "data": 1, "averag": 1, "comparison": 1, "anteced": 1, "mitkov": 1, "algorithm": 1, "boguraev": 1, "provid": 1, "thu": 1, "manual": 1, "employ": 1, "degre": 1, "common": 1, "kennedi": 1, "intervent": 1, "reliabl": 1, "expect": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators."], "label": "Non-Prov", "citing": "P01-1006", "vector": [3, 1, 0, 0.0], "context": ["", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common knowledge-poor philosophy: Kennedy and Boguraevs (1996) parser-free algorithm, Baldwins (1997) CogNiac and Mitkovs (1998b) knowledge-poor approach.", ""], "marker": "1998b", "vector_1": {"cogniac": 1, "mitkov": 1, "algorithm": 1, "evalu": 1, "boguraev": 1, "baldwin": 1, "configur": 1, "share": 1, "three": 1, "knowledgepoor": 2, "paper": 1, "environ": 1, "common": 1, "incorpor": 1, "particular": 1, "new": 1, "kennedi": 1, "philosophi": 1, "approach": 2, "discuss": 1, "parserfre": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["The collocation preference here is restricted to the patterns \"noun phrase (pronoun), verb\" and \"verb, noun phrase (pronoun)\"."], "label": "Non-Prov", "citing": "P01-1006", "vector": [1, 0, 0, 0.0], "context": ["", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common knowledge-poor philosophy: Kennedy and Boguraevs (1996) parser-free algorithm, Baldwins (1997) CogNiac and Mitkovs (1998b) knowledge-poor approach.", ""], "marker": "1998b", "vector_1": {"cogniac": 1, "mitkov": 1, "algorithm": 1, "evalu": 1, "boguraev": 1, "baldwin": 1, "configur": 1, "share": 1, "three": 1, "knowledgepoor": 2, "paper": 1, "environ": 1, "common": 1, "incorpor": 1, "particular": 1, "new": 1, "kennedi": 1, "philosophi": 1, "approach": 2, "discuss": 1, "parserfre": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["Given that our knowledge poor approach is basically an enhancement of a baseline model through a set of antecedent indica tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon."], "label": "Non-Prov", "citing": "P01-1006", "vector": [4, 0, 0, 0.04588314677411235], "context": ["", "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common knowledge-poor philosophy: Kennedy and Boguraevs (1996) parser-free algorithm, Baldwins (1997) CogNiac and Mitkovs (1998b) knowledge-poor approach.", ""], "marker": "1998b", "vector_1": {"cogniac": 1, "mitkov": 1, "algorithm": 1, "evalu": 1, "boguraev": 1, "baldwin": 1, "configur": 1, "share": 1, "three": 1, "knowledgepoor": 2, "paper": 1, "environ": 1, "common": 1, "incorpor": 1, "particular": 1, "new": 1, "kennedi": 1, "philosophi": 1, "approach": 2, "discuss": 1, "parserfre": 1}, "vector_2": [3, 0.5, 3, 2, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta tion."], "label": "Non-Prov", "citing": "P04-1017", "vector": [2, 0, 0, 0.0], "context": ["", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "learningbas": 1, "chain": 1, "frequenc": 1, "resolut": 1, "variant": 1, "system": 1, "tfidf": 1, "length": 1, "candid": 1, "salienc": 1, "factor": 1, "refer": 1, "occurr": 1, "coreferenti": 1}, "vector_2": [6, 0.9360183533010451, 4, 1, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking."], "label": "Non-Prov", "citing": "P04-1017", "vector": [6, 0, 0, 0.05383819020581655], "context": ["", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "learningbas": 1, "chain": 1, "frequenc": 1, "resolut": 1, "variant": 1, "system": 1, "tfidf": 1, "length": 1, "candid": 1, "salienc": 1, "factor": 1, "refer": 1, "occurr": 1, "coreferenti": 1}, "vector_2": [6, 0.9360183533010451, 4, 1, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolv ing anaphors (with critical success rate of 86.2%)."], "label": "Non-Prov", "citing": "P04-1017", "vector": [4, 0, 0, 0.0], "context": ["", "The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "learningbas": 1, "chain": 1, "frequenc": 1, "resolut": 1, "variant": 1, "system": 1, "tfidf": 1, "length": 1, "candid": 1, "salienc": 1, "factor": 1, "refer": 1, "occurr": 1, "coreferenti": 1}, "vector_2": [6, 0.9360183533010451, 4, 1, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%.", "Given that our knowledge poor approach is basically an enhancement of a baseline model through a set of antecedent indica tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon."], "label": "Non-Prov", "citing": "P07-1068", "vector": [3, 0, 0, 0.04279604925109129], "context": ["", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", ""], "marker": "1998", "vector_1": {"knowledgelean": 1, "sourc": 1, "mitkov": 1, "process": 1, "eg": 1, "knowledg": 1, "resolv": 1, "corefer": 1, "employ": 1, "cue": 1, "morphosyntact": 1, "resolut": 1, "approach": 1, "tetreault": 1}, "vector_2": [9, 0.027767724954762298, 2, 0, 5, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.", "The resolution of anaphors was carried out with a suc cess rate of 95.8%."], "label": "Non-Prov", "citing": "P07-1068", "vector": [3, 0, 2, 0.06900655593423542], "context": ["", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", ""], "marker": "1998", "vector_1": {"knowledgelean": 1, "sourc": 1, "mitkov": 1, "process": 1, "eg": 1, "knowledg": 1, "resolv": 1, "corefer": 1, "employ": 1, "cue": 1, "morphosyntact": 1, "resolut": 1, "approach": 1, "tetreault": 1}, "vector_2": [9, 0.027767724954762298, 2, 0, 5, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphors than indefinite ones (definite noun phrases score 0 and indefinite ones are penalised by -1).", "We regard a noun phrase as definite if the head noun is modified by a definite article, or by demonstrative or posses sive pronouns."], "label": "Non-Prov", "citing": "P07-1068", "vector": [3, 0, 0, 0.0], "context": ["", "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).", ""], "marker": "1998", "vector_1": {"knowledgelean": 1, "sourc": 1, "mitkov": 1, "process": 1, "eg": 1, "knowledg": 1, "resolv": 1, "corefer": 1, "employ": 1, "cue": 1, "morphosyntact": 1, "resolut": 1, "approach": 1, "tetreault": 1}, "vector_2": [9, 0.027767724954762298, 2, 0, 5, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the \"non-prepositional noun phrases\" heuristics (penalty) would be overturned by the \"collocational preference\" heuristics."], "label": "Non-Prov", "citing": "P10-2049", "vector": [3, 0, 0, 0.0], "context": ["", "The algorithm with the next-to-highest results in (Char- niak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit.", ""], "marker": "Mitkov, 1998", "vector_1": {"nexttohighest": 1, "mar": 1, "algorithm": 1, "toolkit": 1, "guitar": 1, "result": 1}, "vector_2": [12, 0.44919241402725907, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["In addition, preliminary experiments show that the approach can be success fully adapted for other languages with minimum modifications."], "label": "Non-Prov", "citing": "P10-2049", "vector": [3, 0, 0, 0.0], "context": ["", "The algorithm with the next-to-highest results in (Char- niak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit.", ""], "marker": "Mitkov, 1998", "vector_1": {"nexttohighest": 1, "mar": 1, "algorithm": 1, "toolkit": 1, "guitar": 1, "result": 1}, "vector_2": [12, 0.44919241402725907, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["This preference can be viewed as a modification of the collocation preference."], "label": "Non-Prov", "citing": "P10-2049", "vector": [1, 0, 0, 0.0], "context": ["", "The algorithm with the next-to-highest results in (Char- niak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit.", ""], "marker": "Mitkov, 1998", "vector_1": {"nexttohighest": 1, "mar": 1, "algorithm": 1, "toolkit": 1, "guitar": 1, "result": 1}, "vector_2": [12, 0.44919241402725907, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Note that \"Baseline subject\" can be assessed both in terms of recall and precision because this \"version\" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many im perative zero-subject sentences).", "In the second experiment we evaluated the ap proach from the point of view also of its \"critical success rate\"."], "label": "Non-Prov", "citing": "S10-1019", "vector": [3, 0, 1, 0.0], "context": ["", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", ""], "marker": "Mitkov, 1998", "vector_1": {"concentr": 1, "machin": 1, "major": 1, "made": 1, "last": 1, "rulebas": 1, "eg": 1, "resolut": 1, "embrac": 1, "system": 1, "corefer": 1, "field": 1, "cf": 2, "learn": 1, "progress": 1, "decad": 1, "method": 1}, "vector_2": [12, 0.050087806367870503, 3, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["We regard a noun phrase as definite if the head noun is modified by a definite article, or by demonstrative or posses sive pronouns.", "This rule is ignored if there are no definite articles, possessive or demonstrative pro nouns in the paragraph (this exception is taken into account because some English user's guides tend to omit articles)."], "label": "Non-Prov", "citing": "S10-1019", "vector": [4, 0, 1, 0.0], "context": ["", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", ""], "marker": "Mitkov, 1998", "vector_1": {"concentr": 1, "machin": 1, "major": 1, "made": 1, "last": 1, "rulebas": 1, "eg": 1, "resolut": 1, "embrac": 1, "system": 1, "corefer": 1, "field": 1, "cf": 2, "learn": 1, "progress": 1, "decad": 1, "method": 1}, "vector_2": [12, 0.050087806367870503, 3, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The results from experiment 1 and experiment 2 can be summarised in the following (statistically) slightly more representative figures.", "R ob ust aQ pr oa ch B a s el i n e s u b je ct B as eli ne m os t re ce nt Su cc es s rat e (= Pr ec isi on / Re ca ll) 8 9."], "label": "Non-Prov", "citing": "S10-1019", "vector": [4, 0, 1, 0.0], "context": ["", "Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.", ""], "marker": "Mitkov, 1998", "vector_1": {"concentr": 1, "machin": 1, "major": 1, "made": 1, "last": 1, "rulebas": 1, "eg": 1, "resolut": 1, "embrac": 1, "system": 1, "corefer": 1, "field": 1, "cf": 2, "learn": 1, "progress": 1, "decad": 1, "method": 1}, "vector_2": [12, 0.050087806367870503, 3, 1, 1, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["This case and other cases suggest that it might be worthwhile reconsider ing/refining the weights for the indicator \"referential distance\".", "Similarly to the first evaluation, we found that the robust approach was not very successful on sen tences with too complicated syntax - a price we have to pay for the \"convenience\" of developing a knowl edge-poor system."], "label": "Non-Prov", "citing": "W01-0704", "vector": [5, 0, 1, 0.0], "context": ["", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998)", ""], "marker": "Mitkov, 1998", "vector_1": {"high": 1, "use": 1, "sourc": 1, "success": 1, "cor": 1, "knowledg": 1, "rate": 1, "lexic": 1, "morpholog": 1, "syntacticinform": 1, "detect": 1, "limit": 1, "anteced": 1, "english": 1, "report": 1, "rect": 1, "propos": 1}, "vector_2": [3, 0.7005733005733006, 1, 1, 1, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%.", "Given that our knowledge poor approach is basically an enhancement of a baseline model through a set of antecedent indica tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon."], "label": "Non-Prov", "citing": "W01-0704", "vector": [6, 0, 1, 0.15534712747612348], "context": ["", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998)", ""], "marker": "Mitkov, 1998", "vector_1": {"high": 1, "use": 1, "sourc": 1, "success": 1, "cor": 1, "knowledg": 1, "rate": 1, "lexic": 1, "morpholog": 1, "syntacticinform": 1, "detect": 1, "limit": 1, "anteced": 1, "english": 1, "report": 1, "rect": 1, "propos": 1}, "vector_2": [3, 0.7005733005733006, 1, 1, 1, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["Syntactic parallelism, useful in discrimi nating between identical pronouns on the basis of their syntactic function, also has to be forgone.", "Lack of semantic knowledge rules out the use of verb se mantics and semantic parallelism."], "label": "Non-Prov", "citing": "W01-0704", "vector": [5, 0, 0, 0.09166984970282112], "context": ["", "They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998)", ""], "marker": "Mitkov, 1998", "vector_1": {"high": 1, "use": 1, "sourc": 1, "success": 1, "cor": 1, "knowledg": 1, "rate": 1, "lexic": 1, "morpholog": 1, "syntacticinform": 1, "detect": 1, "limit": 1, "anteced": 1, "english": 1, "report": 1, "rect": 1, "propos": 1}, "vector_2": [3, 0.7005733005733006, 1, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["For the time being, we are using the same scores for Polish."], "label": "Non-Prov", "citing": "W01-0717", "vector": [1, 0, 0, 0.0], "context": ["", "There are many recent approaches to this prob lem, e. g. syntax-based approaches (Lappin and Leass, 1994), cooccurrence-based approaches(Dagan and Itai, 1990), machine-learning approaches (Connolly et al., 1994; Aone and Ben nett, 1996; Soon et al., 1999), uncertainty reasoning approaches (Mitkov, 1995; Mitkov, 1997), and robust knowledge-poor approaches (Kennedy and Boguarev, 1996; Baldwin, 1997; Mitkov, 1998b; Mitkov, 1999).", ""], "marker": "Mitkov, 1998b", "vector_1": {"syntaxbas": 1, "cooccurrencebas": 1, "e": 1, "g": 1, "machinelearn": 1, "lem": 1, "knowledgepoor": 1, "reason": 1, "robust": 1, "mani": 1, "uncertainti": 1, "approach": 6, "prob": 1, "recent": 1}, "vector_2": [3, 0.869720700737857, 11, 6, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["antecedent."], "label": "Non-Prov", "citing": "W01-0717", "vector": [0, 0, 0, 0.0], "context": ["", "There are many recent approaches to this prob lem, e. g. syntax-based approaches (Lappin and Leass, 1994), cooccurrence-based approaches(Dagan and Itai, 1990), machine-learning approaches (Connolly et al., 1994; Aone and Ben nett, 1996; Soon et al., 1999), uncertainty reasoning approaches (Mitkov, 1995; Mitkov, 1997), and robust knowledge-poor approaches (Kennedy and Boguarev, 1996; Baldwin, 1997; Mitkov, 1998b; Mitkov, 1999).", ""], "marker": "Mitkov, 1998b", "vector_1": {"syntaxbas": 1, "cooccurrencebas": 1, "e": 1, "g": 1, "machinelearn": 1, "lem": 1, "knowledgepoor": 1, "reason": 1, "robust": 1, "mani": 1, "uncertainti": 1, "approach": 6, "prob": 1, "recent": 1}, "vector_2": [3, 0.869720700737857, 11, 6, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Press iti again."], "label": "Non-Prov", "citing": "W01-0717", "vector": [0, 0, 0, 0.0], "context": ["", "There are many recent approaches to this prob lem, e. g. syntax-based approaches (Lappin and Leass, 1994), cooccurrence-based approaches(Dagan and Itai, 1990), machine-learning approaches (Connolly et al., 1994; Aone and Ben nett, 1996; Soon et al., 1999), uncertainty reasoning approaches (Mitkov, 1995; Mitkov, 1997), and robust knowledge-poor approaches (Kennedy and Boguarev, 1996; Baldwin, 1997; Mitkov, 1998b; Mitkov, 1999).", ""], "marker": "Mitkov, 1998b", "vector_1": {"syntaxbas": 1, "cooccurrencebas": 1, "e": 1, "g": 1, "machinelearn": 1, "lem": 1, "knowledgepoor": 1, "reason": 1, "robust": 1, "mani": 1, "uncertainti": 1, "approach": 6, "prob": 1, "recent": 1}, "vector_2": [3, 0.869720700737857, 11, 6, 0, 1]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree ment test."], "label": "Non-Prov", "citing": "W04-0707", "vector": [6, 0, 0, 0.0], "context": ["", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovs algorithm for pronoun resolution (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"poesio": 1, "mitkov": 1, "algorithm": 2, "g": 1, "descript": 1, "resolut": 1, "definit": 1, "resolv": 1, "vieira": 1, "r": 1, "u": 1, "includ": 1, "generalpurpos": 1, "anaphor": 1, "implement": 1, "pronoun": 1, "ta": 1}, "vector_2": [6, 0.7486840068122, 2, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in (Dagan & ltai 1990)."], "label": "Non-Prov", "citing": "W04-0707", "vector": [3, 0, 0, 0.0], "context": ["", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovs algorithm for pronoun resolution (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"poesio": 1, "mitkov": 1, "algorithm": 2, "g": 1, "descript": 1, "resolut": 1, "definit": 1, "resolv": 1, "vieira": 1, "r": 1, "u": 1, "includ": 1, "generalpurpos": 1, "anaphor": 1, "implement": 1, "pronoun": 1, "ta": 1}, "vector_2": [6, 0.7486840068122, 2, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["If this indicator does not hold again, go for the most recent candidate."], "label": "Non-Prov", "citing": "W04-0707", "vector": [2, 0, 0, 0.0], "context": ["", "G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovs algorithm for pronoun resolution (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"poesio": 1, "mitkov": 1, "algorithm": 2, "g": 1, "descript": 1, "resolut": 1, "definit": 1, "resolv": 1, "vieira": 1, "r": 1, "u": 1, "includ": 1, "generalpurpos": 1, "anaphor": 1, "implement": 1, "pronoun": 1, "ta": 1}, "vector_2": [6, 0.7486840068122, 2, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing."], "label": "Non-Prov", "citing": "W04-0711", "vector": [3, 0, 0, 0.058823529411764705], "context": ["", "(PMID:9701290) Table 1: A protein domain-referring phrase example ments and lexical features, in addressing problems in the biomedical domain (cf. Mitkov et al (1998)).", ""], "marker": "1998", "vector_1": {"domain": 1, "featur": 1, "mitkov": 1, "ment": 1, "address": 1, "al": 1, "problem": 1, "lexic": 1, "cf": 1, "et": 1, "domainref": 1, "exampl": 1, "biomed": 1, "tabl": 1, "phrase": 1, "pmid": 1, "protein": 1}, "vector_2": [6, 0.12350511761537081, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0)."], "label": "Non-Prov", "citing": "W04-0711", "vector": [1, 0, 0, 0.0], "context": ["", "(PMID:9701290) Table 1: A protein domain-referring phrase example ments and lexical features, in addressing problems in the biomedical domain (cf. Mitkov et al (1998)).", ""], "marker": "1998", "vector_1": {"domain": 1, "featur": 1, "mitkov": 1, "ment": 1, "address": 1, "al": 1, "problem": 1, "lexic": 1, "cf": 1, "et": 1, "domainref": 1, "exampl": 1, "biomed": 1, "tabl": 1, "phrase": 1, "pmid": 1, "protein": 1}, "vector_2": [6, 0.12350511761537081, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Evaluation reports a success rate of 89.7% which is better than the suc cess rates of the approaches selected for comparison and tested on the same data."], "label": "Non-Prov", "citing": "W04-0711", "vector": [3, 0, 0, 0.0], "context": ["", "(PMID:9701290) Table 1: A protein domain-referring phrase example ments and lexical features, in addressing problems in the biomedical domain (cf. Mitkov et al (1998)).", ""], "marker": "1998", "vector_1": {"domain": 1, "featur": 1, "mitkov": 1, "ment": 1, "address": 1, "al": 1, "problem": 1, "lexic": 1, "cf": 1, "et": 1, "domainref": 1, "exampl": 1, "biomed": 1, "tabl": 1, "phrase": 1, "pmid": 1, "protein": 1}, "vector_2": [6, 0.12350511761537081, 1, 0, 0, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\")."], "label": "Non-Prov", "citing": "W04-0714", "vector": [4, 0, 0, 0.04225771273642582], "context": ["", "Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as part- of-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrndez et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003).", ""], "marker": "Mitkov, 1998", "vector_1": {"nlp": 1, "parser": 1, "shallow": 1, "po": 1, "inexpens": 1, "reliabl": 2, "resolut": 1, "tool": 1, "procedur": 1, "cheaper": 1, "reli": 1, "part": 1, "tagger": 1, "fast": 1, "ofspeech": 1, "method": 1, "anaphora": 1}, "vector_2": [6, 0.12737480575467441, 5, 1, 2, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["We regard a noun phrase as definite if the head noun is modified by a definite article, or by demonstrative or posses sive pronouns."], "label": "Non-Prov", "citing": "W04-0714", "vector": [2, 0, 0, 0.0], "context": ["", "Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as part- of-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrndez et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003).", ""], "marker": "Mitkov, 1998", "vector_1": {"nlp": 1, "parser": 1, "shallow": 1, "po": 1, "inexpens": 1, "reliabl": 2, "resolut": 1, "tool": 1, "procedur": 1, "cheaper": 1, "reli": 1, "part": 1, "tagger": 1, "fast": 1, "ofspeech": 1, "method": 1, "anaphora": 1}, "vector_2": [6, 0.12737480575467441, 5, 1, 2, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates"], "label": "Non-Prov", "citing": "W04-0714", "vector": [3, 0, 0, 0.0], "context": ["", "Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as part- of-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrndez et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003).", ""], "marker": "Mitkov, 1998", "vector_1": {"nlp": 1, "parser": 1, "shallow": 1, "po": 1, "inexpens": 1, "reliabl": 2, "resolut": 1, "tool": 1, "procedur": 1, "cheaper": 1, "reli": 1, "part": 1, "tagger": 1, "fast": 1, "ofspeech": 1, "method": 1, "anaphora": 1}, "vector_2": [6, 0.12737480575467441, 5, 1, 2, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Typically, our preference-based model proved superior to both baseline models when the antece dent was neither the most recent subject nor the most recent noun phrase matching the anaphor in gender and number.", "Example: Identify the draweq by the lit paper port LED and add paper to itj.", "The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera tion 0 + section heading 0 + collocation 0 + referen tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4)."], "label": "Non-Prov", "citing": "W04-2310", "vector": [4, 0, 0, 0.0], "context": ["", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", ""], "marker": "Mitkov, 1998", "vector_1": {"differ": 1, "depend": 1, "weight": 1, "relationship": 1, "system": 1, "programm": 1, "anaphoranteced": 1, "assign": 1}, "vector_2": [6, 0.8749348958333333, 2, 2, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["In order to evaluate the effectiveness of the ap proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.", "The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%.", "Given that our knowledge poor approach is basically an enhancement of a baseline model through a set of antecedent indica tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon."], "label": "Non-Prov", "citing": "W04-2310", "vector": [6, 0, 0, 0.0], "context": ["", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", ""], "marker": "Mitkov, 1998", "vector_1": {"differ": 1, "depend": 1, "weight": 1, "relationship": 1, "system": 1, "programm": 1, "anaphoranteced": 1, "assign": 1}, "vector_2": [6, 0.8749348958333333, 2, 2, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["In the second experiment we evaluated the ap proach from the point of view also of its \"critical success rate\".", "This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.", "Our evaluation estab lished the critical success rate as 82%."], "label": "Non-Prov", "citing": "W04-2310", "vector": [3, 1, 0, 0.0], "context": ["", "In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.", ""], "marker": "Mitkov, 1998", "vector_1": {"differ": 1, "depend": 1, "weight": 1, "relationship": 1, "system": 1, "programm": 1, "anaphoranteced": 1, "assign": 1}, "vector_2": [6, 0.8749348958333333, 2, 2, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Note that \"Baseline subject\" can be assessed both in terms of recall and precision because this \"version\" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many im perative zero-subject sentences).", "In the second experiment we evaluated the ap proach from the point of view also of its \"critical success rate\"."], "label": "Non-Prov", "citing": "W04-2310", "vector": [5, 0, 0, 0.04564354645876384], "context": ["", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"corpu": 1, "use": 1, "often": 1, "perform": 1, "resolut": 1, "well": 1, "limit": 1, "tune": 1, "heurist": 1, "domainspecif": 1, "fine": 1, "anaphora": 1}, "vector_2": [6, 0.12135416666666667, 1, 2, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Example: Identify the draweq by the lit paper port LED and add paper to itj.", "The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera tion 0 + section heading 0 + collocation 0 + referen tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4)."], "label": "Non-Prov", "citing": "W04-2310", "vector": [3, 0, 0, 0.0], "context": ["", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"corpu": 1, "use": 1, "often": 1, "perform": 1, "resolut": 1, "well": 1, "limit": 1, "tune": 1, "heurist": 1, "domainspecif": 1, "fine": 1, "anaphora": 1}, "vector_2": [6, 0.12135416666666667, 1, 2, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).", "With a view to avoiding complex syntactic, seman tic and discourse analysis (which is vital for real world applications), we developed a robust, knowl edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors."], "label": "Non-Prov", "citing": "W04-2310", "vector": [7, 0, 0, 0.08703882797784893], "context": ["", "Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"corpu": 1, "use": 1, "often": 1, "perform": 1, "resolut": 1, "well": 1, "limit": 1, "tune": 1, "heurist": 1, "domainspecif": 1, "fine": 1, "anaphora": 1}, "vector_2": [6, 0.12135416666666667, 1, 2, 1, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["If immediate reference does not hold, propose the candidate with higher score for collocational pattern.", "If collocational pattern suggests a tie or does not hold, select the candidate with higher score for indicating verbs."], "label": "Non-Prov", "citing": "W06-2302", "vector": [2, 0, 0, 0.0], "context": ["", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic it occurrences, and assigns animacy.", ""], "marker": "1998", "vector_1": {"poor": 1, "tri": 1, "use": 1, "knowledg": 1, "mitkov": 1, "pleonast": 1, "algorithm": 1, "occurr": 1, "make": 1, "resolut": 1, "chunk": 1, "assign": 1, "r": 1, "animaci": 1, "present": 1, "np": 1, "individu": 1, "approach": 1, "po": 1, "anaphora": 1}, "vector_2": [8, 0.22151446213844653, 0, 0, 1, 1]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3).", "The evaluation indicated 83.6% success rate."], "label": "Non-Prov", "citing": "W06-2302", "vector": [6, 0, 0, 0.0], "context": ["", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic it occurrences, and assigns animacy.", ""], "marker": "1998", "vector_1": {"poor": 1, "tri": 1, "use": 1, "knowledg": 1, "mitkov": 1, "pleonast": 1, "algorithm": 1, "occurr": 1, "make": 1, "resolut": 1, "chunk": 1, "assign": 1, "r": 1, "animaci": 1, "present": 1, "np": 1, "individu": 1, "approach": 1, "po": 1, "anaphora": 1}, "vector_2": [8, 0.22151446213844653, 0, 0, 1, 1]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates", "tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree ment test."], "label": "Non-Prov", "citing": "W06-2302", "vector": [10, 0, 1, 0.01917412472118426], "context": ["", "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic it occurrences, and assigns animacy.", ""], "marker": "1998", "vector_1": {"poor": 1, "tri": 1, "use": 1, "knowledg": 1, "mitkov": 1, "pleonast": 1, "algorithm": 1, "occurr": 1, "make": 1, "resolut": 1, "chunk": 1, "assign": 1, "r": 1, "animaci": 1, "present": 1, "np": 1, "individu": 1, "approach": 1, "po": 1, "anaphora": 1}, "vector_2": [8, 0.22151446213844653, 0, 0, 1, 1]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im mediate reference.", "If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score."], "label": "Non-Prov", "citing": "W09-2411", "vector": [2, 0, 0, 0.0], "context": ["", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", ""], "marker": "Mitkov, 1998", "vector_1": {"tradit": 1, "machin": 1, "corpora": 1, "techniqu": 1, "allow": 1, "could": 1, "knowledg": 1, "annot": 1, "rule": 1, "autom": 1, "acquisit": 1, "base": 1, "limit": 1, "overcom": 1, "learn": 1, "approach": 1}, "vector_2": [11, 0.07112765748995788, 1, 1, 0, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["Here \"the VCR\" is penalised (-1) for being part of the prepositional phrase \"into the VCR\".", "This preference can be explained in terms of sali ence from the point of view of the centering theory."], "label": "Non-Prov", "citing": "W09-2411", "vector": [4, 0, 1, 0.0], "context": ["", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", ""], "marker": "Mitkov, 1998", "vector_1": {"tradit": 1, "machin": 1, "corpora": 1, "techniqu": 1, "allow": 1, "could": 1, "knowledg": 1, "annot": 1, "rule": 1, "autom": 1, "acquisit": 1, "base": 1, "limit": 1, "overcom": 1, "learn": 1, "approach": 1}, "vector_2": [11, 0.07112765748995788, 1, 1, 0, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["3.3 Comparison to similar approaches: compara.", "tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\"."], "label": "Non-Prov", "citing": "W09-2411", "vector": [6, 0, 0, 0.08219949365267865], "context": ["", "Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", ""], "marker": "Mitkov, 1998", "vector_1": {"tradit": 1, "machin": 1, "corpora": 1, "techniqu": 1, "allow": 1, "could": 1, "knowledg": 1, "annot": 1, "rule": 1, "autom": 1, "acquisit": 1, "base": 1, "limit": 1, "overcom": 1, "learn": 1, "approach": 1}, "vector_2": [11, 0.07112765748995788, 1, 1, 0, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["We should point out that the antecedent indicators are preferences and not absolute factors.", "There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent."], "label": "Non-Prov", "citing": "W99-0104", "vector": [3, 0, 0, 0.0], "context": ["", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "set": 1, "anteced": 1, "pronomin": 1, "eitper": 1, "resolut": 1, "knowledgepoor": 1, "candid": 1, "assign": 1, "score": 1, "combin": 1, "promot": 1, "heurist": 1, "gener": 1, "order": 1, "approach": 1, "method": 1}, "vector_2": [1, 0.18110604466269145, 1, 1, 0, 0]}, {"function": "Weak", "cited": "P98-2143", "provenance": ["3.3 Comparison to similar approaches: compara.", "tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's Cog NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\"."], "label": "Non-Prov", "citing": "W99-0104", "vector": [4, 0, 0, 0.039872611141445004], "context": ["", "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "set": 1, "anteced": 1, "pronomin": 1, "eitper": 1, "resolut": 1, "knowledgepoor": 1, "candid": 1, "assign": 1, "score": 1, "combin": 1, "promot": 1, "heurist": 1, "gener": 1, "order": 1, "approach": 1, "method": 1}, "vector_2": [1, 0.18110604466269145, 1, 1, 0, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im mediate reference."], "label": "Non-Prov", "citing": "W99-0104", "vector": [7, 0, 0, 0.04564354645876384], "context": ["", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "resalv": 1, "set": 1, "cal": 1, "algorithm": 2, "cogniac": 1, "eg": 1, "six": 1, "wherea": 1, "l": 1, "rule": 1, "corefer": 1, "base": 1, "limit": 1, "prefer": 1, "heurist": 1, "immedi": 1, "definit": 1, "reiter": 1, "present": 1, "refer": 1}, "vector_2": [1, 0.1865427335437858, 2, 1, 0, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer ences (see below)."], "label": "Non-Prov", "citing": "W99-0104", "vector": [2, 0, 0, 0.0], "context": ["", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "resalv": 1, "set": 1, "cal": 1, "algorithm": 2, "cogniac": 1, "eg": 1, "six": 1, "wherea": 1, "l": 1, "rule": 1, "corefer": 1, "base": 1, "limit": 1, "prefer": 1, "heurist": 1, "immedi": 1, "definit": 1, "reiter": 1, "present": 1, "refer": 1}, "vector_2": [1, 0.1865427335437858, 2, 1, 0, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["The \"Baseline subject\" model tested on the same data scored 33.9% recall and 67.9% precision, whereas \"Baseline most recent\" scored 66.7%."], "label": "Non-Prov", "citing": "W99-0104", "vector": [3, 0, 0, 0.051031036307982884], "context": ["", "The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "resalv": 1, "set": 1, "cal": 1, "algorithm": 2, "cogniac": 1, "eg": 1, "six": 1, "wherea": 1, "l": 1, "rule": 1, "corefer": 1, "base": 1, "limit": 1, "prefer": 1, "heurist": 1, "immedi": 1, "definit": 1, "reiter": 1, "present": 1, "refer": 1}, "vector_2": [1, 0.1865427335437858, 2, 1, 0, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf."], "label": "Non-Prov", "citing": "D09-1138", "vector": [1, 1, 0, 0.0], "context": ["", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"supervis": 1, "classif": 1, "investig": 1, "automat": 1, "extens": 1, "verb": 1, "method": 1}, "vector_2": [6, 0.10682836397342808, 6, 2, 31, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense."], "label": "Non-Prov", "citing": "D09-1138", "vector": [3, 0, 0, 0.22237479499833035], "context": ["", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"supervis": 1, "classif": 1, "investig": 1, "automat": 1, "extens": 1, "verb": 1, "method": 1}, "vector_2": [6, 0.10682836397342808, 6, 2, 31, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["I stole..."], "label": "Non-Prov", "citing": "D09-1138", "vector": [0, 0, 0, 0.0], "context": ["", "Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"supervis": 1, "classif": 1, "investig": 1, "automat": 1, "extens": 1, "verb": 1, "method": 1}, "vector_2": [6, 0.10682836397342808, 6, 2, 31, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery."], "label": "Non-Prov", "citing": "D09-1138", "vector": [2, 0, 0, 0.19611613513818402], "context": ["", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"supervis": 1, "classif": 1, "work": 1, "inherit": 1, "verb": 1, "approach": 1, "spirit": 1, "present": 1}, "vector_2": [6, 0.2115453936865956, 6, 2, 31, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["However, their study used a small set of five features manually devised for a set of three particular classes."], "label": "Non-Prov", "citing": "D09-1138", "vector": [1, 0, 0, 0.0], "context": ["", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"supervis": 1, "classif": 1, "work": 1, "inherit": 1, "verb": 1, "approach": 1, "spirit": 1, "present": 1}, "vector_2": [6, 0.2115453936865956, 6, 2, 31, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure)."], "label": "Non-Prov", "citing": "D09-1138", "vector": [3, 0, 1, 0.0], "context": ["", "The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"supervis": 1, "classif": 1, "work": 1, "inherit": 1, "verb": 1, "approach": 1, "spirit": 1, "present": 1}, "vector_2": [6, 0.2115453936865956, 6, 2, 31, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["We ran 50 experiments using randomly selected sets of features of cardinality , where 5We also tried directly applying the mutual information (MI) measure used in decision-tree induction (Quinlan, 1986)."], "label": "Non-Prov", "citing": "N13-1118", "vector": [2, 0, 0, 0.07071067811865474], "context": ["", "Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"hierarch": 1, "wide": 1, "use": 1, "word": 1, "agg": 1, "cluster": 2, "method": 1}, "vector_2": [10, 0.24975210708973725, 4, 1, 0, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Table 1 above shows the number of verbs in each class at the end of this process."], "label": "Non-Prov", "citing": "N13-1118", "vector": [1, 0, 0, 0.0], "context": ["", "Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"hierarch": 1, "wide": 1, "use": 1, "word": 1, "agg": 1, "cluster": 2, "method": 1}, "vector_2": [10, 0.24975210708973725, 4, 1, 0, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["This is promising for our method, as it shows that the precise selection of a seed set of verbs is not crucial to the success of the semi-supervised approach."], "label": "Non-Prov", "citing": "N13-1118", "vector": [4, 0, 0, 0.09128709291752768], "context": ["", "Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"hierarch": 1, "wide": 1, "use": 1, "word": 1, "agg": 1, "cluster": 2, "method": 1}, "vector_2": [10, 0.24975210708973725, 4, 1, 0, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Optionally Intransitive: Run versus Change of State versus Object Drop."], "label": "Non-Prov", "citing": "W06-2910", "vector": [0, 0, 0, 0.0], "context": ["", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"altern": 1, "corpu": 1, "classif": 2, "eg": 1, "appli": 1, "induc": 1, "manual": 1, "automat": 1, "class": 1, "cluster": 1, "verb": 1, "data": 1, "method": 1, "resourceintens": 1}, "vector_2": [3, 0.04776887871853547, 6, 3, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["A plausible scenario is that researchers would have examples of verbs which they believe fall into different classes of interest, and they want to separate other verbs along the same lines."], "label": "Non-Prov", "citing": "W06-2910", "vector": [4, 0, 0, 0.06085806194501846], "context": ["", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"altern": 1, "corpu": 1, "classif": 2, "eg": 1, "appli": 1, "induc": 1, "manual": 1, "automat": 1, "class": 1, "cluster": 1, "verb": 1, "data": 1, "method": 1, "resourceintens": 1}, "vector_2": [3, 0.04776887871853547, 6, 3, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["*Jane of her money/the money from Jane."], "label": "Non-Prov", "citing": "W06-2910", "vector": [1, 0, 0, 0.0], "context": ["", "As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"altern": 1, "corpu": 1, "classif": 2, "eg": 1, "appli": 1, "induc": 1, "manual": 1, "automat": 1, "class": 1, "cluster": 1, "verb": 1, "data": 1, "method": 1, "resourceintens": 1}, "vector_2": [3, 0.04776887871853547, 6, 3, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.", "We find that the unsupervised method we tried cannot be consistently applied to our data."], "label": "Non-Prov", "citing": "W06-2910", "vector": [2, 0, 0, 0.05698028822981897], "context": ["", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"classif": 1, "featur": 1, "clear": 1, "syntaxsemant": 1, "largerscal": 1, "verb": 1, "interfac": 1, "salient": 1, "model": 1, "similar": 1, "class": 1}, "vector_2": [3, 0.07576516018306637, 3, 3, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["However, a general feature space means that most features will be irrelevant to any given verb discrimination task.", "In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to the curse of dimensionality?"], "label": "Non-Prov", "citing": "W06-2910", "vector": [5, 0, 0, 0.1982726388386439], "context": ["", "In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"classif": 1, "featur": 1, "clear": 1, "syntaxsemant": 1, "largerscal": 1, "verb": 1, "interfac": 1, "salient": 1, "model": 1, "similar": 1, "class": 1}, "vector_2": [3, 0.07576516018306637, 3, 3, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["The theoretical maximum is, of course, 1."], "label": "Non-Prov", "citing": "W06-2910", "vector": [2, 0, 0, 0.0], "context": ["", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003)", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"measur": 1, "evalu": 1, "appli": 1, "cf": 1, "accuraci": 1, "cluster": 3, "calcul": 1, "result": 1, "similar": 1}, "vector_2": [3, 0.6832451372997712, 2, 3, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002)."], "label": "Non-Prov", "citing": "W06-2910", "vector": [4, 0, 1, 0.0700140042014005], "context": ["", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003)", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"measur": 1, "evalu": 1, "appli": 1, "cf": 1, "accuraci": 1, "cluster": 3, "calcul": 1, "result": 1, "similar": 1}, "vector_2": [3, 0.6832451372997712, 2, 3, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["3.2 Verb Selection."], "label": "Non-Prov", "citing": "W06-2910", "vector": [0, 0, 0, 0.0], "context": ["", "For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003)", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"measur": 1, "evalu": 1, "appli": 1, "cf": 1, "accuraci": 1, "cluster": 3, "calcul": 1, "result": 1, "similar": 1}, "vector_2": [3, 0.6832451372997712, 2, 3, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["The results for these feature sets in clustering are given in the second subcolumn (Ling) under each of the , , and measures in Table 2."], "label": "Non-Prov", "citing": "E09-1072", "vector": [1, 0, 0, 0.0], "context": ["", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"corpu": 1, "classif": 1, "set": 1, "noun": 1, "distribut": 1, "work": 1, "classifi": 1, "lemma": 1, "verb": 1, "common": 1, "base": 1, "larger": 1, "morphosyntact": 1, "follow": 1, "line": 1, "consider": 1, "strategi": 1}, "vector_2": [6, 0.3843511927063819, 2, 1, 1, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions."], "label": "Non-Prov", "citing": "E09-1072", "vector": [2, 0, 0, 0.0], "context": ["", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"corpu": 1, "classif": 1, "set": 1, "noun": 1, "distribut": 1, "work": 1, "classifi": 1, "lemma": 1, "verb": 1, "common": 1, "base": 1, "larger": 1, "morphosyntact": 1, "follow": 1, "line": 1, "consider": 1, "strategi": 1}, "vector_2": [6, 0.3843511927063819, 2, 1, 1, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["5.3 Unsupervised Feature Selection."], "label": "Non-Prov", "citing": "E09-1072", "vector": [0, 0, 0, 0.0], "context": ["", "Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"corpu": 1, "classif": 1, "set": 1, "noun": 1, "distribut": 1, "work": 1, "classifi": 1, "lemma": 1, "verb": 1, "common": 1, "base": 1, "larger": 1, "morphosyntact": 1, "follow": 1, "line": 1, "consider": 1, "strategi": 1}, "vector_2": [6, 0.3843511927063819, 2, 1, 1, 0]}]