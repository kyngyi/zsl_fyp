[{"function": "Neut", "cited": "C08-1098", "provenance": ["Our tagger generates a predictor for each feature (such as base POS, number, gender etc.)", "Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc.", "for base POS), the tagger builds a separate decision tree for each value."], "label": "Prov", "citing": "E09-1079", "vector": [3, 0, 1, 0.0], "context": ["", "The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"decisiontre": 1, "featur": 1, "use": 1, "differ": 2, "predic": 1, "context": 1, "tion": 1, "attribut": 1}, "vector_2": [1, 0.5007715133531158, 1, 1, 1, 1]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "P10-1068", "vector": [5, 0, 0, 0.25699702648129413], "context": ["", "With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"current": 1, "morph": 4, "five": 1, "morphisto": 1, "rftagger": 1, "tiger": 1, "annot": 3, "morpholog": 1, "german": 1, "stt": 1, "part": 1, "speech": 1, "morphosyntact": 1, "respect": 1, "model": 1, "po": 4, "avail": 1}, "vector_2": [2, 0.2791806235506313, 4, 3, 1, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets"], "label": "Prov", "citing": "P10-1068", "vector": [1, 0, 0, 0.0], "context": ["", "Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"analysi": 1, "given": 1, "rftagger": 1, "descript": 1, "correspond": 1, "transform": 1, "term": 1, "model": 1, "olia": 1, "analog": 1, "refer": 1}, "vector_2": [2, 0.4341342437516104, 1, 3, 1, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.", "We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "P10-1068", "vector": [3, 0, 0, 0.0], "context": ["", "(iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008)", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"analysi": 1, "perform": 1, "rftagger": 1, "morpholog": 1, "part": 1, "speech": 1, "iii": 1}, "vector_2": [2, 0.46850038649832515, 1, 3, 1, 0]}, {"function": "Neut", "cited": "C08-1098", "provenance": ["We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees."], "label": "Prov", "citing": "W12-3141", "vector": [6, 0, 0, 0.1777046633277277], "context": ["", "All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).", ""], "marker": "Schmid and Laws, 2008", "vector_1": {"finegrain": 1, "use": 1, "preprocess": 1, "german": 1, "rftagger": 1, "corpora": 1, "obtain": 1, "po": 1, "postag": 1, "also": 1, "treetagg": 1, "need": 1, "label": 1, "parallel": 1, "addit": 1}, "vector_2": [4, 0.5, 2, 2, 2, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (6)."], "label": "Prov", "citing": "P12-2002", "vector": [5, 0, 1, 0.2760262237369417], "context": ["", "One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", ""], "marker": "Green and Manning, 2010", "vector_1": {"let": 1, "parser": 1, "argu": 1, "one": 2, "jointli": 1, "prior": 1, "pars": 1, "either": 1, "pick": 1, "path": 1, "segment": 2, "decod": 1, "select": 1, "recent": 1}, "vector_2": [2, 0.13090780642882374, 4, 3, 0, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["Better Arabic Parsing: Baselines, Evaluations, and Analysis"], "label": "Prov", "citing": "P12-2002", "vector": [2, 0, 0, 0.10540925533894598], "context": ["", "2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010).", ""], "marker": "2010", "vector_1": {"set": 1, "word": 1, "provid": 1, "similar": 1, "phenomena": 1, "green": 1, "arab": 1, "may": 1, "exampl": 1, "goldberg": 1, "tsarfati": 1, "found": 1, "man": 1, "analys": 1, "complet": 1}, "vector_2": [2, 0.3632638643588838, 2, 0, 0, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.", "We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton."], "label": "Prov", "citing": "P12-2002", "vector": [8, 0, 0, 0.13043478260869568], "context": ["", "In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).", ""], "marker": "Green and Manning, 2010", "vector_1": {"morpholog": 1, "decid": 1, "practic": 1, "jointli": 1, "may": 1, "local": 1, "lattic": 1, "compon": 1, "base": 1, "done": 1, "statist": 1, "context": 1, "pick": 1, "pars": 1, "path": 1, "segment": 1, "correct": 2, "requir": 1, "linear": 1}, "vector_2": [2, 0.3853055457435535, 7, 3, 0, 0]}, {"function": "Pos", "cited": "C10-1045", "provenance": ["To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply Arabic) because of the unusual opportunity it presents for comparison to English parsing results."], "label": "Prov", "citing": "W13-4916", "vector": [5, 0, 0, 0.16903085094570328], "context": ["", "We also express our gratitude to the treebank providers for each language: Arabic (Maamouri et al., 2004; Habash and Roth, 2009; Habash et al., 2009; Green and Manning, 2010)", ""], "marker": "Green and Manning, 2010", "vector_1": {"provid": 1, "treebank": 1, "express": 1, "gratitud": 1, "arab": 1, "also": 1, "languag": 1}, "vector_2": [3, 0.9921679399114078, 4, 1, 0, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-.", "able at http://nlp.stanford.edu/projects/arabic.shtml."], "label": "Prov", "citing": "W13-4917", "vector": [4, 0, 0, 0.06537204504606135], "context": ["", "The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)", ""], "marker": "2010", "vector_1": {"compat": 1, "constitu": 1, "art": 1, "stanford": 1, "provid": 1, "treebank": 1, "step": 1, "structur": 1, "stay": 1, "arab": 1, "state": 1, "set": 1, "green": 1, "preprocess": 1, "phrase": 1, "data": 1, "order": 1, "man": 1}, "vector_2": [3, 0.3083243299759523, 1, 0, 12, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["At the phrasal level, we remove all function tags and traces."], "label": "Prov", "citing": "W13-4917", "vector": [7, 0, 3, 0.5163977794943223], "context": ["", "We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.", ""], "marker": "2010", "vector_1": {"function": 1, "trace": 1, "remov": 1, "keep": 1, "unlik": 1, "tag": 1, "green": 1, "final": 1, "man": 1}, "vector_2": [3, 0.31348238246262156, 1, 0, 12, 0]}, {"function": "Neut", "cited": "C10-1045", "provenance": ["The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993)."], "label": "Prov", "citing": "W13-4917", "vector": [5, 1, 2, 0.28867513459481287], "context": ["", "Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)", ""], "marker": "Green and Manning, 2010", "vector_1": {"ldc": 1, "catib": 1, "set": 2, "columbia": 1, "deriv": 1, "stanford": 1, "treebank": 3, "tree": 1, "two": 1, "arab": 3, "patb": 2, "version": 1, "contain": 1, "penn": 1, "data": 2, "bank": 1, "depend": 1}, "vector_2": [3, 0.26343533265953367, 3, 4, 5, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints."], "label": "Prov", "citing": "D11-1059", "vector": [0, 0, 0, 0.0], "context": ["", "However, despite a recent proliferation of syntactic class induction systems (Biemann, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Ravi and Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)", ""], "marker": "Lee et al., 2010", "vector_1": {"despit": 1, "syntact": 1, "howev": 1, "system": 1, "prolifer": 1, "induct": 1, "class": 1, "recent": 1}, "vector_2": [1, 0.2444591684611562, 6, 8, 6, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag."], "label": "Prov", "citing": "D11-1059", "vector": [11, 0, 3, 0.21320071635561041], "context": ["", "This property is not strictly true of linguistic data, but is a good approximation: as Lee et al (2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.", ""], "marker": "2010", "vector_1": {"upper": 1, "approxim": 1, "al": 1, "et": 1, "languag": 1, "true": 1, "accuraci": 1, "note": 1, "speech": 1, "strictli": 1, "type": 1, "good": 1, "lee": 1, "bound": 1, "part": 1, "data": 1, "word": 1, "properti": 1, "yield": 1, "linguist": 1, "assign": 1, "frequent": 1}, "vector_2": [1, 0.07152559218154712, 1, 0, 1, 0]}, {"function": "Pos", "cited": "D10-1083", "provenance": ["Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints."], "label": "Prov", "citing": "D11-1059", "vector": [3, 0, 0, 0.14433756729740646], "context": ["", "More recently, Lee et al (2010) presented a new type-based model, and also reported very good results.", ""], "marker": "2010", "vector_1": {"good": 1, "lee": 1, "typebas": 1, "al": 1, "also": 1, "result": 1, "new": 1, "report": 1, "et": 1, "model": 1, "present": 1, "recent": 1}, "vector_2": [1, 0.09074043399039258, 1, 0, 1, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["We consider the unsupervised POS induction problem without the use of a tagging dictionary."], "label": "Prov", "citing": "D11-1059", "vector": [7, 0, 0, 0.3823595564509362], "context": ["", "Sequence models are by far the most common method of supervised part- of-speech tagging, and have also been widely used for unsupervised part-of-speech tagging both with and without a dictionary (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Ravi and Knight, 2009; Lee et al., 2010).", ""], "marker": "Lee et al., 2010", "vector_1": {"supervis": 1, "use": 1, "dictionari": 1, "wide": 1, "far": 1, "sequenc": 1, "unsupervis": 1, "tag": 2, "without": 1, "also": 1, "part": 1, "common": 1, "partofspeech": 1, "ofspeech": 1, "model": 1, "method": 1}, "vector_2": [1, 0.11704488984594998, 6, 8, 6, 0]}, {"function": "Pos", "cited": "D10-1083", "provenance": ["We consider the unsupervised POS induction problem without the use of a tagging dictionary."], "label": "Prov", "citing": "D11-1059", "vector": [6, 0, 3, 0.13608276348795434], "context": ["", "As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.", ""], "marker": "Lee et al., 2010", "vector_1": {"even": 1, "model": 1, "use": 1, "previou": 1, "consider": 1, "compar": 2, "perform": 1, "align": 1, "work": 1, "yield": 1, "without": 1, "restrict": 1, "token": 1, "base": 1, "result": 1, "oneclasspertyp": 1, "morpholog": 1, "boost": 1, "find": 1, "featur": 1, "stateoftheart": 1}, "vector_2": [1, 0.15872121914858373, 1, 8, 6, 0]}, {"function": "Pos", "cited": "D10-1083", "provenance": ["For all languages we do not make use of a tagging dictionary."], "label": "Prov", "citing": "D11-1059", "vector": [2, 0, 0, 0.0], "context": ["", "2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al (2010).", ""], "marker": "2010", "vector_1": {"term": 1, "featur": 1, "word": 1, "lee": 1, "al": 1, "likelihood": 1, "could": 1, "independ": 1, "nj": 1, "one": 1, "token": 1, "approach": 1, "type": 1, "taken": 1, "et": 1, "j": 1, "assum": 1, "approxim": 1}, "vector_2": [1, 0.39400364419413614, 1, 0, 1, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts."], "label": "Prov", "citing": "D11-1059", "vector": [0, 0, 0, 0.0], "context": ["", "Following Lee et al (2010) we used only the training sections for each language.", ""], "marker": "2010", "vector_1": {"use": 1, "lee": 1, "section": 1, "al": 1, "train": 1, "et": 1, "follow": 1, "languag": 1}, "vector_2": [1, 0.6070896140467119, 1, 0, 1, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank."], "label": "Prov", "citing": "D12-1086", "vector": [7, 0, 2, 0.16724840200141816], "context": ["", "Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)", ""], "marker": "Lee et al., 2010", "vector_1": {"given": 1, "word": 1, "data": 1, "label": 1, "tag": 1, "speech": 1, "human": 1, "close": 1, "part": 1, "occurr": 1, "frequent": 1}, "vector_2": [2, 0.30794417776647487, 1, 2, 2, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["We consider the unsupervised POS induction problem without the use of a tagging dictionary."], "label": "Prov", "citing": "D12-1125", "vector": [2, 0, 1, 0.3333333333333333], "context": ["", "vised POS induction algorithm (Lee et al., 2010)", ""], "marker": "Lee et al., 2010", "vector_1": {"induct": 1, "po": 1, "algorithm": 1, "vise": 1}, "vector_2": [2, 0.7261107044315558, 1, 1, 6, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["We consider the unsupervised POS induction problem without the use of a tagging dictionary."], "label": "Prov", "citing": "D12-1127", "vector": [5, 0, 1, 0.3042903097250923], "context": ["", "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; Graca et al., 2011; Lee et al., 2010).", ""], "marker": "Lee et al., 2010", "vector_1": {"despit": 1, "still": 1, "system": 1, "possibl": 1, "far": 1, "supervis": 1, "unsupervis": 2, "offer": 1, "annot": 1, "accuraci": 1, "suitabl": 1, "behind": 1, "tagger": 2, "avoid": 1, "applic": 1, "progress": 1, "costli": 1, "fall": 1, "recent": 1, "po": 2, "induct": 1}, "vector_2": [2, 0.0768143261074458, 3, 1, 0, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon."], "label": "Prov", "citing": "N12-1045", "vector": [2, 0, 0, 0.0890870806374748], "context": ["", "Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)", ""], "marker": "Lee et al., 2010", "vector_1": {"use": 1, "featur": 1, "make": 1, "system": 1, "po": 1, "morpholog": 1, "unsupervis": 1, "sever": 1, "induct": 1}, "vector_2": [2, 0.08225726141078839, 5, 6, 2, 0]}, {"function": "Neut", "cited": "D10-1083", "provenance": ["The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.", "Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag."], "label": "Prov", "citing": "W11-0301", "vector": [11, 0, 1, 0.37670528747840887], "context": ["", "Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).", ""], "marker": "Lee et al., 2010", "vector_1": {"set": 2, "word": 4, "gener": 3, "earlier": 1, "tag": 2, "w": 3, "condit": 1, "type": 2, "refer": 1}, "vector_2": [1, 0.5041514085783692, 1, 1, 6, 0]}, {"function": "Pos", "cited": "E09-2008", "provenance": ["Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples."], "label": "Prov", "citing": "W12-1003", "vector": [8, 0, 0, 0.11846977555181847], "context": ["", "The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.", ""], "marker": "Hulden, 2009", "vector_1": {"softwar": 1, "use": 1, "foma": 1, "bad": 1, "written": 1, "counter": 1, "vers": 1, "syllabl": 1, "figur": 1, "web": 1, "applic": 1, "found": 1, "implement": 2, "homepag": 1}, "vector_2": [3, 0.5317945582390706, 2, 1, 1, 0]}, {"function": "Neut", "cited": "E09-2008", "provenance": ["Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples."], "label": "Prov", "citing": "W12-6211", "vector": [5, 0, 1, 0.07254762501100116], "context": ["", "Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.", ""], "marker": "Hulden, 2009", "vector_1": {"fs": 1, "transduc": 1, "foma": 1, "toolkit": 1, "avail": 1, "automata": 1, "pars": 1, "build": 1, "allow": 1, "freeli": 1}, "vector_2": [3, 0.4502083333333333, 1, 1, 0, 0]}, {"function": "Pos", "cited": "E09-2008", "provenance": ["Foma is largely compatible with the Xerox/PARC finite-state toolkit."], "label": "Prov", "citing": "W12-6212", "vector": [5, 0, 1, 0.33968311024337877], "context": ["", "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).", ""], "marker": "Hulden, 2009", "vector_1": {"use": 1, "chain": 1, "transfer": 1, "modul": 1, "foma": 1, "toolkit": 1, "rule": 1, "finitest": 1, "verb": 1, "replac": 1, "implement": 1, "order": 1, "seri": 1}, "vector_2": [3, 0.2879749443666035, 2, 3, 0, 0]}, {"function": "Pos", "cited": "E09-2008", "provenance": ["Foma is largely compatible with the Xerox/PARC finite-state toolkit."], "label": "Prov", "citing": "W12-6212", "vector": [4, 0, 1, 0.2461829819586655], "context": ["", "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).", ""], "marker": "Hulden, 2009", "vector_1": {"origin": 1, "foma": 1, "reimplement": 1, "work": 1, "toolkit": 1, "rule": 1, "written": 1, "present": 1, "xfst": 1, "expand": 1}, "vector_2": [3, 0.9536800461551141, 1, 3, 0, 0]}, {"function": "Weak", "cited": "N04-1038", "provenance": ["We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.", "BABAR employs information extraction techniques to represent and learn role relationships.", "Each pattern represents the role that a noun phrase plays in the surrounding context."], "label": "Prov", "citing": "N13-1110", "vector": [10, 0, 3, 0.23378595005975902], "context": ["", "Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary.", ""], "marker": "2004", "vector_1": {"corpu": 1, "domain": 1, "show": 1, "inspir": 1, "argument": 1, "ie": 1, "event": 1, "use": 3, "anoth": 1, "two": 1, "corefer": 2, "role": 2, "build": 1, "np": 1, "play": 1, "sourc": 1, "resolut": 1, "decid": 1, "bean": 1, "verb": 1, "dictionari": 1, "relev": 1, "extract": 1, "howev": 1, "work": 1, "contextu": 1, "aim": 1, "riloff": 1, "pattern": 1, "small": 1}, "vector_2": [9, 0.8737076559954788, 1, 0, 0, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.", "2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions."], "label": "Prov", "citing": "P08-1090", "vector": [5, 0, 2, 0.14712247158412492], "context": ["", "Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution.", ""], "marker": "2004", "vector_1": {"use": 1, "learn": 1, "network": 2, "text": 1, "casefram": 1, "resolut": 2, "kind": 1, "contextu": 1, "two": 1, "bean": 2, "role": 1, "topicspecif": 1, "riloff": 2, "knoweldg": 1, "problem": 1, "appli": 1, "anaphora": 2, "propos": 1}, "vector_2": [4, 0.1542071628278525, 1, 0, 0, 0]}, {"function": "CoCo", "cited": "N04-1038", "provenance": ["Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes."], "label": "Prov", "citing": "P09-1068", "vector": [2, 0, 0, 0.09999999999999998], "context": ["", "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)", ""], "marker": "Bean and Riloff, 2004", "vector_1": {"set": 1, "extend": 1, "script": 1, "casefram": 1, "work": 1, "repres": 1, "unlik": 1, "paper": 1, "event": 1, "situationspecif": 1}, "vector_2": [5, 0.10048394480413315, 1, 1, 0, 0]}, {"function": "Neut", "cited": "N04-1038", "provenance": ["Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision."], "label": "Prov", "citing": "P10-1142", "vector": [5, 0, 0, 0.05923488777590923], "context": ["", "The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.", ""], "marker": "2004", "vector_1": {"use": 1, "identifi": 1, "riloff": 1, "neg": 1, "bean": 1, "rule": 1, "np": 1, "dempstershaf": 1, "score": 1, "combin": 1, "decis": 1, "partit": 2, "posit": 1, "pairwis": 1, "kehler": 1, "probabl": 1}, "vector_2": [6, 0.32327575305394673, 3, 0, 25, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.", "For example, the word shot in It was a nice shot.", "should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing."], "label": "Prov", "citing": "P12-1079", "vector": [4, 0, 0, 0.1318760946791574], "context": ["", "To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"machin": 1, "lexicon": 1, "variou": 1, "smt": 1, "qualiti": 1, "research": 1, "topic": 1, "inform": 1, "exploit": 1, "statist": 1, "improv": 1, "model": 1, "propos": 1, "topicspecif": 1, "translat": 3}, "vector_2": [6, 0.04078056014290048, 3, 2, 6, 0]}, {"function": "Neut", "cited": "P06-2124", "provenance": ["We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT."], "label": "Prov", "citing": "P12-1079", "vector": [1, 0, 0, 0.0], "context": ["", "Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).", ""], "marker": "Zhao and Xing, 2006", "vector_1": {"topic": 1, "sentenc": 1, "translat": 1, "consist": 1}, "vector_2": [6, 0.19015200026962353, 3, 2, 6, 0]}, {"function": "Neut", "cited": "W04-0213", "provenance": ["This paper, however, provides a comprehensive overview of the data collection effort and its current state.", "At present, the Potsdam Commentary Corpus (henceforth PCC for short) consists of 170 commentaries from Markische Allgemeine Zeitung, a German regional daily.", "The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus."], "label": "Prov", "citing": "W06-2709", "vector": [13, 0, 6, 0.44644187172305666], "context": ["", "Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.", ""], "marker": "Stede, 2004", "vector_1": {"corpu": 1, "discours": 2, "morphosyn": 1, "consist": 1, "inform": 1, "accord": 1, "commentari": 2, "newspap": 1, "tax": 1, "annot": 1, "structur": 3, "corefer": 1, "rhetor": 1, "potsdam": 1, "pcc": 1, "studi": 1, "theori": 1}, "vector_2": [2, 0.8377781702578594, 1, 1, 3, 0]}, {"function": "Neut", "cited": "W04-0213", "provenance": ["A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.", "Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization)."], "label": "Prov", "citing": "W07-1530", "vector": [6, 0, 0, 0.045175395145262566], "context": ["", "The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12 15 sentences), with 33.000 tokens in total.", ""], "marker": "Stede, 2004", "vector_1": {"purpos": 1, "consist": 1, "request": 1, "sentenc": 1, "upon": 1, "research": 1, "avail": 1, "token": 1, "short": 1, "rel": 1, "commentari": 1, "total": 1, "pcc": 1, "subcorpu": 1}, "vector_2": [3, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "W04-0213", "provenance": ["The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.", "At present, the Potsdam Commentary Corpus (henceforth PCC for short) consists of 170 commentaries from Markische Allgemeine Zeitung, a German regional daily."], "label": "Prov", "citing": "W07-1530", "vector": [8, 0, 3, 0.3038218101251], "context": ["", "(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.", ""], "marker": "Manfred Stede, Potsdam", "vector_1": {"corpu": 1, "manfr": 1, "stede": 1, "ongo": 1, "commentari": 1, "began": 1, "construct": 1, "potsdam": 2, "pcc": 1, "still": 1}, "vector_2": [8, 0.5, 0, 2, 2, 0]}, {"function": "Neut", "cited": "W04-0213", "provenance": ["The Potsdam Commentary Corpus A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure."], "label": "Prov", "citing": "W11-0401", "vector": [5, 0, 1, 0.10369516947304255], "context": ["", "This is due to the fact that at present corpora annotated with RST relations are available only for these languages (for English: Carlson et al., 2002, Taboada and Renkema, 2008; for German: Stede, 2004;", ""], "marker": "Stede, 2004", "vector_1": {"relat": 1, "german": 1, "corpora": 1, "due": 1, "annot": 1, "avail": 1, "fact": 1, "english": 1, "rst": 1, "languag": 1, "present": 1}, "vector_2": [7, 0.06254261451682885, 3, 2, 0, 0]}, {"function": "Neut", "cited": "W04-0213", "provenance": ["At present, the Potsdam Commentary Corpus (henceforth PCC for short) consists of 170 commentaries from Markische Allgemeine Zeitung, a German regional daily.", "The corpus has been annotated with six different types of information, which are characterized in the following subsections.", "Not all the layers have been produced for all the texts yet.", "For the core portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright  but see Sections 3.2 and 3.3 below."], "label": "Prov", "citing": "W11-0401", "vector": [26, 0, 10, 0.28607725128818], "context": ["", "Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003). This corpus includes 173 texts on politics from the online newspaper Mrkische Allgemeine Zeitung. It contains 32,962 words and 2,195 sentences. It is annotated with several data: morphology, syntax, rhetorical structure, connectors, correference and informative structure. This corpus has several advantages: it is annotated at different levels (the annotation of connectors is especially interesting); all the texts were annotated by two people (with a previous RST training phase); it is free for research purposes, and there is a tool for searching over the corpus (although it is not available online). The disadvantages are: the genre and domain of all the texts are the same, the methodology of annotation was quite intuitive (without a manual or specific criteria) and the inter-annotator agreement is not given.", ""], "marker": "Stede, 2004", "vector_1": {"corpu": 5, "domain": 1, "advantag": 1, "german": 1, "text": 3, "newspap": 1, "syntax": 1, "phase": 1, "correfer": 1, "allgemein": 1, "zeitung": 1, "disadvantag": 1, "onlin": 2, "mrkisch": 1, "sever": 2, "connector": 2, "differ": 1, "inform": 1, "anoth": 1, "two": 1, "research": 1, "morpholog": 1, "rhetor": 1, "potsdam": 1, "interest": 1, "peopl": 1, "wellknown": 1, "methodolog": 1, "polit": 1, "criteria": 1, "agreement": 1, "especi": 1, "sentenc": 1, "tool": 1, "previou": 1, "quit": 1, "free": 1, "intuit": 1, "train": 1, "although": 1, "rst": 1, "given": 1, "data": 1, "avail": 1, "manual": 1, "search": 1, "word": 1, "specif": 1, "level": 1, "genr": 1, "annot": 5, "structur": 2, "commentari": 1, "without": 1, "includ": 1, "contain": 1, "purpos": 1, "interannot": 1}, "vector_2": [7, 0.19140891340730182, 2, 2, 0, 0]}, {"function": "Neut", "cited": "W08-2222", "provenance": ["It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic."], "label": "Prov", "citing": "D13-1161", "vector": [3, 0, 1, 0.18257418583505539], "context": ["", "Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains.", ""], "marker": "Bos, 2008", "vector_1": {"represent": 1, "design": 1, "natur": 1, "gener": 1, "previou": 1, "system": 1, "adapt": 1, "understand": 1, "build": 1, "domain": 1, "handengin": 1, "mani": 1, "differ": 1, "languag": 1, "mean": 1}, "vector_2": [5, 0.25901746368532724, 3, 1, 0, 0]}, {"function": "Pos", "cited": "W08-2222", "provenance": ["The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.", "The numerical and date expressions got correct representations."], "label": "Prov", "citing": "P13-1138", "vector": [4, 0, 0, 0.22019275302527214], "context": ["", "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.", ""], "marker": "Bos, 2008", "vector_1": {"use": 1, "semant": 2, "predic": 1, "boxer": 1, "analyz": 1, "date": 1, "extract": 1, "event": 1}, "vector_2": [5, 0.26716586197924197, 1, 1, 0, 0]}, {"function": "Pos", "cited": "W08-2222", "provenance": ["Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.", "Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called boxes because of the way they are graphically displayed) for English sentences and texts."], "label": "Prov", "citing": "S13-1002", "vector": [7, 1, 3, 0.27498597046143514], "context": ["", "Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).", ""], "marker": "Bos, 2008", "vector_1": {"analysi": 1, "softwar": 1, "semant": 2, "form": 1, "represent": 1, "widecoverag": 2, "logicbas": 1, "use": 1, "structur": 1, "packag": 1, "boxer": 1, "logic": 1, "discours": 1, "produc": 1}, "vector_2": [5, 0.17477333986767948, 2, 1, 5, 0]}, {"function": "Neut", "cited": "W08-2222", "provenance": ["Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called boxes because of the way they are graphically displayed) for English sentences and texts.", "It is distributed with the C&C tools for natural language processing (Curran et al., 2007), which are hosted on this site: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer"], "label": "Prov", "citing": "W10-1750", "vector": [8, 1, 2, 0.20683507599800766], "context": ["", "For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).", ""], "marker": "Bos, 2008", "vector_1": {"analysi": 1, "specif": 1, "cc": 1, "text": 1, "metric": 1, "reli": 1, "compon": 1, "discurs": 1, "boxer": 1, "dr": 1, "tool": 1}, "vector_2": [2, 0.36897317968063664, 2, 1, 2, 0]}, {"function": "Neut", "cited": "W08-2222", "provenance": ["Boxer is an open-domain tool for computing and reasoning with semantic representations."], "label": "Prov", "citing": "W13-2101", "vector": [4, 0, 1, 0.4073065399812784], "context": ["", "Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.", ""], "marker": "Bos, 2008", "vector_1": {"represent": 2, "semant": 3, "revers": 1, "consid": 1, "would": 1, "process": 1, "natur": 1, "nowaday": 1, "parser": 1, "abl": 1, "surfac": 1, "produc": 1, "compon": 1, "see": 1, "realiz": 1, "opendomain": 1, "gener": 1, "input": 1, "sinc": 1, "formal": 1}, "vector_2": [5, 0.03624813153961136, 2, 1, 4, 0]}, {"function": "Pos", "cited": "W95-0104", "provenance": ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers."], "label": "Prov", "citing": "A97-1025", "vector": [6, 0, 3, 0.38138503569823695], "context": ["", "A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).", ""], "marker": "Golding, 1995", "vector_1": {"tri": 1, "knowledgebas": 1, "featurebas": 1, "list": 1, "number": 1, "classifi": 1, "includ": 1, "bayesian": 1, "approach": 1, "method": 1, "decis": 1}, "vector_2": [2, 0.1313943812656021, 4, 1, 6, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["Table 1 shows the performance of the baseline method for 18 confusion sets."], "label": "Prov", "citing": "A97-1025", "vector": [3, 1, 1, 0.24999999999999994], "context": ["", "The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).", ""], "marker": "1995", "vector_1": {"set": 1, "describ": 1, "gold": 1, "section": 1, "base": 1, "result": 1, "confus": 1, "select": 1}, "vector_2": [2, 0.5613186987359828, 0, 0, 0, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["A method is presented for doing this, based on Bayesian classifiers.", "\\Ve then apply each of the two component methods mentioned above context words and collocations."], "label": "Prov", "citing": "D07-1012", "vector": [5, 0, 1, 0.18257418583505533], "context": ["", "Golding (1995) builds a classifier based on a rich set of context features.", ""], "marker": "1995", "vector_1": {"set": 1, "context": 1, "gold": 1, "classifi": 1, "base": 1, "build": 1, "rich": 1, "featur": 1}, "vector_2": [12, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers."], "label": "Prov", "citing": "D11-1119", "vector": [6, 0, 2, 0.29019050004400465], "context": ["", "A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)", ""], "marker": "Golding, 1995", "vector_1": {"preposit": 1, "machinelearn": 1, "winnowbas": 1, "list": 1, "spell": 1, "decis": 1, "classifi": 1, "method": 1, "field": 1, "learn": 1, "error": 1, "varieti": 1, "bayesian": 1, "articl": 1, "correct": 2, "propos": 1}, "vector_2": [16, 0.18653709409812752, 4, 2, 3, 0]}, {"function": "Pos", "cited": "W95-0104", "provenance": ["A method is presented for doing this, based on Bayesian classifiers.", "The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction."], "label": "Prov", "citing": "E99-1024", "vector": [7, 0, 1, 0.1720618004029213], "context": ["", "Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Gold ing and Schabes, 1996).", ""], "marker": "Golding, 1995", "vector_1": {"case": 1, "detect": 1, "equival": 1, "appli": 1, "spell": 1, "contextsensit": 1, "succeed": 1, "take": 1, "error": 1, "homophon": 1, "problem": 2, "method": 1, "statist": 1}, "vector_2": [4, 0.17095605757315793, 2, 1, 1, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction."], "label": "Prov", "citing": "P98-2138", "vector": [4, 0, 1, 0.18490006540840973], "context": ["", "For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).", ""], "marker": "Golding, 1995", "vector_1": {"spell": 1, "cope": 1, "error": 1, "number": 1, "correct": 1, "realword": 1, "english": 1, "method": 1, "propos": 1}, "vector_2": [3, 0.5793877254920926, 4, 2, 4, 0]}, {"function": "Pos", "cited": "W95-0104", "provenance": ["\\Ve then apply each of the two component methods mentioned above context words and collocations."], "label": "Prov", "citing": "P98-2138", "vector": [6, 0, 3, 0.4216370213557839], "context": ["", "Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.", ""], "marker": "Golding, 1995", "vector_1": {"tri": 1, "colloc": 1, "previou": 1, "word": 1, "work": 1, "two": 1, "featur": 1, "context": 1, "follow": 1, "type": 1}, "vector_2": [3, 0.7888958746433234, 2, 2, 4, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["A method is presented for doing this, based on Bayesian classifiers."], "label": "Prov", "citing": "W01-0502", "vector": [3, 0, 1, 0.36927447293799825], "context": ["", "A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)", ""], "marker": "Golding, 1995", "vector_1": {"partial": 1, "consist": 1, "hybrid": 1, "list": 2, "classifi": 1, "decis": 1, "bayesian": 2}, "vector_2": [6, 0.04744802135160961, 3, 1, 4, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction.", "\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers."], "label": "Prov", "citing": "W02-1005", "vector": [7, 0, 0, 0.1734944795898721], "context": ["", "Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)", ""], "marker": "Golding, 1995", "vector_1": {"machin": 1, "previou": 1, "work": 1, "list": 1, "cssc": 1, "learn": 1, "includ": 1, "address": 1, "bayesian": 1, "problem": 1, "perspect": 1, "model": 1, "decis": 1}, "vector_2": [7, 0.06659083858959777, 1, 2, 7, 0]}, {"function": "Neut", "cited": "W95-0104", "provenance": ["The second tests for collocations - patterns of words and part-of-speech tags around the target word.", "The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction."], "label": "Prov", "citing": "W02-1005", "vector": [10, 0, 1, 0.1430397079704303], "context": ["", "Previous work in WSD and CSSC (Golding, 1995; Bruce et al., 1996; Yarowsky, 1996; Golding and Roth, 1996; Pedersen, 1998) has found diverse feature types to be useful, including inflected words, lemmas and part-of-speech (POS) in a variety of collocational and syntactic relationships, including local bigrams and trigrams, Figure 2: Example context for the spelling confusion set {piece,peace} and extracted features", ""], "marker": "Golding, 1995", "vector_1": {"featur": 2, "bigram": 1, "cssc": 1, "trigram": 1, "set": 1, "piecepeac": 1, "extract": 1, "use": 1, "lemma": 1, "figur": 1, "includ": 2, "varieti": 1, "local": 1, "po": 1, "spell": 1, "syntact": 1, "relationship": 1, "inflect": 1, "previou": 1, "colloc": 1, "partofspeech": 1, "word": 1, "wsd": 1, "type": 1, "work": 1, "divers": 1, "exampl": 1, "context": 1, "found": 1, "confus": 1}, "vector_2": [7, 0.1558266983765898, 5, 2, 7, 0]}, {"function": "Pos", "cited": "W95-0104", "provenance": ["The work reported here was applied not to accent restoration, but to a related lexical disam biguation task: context-sensitive spelling correction.", "The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera."], "label": "Prov", "citing": "W02-1005", "vector": [4, 0, 2, 0.13900960937138318], "context": ["", "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)", ""], "marker": "1995", "vector_1": {"corpu": 1, "brown": 1, "use": 1, "ident": 1, "gold": 1, "system": 1, "cssc": 1, "test": 1, "data": 1}, "vector_2": [7, 0.7198152552304139, 1, 0, 1, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["We apply a beam search concept as in speech recognition."], "label": "Prov", "citing": "J03-1005", "vector": [4, 0, 1, 0.20412414523193154], "context": ["", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", ""], "marker": "2000", "vector_1": {"search": 1, "ibm": 1, "tillmann": 1, "preliminari": 1, "work": 1, "dpbase": 1, "publish": 1, "articl": 1, "beam": 1, "version": 1, "decod": 1, "translat": 1, "ney": 1, "model": 1, "present": 2}, "vector_2": [3, 0.15982922481236303, 1, 0, 5, 1]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is sucient to consider only the best 50 words."], "label": "Prov", "citing": "P01-1027", "vector": [6, 0, 2, 0.08606629658238703], "context": ["", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"sort": 1, "use": 2, "entropi": 1, "describ": 1, "hypothesi": 2, "provid": 1, "top": 1, "list": 1, "system": 1, "maximum": 1, "accord": 1, "score": 1, "translat": 1, "new": 1, "model": 1, "rescor": 1}, "vector_2": [1, 0.8941605839416058, 1, 1, 4, 0]}, {"function": "Pos", "cited": "C04-1089", "provenance": ["We employ the language modeling approach (Ng, 2000; Ponte and Croft, 1998) for corresponding to that document translation of c .", "C (e* ) is the this retrieval problem."], "label": "Prov", "citing": "D12-1003", "vector": [1, 1, 0, 0.058321184351980436], "context": ["", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", ""], "marker": "2004", "vector_1": {"measur": 1, "use": 2, "ratio": 1, "variou": 1, "etc": 1, "heterogen": 1, "model": 1, "mutual": 1, "pmi": 1, "tfidf": 1, "inform": 1, "correl": 1, "pointwis": 1, "context": 2, "shao": 1, "repres": 1, "ng": 1, "loglikelihood": 1, "languag": 1}, "vector_2": [8, 0.15703845948685122, 6, 0, 0, 0]}, {"function": "CoCo", "cited": "C04-1089", "provenance": ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information."], "label": "Prov", "citing": "D12-1003", "vector": [6, 0, 2, 0.16269784336399212], "context": ["", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (Dejean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"multilingu": 1, "concept": 1, "comput": 1, "transliter": 1, "variou": 1, "align": 1, "gener": 1, "obtain": 1, "clue": 1, "inform": 2, "thesauru": 1, "consid": 1, "cooccurr": 1, "model": 1, "document": 1, "similar": 1, "class": 1}, "vector_2": [8, 0.2056862431322345, 3, 2, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information."], "label": "Prov", "citing": "P06-1011", "vector": [6, 0, 2, 0.3233808333817773], "context": ["", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"involv": 1, "corpora": 1, "word": 1, "compar": 1, "work": 1, "focus": 1, "much": 1, "translat": 1, "extract": 1}, "vector_2": [2, 0.8435580204778157, 7, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query."], "label": "Prov", "citing": "P13-1062", "vector": [9, 0, 1, 0.17641870800832102], "context": ["", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al (2011) and Shao and Ng (2004).", ""], "marker": "2004", "vector_1": {"subset": 1, "vb": 1, "edg": 1, "evalu": 1, "process": 1, "al": 1, "eb": 1, "bipartit": 3, "gb": 1, "year": 1, "et": 1, "ng": 1, "find": 1, "use": 1, "weight": 1, "defin": 1, "graph": 1, "ts": 1, "publish": 1, "articl": 1, "also": 1, "shao": 1, "match": 2, "chines": 1, "xinhua": 1, "entir": 1, "kim": 1, "news": 3, "c": 2, "e": 2, "maximum": 2, "sj": 1, "si": 1, "english": 1, "tion": 1}, "vector_2": [9, 0.7494066949787659, 3, 0, 2, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information."], "label": "Prov", "citing": "W11-1215", "vector": [5, 0, 3, 0.420084025208403], "context": ["", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "word": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "transliter": 1, "new": 1, "rescor": 1, "recent": 1}, "vector_2": [7, 0.18829544618553667, 9, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "The NE tagger is a rule-based system with 140 NE categories [Sekine et al 2004]."], "label": "Prov", "citing": "E09-1025", "vector": [1, 1, 0, 0.0], "context": ["", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"acquir": 1, "number": 1, "automat": 1, "avail": 1, "collect": 1, "ruleparaphras": 1, "infer": 1}, "vector_2": [4, 0.14964451736395953, 2, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.", "We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications."], "label": "Prov", "citing": "P09-2063", "vector": [1, 0, 0, 0.0], "context": ["", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", ""], "marker": "Sekine, 2005", "vector_1": {"retriev": 1, "recognit": 1, "pattern": 1, "gener": 1, "inform": 1, "base": 1, "paraphras": 1, "similarli": 1, "improv": 1, "introduc": 1}, "vector_2": [4, 0.10389138957456076, 1, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs"], "label": "Prov", "citing": "P10-1124", "vector": [1, 0, 0, 0.0], "context": ["", "This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g. (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).", ""], "marker": "Sekine, 2005", "vector_1": {"led": 1, "entail": 1, "eg": 1, "activ": 1, "rule": 1, "research": 1, "acquisit": 1, "predic": 1, "broadscal": 1}, "vector_2": [5, 0.0511044385646758, 3, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["There have been other kinds of efforts to discover paraphrase automatically from corpora."], "label": "Prov", "citing": "P12-1013", "vector": [2, 0, 0, 0.0], "context": ["", "Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).", ""], "marker": "Sekine, 2005", "vector_1": {"made": 1, "sub": 1, "stantial": 1, "rule": 1, "consequ": 1, "learn": 1, "effort": 1}, "vector_2": [7, 0.06604391437730515, 4, 1, 1, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["There have been other kinds of efforts to discover paraphrase automatically from corpora."], "label": "Prov", "citing": "P12-2031", "vector": [3, 0, 0, 0.08164965809277261], "context": ["", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", ""], "marker": "Sekine, 2005", "vector_1": {"led": 1, "algorithm": 1, "gener": 1, "knowledg": 1, "resourc": 1, "system": 1, "rule": 2, "signific": 1, "automat": 1, "substanti": 1, "learn": 1, "develop": 1, "effort": 1, "infer": 3}, "vector_2": [7, 0.09070796460176991, 3, 2, 2, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks."], "label": "Prov", "citing": "P12-2031", "vector": [2, 0, 0, 0.0], "context": ["", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"given": 1, "made": 1, "attempt": 1, "directli": 1, "judg": 2, "annot": 1, "rule": 2, "let": 1, "ask": 1, "correct": 2}, "vector_2": [7, 0.19070796460176992, 2, 2, 2, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Hereafter, each pair of NE categories will be called a domain; e.g.", "the Company  Company domain, which we will call CC- domain (Step 2).", "For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3)."], "label": "Prov", "citing": "W12-4006", "vector": [3, 0, 0, 0.04708816093480111], "context": ["", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 1, "algorithm": 1, "constraint": 1, "eg": 1, "rule": 1, "acquisit": 1, "add": 1, "exist": 1, "paraphras": 1, "learn": 1}, "vector_2": [7, 0.08978978978978978, 2, 2, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications."], "label": "Prov", "citing": "W12-4006", "vector": [10, 0, 5, 0.3731012536223182], "context": ["", "As for paraphrase, Sekines Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", ""], "marker": "Sekine, 2005", "vector_1": {"use": 1, "name": 1, "databas": 1, "unsupervis": 1, "focus": 1, "two": 1, "collect": 1, "paraphras": 2, "sekin": 1, "phrase": 1, "entiti": 1, "method": 1, "connect": 1}, "vector_2": [7, 0.223656990323657, 1, 2, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexi cal rule-based approaches, and approaches that combine lexical information with sta tistical information.", "The present proposal falls into the last group.", "Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach."], "label": "Prov", "citing": "A00-2032", "vector": [13, 1, 3, 0.27255405754769874], "context": ["", "Chinese According to Sproat et al (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pub lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical ap proach.", ""], "marker": "1996", "vector_1": {"shih": 1, "inde": 1, "knowledg": 1, "al": 1, "one": 1, "ap": 1, "exploit": 1, "mutualinform": 1, "et": 1, "previous": 1, "awar": 1, "author": 1, "pure": 1, "method": 1, "accord": 1, "sproat": 2, "proach": 1, "chines": 2, "lexic": 1, "assert": 1, "base": 1, "pub": 1, "segment": 1, "work": 1, "lish": 1, "prior": 1, "instanc": 1, "statist": 1}, "vector_2": [4, 0.8966613672496025, 2, 0, 10, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Chinese word segmentation can be viewed as a stochastic transduction problem.", "More formally, we start by representing the dictionary D as a Weighted Finite State Trans ducer (WFST) (Pereira, Riley, and Sproat 1994)."], "label": "Prov", "citing": "A00-2032", "vector": [2, 1, 0, 0.04999999999999999], "context": ["", "Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) er rors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Fung, 1998).", ""], "marker": "Sproat et al., 1996", "vector_1": {"applic": 1, "term": 1, "charact": 1, "retriev": 1, "er": 1, "optic": 1, "index": 1, "segment": 1, "recognit": 1, "technolog": 1, "correct": 1, "inform": 1, "includ": 1, "ocr": 1, "new": 1, "ror": 1, "document": 1, "extract": 1, "technic": 1, "propos": 1}, "vector_2": [4, 0.033177139988285496, 6, 2, 1, 0]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.", "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 On"], "label": "Prov", "citing": "E09-1063", "vector": [11, 0, 1, 0.17574991006549875], "context": ["", "First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"given": 1, "task": 1, "nativ": 1, "reliabl": 1, "object": 1, "agreement": 1, "realli": 1, "goldstandard": 1, "speaker": 1, "build": 1, "first": 1, "fact": 1, "difficult": 1}, "vector_2": [13, 0.5441719734018224, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.", "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15"], "label": "Prov", "citing": "J00-3004", "vector": [10, 0, 2, 0.11314714296070712], "context": ["", "According to Sproat et al {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the \"correct\" segmentation, and the figure reduces as more people become involved.", ""], "marker": "1996", "vector_1": {"reduc": 1, "involv": 1, "accord": 1, "becom": 1, "figur": 1, "sproat": 1, "show": 1, "peopl": 1, "fung": 1, "al": 1, "agreement": 1, "nativ": 1, "wu": 1, "speaker": 1, "expect": 1, "et": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [4, 0.10996673107954659, 0, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration."], "label": "Prov", "citing": "J00-3004", "vector": [7, 0, 3, 0.26352313834736496], "context": ["", "Sproat et al (1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.", ""], "marker": "1996", "vector_1": {"morpholog": 1, "recogn": 1, "sproat": 1, "word": 1, "chines": 1, "well": 1, "al": 1, "foreign": 1, "compon": 1, "transliter": 1, "et": 1, "implement": 1, "obtain": 1, "special": 1, "name": 2}, "vector_2": [4, 0.247504830965994, 1, 0, 1, 1]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.", "The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text."], "label": "Prov", "citing": "J11-3001", "vector": [2, 0, 1, 0.10206207261596574], "context": ["", "Gold standards, however, 435 cannot be unied into a single standard (Fung and Wu 1994; Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"sproat": 1, "gold": 1, "al": 1, "howev": 1, "wu": 1, "standard": 2, "fung": 1, "cannot": 1, "et": 1, "uni": 1, "singl": 1}, "vector_2": [15, 0.47331441068354196, 0, 0, 4, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Chinese word segmentation can be viewed as a stochastic transduction problem."], "label": "Prov", "citing": "J96-4004", "vector": [3, 0, 2, 0.1679842102263232], "context": ["", "Our ap . proach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994; Lai et al 1992; Sproat et al 1994; Sproat et al 1996) primarily in that our system performs sentence inter pretation, in addition to word boundary identification.", ""], "marker": "Sproat et al. 1996", "vector_1": {"shih": 1, "wang": 2, "primarili": 1, "al": 4, "ap": 1, "exist": 1, "chen": 2, "et": 4, "lai": 1, "identif": 1, "differ": 1, "perform": 1, "boundari": 1, "system": 1, "pretat": 1, "lua": 1, "liang": 1, "sentenc": 1, "sproat": 3, "proach": 1, "chines": 1, "gan": 1, "wu": 1, "fan": 1, "segment": 1, "addit": 1, "word": 2, "tsai": 1, "work": 1, "su": 1, "chiang": 1, "chang": 1, "inter": 1, "bai": 1}, "vector_2": [0, 0.06607004165784085, 0, 0, 5, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !", ":i*m xia4mi3-er3 'Shamir,' which is a legal Chi nese personal name, retains a foreign flavor because of liM.", "As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate."], "label": "Prov", "citing": "P06-1010", "vector": [10, 0, 4, 0.20965696734438366], "context": ["", "Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names. As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.", ""], "marker": "Sproat et al., 1996", "vector_1": {"subset": 1, "discuss": 1, "use": 2, "name": 2, "overwhelmingli": 1, "chines": 2, "elsewher": 1, "list": 1, "tend": 1, "charact": 2, "thousand": 1, "candid": 1, "sever": 1, "transliter": 3, "consult": 1, "foreign": 2, "hundr": 1, "gener": 1, "frequent": 1}, "vector_2": [10, 0.29509921828021646, 1, 1, 0, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !", ":i*m xia4mi3-er3 'Shamir,' which is a legal Chi nese personal name, retains a foreign flavor because of liM.", "As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate."], "label": "Prov", "citing": "P07-1015", "vector": [11, 0, 6, 0.18523964340873708], "context": ["", "Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 2, "name": 1, "possibl": 1, "sequenc": 1, "chines": 1, "list": 1, "three": 1, "foreign": 1, "candid": 1, "transliter": 1, "taken": 1, "charact": 2, "frequent": 1}, "vector_2": [11, 0.6221020092735703, 1, 2, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented."], "label": "Prov", "citing": "P12-1110", "vector": [11, 0, 4, 0.2156143862717941], "context": ["", "3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.", ""], "marker": "Sproat et al., 1996", "vector_1": {"featur": 1, "set": 1, "expect": 1, "contribut": 1, "use": 2, "depend": 1, "make": 1, "baselin": 1, "investig": 1, "syntact": 1, "chines": 1, "joint": 1, "dictionari": 3, "robust": 1, "strong": 1, "segment": 2, "serv": 1, "word": 1, "enabl": 1, "realist": 1, "alon": 1, "us": 1, "model": 1}, "vector_2": [16, 0.4850732373386296, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The performance was 80.99% recall and 61.83% precision."], "label": "Prov", "citing": "P99-1036", "vector": [2, 0, 0, 0.0], "context": ["", "In Japanese, around 95% word segmentation ac curacy is reported by using a word-based lan guage model and the Viterbi-like dynamic program ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Mat sumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"guag": 1, "curaci": 1, "ac": 1, "word": 1, "around": 1, "japanes": 1, "chines": 1, "viterbilik": 1, "use": 1, "procedur": 1, "ming": 1, "lan": 1, "program": 1, "dynam": 1, "report": 2, "model": 1, "accuraci": 1, "segment": 1, "method": 1, "wordbas": 1, "statist": 1}, "vector_2": [3, 0.034519797029519614, 5, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The major problem for our seg menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).", "We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted.", "However, there will remain a large number of words that are not readily adduced to any produc tive pattern and that would simply have to be added to the dictionary."], "label": "Prov", "citing": "P99-1036", "vector": [12, 1, 2, 0.3202563076101743], "context": ["", "There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "unknown": 1, "two": 1, "coverag": 1, "better": 1, "increas": 1, "design": 1, "dictionari": 1, "solv": 1, "problem": 1, "approach": 1, "model": 1}, "vector_2": [3, 0.041668209933701246, 5, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In this paper we have argued that Chinese word segmentation can be modeled ef fectively using weighted finite-state transducers.", "This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration."], "label": "Prov", "citing": "P99-1036", "vector": [9, 0, 2, 0.254000254000381], "context": ["", "To improve word segmenta tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.", ""], "marker": "Sproat et al., 1996", "vector_1": {"foreign": 1, "use": 2, "set": 1, "word": 4, "name": 1, "specif": 1, "unknown": 1, "gener": 1, "accuraci": 1, "person": 1, "transliter": 1, "segmenta": 1, "improv": 1, "tion": 1, "purpos": 1, "plural": 1, "model": 2, "singl": 1}, "vector_2": [3, 0.05311307826215786, 2, 5, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recal"], "label": "Prov", "citing": "P99-1036", "vector": [8, 0, 2, 0.253546276418555], "context": ["", "Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"term": 1, "previou": 1, "recal": 1, "express": 1, "accuraci": 1, "precis": 1, "done": 1, "word": 1, "research": 1, "segment": 1}, "vector_2": [3, 0.7816585799474055, 1, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.", "Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide."], "label": "Prov", "citing": "W01-0513", "vector": [10, 0, 3, 0.12670850749479937], "context": ["", "The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et. al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).", ""], "marker": "Sproat, et al, 1996", "vector_1": {"identifi": 1, "stream": 1, "pont": 1, "focus": 1, "phonet": 1, "asian": 1, "orthographi": 1, "et": 2, "languag": 1, "croft": 1, "token": 1, "indian": 1, "includ": 1, "saffran": 1, "sproat": 1, "normal": 1, "segment": 1, "word": 2, "delimit": 1, "work": 1, "princip": 1, "either": 1, "mani": 1, "other": 1}, "vector_2": [5, 0.1871841291393226, 6, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement"], "label": "Prov", "citing": "W03-1025", "vector": [5, 0, 1, 0.21764287503300347], "context": ["", "There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower", ""], "marker": "Sproat et al., 1996", "vector_1": {"upper": 1, "lower": 1, "multipl": 1, "show": 1, "agreement": 1, "two": 1, "nativ": 1, "speaker": 1, "untrain": 1, "studi": 1}, "vector_2": [7, 0.04833787687675469, 3, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["A Stochastic Finite-State Word-Segmentation Algorithm for Chinese The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement"], "label": "Prov", "citing": "W03-1025", "vector": [6, 0, 0, 0.2946278254943948], "context": ["", "Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "rel": 1, "wellknown": 1, "chines": 1, "agreement": 1, "extens": 1, "low": 1, "human": 1, "known": 1, "problem": 1, "segment": 1, "studi": 1}, "vector_2": [7, 0.8905480734019612, 3, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["More formally, we start by representing the dictionary D as a Weighted Finite State Trans ducer (WFST) (Pereira, Riley, and Sproat 1994).", "This FSA I can be segmented into words by composing Id(I) with D*, to form the WFST shown in Figure 2(c), then selecting the best path through this WFST to produce the WFST in Figure 2(d).", "This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels."], "label": "Prov", "citing": "W03-1025", "vector": [6, 0, 2, 0.1764461683101464], "context": ["", "Sproat et al (1996) employs stochastic finite state machines to find word boundaries.", ""], "marker": "1996", "vector_1": {"machin": 1, "word": 1, "sproat": 1, "boundari": 1, "finit": 1, "al": 1, "employ": 1, "state": 1, "et": 1, "stochast": 1, "find": 1}, "vector_2": [7, 0.9157749115026244, 1, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point."], "label": "Prov", "citing": "W10-3212", "vector": [1, 0, 0, 0.032826608214930636], "context": ["", "In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.", ""], "marker": "Sproat et al., 1996", "vector_1": {"lexicon": 1, "knowledg": 1, "ii": 1, "dictionarylexicon": 1, "languag": 1, "use": 1, "techniqu": 1, "iii": 1, "segment": 1, "three": 1, "approachesstatist": 1, "approach": 4, "method": 1, "match": 2, "machin": 1, "advanc": 1, "base": 4, "longest": 1, "categor": 1, "word": 1, "maximum": 1, "exampl": 1, "learn": 1, "linguist": 1}, "vector_2": [14, 0.13642488210195375, 5, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement"], "label": "Prov", "citing": "W10-3708", "vector": [4, 0, 1, 0.29019050004400465], "context": ["", "Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"among": 1, "shown": 1, "word": 1, "regard": 1, "agreement": 1, "nativ": 1, "speaker": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [14, 0.08489433674026116, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.", "Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.", "Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary."], "label": "Prov", "citing": "W11-0823", "vector": [5, 0, 1, 0.20628424925175873], "context": ["", "There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.", ""], "marker": "1996", "vector_1": {"altern": 1, "cha": 1, "mutual": 1, "juman": 1, "sproat": 1, "dictionarybas": 1, "solut": 2, "number": 1, "al": 1, "inform": 1, "base": 1, "statist": 1, "et": 1, "popular": 1, "sen": 1, "distribut": 1, "propos": 1}, "vector_2": [15, 0.5, 1, 2, 2, 0]}, {"function": "CoCo", "cited": "N06-2049", "provenance": ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters."], "label": "Prov", "citing": "J11-1005", "vector": [5, 0, 0, 0.055555555555555566], "context": ["", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", ""], "marker": "2006", "vector_1": {"sumita": 1, "chose": 1, "zhang": 1, "emerson": 1, "well": 1, "three": 1, "least": 1, "one": 1, "kikui": 1, "achiev": 1, "score": 1, "subwordbas": 1, "test": 1, "close": 1, "model": 2, "comparison": 1, "best": 1}, "vector_2": [5, 0.28068530583135004, 2, 0, 2, 1]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters."], "label": "Prov", "citing": "W06-0118", "vector": [3, 0, 0, 0.1781741612749496], "context": ["", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 1, "maximum": 1, "subwordbas": 1, "also": 1, "tag": 1, "crf": 1, "model": 1}, "vector_2": [0, 0.1024783777890835, 1, 5, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.", "We also successfully employed the confidence measure to make a confidence-dependent word segmentation.", "We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections."], "label": "Prov", "citing": "W06-0118", "vector": [12, 0, 2, 0.2869984013336701], "context": ["", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", ""], "marker": "Zhang et al., 2006", "vector_1": {"competit": 1, "task": 1, "word": 1, "accuraci": 1, "appli": 1, "chines": 1, "previou": 1, "obtain": 1, "maximum": 1, "subwordbas": 1, "system": 1, "high": 1, "approach": 1, "tagger": 1, "data": 1, "sighan": 1, "share": 1, "recent": 1, "segment": 1, "iob": 1, "propos": 1}, "vector_2": [0, 0.24304091867141508, 1, 5, 0, 0]}, {"function": "CoCo", "cited": "N06-2049", "provenance": ["In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters."], "label": "Prov", "citing": "W10-4128", "vector": [6, 0, 0, 0.26352313834736496], "context": ["", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 1, "previou": 1, "word": 1, "illustr": 1, "work": 1, "effect": 1, "lexic": 1, "charact": 1, "employ": 1, "tag": 2, "subword": 1, "focu": 1, "literatur": 1, "unit": 2}, "vector_2": [4, 0.10215770890545312, 6, 11, 0, 1]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["For a character-based IOB tagger, there is only one possibility of re-segmentation.", "However, there are multiple choices for a subword-based IOB tagger."], "label": "Prov", "citing": "W10-4138", "vector": [5, 0, 0, 0.0], "context": ["", "Thus, the bigram RAIL ENQUIRIES gives a misleading probability that RAIL is followed by ENQUIRIES irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al (2006) were proposed.", ""], "marker": "2006", "vector_1": {"bigram": 1, "give": 1, "al": 1, "tag": 1, "et": 1, "follow": 1, "happen": 1, "still": 1, "unit": 1, "probabl": 1, "mislead": 1, "appear": 1, "solut": 1, "overlap": 1, "also": 1, "enquiri": 2, "therefor": 1, "zhang": 1, "rail": 2, "compound": 1, "sinc": 1, "wordtoken": 1, "thu": 1, "corpora": 2, "correspond": 1, "n": 1, "irrespect": 1, "preced": 1, "gram": 1, "problem": 1, "propos": 1}, "vector_2": [4, 0.4131716542440761, 1, 0, 0, 1]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns."], "label": "Prov", "citing": "J07-4005", "vector": [7, 0, 1, 0.2291746242570528], "context": ["", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", ""], "marker": "2005", "vector_1": {"text": 1, "curran": 1, "automat": 2, "one": 1, "find": 1, "wordnet": 2, "use": 2, "inventori": 1, "unknown": 1, "also": 1, "adapt": 1, "synset": 1, "sens": 1, "might": 1, "method": 2, "induc": 1, "relat": 1, "although": 1, "new": 1, "noun": 1, "johnson": 1, "could": 1, "combin": 1, "ciaramita": 1}, "vector_2": [2, 0.3230273921992928, 2, 0, 1, 1]}, {"function": "Weak", "cited": "P05-1004", "provenance": ["Some specialist topics are better covered in WORD- NET than others, e.g.", "dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses."], "label": "Prov", "citing": "J09-3004", "vector": [4, 0, 0, 0.040824829046386304], "context": ["", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", ""], "marker": "Curran 2005", "vector_1": {"curran": 1, "automat": 2, "acquisit": 1, "seem": 1, "suffer": 1, "sever": 1, "acquir": 1, "rather": 1, "overlap": 1, "wordnet": 1, "method": 1, "relationship": 1, "extent": 1, "found": 1, "addit": 1, "seriou": 1, "inform": 1, "limit": 2, "integr": 1, "potenti": 1, "output": 1, "typic": 1}, "vector_2": [4, 0.9903861370470011, 0, 0, 1, 1]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems."], "label": "Prov", "citing": "S10-1090", "vector": [4, 1, 1, 0.06428243465332249], "context": ["", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"use": 1, "set": 1, "wsd": 1, "classbas": 1, "sensegroup": 1, "focus": 1, "classifi": 1, "research": 1, "learn": 1, "predefin": 1, "contrast": 1}, "vector_2": [5, 0.17027141645462257, 6, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Prov", "citing": "D09-1149", "vector": [8, 0, 6, 0.17040572913692198], "context": ["", "This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "ace": 1, "semisupervis": 1, "unsupervis": 1, "focus": 1, "subtask": 1, "paper": 1, "includ": 1, "rdc": 1, "learn": 1, "mani": 1, "method": 4, "propos": 1}, "vector_2": [4, 0.055550295110942775, 14, 4, 6, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["For each pair of mentions3, we compute various lexical, syntactic and semantic features.", "This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Prov", "citing": "D09-1149", "vector": [15, 0, 7, 0.511766315719159], "context": ["", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "comput": 1, "syntact": 1, "variou": 1, "relat": 1, "system": 1, "lexic": 1, "employ": 1, "mention": 1, "stateoftheart": 1, "pair": 1, "extract": 2, "entiti": 1}, "vector_2": [4, 0.2651895338193984, 1, 4, 6, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype Founder under the type ROLE.", "It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes Based-In, Located and Residence under the type AT, which are difficult even for human experts to differentiate."], "label": "Prov", "citing": "D09-1149", "vector": [10, 0, 3, 0.3181980515339463], "context": ["", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", ""], "marker": "Zhou et al., 2005", "vector_1": {"corpu": 1, "unbalanc": 1, "corpora": 1, "greatli": 1, "ace": 2, "relat": 1, "well": 1, "number": 1, "instanc": 1, "tabl": 1, "rdc": 2, "shown": 1, "known": 1, "type": 1}, "vector_2": [4, 0.4024240128775684, 1, 4, 6, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998)."], "label": "Prov", "citing": "P06-1017", "vector": [1, 1, 0, 0.0], "context": ["", "Many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithms (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "algorithm": 3, "semisupervis": 1, "eg": 1, "learn": 4, "unsupervis": 1, "address": 1, "mani": 1, "problem": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.0491425646275915, 9, 18, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Prov", "citing": "P06-1017", "vector": [6, 0, 2, 0.1720618004029213], "context": ["", "Bootstrapping Currently most of works on the RDC task of ACE focused on supervised learning methods Culotta and Soresen (2004; Kambhatla (2004; Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"supervis": 1, "work": 1, "zhou": 1, "ace": 1, "soresen": 1, "bootstrap": 1, "focus": 1, "al": 1, "current": 1, "task": 1, "kambhatla": 1, "rdc": 1, "learn": 1, "et": 1, "culotta": 1, "method": 1}, "vector_2": [1, 0.9191621875266616, 1, 0, 18, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information."], "label": "Prov", "citing": "P06-1017", "vector": [1, 0, 0, 0.0], "context": ["", "In the future, we would like to use more effective feature sets Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "like": 1, "would": 1, "effect": 1, "al": 1, "set": 1, "futur": 1, "zhou": 1, "et": 1}, "vector_2": [1, 0.948895145465404, 1, 0, 18, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.", "This is largely due to incorporation of two semantic resources, i.e.", "the country name list and the personal relative trigger word list."], "label": "Prov", "citing": "P08-2023", "vector": [13, 0, 11, 0.4522670168666454], "context": ["", "Based on his work, Zhou et al (2005) further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", ""], "marker": "2005", "vector_1": {"person": 1, "word": 1, "name": 1, "zhou": 1, "collect": 1, "semiautomat": 1, "trigger": 1, "chunk": 1, "work": 1, "list": 2, "al": 1, "rel": 1, "inform": 1, "base": 2, "incorpor": 1, "phrase": 1, "et": 1, "countri": 1}, "vector_2": [3, 0.2484637201607185, 1, 0, 0, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs)."], "label": "Prov", "citing": "P11-3012", "vector": [2, 0, 0, 0.061313933948496574], "context": ["", "We used Zhou et al.s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", ""], "marker": "Zhou et al., 2005", "vector_1": {"use": 1, "featur": 2, "zhou": 1, "al": 1, "lexic": 1, "system": 1, "done": 1, "et": 1, "research": 1, "similar": 1, "basi": 1}, "vector_2": [6, 0.31043752940385777, 2, 2, 0, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": ["It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features."], "label": "Prov", "citing": "P11-3012", "vector": [5, 1, 0, 0.12909944487358055], "context": ["", "Although a bit lower than Zhou et al.s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", ""], "marker": "Zhou et al., 2005", "vector_1": {"lower": 1, "parser": 1, "zhou": 1, "semant": 1, "use": 2, "al": 1, "inform": 1, "token": 1, "featur": 1, "result": 1, "although": 1, "et": 1, "bit": 1, "differ": 3, "attribut": 1}, "vector_2": [6, 0.41313260299751325, 1, 2, 0, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": ["This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).", "In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a NONE class for the case where the two mentions are not related."], "label": "Prov", "citing": "P13-1147", "vector": [7, 0, 1, 0.025657900289539272], "context": ["", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "featur": 1, "featuresinst": 1, "parser": 1, "ii": 1, "phrase": 1, "ie": 1, "differ": 2, "depend": 1, "iii": 1, "behind": 1, "threefold": 1, "preprocess": 1, "might": 1, "sourc": 1, "resourc": 1, "partit": 1, "zhang": 1, "use": 1, "reason": 1, "data": 1, "averag": 1, "addit": 1, "slightli": 1, "chunker": 1, "incorpor": 1}, "vector_2": [8, 0.6765932650432436, 2, 3, 11, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998)."], "label": "Prov", "citing": "W06-1667", "vector": [1, 1, 0, 0.0], "context": ["", "Prior work on automatic relation extraction come in three kinds: supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "kind": 1, "work": 1, "algorithm": 3, "semisupervis": 1, "extract": 1, "unsupervis": 1, "relat": 1, "three": 1, "automat": 1, "prior": 1, "learn": 3, "come": 1}, "vector_2": [1, 0.04478182722092623, 9, 23, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance."], "label": "Prov", "citing": "W06-1667", "vector": [11, 0, 6, 0.524863881081478], "context": ["", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "captur": 1, "chunk": 1, "still": 1, "entiti": 1, "supervis": 1, "depend": 1, "concern": 1, "simpl": 1, "method": 1, "full": 1, "especi": 1, "complement": 1, "pars": 1, "although": 1, "pair": 1, "characterist": 1, "word": 1, "provid": 1, "tree": 2, "inform": 3, "incorpor": 1}, "vector_2": [1, 0.8953688477570844, 4, 23, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities."], "label": "Prov", "citing": "W08-0602", "vector": [1, 0, 0, 0.0], "context": ["", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", ""], "marker": "2005", "vector_1": {"nlp": 1, "zhou": 1, "success": 1, "gener": 1, "al": 1, "see": 1, "exampl": 1, "et": 1, "follow": 1, "method": 1}, "vector_2": [3, 0.15162721893491124, 1, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information."], "label": "Prov", "citing": "W08-0602", "vector": [1, 0, 0, 0.0], "context": ["", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "develop": 1, "describ": 1, "zhou": 1, "al": 2, "part": 1, "et": 2, "wang": 1}, "vector_2": [3, 0.5425778235142784, 2, 0, 1, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses."], "label": "Prov", "citing": "J05-1004", "vector": [4, 0, 0, 0.05270462766947299], "context": ["", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levins original classes, adding an additional level to the hierarchy (Dang et al 1998).", ""], "marker": "Dang et al. 1998", "vector_1": {"case": 1, "origin": 1, "use": 1, "ad": 1, "level": 1, "provid": 1, "levin": 1, "al": 1, "caus": 1, "inform": 1, "dang": 1, "subdivid": 1, "intersect": 1, "et": 1, "hierarchi": 1, "mani": 1, "addit": 2, "class": 2, "verbnet": 1}, "vector_2": [7, 0.5, 0, 2, 2, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class."], "label": "Prov", "citing": "W02-1108", "vector": [2, 0, 0, 0.0], "context": ["", "Manual classification of verbs to semanticclasses yields accurate results but is time con suming (Levin, 1993; Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"classif": 1, "accur": 1, "manual": 1, "yield": 1, "verb": 1, "result": 1, "time": 1, "semanticclass": 1, "sume": 1, "con": 1}, "vector_2": [4, 0.5, 2, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses."], "label": "Prov", "citing": "W02-1108", "vector": [5, 0, 0, 0.19364916731037085], "context": ["", "Dang et al (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", ""], "marker": "1998", "vector_1": {"classif": 1, "creat": 1, "levin": 1, "share": 1, "al": 1, "one": 1, "current": 1, "verb": 1, "membership": 1, "exampl": 1, "intersect": 1, "refin": 1, "dang": 1, "et": 1, "class": 2}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["The fundamental assumption is that the syntactic frames are a direct reflection of the un derlying semantics."], "label": "Prov", "citing": "W04-2606", "vector": [3, 0, 0, 0.07856742013183861], "context": ["", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "comput": 1, "consider": 1, "relationship": 1, "captur": 1, "eg": 1, "lexicalsemant": 1, "syntax": 1, "aim": 1, "verb": 1, "interest": 1, "close": 1, "linguist": 2, "attract": 1, "class": 1}, "vector_2": [6, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Investigating regular sense extensions based on intersective Levin classes"], "label": "Prov", "citing": "W11-0910", "vector": [3, 0, 2, 0.35355339059327373], "context": ["", "Complicating the issue is the phenomenon of regular sense extensions (Dang et al., 1998), where what once may have been coercion has become entrenched and is now seen as a different sense of the verb.", ""], "marker": "Dang et al., 1998", "vector_1": {"differ": 1, "entrench": 1, "phenomenon": 1, "may": 1, "coercion": 1, "verb": 1, "issu": 1, "extens": 1, "regular": 1, "seen": 1, "sens": 2, "becom": 1, "complic": 1}, "vector_2": [13, 0.270500710524116, 1, 2, 4, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses."], "label": "Prov", "citing": "W99-0632", "vector": [6, 0, 0, 0.06085806194501846], "context": ["", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim plify the definition of different verb senses.", ""], "marker": "1998", "vector_1": {"use": 1, "syntact": 1, "frame": 1, "palmer": 1, "definit": 1, "argu": 1, "verb": 2, "sens": 1, "dang": 1, "et": 1, "differ": 1, "class": 1, "sim": 1, "plifi": 1}, "vector_2": [1, 0.16959409014106644, 2, 0, 4, 1]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["Two current approaches to English verb classi fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993)."], "label": "Prov", "citing": "W99-0632", "vector": [6, 1, 2, 0.3651483716701107], "context": ["", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"differ": 1, "scheme": 1, "levin": 1, "semant": 1, "classif": 1, "also": 1, "verb": 1, "plan": 1, "intersect": 1, "experi": 1, "class": 1, "wordnet": 1}, "vector_2": [1, 0.9864434751856654, 2, 2, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\")."], "label": "Prov", "citing": "P00-1022", "vector": [2, 0, 0, 0.0], "context": ["", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", ""], "marker": "1998", "vector_1": {"origin": 1, "restrict": 1, "semant": 1, "shallow": 1, "mitkov": 1, "see": 1, "inform": 1, "resolut": 1, "prefer": 1, "employ": 1, "current": 1, "consequ": 1, "reli": 1, "exampl": 1, "heurist": 1, "morphosyntact": 1, "analysi": 1, "mainli": 1, "method": 1, "anaphora": 1}, "vector_2": [2, 0.134280882589789, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["This paper pres ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Evaluation reports a success rate of 89.7% which is better than the suc cess rates of the approaches selected for comparison and tested on the same data."], "label": "Prov", "citing": "P00-1022", "vector": [7, 1, 4, 0.23904572186687872], "context": ["", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", ""], "marker": "1998", "vector_1": {"mitkov": 1, "success": 1, "work": 1, "manual": 1, "obtain": 1, "rate": 1, "english": 1, "pronomin": 1, "technic": 1, "refer": 1}, "vector_2": [2, 0.9156466419713319, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["With a view to avoiding complex syntactic, seman tic and discourse analysis (which is vital for real world applications), we developed a robust, knowl edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors."], "label": "Prov", "citing": "P00-1022", "vector": [5, 0, 1, 0.18670401120373464], "context": ["", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", ""], "marker": "1998", "vector_1": {"knowledg": 1, "mitkov": 1, "evalu": 1, "pronoun": 1, "resolut": 2, "limit": 1, "anaphora": 1, "th": 1, "robust": 1, "implement": 1, "pronomin": 1, "ruslan": 1, "sever": 1, "baselin": 1}, "vector_2": [2, 0.9758817845063618, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["This paper pres ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent."], "label": "Prov", "citing": "P03-1023", "vector": [7, 0, 0, 0.22645540682891918], "context": ["", "Mitkovs knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "set": 1, "mitkov": 1, "score": 1, "pronoun": 1, "resolut": 1, "indic": 1, "method": 1, "candid": 1, "exampl": 1, "anteced": 1, "rank": 1, "knowledgepoor": 1}, "vector_2": [5, 0.055885805156942185, 1, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["With a view to avoiding complex syntactic, seman tic and discourse analysis (which is vital for real world applications), we developed a robust, knowl edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors."], "label": "Prov", "citing": "P04-1018", "vector": [3, 0, 1, 0.12830005981991682], "context": ["", "Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)", ""], "marker": "Mitkov, 1998", "vector_1": {"earli": 1, "work": 1, "pronoun": 1, "resolut": 1, "focus": 1, "anteced": 1, "ing": 1, "find": 1, "anaphora": 1}, "vector_2": [6, 0.04756803802765039, 3, 1, 0, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing."], "label": "Prov", "citing": "P05-1021", "vector": [1, 0, 0, 0.05716619504750295], "context": ["", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al", ""], "marker": "1998", "vector_1": {"use": 1, "learningbas": 1, "knowledg": 1, "perform": 1, "shallow": 1, "resolut": 1, "well": 1, "soon": 1, "al": 1, "inde": 1, "reason": 1, "exist": 1, "et": 1, "limit": 1, "anaphor": 1, "mitkov": 1, "approach": 1, "eg": 1}, "vector_2": [7, 0.0890668178275349, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The antecedent indicators have been identi fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms."], "label": "Prov", "citing": "P05-1021", "vector": [4, 0, 0, 0.03225806451612904], "context": ["", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"c": 2, "word": 1, "ana": 3, "evalu": 1, "whether": 1, "max": 1, "surround": 1, "ilar": 1, "also": 1, "candid": 2, "salienc": 1, "factor": 1, "anaphor": 1, "statsemn": 1, "paralstuctmark": 1, "statsemci": 1, "sim": 1}, "vector_2": [7, 0.4518883299085949, 1, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\")."], "label": "Prov", "citing": "P06-1006", "vector": [4, 0, 0, 0.04583492485141056], "context": ["", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 3, "featur": 1, "machinelearn": 1, "manual": 1, "could": 1, "resolut": 1, "tree": 1, "mine": 1, "rule": 1, "calcul": 1, "pars": 1, "design": 1, "method": 1}, "vector_2": [8, 0.05915543575920935, 6, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The antecedent indicators have been identi fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.", "Top symptoms like \"lexical reiteration\" as sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\".", "We should point out that the antecedent indicators are preferences and not absolute factors."], "label": "Prov", "citing": "P13-3012", "vector": [7, 0, 0, 0.05063696835418333], "context": ["", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", ""], "marker": "Mitkov, 1998", "vector_1": {"graphbas": 1, "resolut": 2, "develop": 1, "edg": 1, "pronoun": 2, "cluster": 1, "neg": 1, "regard": 1, "within": 1, "factorbas": 1, "weight": 1, "repres": 1, "multigraph": 1, "framework": 1, "factor": 1, "posit": 1, "approach": 1, "greedi": 1}, "vector_2": [15, 0.09670611982750711, 1, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["In order to evaluate the effectiveness of the ap proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.", "The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%."], "label": "Prov", "citing": "W99-0207", "vector": [13, 1, 6, 0.2545875386086578], "context": ["", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro nouns in English technical manuals to the most re cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an tecedent (cf.", ""], "marker": "Mitkov, 1998", "vector_1": {"pro": 1, "cf": 1, "rate": 1, "result": 1, "baselin": 1, "correctli": 1, "cent": 1, "verifi": 1, "experi": 2, "difficulti": 1, "accord": 1, "wherea": 1, "candid": 2, "teced": 1, "report": 1, "technic": 1, "recent": 1, "task": 1, "noun": 1, "success": 1, "howev": 1, "manual": 1, "resolv": 2, "achiev": 1, "english": 1}, "vector_2": [1, 0.8277052445231246, 1, 2, 0, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph ora resolution.", "It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing."], "label": "Prov", "citing": "W99-0207", "vector": [9, 0, 3, 0.19695964928958382], "context": ["", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen sive in the cost of human effort at development time and limited ability to scale to new domains, more re cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", ""], "marker": "Mitkov, 1998", "vector_1": {"domain": 1, "expen": 1, "knowledg": 1, "cost": 1, "human": 1, "limit": 1, "abil": 1, "scale": 1, "multipl": 1, "system": 1, "cent": 1, "without": 1, "new": 1, "approach": 1, "strategi": 1, "knowledgebas": 1, "resolut": 1, "wherea": 1, "sophist": 1, "address": 1, "develop": 1, "effort": 1, "knowledgepoor": 1, "like": 2, "sive": 1, "combin": 1, "time": 1, "problem": 1, "linguist": 1}, "vector_2": [1, 0.8375746846647488, 4, 2, 0, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs."], "label": "Prov", "citing": "D07-1018", "vector": [1, 0, 0, 0.16666666666666666], "context": ["", "Stevenson and Joanis, 2003 for English semantic verb classes", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"semant": 1, "verb": 1, "class": 1, "english": 1}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments."], "label": "Prov", "citing": "D11-1095", "vector": [7, 0, 1, 0.33633639699815626], "context": ["", "We adopt as our baseline method a well-known hierarchical method  agglomerative clustering (AGG)  which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"hierarch": 1, "flat": 1, "use": 1, "levinstyl": 1, "acquir": 1, "agg": 1, "adopt": 1, "classif": 1, "cluster": 1, "agglom": 1, "previous": 1, "wellknown": 1, "method": 2, "baselin": 1}, "vector_2": [8, 0.07637231503579953, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research"], "label": "Prov", "citing": "D11-1095", "vector": [7, 0, 1, 0.2396169191492615], "context": ["", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levins original taxonomy (Stevenson and Joanis, 2003).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"origin": 1, "flat": 1, "use": 1, "set": 1, "resourc": 1, "includ": 1, "gold": 3, "levin": 1, "correspond": 1, "first": 1, "three": 1, "appear": 1, "3": 1, "experi": 1, "test": 1, "taxonomi": 1, "standard": 3, "extract": 1, "class": 1, "t": 1}, "vector_2": [8, 0.1730461315368116, 1, 10, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments)."], "label": "Prov", "citing": "D11-1095", "vector": [3, 1, 0, 0.1632993161855452], "context": ["", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"corpu": 1, "joani": 1, "stevenson": 1, "least": 1, "occur": 1, "verb": 1, "time": 1, "follow": 1, "class": 1, "select": 1}, "vector_2": [8, 0.17996435153016524, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below)."], "label": "Prov", "citing": "D11-1095", "vector": [3, 0, 0, 0.19289712886816485], "context": ["", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"style": 1, "previou": 1, "optim": 1, "levin": 1, "work": 1, "classif": 1, "task": 1, "verb": 1, "investig": 1, "featur": 1}, "vector_2": [8, 0.2505060270082475, 3, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff."], "label": "Prov", "citing": "D11-1095", "vector": [4, 0, 0, 0.06537204504606135], "context": ["", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"cutbas": 1, "cutoff": 1, "use": 1, "set": 1, "lost": 1, "cluster": 1, "remov": 1, "amount": 1, "predefin": 1, "inform": 1, "signific": 1, "pairwis": 1, "valu": 1, "although": 1, "addit": 1, "method": 1, "requir": 1, "difficult": 1}, "vector_2": [8, 0.31213558502764266, 1, 10, 3, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features)."], "label": "Prov", "citing": "D11-1095", "vector": [3, 0, 0, 0.05270462766947299], "context": ["", "Table 1: Comparison against Stevenson and Joanis (2003)s result on T1 (using similar features).", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"comparison": 1, "use": 1, "featur": 1, "joani": 1, "stevenson": 1, "t": 1, "tabl": 1, "similar": 1, "result": 1}, "vector_2": [8, 0.6423975106492251, 1, 10, 3, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993)."], "label": "Prov", "citing": "D11-1095", "vector": [3, 0, 0, 0.0], "context": ["", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"use": 1, "joani": 1, "show": 1, "agg": 1, "stevenson": 1, "employ": 1, "linkag": 1, "criterion": 1, "t": 1, "tabl": 1, "ward": 1, "result": 2}, "vector_2": [8, 0.6455394096855079, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2)."], "label": "Prov", "citing": "D11-1095", "vector": [6, 1, 1, 0.21629522817435007], "context": ["", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"therefor": 1, "use": 1, "featur": 1, "joani": 1, "agg": 1, "reproduc": 1, "stevenson": 1, "abl": 1, "differ": 1, "see": 1, "set": 2, "result": 1, "experi": 1, "b": 1, "section": 1, "smaller": 1}, "vector_2": [8, 0.6489229932630434, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes."], "label": "Prov", "citing": "J06-2001", "vector": [10, 0, 6, 0.20851441405707477], "context": ["", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", ""], "marker": "Stevenson and Joanis 2003", "vector_1": {"class": 1, "measur": 1, "korhonen": 1, "joani": 1, "gold": 1, "evalu": 1, "accuracypur": 1, "whether": 1, "stevenson": 1, "krymolowski": 1, "correct": 1, "member": 1, "cluster": 2, "exampl": 1, "verb": 1, "respect": 1, "major": 1, "standard": 1, "marx": 1, "assign": 1}, "vector_2": [3, 0.49879795850000985, 0, 0, 5, 1]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).", "This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder.", "Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks."], "label": "Prov", "citing": "J06-2001", "vector": [16, 0, 2, 0.21964027974653352], "context": ["", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"classif": 1, "enlarg": 1, "entropi": 1, "gold": 1, "number": 1, "dash": 1, "seed": 1, "exclud": 1, "total": 1, "featur": 1, "select": 1, "supervis": 1, "use": 1, "joani": 1, "techniqu": 1, "compar": 1, "semisupervis": 2, "frequenc": 1, "classifi": 1, "liu": 1, "yao": 1, "low": 1, "outperform": 1, "experi": 1, "approach": 2, "method": 1, "relat": 1, "standard": 1, "verb": 6, "five": 1, "multidimension": 1, "data": 1, "class": 5, "recent": 1, "measur": 1, "organ": 1, "levin": 1, "stevenson": 1, "work": 1, "manual": 1, "space": 1, "ambigu": 1, "train": 1, "english": 1, "found": 1, "unsupervis": 2}, "vector_2": [3, 0.8416852227717895, 2, 1, 10, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size."], "label": "Prov", "citing": "P03-1009", "vector": [11, 0, 9, 0.5345224838248488], "context": ["", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", ""], "marker": "(Stevenson and Joanis, 2003)", "vector_1": {"size": 1, "measur": 2, "accord": 1, "deriv": 1, "evalu": 1, "global": 1, "weight": 1, "precis": 1, "cluster": 2, "second": 1, "mean": 1, "puriti": 1}, "vector_2": [0, 0.5941138544392612, 1, 2, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results."], "label": "Prov", "citing": "P03-1009", "vector": [2, 0, 0, 0.0936585811581694], "context": ["", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R  29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"involv": 1, "impli": 1, "task": 1, "joani": 1, "structur": 1, "stevenson": 1, "predicateargu": 1, "differ": 1, "classifi": 1, "accuraci": 1, "exampl": 1, "u": 1, "mp": 1, "base": 1, "verb": 1, "report": 1, "r": 1, "class": 1}, "vector_2": [0, 0.698524670990253, 1, 2, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs."], "label": "Prov", "citing": "P04-2007", "vector": [2, 0, 0, 0.20100756305184242], "context": ["", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"machin": 1, "use": 1, "attempt": 1, "techniqu": 1, "variou": 1, "automat": 1, "reason": 1, "verb": 1, "classifi": 1, "learn": 1, "method": 1}, "vector_2": [1, 0.07063896151552682, 3, 3, 1, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["4.1 Clustering Parameters."], "label": "Prov", "citing": "P04-2007", "vector": [0, 0, 0, 0.0], "context": ["", "Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"work": 1, "choic": 1, "set": 1, "motiv": 1, "paramet": 1}, "vector_2": [1, 0.5300674280996285, 1, 3, 1, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good.", "Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification."], "label": "Prov", "citing": "P04-2007", "vector": [9, 1, 1, 0.16803361008336115], "context": ["", "Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"subset": 1, "rand": 1, "adjust": 1, "compar": 1, "cluster": 1, "nevertheless": 1, "gener": 1, "measur": 1, "descript": 1, "obtain": 1, "perform": 1, "verb": 2, "result": 1, "base": 1, "english": 1, "report": 1, "experi": 2, "similar": 1, "averag": 1}, "vector_2": [1, 0.8445025457547819, 1, 3, 1, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to the curse of dimensionality?"], "label": "Prov", "citing": "P07-3016", "vector": [8, 0, 1, 0.3928371006591931], "context": ["", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", ""], "marker": "(Stevenson and Joanis, 2003)", "vector_1": {"applic": 1, "verb": 1, "investig": 1, "space": 1, "gener": 1, "task": 1, "cluster": 1, "featur": 1, "unsupervis": 1}, "vector_2": [4, 0.25432923012726893, 1, 1, 4, 0]}, {"function": "Pos", "cited": "C90-2039", "provenance": ["The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation."], "label": "Prov", "citing": "P91-1031", "vector": [4, 0, 0, 0.13245323570650439], "context": ["", "This observation is the basis for a reordering method proposed by Kogure [1990].", ""], "marker": "1990", "vector_1": {"kogur": 1, "observ": 1, "basi": 1, "method": 1, "reorder": 1, "propos": 1}, "vector_2": [1, 0.24332079321558375, 1, 0, 7, 0]}, {"function": "Neut", "cited": "C90-2039", "provenance": ["in this method, theretbre, the failure tendency information is acquired by a learning process.", "That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process.", "in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction."], "label": "Prov", "citing": "P91-1031", "vector": [4, 0, 0, 0.059391387091649865], "context": ["", "Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).", ""], "marker": "Kogure 1990", "vector_1": {"count": 1, "stabil": 1, "conjunct": 1, "shuffl": 1, "kogur": 1, "reshuffl": 1, "thu": 1, "automat": 1, "also": 1, "see": 1, "appli": 1, "need": 1, "constant": 1, "scheme": 1, "order": 2}, "vector_2": [1, 0.30367276669004656, 0, 0, 7, 0]}, {"function": "Neut", "cited": "C90-2039", "provenance": ["This paper proposes an FS unification method that allows structure sharing with constant m'der node access time.", "This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.", "Then, the unification of tl anti t2 is defined as their greatest lower bound or the meet."], "label": "Prov", "citing": "E93-1008", "vector": [6, 0, 1, 0.20801257358446093], "context": ["", "The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.", ""], "marker": "Kogure, 1990", "vector_1": {"backtrack": 1, "copi": 3, "histor": 1, "overlap": 1, "lazi": 2, "part": 1, "structur": 1, "tf": 1, "approach": 1}, "vector_2": [3, 0.29287890159374863, 2, 1, 0, 0]}, {"function": "Neut", "cited": "C94-2154", "provenance": ["THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES"], "label": "Prov", "citing": "J99-1002", "vector": [3, 0, 0, 0.07216878364870323], "context": ["", "there are a number of different constraint languages within feature structure frameworks (e.g., Alshawi et al 1991; Carpenter 1992; Dorre and Eisle 1991; Emele and Zajac 1990; Gerdemann and King 1994; Krieger and Schafer 1994; de Paiva 1993; Smolka 1989).", ""], "marker": "Gerdemann and King 1994", "vector_1": {"emel": 1, "featur": 1, "eg": 1, "eisl": 1, "number": 1, "alshawi": 1, "et": 1, "languag": 1, "differ": 1, "carpent": 1, "gerdemann": 1, "within": 1, "dorr": 1, "de": 1, "al": 1, "framework": 1, "krieger": 1, "king": 1, "constraint": 1, "paiva": 1, "structur": 1, "zajac": 1, "smolka": 1, "schafer": 1}, "vector_2": [5, 0.36723571305637737, 0, 0, 72, 1]}, {"function": "Neut", "cited": "C94-2154", "provenance": ["1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing"], "label": "Prov", "citing": "W97-1506", "vector": [1, 0, 0, 0.08333333333333333], "context": ["", "The practical effect of this is that Troll implements an exhaustive typing strategy which provides the stronger kind of inferencing over descriptions (Gerdemann and King, 1993, 1994) required by standard HPSG theories.", ""], "marker": "Gerdemann and King, 1993, 1994", "vector_1": {"troll": 1, "exhaust": 1, "kind": 1, "practic": 1, "provid": 1, "descript": 1, "stronger": 1, "effect": 1, "standard": 1, "inferenc": 1, "hpsg": 1, "theori": 1, "implement": 1, "type": 1, "requir": 1, "strategi": 1}, "vector_2": [4, 0.0373710764834576, 1, 6, 0, 1]}, {"function": "Pos", "cited": "E03-1020", "provenance": ["To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000)."], "label": "Prov", "citing": "W11-1104", "vector": [2, 0, 0, 0.07453559924999299], "context": ["", "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.", ""], "marker": "Dorow and Widdows, 2003", "vector_1": {"use": 1, "identifi": 1, "subgraph": 1, "graph": 1, "current": 1, "cluster": 1, "statist": 1, "model": 1, "approach": 1, "sensespecif": 1}, "vector_2": [8, 0.08528910449718106, 3, 3, 0, 0]}, {"function": "Neut", "cited": "E03-1020", "provenance": ["Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1."], "label": "Prov", "citing": "D10-1073", "vector": [4, 0, 0, 0.06262242910851494], "context": ["", "Another graph-based method is presented in(Dorow and Widdows, 2003). They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word. Additionally, theyextract second-order co-occurrences.", ""], "marker": "Dorow and Widdows, 2003", "vector_1": {"graphbas": 1, "word": 1, "appear": 1, "disjunct": 1, "anoth": 1, "secondord": 1, "conjunct": 1, "neighbour": 1, "theyextract": 1, "cooccurr": 1, "onlynoun": 1, "addit": 1, "extract": 1, "method": 1, "present": 1, "target": 1}, "vector_2": [7, 0.16890536680240686, 1, 1, 2, 0]}, {"function": "Neut", "cited": "H05-1115", "provenance": ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method."], "label": "Prov", "citing": "N06-1027", "vector": [2, 0, 1, 0.2886751345948129], "context": ["", "and sentence retrieval for question answering (Otterbacher et al., 2005).", ""], "marker": "Otterbacher et al., 2005", "vector_1": {"answer": 1, "sentenc": 1, "retriev": 1, "question": 1}, "vector_2": [1, 0.2136378658245511, 1, 1, 1, 0]}, {"function": "Pos", "cited": "H05-1115", "provenance": ["Using Random Walks for Question-focused Sentence Retrieval In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary."], "label": "Prov", "citing": "P06-1039", "vector": [6, 0, 0, 0.13900960937138318], "context": ["", "Our model is similar in spirit to the random- walk summarization model (Otterbacher et al., 2005).", ""], "marker": "Otterbacher et al., 2005", "vector_1": {"random": 1, "summar": 1, "model": 2, "walk": 1, "similar": 1, "spirit": 1}, "vector_2": [1, 0.9521811782273261, 1, 1, 0, 0]}, {"function": "Neut", "cited": "H05-1115", "provenance": ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method."], "label": "Prov", "citing": "P08-2003", "vector": [3, 0, 0, 0.14433756729740646], "context": ["", "A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).", ""], "marker": "Otterbacher et al., 2005", "vector_1": {"topic": 1, "sensit": 1, "lexrank": 1, "propos": 1}, "vector_2": [3, 0.17362862547288777, 1, 2, 2, 0]}, {"function": "Neut", "cited": "H05-1115", "provenance": ["Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method."], "label": "Prov", "citing": "P08-2003", "vector": [6, 0, 2, 0.3651483716701107], "context": ["", "To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).", ""], "marker": "Otterbacher et al., 2005", "vector_1": {"topicsensit": 1, "appli": 1, "lexrank": 2, "version": 1, "context": 1, "queryfocus": 1, "propos": 1}, "vector_2": [3, 0.3230611601513241, 1, 2, 2, 0]}, {"function": "Neut", "cited": "H89-2014", "provenance": ["The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.", "A stochastic method for assigning part-of-speech categories to unrestricted English text has been described."], "label": "Prov", "citing": "J93-1001", "vector": [4, 0, 1, 0.0716114874039433], "context": ["", "the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mer cer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al", ""], "marker": "1989", "vector_1": {"almost": 1, "al": 1, "empir": 1, "church": 1, "et": 1, "contemporari": 1, "atwel": 1, "hindl": 1, "program": 1, "merialdo": 1, "approach": 1, "leech": 2, "kupiec": 1, "garsid": 2, "jelinek": 1, "deros": 1, "cer": 1, "partofspeech": 1, "mer": 1, "bahl": 1, "sampson": 1, "adopt": 1, "ayuso": 1, "deroualt": 1}, "vector_2": [4, 0.327623323902079, 9, 0, 5, 1]}, {"function": "Neut", "cited": "J00-3003", "provenance": ["For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.", "Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech."], "label": "Prov", "citing": "W12-1634", "vector": [5, 0, 0, 0.09656090991705352], "context": ["", "To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).", ""], "marker": "Stolcke et al ... 2000", "vector_1": {"major": 1, "spoken": 1, "work": 1, "dialogu": 2, "act": 1, "date": 1, "model": 1, "address": 1}, "vector_2": [12, 0.04749810337434443, 6, 0, 2, 0]}, {"function": "Neut", "cited": "J00-3003", "provenance": ["The computation of likelihoods P(EIU ) depends on the types of evidence used.", "Prosodic features-Evidence is given by the acoustic features F capturing various aspects of pitch, duration, energy, etc., of the speech signal; the associated likelihoods are P(F I U)."], "label": "Prov", "citing": "W12-1634", "vector": [3, 0, 1, 0.0890870806374748], "context": ["", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", ""], "marker": "Stolcke et al., 2000", "vector_1": {"remain": 1, "dialogu": 1, "express": 1, "prosod": 1, "previou": 1, "classif": 1, "automat": 1, "research": 1, "nonverb": 1, "cue": 2, "facial": 1, "act": 1, "type": 1, "unexplor": 1, "leverag": 1}, "vector_2": [12, 0.07477652802058252, 3, 6, 1, 0]}, {"function": "Pos", "cited": "J00-3003", "provenance": ["Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm in the legal department."], "label": "Prov", "citing": "W01-1627", "vector": [2, 0, 0, 0.02405626121623441], "context": ["", "Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.", ""], "marker": "2000", "vector_1": {"remain": 1, "dialogu": 1, "express": 1, "prosod": 1, "previou": 1, "classif": 1, "automat": 1, "research": 1, "nonverb": 1, "cue": 2, "facial": 1, "act": 1, "type": 1, "unexplor": 1, "leverag": 1}, "vector_2": [1, 0.6141081719086301, 3, 0, 0, 1]}, {"function": "Neut", "cited": "J00-3003", "provenance": ["Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand.", "In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words."], "label": "Prov", "citing": "W12-1616", "vector": [5, 0, 1, 0.13068205256070964], "context": ["", "By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).", ""], "marker": "Stolcke et al., 2000", "vector_1": {"utterancesdur": 1, "dialogu": 1, "byspoken": 1, "areb": 1, "convers": 1, "level": 1, "provid": 1, "enrich": 1, "use": 1, "repres": 1, "inform": 1, "intent": 1, "human": 1, "act": 1, "word": 1, "label": 1, "higher": 1}, "vector_2": [12, 0.050686897683001844, 1, 4, 7, 0]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?", "We will show that in both cases the estimated probability is tight."], "label": "Prov", "citing": "P06-1109", "vector": [3, 0, 1, 0.05063696835418333], "context": ["", "a treebank PCFG whose simple relativefrequency estimator corresponds to maximumlikelihood (Chi and Geman 1998), and which weshall refer to as \"MLPCFG\".", ""], "marker": "Chi and Geman 1998", "vector_1": {"whose": 1, "mlpcfg": 1, "chi": 1, "treebank": 1, "correspond": 1, "estim": 1, "maximumlikelihood": 1, "geman": 1, "relativefrequ": 1, "simpl": 1, "weshal": 1, "pcfg": 1, "refer": 1}, "vector_2": [8, 0.7864881071357186, 0, 0, 0, 0]}, {"function": "Pos", "cited": "J98-2005", "provenance": ["Estimation of Probabilistic Context-Free Grammars"], "label": "Prov", "citing": "P13-1102", "vector": [1, 0, 0, 0.0], "context": ["", "we follow Chi and Geman (1998)in calling them non-tight to avoid confusionwith the consistency of statistical estimators).", ""], "marker": "1998", "vector_1": {"consist": 1, "geman": 1, "avoid": 1, "statist": 1, "estim": 1, "call": 1, "confusionwith": 1, "chi": 1, "follow": 1, "nontight": 1}, "vector_2": [15, 0.10018854430791235, 1, 0, 1, 0]}, {"function": "Neut", "cited": "J98-2005", "provenance": ["If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?", "We will show that in both cases the estimated probability is tight."], "label": "Prov", "citing": "P13-1102", "vector": [9, 0, 0, 0.12006004503753286], "context": ["", "Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).", ""], "marker": "1998", "vector_1": {"show": 1, "termin": 1, "andth": 1, "es": 1, "alway": 1, "chi": 1, "question": 1, "tight": 1, "estim": 1, "ofyield": 1, "supervisedcas": 1, "string": 1, "geman": 1, "timat": 1, "input": 2, "likelihood": 1, "case": 1, "consist": 2, "ml": 2, "unsupervis": 1, "tree": 1, "maximum": 1, "pars": 1, "studi": 1}, "vector_2": [15, 0.12700734672648073, 1, 0, 1, 0]}, {"function": "Neut", "cited": "N01-1011", "provenance": ["This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation."], "label": "Prov", "citing": "N03-3004", "vector": [5, 0, 2, 0.31426968052735443], "context": ["", "Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001).", ""], "marker": "Pedersen, 2001", "vector_1": {"supervis": 1, "shown": 1, "featur": 1, "bigram": 1, "success": 1, "disambigu": 1, "sens": 1, "word": 1, "recent": 1}, "vector_2": [2, 0.2857482561826252, 1, 1, 3, 0]}, {"function": "Neut", "cited": "N01-1011", "provenance": ["However, (Cressie and Read, 1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other.", "Unfortunately it is usually not clear which test is most appropriate for a particular sample of data."], "label": "Prov", "citing": "J02-2003", "vector": [19, 1, 12, 0.2862512870383358], "context": ["", "In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.", ""], "marker": "2001", "vector_1": {"case": 1, "task": 1, "bigram": 1, "whether": 1, "pedersen": 1, "reliabl": 1, "question": 1, "argu": 1, "one": 1, "acquisit": 1, "pearson": 1, "read": 1, "statist": 3, "prefer": 1, "loglikelihood": 1, "cite": 1, "cressi": 1, "addit": 1}, "vector_2": [1, 0.4586071462783792, 2, 0, 4, 1]}, {"function": "Error", "cited": "P98-1081", "provenance": ["Table 2: Accuracy of individual taggers and combination methods."], "label": "Prov", "citing": "W01-0712", "vector": [2, 0, 1, 0.2721655269759087], "context": ["", "Nine combination methods were originally suggested by Van Halteren et al", ""], "marker": "1998", "vector_1": {"origin": 1, "van": 1, "et": 1, "suggest": 1, "al": 1, "combin": 1, "halteren": 1, "nine": 1, "method": 1}, "vector_2": [3, 0.5932360693469948, 0, 0, 1, 1]}, {"function": "Pos", "cited": "P98-1081", "provenance": ["A next step is to examine them in pairs.", "We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx."], "label": "Prov", "citing": "W01-0712", "vector": [4, 0, 1, 0.0512989176042577], "context": ["", "And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).", ""], "marker": "van Halteren et al., 1998", "vector_1": {"use": 1, "weight": 1, "classic": 3, "predict": 1, "tagpair": 1, "nalli": 1, "base": 1, "pair": 2, "probabl": 1}, "vector_2": [3, 0.6166982217767383, 1, 3, 3, 0]}, {"function": "CoCo", "cited": "P98-1081", "provenance": ["In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.", "We accept that we are measuring quality in relation to a specific tagging"], "label": "Prov", "citing": "W01-0712", "vector": [1, 0, 0, 0.0], "context": ["", "Like Van Halteren et al (1998), we evaluated two features combinations. Like Van Halteren et al (1998), we evaluated two features combinations", ""], "marker": "1998", "vector_1": {"featur": 2, "van": 2, "like": 2, "evalu": 2, "halteren": 2, "al": 2, "two": 2, "combin": 2, "et": 2}, "vector_2": [3, 0.6319560455878531, 2, 0, 1, 1]}, {"function": "Pos", "cited": "P98-1081", "provenance": ["However, it appears more useful to give more weight to taggers which have proved their quality.", "This can be general quality, e.g.", "each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g.", "each tagger votes its precision on the suggested tag (Tag- Precision)."], "label": "Prov", "citing": "W05-1518", "vector": [4, 0, 0, 0.0], "context": ["", "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results.", ""], "marker": "van Halteren et ... al., 1998", "vector_1": {"case": 1, "investig": 1, "techniqu": 1, "success": 1, "appli": 1, "previou": 1, "tag": 2, "signific": 1, "abl": 1, "achiev": 1, "part": 1, "speech": 1, "result": 1, "improv": 1, "combin": 1, "best": 1}, "vector_2": [7, 0.06969867642917488, 3, 0, 6, 0]}, {"function": "Neut", "cited": "P98-1081", "provenance": ["The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.", "Also of note is the improvement yielded by the best combination.", "The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz."], "label": "Prov", "citing": "W05-1518", "vector": [6, 0, 0, 0.03513641844631533], "context": ["", "Van Halteren et al (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy.", ""], "marker": "1998", "vector_1": {"van": 1, "weight": 1, "halteren": 1, "gener": 1, "parser": 1, "al": 1, "classifi": 2, "number": 1, "respect": 1, "vote": 2, "et": 1, "totprecis": 1, "accuraci": 1, "approach": 1, "method": 1, "higher": 1}, "vector_2": [7, 0.43781094527363185, 1, 0, 6, 0]}, {"function": "CoCo", "cited": "P98-1081", "provenance": ["However, it appears more useful to give more weight to taggers which have proved their quality.", "This can be general quality, e.g.", "each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g.", "each tagger votes its precision on the suggested tag (Tag- Precision)."], "label": "Prov", "citing": "W05-1518", "vector": [1, 0, 0, 0.0], "context": ["", "Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.", ""], "marker": "van Halteren et al., 1998", "vector_1": {"decisiontreebas": 1, "ran": 1, "memorybas": 1, "two": 1, "classifi": 1, "parallel": 1, "experi": 1, "stack": 1}, "vector_2": [7, 0.4640007509621703, 1, 4, 1, 0]}, {"function": "Neut", "cited": "P98-1081", "provenance": ["The first choice for this is to use a Memory- Based second level learner.", "To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material."], "label": "Prov", "citing": "W05-1518", "vector": [3, 0, 0, 0.07352146220938077], "context": ["", "In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.", ""], "marker": "van Halteren et al., 1998", "vector_1": {"vote": 1, "scheme": 1, "totprecis": 1, "experi": 1, "use": 1}, "vector_2": [7, 0.561391157420445, 1, 4, 1, 0]}, {"function": "Pos", "cited": "P98-1081", "provenance": ["A next step is to examine them in pairs.", "We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx."], "label": "Prov", "citing": "W00-0733", "vector": [7, 0, 0, 0.0899842541331695], "context": ["", "The most advanced voting method ex amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag Pair, Van Halteren et al., (1998)).", ""], "marker": "1998", "vector_1": {"amin": 1, "van": 1, "often": 1, "weight": 1, "advanc": 1, "halteren": 1, "al": 1, "classifi": 1, "assign": 1, "pair": 3, "tag": 2, "ex": 1, "base": 1, "et": 1, "vote": 1, "output": 1, "tune": 1, "data": 1, "method": 1, "valu": 1, "appear": 1}, "vector_2": [2, 0.3231789451870273, 1, 0, 1, 1]}, {"function": "Pos", "cited": "P98-1081", "provenance": ["Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers.", "However, it appears more useful to give more weight to taggers which have proved their quality."], "label": "Prov", "citing": "W00-0733", "vector": [7, 0, 1, 0.06388765649999399], "context": ["", "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.", ""], "marker": "Van Halteren et al., 1998", "vector_1": {"system": 1, "differ": 1, "assign": 1, "weight": 2, "evalu": 1, "individu": 1, "use": 1, "chunker": 1, "socal": 1, "five": 2, "tag": 1, "combin": 1, "determin": 1, "output": 3, "vote": 1, "nine": 1, "method": 2, "probabl": 1}, "vector_2": [2, 0.258729665319656, 1, 3, 5, 0]}, {"function": "Pos", "cited": "P98-1081", "provenance": ["The second stage can be provided with the first level outputs, and with additional information, e.g.", "about the original input pattern."], "label": "Prov", "citing": "W00-0733", "vector": [4, 0, 1, 0.25000000000000006], "context": ["", "For this purpose we have used the part-of-speech tag of the cur rent word as compressed representation of the first stage input (Van Halteren et al., 1998).", ""], "marker": "Van Halteren et al., 1998", "vector_1": {"represent": 1, "use": 1, "word": 1, "cur": 1, "compress": 1, "tag": 1, "rent": 1, "partofspeech": 1, "input": 1, "purpos": 1, "stage": 1, "first": 1}, "vector_2": [2, 0.3979898456118537, 1, 3, 5, 0]}, {"function": "Weak", "cited": "P98-1081", "provenance": ["This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.", "The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.", "Also of note is the improvement yielded by the best combination.", "The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.", "Regardless of such closer investigation, we feel that our results are encouraging enough to extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and/or languages."], "label": "Prov", "citing": "J01-2002", "vector": [14, 1, 4, 0.08576900278702357], "context": ["", "First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners. This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.", ""], "marker": "van Halteren, Zavrel, and Daelemans 1998", "vector_1": {"individu": 1, "restrict": 1, "rate": 1, "tag": 1, "tagger": 1, "in": 1, "daeleman": 3, "best": 2, "van": 3, "confirm": 1, "perform": 1, "latter": 1, "valid": 1, "languag": 1, "tagset": 1, "basic": 1, "limit": 1, "experi": 2, "approach": 1, "singl": 2, "importantli": 1, "hypothesi": 1, "halteren": 3, "previou": 1, "zavrel": 3, "extens": 1, "wu": 1, "train": 1, "half": 1, "data": 1, "demonstr": 1, "brill": 1, "lower": 1, "howev": 1, "work": 1, "us": 1, "amount": 1, "combin": 2, "unabl": 1, "error": 1, "led": 1, "unequivoc": 1, "first": 1}, "vector_2": [3, 0.06565294002479932, 0, 0, 8, 1]}, {"function": "CoCo", "cited": "P98-1081", "provenance": ["The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.", "All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune.", "This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags."], "label": "Prov", "citing": "J01-2002", "vector": [11, 0, 0, 0.16598500055174645], "context": ["", "Compare this to the \"tune\" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K. tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.", ""], "marker": "1998", "vector_1": {"set": 1, "less": 1, "lack": 1, "four": 1, "tagger": 1, "daeleman": 1, "use": 1, "van": 1, "compar": 1, "perform": 1, "suspect": 1, "rel": 1, "main": 1, "disagr": 1, "halteren": 1, "agreement": 1, "zavrel": 1, "sophist": 1, "reason": 1, "train": 1, "tune": 1, "consist": 1, "k": 2, "resolv": 1, "yield": 1, "token": 2, "combin": 1, "materi": 1}, "vector_2": [3, 0.31463595031434227, 1, 0, 8, 1]}, {"function": "Neut", "cited": "P98-1081", "provenance": ["Improving Data Driven Wordclass Tagging by System Combination Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.", "After comparison, their outputs are combined using several voting strategies and second stage classifiers.", "Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement."], "label": "Prov", "citing": "J01-2002", "vector": [16, 0, 2, 0.16770509831248423], "context": ["", "For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998). In both approaches, different tagger gen erators were applied to the same training data and their predictions combined using different combination methods, including stacking. As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.", ""], "marker": "1998", "vector_1": {"predict": 1, "appli": 2, "brill": 1, "signific": 1, "tag": 1, "tagger": 2, "daeleman": 2, "stack": 1, "differ": 3, "van": 2, "make": 1, "accuraci": 1, "includ": 1, "approach": 1, "method": 2, "halteren": 2, "use": 1, "zavrel": 2, "wu": 1, "train": 1, "partofspeech": 1, "increas": 1, "data": 1, "gen": 1, "demonstr": 1, "comparison": 1, "wsj": 1, "well": 1, "erat": 1, "easier": 1, "combin": 3, "output": 1, "first": 1}, "vector_2": [3, 0.9090909090909091, 3, 0, 8, 1]}, {"function": "Pos", "cited": "P98-1081", "provenance": ["A next step is to examine them in pairs.", "We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.", "The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0.", "is usually called stacking (Wolpert 1992).", "Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair."], "label": "Prov", "citing": "J01-2002", "vector": [25, 2, 18, 0.29193710406057116], "context": ["", "One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele mans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.", ""], "marker": "van Halteren, Zavrel, and Daelemans 1998", "vector_1": {"situat": 2, "one": 3, "tag": 4, "tagger": 3, "classifi": 1, "vote": 1, "best": 1, "probabl": 1, "van": 1, "suggest": 2, "tagpair": 1, "dael": 1, "compon": 1, "also": 1, "estim": 1, "select": 1, "method": 2, "halteren": 1, "tagx": 1, "zavrel": 1, "although": 1, "necessarili": 1, "stack": 1, "present": 1, "man": 1, "actual": 1, "look": 1, "paper": 1, "combin": 1, "variant": 1, "fact": 1}, "vector_2": [3, 0.17130022406404316, 0, 0, 8, 1]}, {"function": "CoCo", "cited": "P98-1081", "provenance": ["When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).", "Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair."], "label": "Prov", "citing": "J01-2002", "vector": [6, 0, 1, 0.16971105832553268], "context": ["", "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed.", ""], "marker": "1998", "vector_1": {"mbl": 2, "undergon": 1, "result": 1, "daeleman": 1, "current": 1, "use": 1, "van": 1, "tagpair": 2, "system": 1, "accuraci": 1, "better": 1, "role": 1, "rel": 1, "import": 1, "experi": 1, "halteren": 1, "zavrel": 1, "stack": 1, "revers": 1, "well": 1, "chang": 1, "significantli": 1}, "vector_2": [3, 0.7876394961822097, 1, 0, 8, 1]}, {"function": "Pos", "cited": "P98-1081", "provenance": ["The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986)."], "label": "Prov", "citing": "J01-2002", "vector": [5, 1, 2, 0.2651650429449553], "context": ["", "The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.", ""], "marker": "van Halteren, Zavrel, and Daelemans 1998", "vector_1": {"lob": 1, "corpu": 1, "johansson": 1, "good": 1, "van": 1, "prove": 1, "ground": 1, "halteren": 1, "earlier": 1, "well": 1, "use": 1, "zavrel": 1, "test": 1, "daeleman": 1, "experi": 1, "first": 1}, "vector_2": [3, 0.3024429506841567, 0, 0, 8, 1]}, {"function": "Weak", "cited": "P98-1081", "provenance": ["Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.", "Table 2: Accuracy of individual taggers and combination methods."], "label": "Prov", "citing": "J01-2002", "vector": [6, 0, 1, 0.1690308509457033], "context": ["", "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im plementation of HMM's, which turned out to have the worst accuracy of the four competing methods.", ""], "marker": "1998", "vector_1": {"worst": 1, "use": 1, "van": 1, "straightforward": 1, "turn": 1, "halteren": 1, "plement": 1, "zavrel": 1, "accuraci": 1, "four": 1, "hmm": 1, "im": 1, "daeleman": 1, "compet": 1, "method": 1}, "vector_2": [3, 0.5119210772477104, 1, 0, 8, 1]}, {"function": "Neut", "cited": "P98-1081", "provenance": ["The first choice for this is to use a Memory- Based second level learner.", "Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.", "To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.", "1 Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse."], "label": "Prov", "citing": "J01-2002", "vector": [6, 0, 1, 0.0899842541331695], "context": ["", "With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word", ""], "marker": "van Halteren,Zavrel, and Daelemans 1998", "vector_1": {"lob": 1, "mbl": 2, "set": 1, "van": 1, "ad": 2, "halteren": 1, "k": 1, "tree": 1, "zavrel": 1, "significantli": 1, "singl": 1, "context": 1, "daeleman": 1, "word": 1, "degrad": 2, "tune": 1, "decis": 1}, "vector_2": [3, 0.6405186103678566, 0, 0, 8, 1]}, {"function": "Neut", "cited": "X96-1048", "provenance": ["The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time."], "label": "Prov", "citing": "J00-4003", "vector": [6, 0, 1, 0.14770978917519928], "context": ["", "This so-called named entity recognition task has received considerable attention recently (Mani and MacMillan 1996; McDonald 1996; Paik et al 1996; Bike! et al 1997; Palmer and Day 1997; Wacholder and Ravin 1997; Mikheev, Moens, and Grover 1999) and was one of the tasks evaluated in the Sixth and Seventh Message Under standing Conferences. In MUC6, 15 different systems participated in the competition (Sundheim 1995). This so-called named entity recognition task has received considerable attention recently ... and was one of the tasks evaluated in the Sixth and Seventh Message Understanding Conferences. In MUC-6, 15 different systems participated in the competition (Sundheim 1995)", ""], "marker": "Sundheim 1995", "vector_1": {"consider": 2, "evalu": 2, "competit": 2, "understand": 1, "al": 2, "macmillan": 1, "et": 2, "entiti": 2, "one": 2, "sundheim": 2, "differ": 2, "mcdonald": 1, "particip": 2, "system": 2, "messag": 2, "recognit": 2, "bike": 1, "muc": 2, "moen": 1, "paik": 1, "sixth": 2, "mikheev": 1, "palmer": 1, "seventh": 2, "confer": 2, "socal": 2, "attent": 2, "ravin": 1, "day": 1, "recent": 2, "task": 4, "name": 2, "wachold": 1, "receiv": 2, "grover": 1, "stand": 1, "mani": 1}, "vector_2": [5, 0.4119513724660567, 0, 0, 62, 1]}, {"function": "Neut", "cited": "X96-1048", "provenance": ["The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.", "In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus."], "label": "Prov", "citing": "J00-4003", "vector": [14, 0, 1, 0.1107273064716533], "context": ["", "As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic  interpretation as well, for example, at the Sixth and Seventh Message Understa nd ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.", ""], "marker": "Sundheim 1995", "vector_1": {"semant": 1, "199": 1, "evalu": 3, "definit": 1, "socal": 1, "result": 1, "ing": 1, "languag": 1, "sundheim": 1, "engin": 1, "techniqu": 1, "area": 1, "nd": 1, "system": 1, "messag": 1, "corefer": 1, "also": 1, "includ": 1, "muc": 2, "commonplac": 1, "sixth": 1, "resolut": 1, "seventh": 1, "confer": 1, "subtask": 1, "pars": 1, "quantit": 2, "interpret": 1, "task": 1, "well": 1, "chinchor": 1, "exampl": 1, "descript": 1, "understa": 1, "propos": 1}, "vector_2": [5, 0.025265766548548202, 0, 0, 62, 1]}, {"function": "Neut", "cited": "X96-1048", "provenance": ["In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted.", "There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc..", "Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns."], "label": "Prov", "citing": "J00-4003", "vector": [7, 0, 1, 0.021281413268968714], "context": ["", "The third main result was that we found very little agreement between our sub jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for. example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.", ""], "marker": "Sundheim 1995", "vector_1": {"identifi": 1, "second": 1, "result": 2, "199": 1, "sundheim": 1, "sub": 1, "due": 1, "littl": 1, "low": 1, "muc": 1, "anaphor": 1, "experi": 2, "difficulti": 1, "previou": 1, "relat": 2, "agreement": 3, "ident": 1, "corefercnc": 1, "main": 1, "attempt": 1, "ject": 1, "third": 1, "level": 1, "drop": 1, "descript": 1, "annot": 3, "bride": 1, "exampl": 1, "found": 1}, "vector_2": [5, 0.10137284383429163, 0, 0, 62, 1]}, {"function": "Pos", "cited": "X96-1048", "provenance": ["5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision.."], "label": "Prov", "citing": "P06-1059", "vector": [6, 1, 0, 0.07715167498104596], "context": ["", "For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).", ""], "marker": "Sundheim, 1995", "vector_1": {"tri": 1, "accuraci": 1, "bioner": 1, "identifi": 1, "system": 1, "organ": 1, "perform": 1, "wherea": 1, "gener": 1, "share": 1, "jnlpba": 1, "task": 1, "person": 1, "exampl": 1, "best": 2, "muc": 1, "entiti": 1, "cole": 1, "fscore": 1, "name": 2}, "vector_2": [11, 0.07684405177727553, 3, 1, 0, 0]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["The German finite verbs 'bin' (second example) and 'konnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions)."], "label": "Non-Prov", "citing": "J03-1005", "vector": [2, 0, 0, 0.0], "context": ["", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", ""], "marker": "2000", "vector_1": {"search": 1, "ibm": 1, "tillmann": 1, "preliminari": 1, "work": 1, "dpbase": 1, "publish": 1, "articl": 1, "beam": 1, "version": 1, "decod": 1, "translat": 1, "ney": 1, "model": 1, "present": 2}, "vector_2": [3, 0.15982922481236303, 1, 0, 5, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["The approach recursively evaluates a quantity Q(C; j), where C is the set of already visited cities and sj is the last visited city."], "label": "Non-Prov", "citing": "J03-1005", "vector": [4, 0, 0, 0.0], "context": ["", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", ""], "marker": "2000", "vector_1": {"search": 1, "ibm": 1, "tillmann": 1, "preliminari": 1, "work": 1, "dpbase": 1, "publish": 1, "articl": 1, "beam": 1, "version": 1, "decod": 1, "translat": 1, "ney": 1, "model": 1, "present": 2}, "vector_2": [3, 0.15982922481236303, 1, 0, 5, 1]}, {"function": "Neut", "cited": "C00-2123", "provenance": ["the number of permutations carried out for the word reordering is given."], "label": "Non-Prov", "citing": "J03-1005", "vector": [3, 0, 1, 0.0], "context": ["", "This article will present a DP-based beam search decoder for the IBM4 translation model. A preliminary version of the work presented here was published in Tillmann and Ney (2000).", ""], "marker": "2000", "vector_1": {"search": 1, "ibm": 1, "tillmann": 1, "preliminari": 1, "work": 1, "dpbase": 1, "publish": 1, "articl": 1, "beam": 1, "version": 1, "decod": 1, "translat": 1, "ney": 1, "model": 1, "present": 2}, "vector_2": [3, 0.15982922481236303, 1, 0, 5, 1]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["We apply a beam search concept as in speech recognition."], "label": "Non-Prov", "citing": "P01-1027", "vector": [2, 0, 0, 0.0], "context": ["", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"sort": 1, "use": 2, "entropi": 1, "describ": 1, "hypothesi": 2, "provid": 1, "top": 1, "list": 1, "system": 1, "maximum": 1, "accord": 1, "score": 1, "translat": 1, "new": 1, "model": 1, "rescor": 1}, "vector_2": [1, 0.8941605839416058, 1, 1, 4, 0]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training."], "label": "Non-Prov", "citing": "P01-1027", "vector": [5, 0, 2, 0.07071067811865474], "context": ["", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"sort": 1, "use": 2, "entropi": 1, "describ": 1, "hypothesi": 2, "provid": 1, "top": 1, "list": 1, "system": 1, "maximum": 1, "accord": 1, "score": 1, "translat": 1, "new": 1, "model": 1, "rescor": 1}, "vector_2": [1, 0.8941605839416058, 1, 1, 4, 0]}, {"function": "Pos", "cited": "C00-2123", "provenance": ["In general, m; l; l0 6= fl1; l2; l3g and in line umber 3 and 4, l0 must be chosen not to violate the above reordering restriction."], "label": "Non-Prov", "citing": "P01-1027", "vector": [4, 0, 0, 0.0], "context": ["", "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.", ""], "marker": "Tillmann and Ney, 2000", "vector_1": {"sort": 1, "use": 2, "entropi": 1, "describ": 1, "hypothesi": 2, "provid": 1, "top": 1, "list": 1, "system": 1, "maximum": 1, "accord": 1, "score": 1, "translat": 1, "new": 1, "model": 1, "rescor": 1}, "vector_2": [1, 0.8941605839416058, 1, 1, 4, 0]}, {"function": "Pos", "cited": "C04-1089", "provenance": ["Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.", "We fully implemented our method and tested it on ChineseEnglish comparable corpora."], "label": "Non-Prov", "citing": "D12-1003", "vector": [4, 0, 0, 0.1745743121887939], "context": ["", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", ""], "marker": "2004", "vector_1": {"measur": 1, "use": 2, "ratio": 1, "variou": 1, "etc": 1, "heterogen": 1, "model": 1, "mutual": 1, "pmi": 1, "tfidf": 1, "inform": 1, "correl": 1, "pointwis": 1, "context": 2, "shao": 1, "repres": 1, "ng": 1, "loglikelihood": 1, "languag": 1}, "vector_2": [8, 0.15703845948685122, 6, 0, 0, 0]}, {"function": "Pos", "cited": "C04-1089", "provenance": ["On the other hand, using our method of combining both sources of information and setting M = , 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except ,,) have their correct English translations at rank one position.", "If M = 10, 15 Chinese words (i.e., the first 19 Chinese words in Table 3 except ,,,) have their correct English translations at rank one position."], "label": "Non-Prov", "citing": "D12-1003", "vector": [4, 0, 0, 0.04910286161402363], "context": ["", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", ""], "marker": "2004", "vector_1": {"measur": 1, "use": 2, "ratio": 1, "variou": 1, "etc": 1, "heterogen": 1, "model": 1, "mutual": 1, "pmi": 1, "tfidf": 1, "inform": 1, "correl": 1, "pointwis": 1, "context": 2, "shao": 1, "repres": 1, "ng": 1, "loglikelihood": 1, "languag": 1}, "vector_2": [8, 0.15703845948685122, 6, 0, 0, 0]}, {"function": "Pos", "cited": "C04-1089", "provenance": ["To avoid accidentally using parallel texts, we did not use the texts of Xinhua News Agency them English translation candidate words.", "For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates."], "label": "Non-Prov", "citing": "D12-1003", "vector": [1, 0, 0, 0.028903665650804], "context": ["", "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.", ""], "marker": "2004", "vector_1": {"measur": 1, "use": 2, "ratio": 1, "variou": 1, "etc": 1, "heterogen": 1, "model": 1, "mutual": 1, "pmi": 1, "tfidf": 1, "inform": 1, "correl": 1, "pointwis": 1, "context": 2, "shao": 1, "repres": 1, "ng": 1, "loglikelihood": 1, "languag": 1}, "vector_2": [8, 0.15703845948685122, 6, 0, 0, 0]}, {"function": "CoCo", "cited": "C04-1089", "provenance": ["Both were correct."], "label": "Non-Prov", "citing": "D12-1003", "vector": [0, 0, 0, 0.0], "context": ["", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (Dejean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"multilingu": 1, "concept": 1, "comput": 1, "transliter": 1, "variou": 1, "align": 1, "gener": 1, "obtain": 1, "clue": 1, "inform": 2, "thesauru": 1, "consid": 1, "cooccurr": 1, "model": 1, "document": 1, "similar": 1, "class": 1}, "vector_2": [8, 0.2056862431322345, 3, 2, 0, 0]}, {"function": "CoCo", "cited": "C04-1089", "provenance": ["We use backoff and linear interpolation for probability estimation: P(tc | Tc (C (e))) =   Pml (tc | Tc (C (e))) + (1  )  Pml (tc ) that the ith pinyin syllable maps to in the particular alignment a. Since most Chinese characters have only one pronunciation and hence one pinyin form, we assume that Chinese character-to-pinyin mapping is one-to-one to simplify the problem."], "label": "Non-Prov", "citing": "D12-1003", "vector": [4, 0, 0, 0.0], "context": ["", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (Dejean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"multilingu": 1, "concept": 1, "comput": 1, "transliter": 1, "variou": 1, "align": 1, "gener": 1, "obtain": 1, "clue": 1, "inform": 2, "thesauru": 1, "consid": 1, "cooccurr": 1, "model": 1, "document": 1, "similar": 1, "class": 1}, "vector_2": [8, 0.2056862431322345, 3, 2, 0, 0]}, {"function": "CoCo", "cited": "C04-1089", "provenance": ["If an English word e is the translation of a Chinese word c , they will have similar contexts."], "label": "Non-Prov", "citing": "D12-1003", "vector": [3, 0, 0, 0.0], "context": ["", "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (Dejean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"multilingu": 1, "concept": 1, "comput": 1, "transliter": 1, "variou": 1, "align": 1, "gener": 1, "obtain": 1, "clue": 1, "inform": 2, "thesauru": 1, "consid": 1, "cooccurr": 1, "model": 1, "document": 1, "similar": 1, "class": 1}, "vector_2": [8, 0.2056862431322345, 3, 2, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["So we estimate that English translations are present in the English part of the corpus for Table 2."], "label": "Non-Prov", "citing": "P06-1011", "vector": [3, 0, 1, 0.10540925533894598], "context": ["", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"involv": 1, "corpora": 1, "word": 1, "compar": 1, "work": 1, "focus": 1, "much": 1, "translat": 1, "extract": 1}, "vector_2": [2, 0.8435580204778157, 7, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["We evaluated our method on each of the 12 half- month periods."], "label": "Non-Prov", "citing": "P06-1011", "vector": [3, 0, 1, 0.0], "context": ["", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"involv": 1, "corpora": 1, "word": 1, "compar": 1, "work": 1, "focus": 1, "much": 1, "translat": 1, "extract": 1}, "vector_2": [2, 0.8435580204778157, 7, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["Rank of correct translation for period Dec 01  Dec 15 and Dec 16  Dec 31."], "label": "Non-Prov", "citing": "P06-1011", "vector": [1, 0, 0, 0.0], "context": ["", "Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"involv": 1, "corpora": 1, "word": 1, "compar": 1, "work": 1, "focus": 1, "much": 1, "translat": 1, "extract": 1}, "vector_2": [2, 0.8435580204778157, 7, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both."], "label": "Non-Prov", "citing": "P13-1062", "vector": [7, 0, 0, 0.02017425108896008], "context": ["", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al (2011) and Shao and Ng (2004).", ""], "marker": "2004", "vector_1": {"subset": 1, "vb": 1, "edg": 1, "evalu": 1, "process": 1, "al": 1, "eb": 1, "bipartit": 3, "gb": 1, "year": 1, "et": 1, "ng": 1, "find": 1, "use": 1, "weight": 1, "defin": 1, "graph": 1, "ts": 1, "publish": 1, "articl": 1, "also": 1, "shao": 1, "match": 2, "chines": 1, "xinhua": 1, "entir": 1, "kim": 1, "news": 3, "c": 2, "e": 2, "maximum": 2, "sj": 1, "si": 1, "english": 1, "tion": 1}, "vector_2": [9, 0.7494066949787659, 3, 0, 2, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["The corpus of the period Jan to Jun 1995 was just used to determine if a Chinese word c from Jul to Dec 1995 was new, i.e., not occurring from Jan to Jun 1995."], "label": "Non-Prov", "citing": "P13-1062", "vector": [6, 0, 0, 0.1126872339638022], "context": ["", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al (2011) and Shao and Ng (2004).", ""], "marker": "2004", "vector_1": {"subset": 1, "vb": 1, "edg": 1, "evalu": 1, "process": 1, "al": 1, "eb": 1, "bipartit": 3, "gb": 1, "year": 1, "et": 1, "ng": 1, "find": 1, "use": 1, "weight": 1, "defin": 1, "graph": 1, "ts": 1, "publish": 1, "articl": 1, "also": 1, "shao": 1, "match": 2, "chines": 1, "xinhua": 1, "entir": 1, "kim": 1, "news": 3, "c": 2, "e": 2, "maximum": 2, "sj": 1, "si": 1, "english": 1, "tion": 1}, "vector_2": [9, 0.7494066949787659, 3, 0, 2, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": [" is set to 0.6 in our experiments."], "label": "Non-Prov", "citing": "P13-1062", "vector": [1, 0, 0, 0.0], "context": ["", "Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al (2011) and Shao and Ng (2004).", ""], "marker": "2004", "vector_1": {"subset": 1, "vb": 1, "edg": 1, "evalu": 1, "process": 1, "al": 1, "eb": 1, "bipartit": 3, "gb": 1, "year": 1, "et": 1, "ng": 1, "find": 1, "use": 1, "weight": 1, "defin": 1, "graph": 1, "ts": 1, "publish": 1, "articl": 1, "also": 1, "shao": 1, "match": 2, "chines": 1, "xinhua": 1, "entir": 1, "kim": 1, "news": 3, "c": 2, "e": 2, "maximum": 2, "sj": 1, "si": 1, "english": 1, "tion": 1}, "vector_2": [9, 0.7494066949787659, 3, 0, 2, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["Hence, our method of using both sources of information outperforms using either information source alone."], "label": "Non-Prov", "citing": "W11-1215", "vector": [0, 0, 0, 0.0], "context": ["", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "word": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "transliter": 1, "new": 1, "rescor": 1, "recent": 1}, "vector_2": [7, 0.18829544618553667, 9, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": ["Rank of correct translation for period Dec 01  Dec 15 and Dec 16  Dec 31."], "label": "Non-Prov", "citing": "W11-1215", "vector": [0, 0, 0, 0.0], "context": ["", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "word": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "transliter": 1, "new": 1, "rescor": 1, "recent": 1}, "vector_2": [7, 0.18829544618553667, 9, 1, 0, 0]}, {"function": "Neut", "cited": "C04-1089", "provenance": [" t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output."], "label": "Non-Prov", "citing": "W11-1215", "vector": [1, 0, 0, 0.0], "context": ["", "Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).", ""], "marker": "Shao and Ng, 2004", "vector_1": {"use": 1, "word": 1, "name": 1, "compar": 1, "corpora": 1, "mine": 1, "research": 1, "translat": 1, "transliter": 1, "new": 1, "rescor": 1, "recent": 1}, "vector_2": [7, 0.18829544618553667, 9, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["3.1 Corpora.", "For the experiments, we used four newswire corpora, the Los Angeles Times/Washington Post, The New York Times, Reuters and the Wall Street Journal, all published in 1995."], "label": "Non-Prov", "citing": "E09-1025", "vector": [0, 0, 0, 0.0], "context": ["", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"acquir": 1, "number": 1, "automat": 1, "avail": 1, "collect": 1, "ruleparaphras": 1, "infer": 1}, "vector_2": [4, 0.14964451736395953, 2, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["x EG, has agreed to be bought by H x EG, now owned by H x H to acquire EG x Hs agreement to buy EG Three of those phrases are actually paraphrases, but sometime there could be some noise; such as the second phrase above.", "So, we set a threshold that at least two examples are required to build a link."], "label": "Non-Prov", "citing": "E09-1025", "vector": [5, 0, 1, 0.0], "context": ["", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"acquir": 1, "number": 1, "automat": 1, "avail": 1, "collect": 1, "ruleparaphras": 1, "infer": 1}, "vector_2": [4, 0.14964451736395953, 2, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["They first collect the NE instance pairs and contexts, just like our method.", "However, the next step is clearly different."], "label": "Non-Prov", "citing": "E09-1025", "vector": [0, 0, 0, 0.0], "context": ["", "A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"acquir": 1, "number": 1, "automat": 1, "avail": 1, "collect": 1, "ruleparaphras": 1, "infer": 1}, "vector_2": [4, 0.14964451736395953, 2, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["We will describe the evaluation of such clusters in the next subsection.", "3.3 Evaluation Results."], "label": "Non-Prov", "citing": "P09-2063", "vector": [0, 0, 0, 0.0], "context": ["", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", ""], "marker": "Sekine, 2005", "vector_1": {"retriev": 1, "recognit": 1, "pattern": 1, "gener": 1, "inform": 1, "base": 1, "paraphras": 1, "similarli": 1, "improv": 1, "introduc": 1}, "vector_2": [4, 0.10389138957456076, 1, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Once we figure out the important word (e.g. keyword), we believe we can capture the meaning of the phrase by the keyword.", "We used the TF/ITF metric to identify keywords."], "label": "Non-Prov", "citing": "P09-2063", "vector": [1, 0, 0, 0.0], "context": ["", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", ""], "marker": "Sekine, 2005", "vector_1": {"retriev": 1, "recognit": 1, "pattern": 1, "gener": 1, "inform": 1, "base": 1, "paraphras": 1, "similarli": 1, "improv": 1, "introduc": 1}, "vector_2": [4, 0.10389138957456076, 1, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Smith estimates Lotus will make profit this quarter.", "In order to solve this problem, a parse tree is needed to understand that Lotus is not the object of estimates."], "label": "Non-Prov", "citing": "P09-2063", "vector": [0, 0, 0, 0.0], "context": ["", "Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.", ""], "marker": "Sekine, 2005", "vector_1": {"retriev": 1, "recognit": 1, "pattern": 1, "gener": 1, "inform": 1, "base": 1, "paraphras": 1, "similarli": 1, "improv": 1, "introduc": 1}, "vector_2": [4, 0.10389138957456076, 1, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["2.1 Overview."], "label": "Non-Prov", "citing": "P10-1124", "vector": [0, 0, 0, 0.0], "context": ["", "This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g. (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).", ""], "marker": "Sekine, 2005", "vector_1": {"led": 1, "entail": 1, "eg": 1, "activ": 1, "rule": 1, "research": 1, "acquisit": 1, "predic": 1, "broadscal": 1}, "vector_2": [5, 0.0511044385646758, 3, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["All the sentences have been analyzed by our chunker and NE tag- ger."], "label": "Non-Prov", "citing": "P10-1124", "vector": [0, 0, 0, 0.0], "context": ["", "This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g. (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).", ""], "marker": "Sekine, 2005", "vector_1": {"led": 1, "entail": 1, "eg": 1, "activ": 1, "rule": 1, "research": 1, "acquisit": 1, "predic": 1, "broadscal": 1}, "vector_2": [5, 0.0511044385646758, 3, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["D o m ai n # of ph ras es t o t a l p h r a s e s ac cu ra cy C C 7 o r m o r e 1 0 5 8 7 . 6 % 6 o r l e s s 1 0 6 6 7 . 0 % P C 7 o r m o r e 3 5 9 9 9 . 2 % 6 o r l e s s 2 5 5 6 5 . 1 % Table 1."], "label": "Non-Prov", "citing": "P10-1124", "vector": [1, 0, 0, 0.0], "context": ["", "This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g. (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).", ""], "marker": "Sekine, 2005", "vector_1": {"led": 1, "entail": 1, "eg": 1, "activ": 1, "rule": 1, "research": 1, "acquisit": 1, "predic": 1, "broadscal": 1}, "vector_2": [5, 0.0511044385646758, 3, 1, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["For example, the phrase 's New York-based trust unit, is not a paraphrase of the other phrases in the unit set."], "label": "Non-Prov", "citing": "P12-1013", "vector": [0, 0, 0, 0.0], "context": ["", "Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).", ""], "marker": "Sekine, 2005", "vector_1": {"made": 1, "sub": 1, "stantial": 1, "rule": 1, "consequ": 1, "learn": 1, "effort": 1}, "vector_2": [7, 0.06604391437730515, 4, 1, 1, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["For the experiments, we used four newswire corpora, the Los Angeles Times/Washington Post, The New York Times, Reuters and the Wall Street Journal, all published in 1995."], "label": "Non-Prov", "citing": "P12-1013", "vector": [0, 0, 0, 0.0], "context": ["", "Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).", ""], "marker": "Sekine, 2005", "vector_1": {"made": 1, "sub": 1, "stantial": 1, "rule": 1, "consequ": 1, "learn": 1, "effort": 1}, "vector_2": [7, 0.06604391437730515, 4, 1, 1, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["In IE, creating the patterns which express the requested scenario, e.g. management succession or corporate merger and acquisition is regarded as the hardest task."], "label": "Non-Prov", "citing": "P12-1013", "vector": [0, 0, 0, 0.0], "context": ["", "Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).", ""], "marker": "Sekine, 2005", "vector_1": {"made": 1, "sub": 1, "stantial": 1, "rule": 1, "consequ": 1, "learn": 1, "effort": 1}, "vector_2": [7, 0.06604391437730515, 4, 1, 1, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["The frequency of the Company  Company domain ranks 11th with 35,567 examples."], "label": "Non-Prov", "citing": "P12-2031", "vector": [2, 0, 0, 0.0], "context": ["", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", ""], "marker": "Sekine, 2005", "vector_1": {"led": 1, "algorithm": 1, "gener": 1, "knowledg": 1, "resourc": 1, "system": 1, "rule": 2, "signific": 1, "automat": 1, "substanti": 1, "learn": 1, "develop": 1, "effort": 1, "infer": 3}, "vector_2": [7, 0.09070796460176991, 3, 2, 2, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Other errors include NE tagging errors and errors due to a phrase which includes other NEs."], "label": "Non-Prov", "citing": "P12-2031", "vector": [2, 0, 0, 0.0], "context": ["", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", ""], "marker": "Sekine, 2005", "vector_1": {"led": 1, "algorithm": 1, "gener": 1, "knowledg": 1, "resourc": 1, "system": 1, "rule": 2, "signific": 1, "automat": 1, "substanti": 1, "learn": 1, "develop": 1, "effort": 1, "infer": 3}, "vector_2": [7, 0.09070796460176991, 3, 2, 2, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["These 140 NE categories are designed by extending MUCs 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object)."], "label": "Non-Prov", "citing": "P12-2031", "vector": [3, 0, 0, 0.0], "context": ["", "The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.", ""], "marker": "Sekine, 2005", "vector_1": {"led": 1, "algorithm": 1, "gener": 1, "knowledg": 1, "resourc": 1, "system": 1, "rule": 2, "signific": 1, "automat": 1, "substanti": 1, "learn": 1, "develop": 1, "effort": 1, "infer": 3}, "vector_2": [7, 0.09070796460176991, 3, 2, 2, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Instead, we focused on phrases and set the frequency threshold to 2, and so were able to utilize a lot of phrases while minimizing noise."], "label": "Non-Prov", "citing": "P12-2031", "vector": [5, 0, 0, 0.0], "context": ["", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"given": 1, "made": 1, "attempt": 1, "directli": 1, "judg": 2, "annot": 1, "rule": 2, "let": 1, "ask": 1, "correct": 2}, "vector_2": [7, 0.19070796460176992, 2, 2, 2, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus."], "label": "Non-Prov", "citing": "P12-2031", "vector": [1, 0, 0, 0.0], "context": ["", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"given": 1, "made": 1, "attempt": 1, "directli": 1, "judg": 2, "annot": 1, "rule": 2, "let": 1, "ask": 1, "correct": 2}, "vector_2": [7, 0.19070796460176992, 2, 2, 2, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Sometime, multiple words are needed, like vice chairman, prime minister or pay for (pay and pay for are different senses in the CC-domain)."], "label": "Non-Prov", "citing": "P12-2031", "vector": [1, 0, 0, 0.0], "context": ["", "Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).", ""], "marker": "Sekine, 2005", "vector_1": {"given": 1, "made": 1, "attempt": 1, "directli": 1, "judg": 2, "annot": 1, "rule": 2, "let": 1, "ask": 1, "correct": 2}, "vector_2": [7, 0.19070796460176992, 2, 2, 2, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["2.1 Overview.", "Before explaining our method in detail, we present a brief overview in this subsection.", "First, from a large corpus, we extract all the NE instance pairs."], "label": "Non-Prov", "citing": "W12-4006", "vector": [1, 0, 0, 0.0], "context": ["", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 1, "algorithm": 1, "constraint": 1, "eg": 1, "rule": 1, "acquisit": 1, "add": 1, "exist": 1, "paraphras": 1, "learn": 1}, "vector_2": [7, 0.08978978978978978, 2, 2, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Step 2.", "Find keywords for each NE pair The keywords are found for each NE category pair.", "For example, in the CC-domain, 96 keywords are found which have TF/ITF scores above a threshold; some of them are shown in Figure 3."], "label": "Non-Prov", "citing": "W12-4006", "vector": [2, 0, 0, 0.0], "context": ["", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 1, "algorithm": 1, "constraint": 1, "eg": 1, "rule": 1, "acquisit": 1, "add": 1, "exist": 1, "paraphras": 1, "learn": 1}, "vector_2": [7, 0.08978978978978978, 2, 2, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Sometime, multiple words are needed, like vice chairman, prime minister or pay for (pay and pay for are different senses in the CC-domain).", "One possibility is to use n-grams based on mutual information.", "If there is a frequent multi-word sequence in a domain, we could use it as a keyword candidate."], "label": "Non-Prov", "citing": "W12-4006", "vector": [3, 0, 0, 0.0], "context": ["", "Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (CallisonBurch, 2008)), but most do not.", ""], "marker": "Sekine, 2005", "vector_1": {"entail": 1, "algorithm": 1, "constraint": 1, "eg": 1, "rule": 1, "acquisit": 1, "add": 1, "exist": 1, "paraphras": 1, "learn": 1}, "vector_2": [7, 0.08978978978978978, 2, 2, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["The number of NE instance pairs used in their experiment is less than half of our method."], "label": "Non-Prov", "citing": "W12-4006", "vector": [2, 0, 0, 0.08333333333333333], "context": ["", "As for paraphrase, Sekines Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", ""], "marker": "Sekine, 2005", "vector_1": {"use": 1, "name": 1, "databas": 1, "unsupervis": 1, "focus": 1, "two": 1, "collect": 1, "paraphras": 2, "sekin": 1, "phrase": 1, "entiti": 1, "method": 1, "connect": 1}, "vector_2": [7, 0.223656990323657, 1, 2, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["Evaluation results for links"], "label": "Non-Prov", "citing": "W12-4006", "vector": [1, 0, 0, 0.0], "context": ["", "As for paraphrase, Sekines Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", ""], "marker": "Sekine, 2005", "vector_1": {"use": 1, "name": 1, "databas": 1, "unsupervis": 1, "focus": 1, "two": 1, "collect": 1, "paraphras": 2, "sekin": 1, "phrase": 1, "entiti": 1, "method": 1, "connect": 1}, "vector_2": [7, 0.223656990323657, 1, 2, 0, 0]}, {"function": "Neut", "cited": "I05-5011", "provenance": ["We concentrate on those sets."], "label": "Non-Prov", "citing": "W12-4006", "vector": [1, 0, 0, 0.0], "context": ["", "As for paraphrase, Sekines Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.", ""], "marker": "Sekine, 2005", "vector_1": {"use": 1, "name": 1, "databas": 1, "unsupervis": 1, "focus": 1, "two": 1, "collect": 1, "paraphras": 2, "sekin": 1, "phrase": 1, "entiti": 1, "method": 1, "connect": 1}, "vector_2": [7, 0.223656990323657, 1, 2, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["It may seem surprising to some readers that the interhuman agreement scores reported here are so low.", "However, this result is consistent with the results of ex periments discussed in Wu and Fung (1994).", "Wu and Fung introduce an evaluation method they call nk-blind."], "label": "Non-Prov", "citing": "A00-2032", "vector": [8, 0, 0, 0.03138824102871723], "context": ["", "Chinese According to Sproat et al (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pub lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical ap proach.", ""], "marker": "1996", "vector_1": {"shih": 1, "inde": 1, "knowledg": 1, "al": 1, "one": 1, "ap": 1, "exploit": 1, "mutualinform": 1, "et": 1, "previous": 1, "awar": 1, "author": 1, "pure": 1, "method": 1, "accord": 1, "sproat": 2, "proach": 1, "chines": 2, "lexic": 1, "assert": 1, "base": 1, "pub": 1, "segment": 1, "work": 1, "lish": 1, "prior": 1, "instanc": 1, "statist": 1}, "vector_2": [4, 0.8966613672496025, 2, 0, 10, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["To this end, we picked 100 sentences at random containing 4,372 total hanzi from a test corpus.14 (There were 487 marks of punctuation in the test sentences, including the sentence-final periods, meaning that the average inter-punctuation distance was about 9 hanzi.)", "We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.", "Since we could not bias the subjects towards a particular segmentation and did not presume linguistic sophistication on their part, the instructions were simple: subjects were to mark all places they might plausibly pause if they were reading the text aloud."], "label": "Non-Prov", "citing": "A00-2032", "vector": [10, 0, 1, 0.020965696734438367], "context": ["", "Chinese According to Sproat et al (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pub lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical ap proach.", ""], "marker": "1996", "vector_1": {"shih": 1, "inde": 1, "knowledg": 1, "al": 1, "one": 1, "ap": 1, "exploit": 1, "mutualinform": 1, "et": 1, "previous": 1, "awar": 1, "author": 1, "pure": 1, "method": 1, "accord": 1, "sproat": 2, "proach": 1, "chines": 2, "lexic": 1, "assert": 1, "base": 1, "pub": 1, "segment": 1, "work": 1, "lish": 1, "prior": 1, "instanc": 1, "statist": 1}, "vector_2": [4, 0.8966613672496025, 2, 0, 10, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Thus we have some confidence that our own performance is at least as good as that of Chang et al.", "(1992).", "In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system."], "label": "Non-Prov", "citing": "A00-2032", "vector": [7, 0, 1, 0.10101525445522107], "context": ["", "Chinese According to Sproat et al (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pub lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical ap proach.", ""], "marker": "1996", "vector_1": {"shih": 1, "inde": 1, "knowledg": 1, "al": 1, "one": 1, "ap": 1, "exploit": 1, "mutualinform": 1, "et": 1, "previous": 1, "awar": 1, "author": 1, "pure": 1, "method": 1, "accord": 1, "sproat": 2, "proach": 1, "chines": 2, "lexic": 1, "assert": 1, "base": 1, "pub": 1, "segment": 1, "work": 1, "lish": 1, "prior": 1, "instanc": 1, "statist": 1}, "vector_2": [4, 0.8966613672496025, 2, 0, 10, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)", "We have argued that the proposed method performs well."], "label": "Non-Prov", "citing": "A00-2032", "vector": [3, 1, 0, 0.05773502691896257], "context": ["", "Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) er rors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Fung, 1998).", ""], "marker": "Sproat et al., 1996", "vector_1": {"applic": 1, "term": 1, "charact": 1, "retriev": 1, "er": 1, "optic": 1, "index": 1, "segment": 1, "recognit": 1, "technolog": 1, "correct": 1, "inform": 1, "includ": 1, "ocr": 1, "new": 1, "ror": 1, "document": 1, "extract": 1, "technic": 1, "propos": 1}, "vector_2": [4, 0.033177139988285496, 6, 2, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["However, it is almost universally the case that no clear definition of what constitutes a \"correct\" segmentation is given, so these performance measures are hard to evaluate.", "Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures."], "label": "Non-Prov", "citing": "A00-2032", "vector": [2, 0, 0, 0.06454972243679027], "context": ["", "Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) er rors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Fung, 1998).", ""], "marker": "Sproat et al., 1996", "vector_1": {"applic": 1, "term": 1, "charact": 1, "retriev": 1, "er": 1, "optic": 1, "index": 1, "segment": 1, "recognit": 1, "technolog": 1, "correct": 1, "inform": 1, "includ": 1, "ocr": 1, "new": 1, "ror": 1, "document": 1, "extract": 1, "technic": 1, "propos": 1}, "vector_2": [4, 0.033177139988285496, 6, 2, 1, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["For each pair of judges, consider one judge as the standard,.", "computing the recall of the other's judgments relative to this standard."], "label": "Non-Prov", "citing": "A00-2032", "vector": [2, 0, 0, 0.0], "context": ["", "Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) er rors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Fung, 1998).", ""], "marker": "Sproat et al., 1996", "vector_1": {"applic": 1, "term": 1, "charact": 1, "retriev": 1, "er": 1, "optic": 1, "index": 1, "segment": 1, "recognit": 1, "technolog": 1, "correct": 1, "inform": 1, "includ": 1, "ocr": 1, "new": 1, "ror": 1, "document": 1, "extract": 1, "technic": 1, "propos": 1}, "vector_2": [4, 0.033177139988285496, 6, 2, 1, 0]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["Full Chinese personal names are in one respect simple: they are always of the form family+given.", "The family name set is restricted: there are a few hundred single-hanzi family names, and about ten double-hanzi ones."], "label": "Non-Prov", "citing": "E09-1063", "vector": [6, 0, 0, 0.0], "context": ["", "First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"given": 1, "task": 1, "nativ": 1, "reliabl": 1, "object": 1, "agreement": 1, "realli": 1, "goldstandard": 1, "speaker": 1, "build": 1, "first": 1, "fact": 1, "difficult": 1}, "vector_2": [13, 0.5441719734018224, 1, 1, 0, 0]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["In (2a), we want to split the two morphemes since the correct analysis is that we have the adverb :1 cai2 'just,' the modal verb neng2 'be able' and the main verb R: Hke4fu2 'overcome'; the competing analysis is, of course, that we have the noun :1 cai2neng2 'talent,' followed by }'lijke4fu2 'overcome.'", "Clearly it is possible to write a rule that states that if an analysis Modal+ Verb is available, then that is to be preferred over Noun+ Verb: such a rule could be stated in terms of (finite-state) local grammars in the sense of Mohri (1993)."], "label": "Non-Prov", "citing": "E09-1063", "vector": [8, 0, 1, 0.0], "context": ["", "First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"given": 1, "task": 1, "nativ": 1, "reliabl": 1, "object": 1, "agreement": 1, "realli": 1, "goldstandard": 1, "speaker": 1, "build": 1, "first": 1, "fact": 1, "difficult": 1}, "vector_2": [13, 0.5441719734018224, 1, 1, 0, 0]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["gaolbu4-gaolxing4 (hap-not-happy) 'happy?'", "(b) F.i'JJI!"], "label": "Non-Prov", "citing": "E09-1063", "vector": [0, 0, 0, 0.0], "context": ["", "First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"given": 1, "task": 1, "nativ": 1, "reliabl": 1, "object": 1, "agreement": 1, "realli": 1, "goldstandard": 1, "speaker": 1, "build": 1, "first": 1, "fact": 1, "difficult": 1}, "vector_2": [13, 0.5441719734018224, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": [" In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to \"reconstruct\" the word-boundary information.", " In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer."], "label": "Non-Prov", "citing": "J00-3004", "vector": [4, 0, 1, 0.0], "context": ["", "According to Sproat et al {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the \"correct\" segmentation, and the figure reduces as more people become involved.", ""], "marker": "1996", "vector_1": {"reduc": 1, "involv": 1, "accord": 1, "becom": 1, "figur": 1, "sproat": 1, "show": 1, "peopl": 1, "fung": 1, "al": 1, "agreement": 1, "nativ": 1, "wu": 1, "speaker": 1, "expect": 1, "et": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [4, 0.10996673107954659, 0, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["An example of a fairly low-level relation is the affix relation, which holds between a stem morpheme and an affix morpheme, such as f1 -menD (PL).", "A high-level relation is agent, which relates an animate nominal to a predicate."], "label": "Non-Prov", "citing": "J00-3004", "vector": [6, 0, 0, 0.0], "context": ["", "According to Sproat et al {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the \"correct\" segmentation, and the figure reduces as more people become involved.", ""], "marker": "1996", "vector_1": {"reduc": 1, "involv": 1, "accord": 1, "becom": 1, "figur": 1, "sproat": 1, "show": 1, "peopl": 1, "fung": 1, "al": 1, "agreement": 1, "nativ": 1, "wu": 1, "speaker": 1, "expect": 1, "et": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [4, 0.10996673107954659, 0, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["\"c' 0 + 0 \"0 '  + a n t i g r e e d y x g r e e d y < > c u r r e n t m e t h o d o d i e t . o n l y  Taiwan 0 ;; 0 c CD E i5 0\"' 9 9  Mainland     -0.30.20.1 0.0 0.1 0.2 Dimension 1 (62%) Figure 7 Classical metric multidimensional scaling of distance matrix, showing the two most significant dimensions.", "The percentage scores on the axis labels represent the amount of variation in the data explained by the dimension in question."], "label": "Non-Prov", "citing": "J00-3004", "vector": [3, 0, 1, 0.01960030890330249], "context": ["", "According to Sproat et al {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the \"correct\" segmentation, and the figure reduces as more people become involved.", ""], "marker": "1996", "vector_1": {"reduc": 1, "involv": 1, "accord": 1, "becom": 1, "figur": 1, "sproat": 1, "show": 1, "peopl": 1, "fung": 1, "al": 1, "agreement": 1, "nativ": 1, "wu": 1, "speaker": 1, "expect": 1, "et": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [4, 0.10996673107954659, 0, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Other kinds of productive word classes, such as company names, abbreviations (termed fijsuolxie3 in Mandarin), and place names can easily be 20 Note that 7 in E 7 is normally pronounced as leO, but as part of a resultative it is liao3.."], "label": "Non-Prov", "citing": "J00-3004", "vector": [5, 0, 0, 0.19658927487319622], "context": ["", "Sproat et al (1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.", ""], "marker": "1996", "vector_1": {"morpholog": 1, "recogn": 1, "sproat": 1, "word": 1, "chines": 1, "well": 1, "al": 1, "foreign": 1, "compon": 1, "transliter": 1, "et": 1, "implement": 1, "obtain": 1, "special": 1, "name": 2}, "vector_2": [4, 0.247504830965994, 1, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words."], "label": "Non-Prov", "citing": "J00-3004", "vector": [3, 0, 0, 0.07106690545187015], "context": ["", "Sproat et al (1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.", ""], "marker": "1996", "vector_1": {"morpholog": 1, "recogn": 1, "sproat": 1, "word": 1, "chines": 1, "well": 1, "al": 1, "foreign": 1, "compon": 1, "transliter": 1, "et": 1, "implement": 1, "obtain": 1, "special": 1, "name": 2}, "vector_2": [4, 0.247504830965994, 1, 0, 1, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary."], "label": "Non-Prov", "citing": "J00-3004", "vector": [6, 0, 0, 0.05716619504750295], "context": ["", "Sproat et al (1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.", ""], "marker": "1996", "vector_1": {"morpholog": 1, "recogn": 1, "sproat": 1, "word": 1, "chines": 1, "well": 1, "al": 1, "foreign": 1, "compon": 1, "transliter": 1, "et": 1, "implement": 1, "obtain": 1, "special": 1, "name": 2}, "vector_2": [4, 0.247504830965994, 1, 0, 1, 1]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["Table 2 Similarity matrix for segmentation judgments.", "Jud ges A G G R ST M 1 M 2 M 3 T1 T2 T3 AG 0.7 0 0.7 0 0 . 4 3 0.4 2 0.6 0 0.6 0 0.6 2 0.5 9 GR 0.9 9 0 . 6 2 0.6 4 0.7 9 0.8 2 0.8 1 0.7 2 ST 0 . 6 4 0.6 7 0.8 0 0.8 4 0.8 2 0.7 4 M1 0.7 7 0.6 9 0.7 1 0.6 9 0.7 0 M2 0.7 2 0.7 3 0.7 1 0.7 0 M3 0.8 9 0.8 7 0.8 0 T1 0.8 8 0.8 2 T2 0.7 8 respectively, the recall and precision."], "label": "Non-Prov", "citing": "J11-3001", "vector": [2, 0, 0, 0.0], "context": ["", "Gold standards, however, 435 cannot be unied into a single standard (Fung and Wu 1994; Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"sproat": 1, "gold": 1, "al": 1, "howev": 1, "wu": 1, "standard": 2, "fung": 1, "cannot": 1, "et": 1, "uni": 1, "singl": 1}, "vector_2": [15, 0.47331441068354196, 0, 0, 4, 1]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["Particular relations are also consistent with particular hypotheses about the segmentation of a given sentence, and the scores for particular relations can be incremented or decremented depending upon whether the segmentations with which they are consistent are \"popular\" or not.", "While Gan's system incorporates fairly sophisticated models of various linguistic information, it has the drawback that it has only been tested with a very small lexicon (a few hundred words) and on a very small test set (thirty sentences); there is therefore serious concern as to whether the methods that he discusses are scalable."], "label": "Non-Prov", "citing": "J11-3001", "vector": [3, 0, 0, 0.0], "context": ["", "Gold standards, however, 435 cannot be unied into a single standard (Fung and Wu 1994; Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"sproat": 1, "gold": 1, "al": 1, "howev": 1, "wu": 1, "standard": 2, "fung": 1, "cannot": 1, "et": 1, "uni": 1, "singl": 1}, "vector_2": [15, 0.47331441068354196, 0, 0, 4, 1]}, {"function": "Weak", "cited": "J96-3004", "provenance": ["In (b) is a plausible segmentation for this sentence; in (c) is an implausible segmentation.", "orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words."], "label": "Non-Prov", "citing": "J11-3001", "vector": [4, 0, 0, 0.0], "context": ["", "Gold standards, however, 435 cannot be unied into a single standard (Fung and Wu 1994; Sproat et al 1996).", ""], "marker": "Sproat et al. 1996", "vector_1": {"sproat": 1, "gold": 1, "al": 1, "howev": 1, "wu": 1, "standard": 2, "fung": 1, "cannot": 1, "et": 1, "uni": 1, "singl": 1}, "vector_2": [15, 0.47331441068354196, 0, 0, 4, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Note that the backoff model assumes that there is a positive correlation between the frequency of a singular noun and its plural."], "label": "Non-Prov", "citing": "J96-4004", "vector": [2, 0, 0, 0.0], "context": ["", "Our ap . proach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994; Lai et al 1992; Sproat et al 1994; Sproat et al 1996) primarily in that our system performs sentence inter pretation, in addition to word boundary identification.", ""], "marker": "Sproat et al. 1996", "vector_1": {"shih": 1, "wang": 2, "primarili": 1, "al": 4, "ap": 1, "exist": 1, "chen": 2, "et": 4, "lai": 1, "identif": 1, "differ": 1, "perform": 1, "boundari": 1, "system": 1, "pretat": 1, "lua": 1, "liang": 1, "sentenc": 1, "sproat": 3, "proach": 1, "chines": 1, "gan": 1, "wu": 1, "fan": 1, "segment": 1, "addit": 1, "word": 2, "tsai": 1, "work": 1, "su": 1, "chiang": 1, "chang": 1, "inter": 1, "bai": 1}, "vector_2": [0, 0.06607004165784085, 0, 0, 5, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Finally, we wish to reiterate an important point."], "label": "Non-Prov", "citing": "J96-4004", "vector": [1, 0, 0, 0.0], "context": ["", "Our ap . proach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994; Lai et al 1992; Sproat et al 1994; Sproat et al 1996) primarily in that our system performs sentence inter pretation, in addition to word boundary identification.", ""], "marker": "Sproat et al. 1996", "vector_1": {"shih": 1, "wang": 2, "primarili": 1, "al": 4, "ap": 1, "exist": 1, "chen": 2, "et": 4, "lai": 1, "identif": 1, "differ": 1, "perform": 1, "boundari": 1, "system": 1, "pretat": 1, "lua": 1, "liang": 1, "sentenc": 1, "sproat": 3, "proach": 1, "chines": 1, "gan": 1, "wu": 1, "fan": 1, "segment": 1, "addit": 1, "word": 2, "tsai": 1, "work": 1, "su": 1, "chiang": 1, "chang": 1, "inter": 1, "bai": 1}, "vector_2": [0, 0.06607004165784085, 0, 0, 5, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects."], "label": "Non-Prov", "citing": "J96-4004", "vector": [2, 0, 0, 0.0], "context": ["", "Our ap . proach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994; Lai et al 1992; Sproat et al 1994; Sproat et al 1996) primarily in that our system performs sentence inter pretation, in addition to word boundary identification.", ""], "marker": "Sproat et al. 1996", "vector_1": {"shih": 1, "wang": 2, "primarili": 1, "al": 4, "ap": 1, "exist": 1, "chen": 2, "et": 4, "lai": 1, "identif": 1, "differ": 1, "perform": 1, "boundari": 1, "system": 1, "pretat": 1, "lua": 1, "liang": 1, "sentenc": 1, "sproat": 3, "proach": 1, "chines": 1, "gan": 1, "wu": 1, "fan": 1, "segment": 1, "addit": 1, "word": 2, "tsai": 1, "work": 1, "su": 1, "chiang": 1, "chang": 1, "inter": 1, "bai": 1}, "vector_2": [0, 0.06607004165784085, 0, 0, 5, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994).", "Since the transducers are built from human-readable descriptions using a lexical toolkit (Sproat 1995), the system is easily maintained and extended.", "While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf."], "label": "Non-Prov", "citing": "P06-1010", "vector": [8, 0, 0, 0.039722906114947866], "context": ["", "Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names. As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.", ""], "marker": "Sproat et al., 1996", "vector_1": {"subset": 1, "discuss": 1, "use": 2, "name": 2, "overwhelmingli": 1, "chines": 2, "elsewher": 1, "list": 1, "tend": 1, "charact": 2, "thousand": 1, "candid": 1, "sever": 1, "transliter": 3, "consult": 1, "foreign": 2, "hundr": 1, "gener": 1, "frequent": 1}, "vector_2": [10, 0.29509921828021646, 1, 1, 0, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model.", "There are two weaknesses in Chang et al.'s model, which we improve upon.", "First, the model assumes independence between the first and second hanzi of a double given name."], "label": "Non-Prov", "citing": "P06-1010", "vector": [4, 0, 1, 0.0], "context": ["", "Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names. As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.", ""], "marker": "Sproat et al., 1996", "vector_1": {"subset": 1, "discuss": 1, "use": 2, "name": 2, "overwhelmingli": 1, "chines": 2, "elsewher": 1, "list": 1, "tend": 1, "charact": 2, "thousand": 1, "candid": 1, "sever": 1, "transliter": 3, "consult": 1, "foreign": 2, "hundr": 1, "gener": 1, "frequent": 1}, "vector_2": [10, 0.29509921828021646, 1, 1, 0, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The less favored reading may be selected in certain contexts, however; in the case of , for example, the nominal reading jiang4 will be selected if there is morphological information, such as a following plural affix ir, menD that renders the nominal reading likely, as we shall see in Section 4.3.", "Figure 3 shows a small fragment of the WFST encoding the dictionary, containing both entries forjust discussed, g:t zhonglhua2 min2guo2 (China Republic) 'Republic of China,' and iinl.", "nan2gual 'pumpkin.'"], "label": "Non-Prov", "citing": "P06-1010", "vector": [7, 0, 0, 0.02017425108896008], "context": ["", "Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names. As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.", ""], "marker": "Sproat et al., 1996", "vector_1": {"subset": 1, "discuss": 1, "use": 2, "name": 2, "overwhelmingli": 1, "chines": 2, "elsewher": 1, "list": 1, "tend": 1, "charact": 2, "thousand": 1, "candid": 1, "sever": 1, "transliter": 3, "consult": 1, "foreign": 2, "hundr": 1, "gener": 1, "frequent": 1}, "vector_2": [10, 0.29509921828021646, 1, 1, 0, 1]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Table 1 The cost as a novel given name (second position) for hanzi from various radical classes.", "JA DE G O L D G R AS S SI C K NE SS DE AT H R A T 14.", "98 15."], "label": "Non-Prov", "citing": "P07-1015", "vector": [5, 0, 1, 0.0], "context": ["", "Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 2, "name": 1, "possibl": 1, "sequenc": 1, "chines": 1, "list": 1, "three": 1, "foreign": 1, "candid": 1, "transliter": 1, "taken": 1, "charact": 2, "frequent": 1}, "vector_2": [11, 0.6221020092735703, 1, 2, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The first concerns how to deal with ambiguities in segmentation.", "The second concerns the methods used (if any) to ex tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.", "The most popular approach to dealing with seg mentation ambiguities is the maximum matching method, possibly augmented with further heuristics."], "label": "Non-Prov", "citing": "P07-1015", "vector": [4, 0, 0, 0.07767356373806174], "context": ["", "Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 2, "name": 1, "possibl": 1, "sequenc": 1, "chines": 1, "list": 1, "three": 1, "foreign": 1, "candid": 1, "transliter": 1, "taken": 1, "charact": 2, "frequent": 1}, "vector_2": [11, 0.6221020092735703, 1, 2, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The first probability is estimated from a name count in a text database, and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al.'s model the p(rule 9) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name, and we use essentially the same estimate here, with some modifications as described later on.", "This model is easily incorporated into the segmenter by building a WFST restrict ing the names to the four licit types, with costs on the arcs for any particular name summing to an estimate of the cost of that name.", "This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model."], "label": "Non-Prov", "citing": "P07-1015", "vector": [10, 0, 0, 0.03859019663395365], "context": ["", "Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.", ""], "marker": "Sproat et al., 1996", "vector_1": {"use": 2, "name": 1, "possibl": 1, "sequenc": 1, "chines": 1, "list": 1, "three": 1, "foreign": 1, "candid": 1, "transliter": 1, "taken": 1, "charact": 2, "frequent": 1}, "vector_2": [11, 0.6221020092735703, 1, 2, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Much confusion has been sown about Chinese writing by the use of the term ideograph, suggesting that hanzi somehow directly represent ideas."], "label": "Non-Prov", "citing": "P12-1110", "vector": [4, 0, 3, 0.09600307214746388], "context": ["", "3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.", ""], "marker": "Sproat et al., 1996", "vector_1": {"featur": 1, "set": 1, "expect": 1, "contribut": 1, "use": 2, "depend": 1, "make": 1, "baselin": 1, "investig": 1, "syntact": 1, "chines": 1, "joint": 1, "dictionari": 3, "robust": 1, "strong": 1, "segment": 2, "serv": 1, "word": 1, "enabl": 1, "realist": 1, "alon": 1, "us": 1, "model": 1}, "vector_2": [16, 0.4850732373386296, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["yu2 'fish.'"], "label": "Non-Prov", "citing": "P12-1110", "vector": [0, 0, 0, 0.0], "context": ["", "3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.", ""], "marker": "Sproat et al., 1996", "vector_1": {"featur": 1, "set": 1, "expect": 1, "contribut": 1, "use": 2, "depend": 1, "make": 1, "baselin": 1, "investig": 1, "syntact": 1, "chines": 1, "joint": 1, "dictionari": 3, "robust": 1, "strong": 1, "segment": 2, "serv": 1, "word": 1, "enabl": 1, "realist": 1, "alon": 1, "us": 1, "model": 1}, "vector_2": [16, 0.4850732373386296, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The morphological analysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up."], "label": "Non-Prov", "citing": "P12-1110", "vector": [5, 0, 1, 0.04356068418690322], "context": ["", "3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.", ""], "marker": "Sproat et al., 1996", "vector_1": {"featur": 1, "set": 1, "expect": 1, "contribut": 1, "use": 2, "depend": 1, "make": 1, "baselin": 1, "investig": 1, "syntact": 1, "chines": 1, "joint": 1, "dictionari": 3, "robust": 1, "strong": 1, "segment": 2, "serv": 1, "word": 1, "enabl": 1, "realist": 1, "alon": 1, "us": 1, "model": 1}, "vector_2": [16, 0.4850732373386296, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexi cal rule-based approaches, and approaches that combine lexical information with sta tistical information."], "label": "Non-Prov", "citing": "P99-1036", "vector": [2, 0, 0, 0.035533452725935076], "context": ["", "In Japanese, around 95% word segmentation ac curacy is reported by using a word-based lan guage model and the Viterbi-like dynamic program ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Mat sumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"guag": 1, "curaci": 1, "ac": 1, "word": 1, "around": 1, "japanes": 1, "chines": 1, "viterbilik": 1, "use": 1, "procedur": 1, "ming": 1, "lan": 1, "program": 1, "dynam": 1, "report": 2, "model": 1, "accuraci": 1, "segment": 1, "method": 1, "wordbas": 1, "statist": 1}, "vector_2": [3, 0.034519797029519614, 5, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair."], "label": "Non-Prov", "citing": "P99-1036", "vector": [4, 0, 0, 0.09622504486493764], "context": ["", "In Japanese, around 95% word segmentation ac curacy is reported by using a word-based lan guage model and the Viterbi-like dynamic program ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Mat sumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"guag": 1, "curaci": 1, "ac": 1, "word": 1, "around": 1, "japanes": 1, "chines": 1, "viterbilik": 1, "use": 1, "procedur": 1, "ming": 1, "lan": 1, "program": 1, "dynam": 1, "report": 2, "model": 1, "accuraci": 1, "segment": 1, "method": 1, "wordbas": 1, "statist": 1}, "vector_2": [3, 0.034519797029519614, 5, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or cost based scoring mechanism."], "label": "Non-Prov", "citing": "P99-1036", "vector": [6, 0, 1, 0.11572751247156893], "context": ["", "In Japanese, around 95% word segmentation ac curacy is reported by using a word-based lan guage model and the Viterbi-like dynamic program ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and Mat sumoto, 1997). About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"guag": 1, "curaci": 1, "ac": 1, "word": 1, "around": 1, "japanes": 1, "chines": 1, "viterbilik": 1, "use": 1, "procedur": 1, "ming": 1, "lan": 1, "program": 1, "dynam": 1, "report": 2, "model": 1, "accuraci": 1, "segment": 1, "method": 1, "wordbas": 1, "statist": 1}, "vector_2": [3, 0.034519797029519614, 5, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["each word in the lexicon whether or not each string is actually an instance of the word in question.", " : _ADV: 5.88 If:!", ":zhong1 : 0.0 tjl :huo2 :0.0 (R:spub:/ic of Ch:ina) + .,_,...I : jlong4 :0.0 (mUifaty genG181) 0 : _NC: 40.0 Figure 3 Partial Chinese Lexicon (NC = noun; NP = proper noun).c=- - I =- :il: .;ss:;zhangt  '-:."], "label": "Non-Prov", "citing": "P99-1036", "vector": [2, 0, 1, 0.0], "context": ["", "There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "unknown": 1, "two": 1, "coverag": 1, "better": 1, "increas": 1, "design": 1, "dictionari": 1, "solv": 1, "problem": 1, "approach": 1, "model": 1}, "vector_2": [3, 0.041668209933701246, 5, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["example, in Northern Mandarin dialects there is a morpheme -r that attaches mostly to nouns, and which is phonologically incorporated into the syllable to which it attaches: thus men2+r (door+R) 'door' is realized as mer2.", "This is orthographically represented as 7C.", "so that 'door' would be and in this case the hanzi 7C, does not represent a syllable."], "label": "Non-Prov", "citing": "P99-1036", "vector": [6, 0, 0, 0.0], "context": ["", "There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "unknown": 1, "two": 1, "coverag": 1, "better": 1, "increas": 1, "design": 1, "dictionari": 1, "solv": 1, "problem": 1, "approach": 1, "model": 1}, "vector_2": [3, 0.041668209933701246, 5, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Clearly it is possible to write a rule that states that if an analysis Modal+ Verb is available, then that is to be preferred over Noun+ Verb: such a rule could be stated in terms of (finite-state) local grammars in the sense of Mohri (1993).", "Turning now to (1), we have the similar problem that splitting.into.ma3 'horse' andlu4 'way' is more costly than retaining this as one word .ma3lu4 'road.'", "However, there is again local grammatical information that should favor the split in the case of (1a): both .ma3 'horse' and .ma3 lu4 are nouns, but only .ma3 is consistent with the classifier pil, the classifier for horses.21 By a similar argument, the preference for not splitting , lm could be strengthened in (lb) by the observation that the classifier 1'1* tiao2 is consistent with long or winding objects like , lm ma3lu4 'road' but not with,ma3 'horse.'"], "label": "Non-Prov", "citing": "P99-1036", "vector": [10, 0, 0, 0.027524094128159017], "context": ["", "There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "unknown": 1, "two": 1, "coverag": 1, "better": 1, "increas": 1, "design": 1, "dictionari": 1, "solv": 1, "problem": 1, "approach": 1, "model": 1}, "vector_2": [3, 0.041668209933701246, 5, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["However, this result is consistent with the results of ex periments discussed in Wu and Fung (1994).", "Wu and Fung introduce an evaluation method they call nk-blind."], "label": "Non-Prov", "citing": "P99-1036", "vector": [2, 0, 0, 0.0], "context": ["", "To improve word segmenta tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.", ""], "marker": "Sproat et al., 1996", "vector_1": {"foreign": 1, "use": 2, "set": 1, "word": 4, "name": 1, "specif": 1, "unknown": 1, "gener": 1, "accuraci": 1, "person": 1, "transliter": 1, "segmenta": 1, "improv": 1, "tion": 1, "purpos": 1, "plural": 1, "model": 2, "singl": 1}, "vector_2": [3, 0.05311307826215786, 2, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Table 2 shows these similarity measures.", "The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that dis tance matrix, and plotting the first two most significant dimensions."], "label": "Non-Prov", "citing": "P99-1036", "vector": [3, 0, 0, 0.0], "context": ["", "To improve word segmenta tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.", ""], "marker": "Sproat et al., 1996", "vector_1": {"foreign": 1, "use": 2, "set": 1, "word": 4, "name": 1, "specif": 1, "unknown": 1, "gener": 1, "accuraci": 1, "person": 1, "transliter": 1, "segmenta": 1, "improv": 1, "tion": 1, "purpos": 1, "plural": 1, "model": 2, "singl": 1}, "vector_2": [3, 0.05311307826215786, 2, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.", "The best analysis of the corpus is taken to be the true analysis, the frequencies are re-estimated, and the algorithm is repeated until it converges."], "label": "Non-Prov", "citing": "P99-1036", "vector": [11, 0, 2, 0.3177354158795988], "context": ["", "To improve word segmenta tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.", ""], "marker": "Sproat et al., 1996", "vector_1": {"foreign": 1, "use": 2, "set": 1, "word": 4, "name": 1, "specif": 1, "unknown": 1, "gener": 1, "accuraci": 1, "person": 1, "transliter": 1, "segmenta": 1, "improv": 1, "tion": 1, "purpos": 1, "plural": 1, "model": 2, "singl": 1}, "vector_2": [3, 0.05311307826215786, 2, 5, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["12 One class of full personal names that this characterization does not cover are married women's names."], "label": "Non-Prov", "citing": "P99-1036", "vector": [1, 0, 0, 0.0], "context": ["", "Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"term": 1, "previou": 1, "recal": 1, "express": 1, "accuraci": 1, "precis": 1, "done": 1, "word": 1, "research": 1, "segment": 1}, "vector_2": [3, 0.7816585799474055, 1, 5, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["42 nator, the N31s can be measured well by counting, and we replace the expectation by the observation."], "label": "Non-Prov", "citing": "P99-1036", "vector": [2, 0, 0, 0.0], "context": ["", "Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"term": 1, "previou": 1, "recal": 1, "express": 1, "accuraci": 1, "precis": 1, "done": 1, "word": 1, "research": 1, "segment": 1}, "vector_2": [3, 0.7816585799474055, 1, 5, 0, 0]}, {"function": "Pos", "cited": "J96-3004", "provenance": ["Nonetheless, the results of the comparison with human judges demonstrates that there is mileage being gained by incorporating models of these types of words."], "label": "Non-Prov", "citing": "P99-1036", "vector": [3, 0, 0, 0.0], "context": ["", "Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"term": 1, "previou": 1, "recal": 1, "express": 1, "accuraci": 1, "precis": 1, "done": 1, "word": 1, "research": 1, "segment": 1}, "vector_2": [3, 0.7816585799474055, 1, 5, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision.", "Note that it is in precision that our over all performance would appear to be poorer than the reported performance of Chang et al., yet based on their published examples, our system appears to be doing better precisionwise."], "label": "Non-Prov", "citing": "W01-0513", "vector": [7, 0, 0, 0.0576390417704235], "context": ["", "The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et. al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).", ""], "marker": "Sproat, et al, 1996", "vector_1": {"identifi": 1, "stream": 1, "pont": 1, "focus": 1, "phonet": 1, "asian": 1, "orthographi": 1, "et": 2, "languag": 1, "croft": 1, "token": 1, "indian": 1, "includ": 1, "saffran": 1, "sproat": 1, "normal": 1, "segment": 1, "word": 2, "delimit": 1, "work": 1, "princip": 1, "either": 1, "mani": 1, "other": 1}, "vector_2": [5, 0.1871841291393226, 6, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Mohri [1995]) shows promise for improving this situation.", "The model described here thus demonstrates great potential for use in widespread applications."], "label": "Non-Prov", "citing": "W01-0513", "vector": [2, 0, 0, 0.0], "context": ["", "The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et. al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).", ""], "marker": "Sproat, et al, 1996", "vector_1": {"identifi": 1, "stream": 1, "pont": 1, "focus": 1, "phonet": 1, "asian": 1, "orthographi": 1, "et": 2, "languag": 1, "croft": 1, "token": 1, "indian": 1, "includ": 1, "saffran": 1, "sproat": 1, "normal": 1, "segment": 1, "word": 2, "delimit": 1, "work": 1, "princip": 1, "either": 1, "mani": 1, "other": 1}, "vector_2": [5, 0.1871841291393226, 6, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised; indeed, such standards have been proposed and include the published PRCNSC (1994) and ROCLING (1993), as well as the unpublished Linguistic Data Consortium standards (ca.", "May 1995)."], "label": "Non-Prov", "citing": "W01-0513", "vector": [6, 0, 0, 0.0657951694959769], "context": ["", "The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et. al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).", ""], "marker": "Sproat, et al, 1996", "vector_1": {"identifi": 1, "stream": 1, "pont": 1, "focus": 1, "phonet": 1, "asian": 1, "orthographi": 1, "et": 2, "languag": 1, "croft": 1, "token": 1, "indian": 1, "includ": 1, "saffran": 1, "sproat": 1, "normal": 1, "segment": 1, "word": 2, "delimit": 1, "work": 1, "princip": 1, "either": 1, "mani": 1, "other": 1}, "vector_2": [5, 0.1871841291393226, 6, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words."], "label": "Non-Prov", "citing": "W03-1025", "vector": [2, 0, 0, 0.0], "context": ["", "There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower", ""], "marker": "Sproat et al., 1996", "vector_1": {"upper": 1, "lower": 1, "multipl": 1, "show": 1, "agreement": 1, "two": 1, "nativ": 1, "speaker": 1, "untrain": 1, "studi": 1}, "vector_2": [7, 0.04833787687675469, 3, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Email: rlls@bell-labs."], "label": "Non-Prov", "citing": "W03-1025", "vector": [0, 0, 0, 0.0], "context": ["", "There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower", ""], "marker": "Sproat et al., 1996", "vector_1": {"upper": 1, "lower": 1, "multipl": 1, "show": 1, "agreement": 1, "two": 1, "nativ": 1, "speaker": 1, "untrain": 1, "studi": 1}, "vector_2": [7, 0.04833787687675469, 3, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["(1992)."], "label": "Non-Prov", "citing": "W03-1025", "vector": [0, 0, 0, 0.0], "context": ["", "There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower", ""], "marker": "Sproat et al., 1996", "vector_1": {"upper": 1, "lower": 1, "multipl": 1, "show": 1, "agreement": 1, "two": 1, "nativ": 1, "speaker": 1, "untrain": 1, "studi": 1}, "vector_2": [7, 0.04833787687675469, 3, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary."], "label": "Non-Prov", "citing": "W03-1025", "vector": [3, 0, 0, 0.09622504486493763], "context": ["", "Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "rel": 1, "wellknown": 1, "chines": 1, "agreement": 1, "extens": 1, "low": 1, "human": 1, "known": 1, "problem": 1, "segment": 1, "studi": 1}, "vector_2": [7, 0.8905480734019612, 3, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Fortunately, we were able to obtain a copy of the full set of sentences from Chang et al. on which Wang, Li, and Chang tested their system, along with the output of their system.18 In what follows we will discuss all cases from this set where our performance on names differs from that of Wang, Li, and Chang."], "label": "Non-Prov", "citing": "W03-1025", "vector": [3, 0, 0, 0.0], "context": ["", "Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "rel": 1, "wellknown": 1, "chines": 1, "agreement": 1, "extens": 1, "low": 1, "human": 1, "known": 1, "problem": 1, "segment": 1, "studi": 1}, "vector_2": [7, 0.8905480734019612, 3, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Let us notate the set of previously unseen, or novel, members of a category X as unseen(X); thus, novel members of the set of words derived in f, menO will be de noted unseen(f,)."], "label": "Non-Prov", "citing": "W03-1025", "vector": [1, 0, 0, 0.0], "context": ["", "Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.", ""], "marker": "Sproat et al., 1996", "vector_1": {"word": 1, "rel": 1, "wellknown": 1, "chines": 1, "agreement": 1, "extens": 1, "low": 1, "human": 1, "known": 1, "problem": 1, "segment": 1, "studi": 1}, "vector_2": [7, 0.8905480734019612, 3, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Therefore in cases where the segmentation is identical between the two systems we assume that tagging is also identical.", "Table 4 Differences in performance between our system and Wang, Li, and Chang (1992).", "Our System Wang, Li, and Chang a. 1\\!f!IP Eflltii /1\\!f!J:P $1til I b. agm: I a m: c. 5 Bf is Bf 1 d. \"*:t: w _t ff 1 \"* :t: w_tff 1 g., , Transliteration/Translation chen2zhongl-shenl qu3 'music by Chen Zhongshen ' huang2rong2 youlyoul de dao4 'Huang Rong said soberly' zhangl qun2 Zhang Qun xian4zhang3 you2qingl shang4ren2 hou4 'after the county president You Qing had assumed the position' lin2 quan2 'Lin Quan' wang2jian4 'Wang Jian' oulyang2-ke4 'Ouyang Ke' yinl qi2 bu4 ke2neng2 rong2xu3 tai2du2 er2 'because it cannot permit Taiwan Independence so' silfa3-yuan4zhang3 lin2yang2-gang3 'president of the Judicial Yuan, Lin Yanggang' lin2zhangl-hu2 jiangl zuo4 xian4chang3 jie3shuol 'Lin Zhanghu will give an ex planation live' jin4/iang3 nian2 nei4 sa3 xia4 de jinlqian2 hui4 ting2zhi3 'in two years the distributed money will stop' gaoltangl da4chi2 ye1zi0 fen3 'chicken stock, a tablespoon of coconut flakes' you2qingl ru4zhu3 xian4fu3 lwu4 'after You Qing headed the county government' Table 5 Performance on morphological analysis."], "label": "Non-Prov", "citing": "W03-1025", "vector": [0, 0, 0, 0.0], "context": ["", "Sproat et al (1996) employs stochastic finite state machines to find word boundaries.", ""], "marker": "1996", "vector_1": {"machin": 1, "word": 1, "sproat": 1, "boundari": 1, "finit": 1, "al": 1, "employ": 1, "state": 1, "et": 1, "stochast": 1, "find": 1}, "vector_2": [7, 0.9157749115026244, 1, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["JI!", "gaolxing4 'happy' => F.i'JF.i'J Jl!", "gaolbu4-gaolxing4 (hap-not-happy) 'happy?'"], "label": "Non-Prov", "citing": "W03-1025", "vector": [0, 0, 0, 0.0], "context": ["", "Sproat et al (1996) employs stochastic finite state machines to find word boundaries.", ""], "marker": "1996", "vector_1": {"machin": 1, "word": 1, "sproat": 1, "boundari": 1, "finit": 1, "al": 1, "employ": 1, "state": 1, "et": 1, "stochast": 1, "find": 1}, "vector_2": [7, 0.9157749115026244, 1, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Figure 4 Input lattice (top) and two segmentations (bottom) of the sentence 'How do you say octopus in Japanese?'.", "A non-optimal analysis is shown with dotted lines in the bottom frame.", "ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t:-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,."], "label": "Non-Prov", "citing": "W03-1025", "vector": [3, 0, 0, 0.08891084489487741], "context": ["", "Sproat et al (1996) employs stochastic finite state machines to find word boundaries.", ""], "marker": "1996", "vector_1": {"machin": 1, "word": 1, "sproat": 1, "boundari": 1, "finit": 1, "al": 1, "employ": 1, "state": 1, "et": 1, "stochast": 1, "find": 1}, "vector_2": [7, 0.9157749115026244, 1, 0, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong."], "label": "Non-Prov", "citing": "W10-3212", "vector": [2, 0, 0, 0.0], "context": ["", "In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.", ""], "marker": "Sproat et al., 1996", "vector_1": {"lexicon": 1, "knowledg": 1, "ii": 1, "dictionarylexicon": 1, "languag": 1, "use": 1, "techniqu": 1, "iii": 1, "segment": 1, "three": 1, "approachesstatist": 1, "approach": 4, "method": 1, "match": 2, "machin": 1, "advanc": 1, "base": 4, "longest": 1, "categor": 1, "word": 1, "maximum": 1, "exampl": 1, "learn": 1, "linguist": 1}, "vector_2": [14, 0.13642488210195375, 5, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["(b) 89 :1 t& tal de cai2neng2 hen3 he DE talent very 'He has great talent' f.b ga ol hig h While the current algorithm correctly handles the (b) sentences, it fails to handle the (a) sentences, since it does not have enough information to know not to group the sequences.ma3lu4 and?]cai2neng2 respectively."], "label": "Non-Prov", "citing": "W10-3212", "vector": [0, 0, 0, 0.0], "context": ["", "In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.", ""], "marker": "Sproat et al., 1996", "vector_1": {"lexicon": 1, "knowledg": 1, "ii": 1, "dictionarylexicon": 1, "languag": 1, "use": 1, "techniqu": 1, "iii": 1, "segment": 1, "three": 1, "approachesstatist": 1, "approach": 4, "method": 1, "match": 2, "machin": 1, "advanc": 1, "base": 4, "longest": 1, "categor": 1, "word": 1, "maximum": 1, "exampl": 1, "learn": 1, "linguist": 1}, "vector_2": [14, 0.13642488210195375, 5, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Evaluation of the Segmentation as a Whole."], "label": "Non-Prov", "citing": "W10-3212", "vector": [1, 0, 0, 0.0], "context": ["", "In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou & Baosheng, 1998) are examples of lexicon based approaches.", ""], "marker": "Sproat et al., 1996", "vector_1": {"lexicon": 1, "knowledg": 1, "ii": 1, "dictionarylexicon": 1, "languag": 1, "use": 1, "techniqu": 1, "iii": 1, "segment": 1, "three": 1, "approachesstatist": 1, "approach": 4, "method": 1, "match": 2, "machin": 1, "advanc": 1, "base": 4, "longest": 1, "categor": 1, "word": 1, "maximum": 1, "exampl": 1, "learn": 1, "linguist": 1}, "vector_2": [14, 0.13642488210195375, 5, 3, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["In any event, to date, we have not compared different methods for deriving the set of initial frequency estimates."], "label": "Non-Prov", "citing": "W10-3708", "vector": [2, 0, 0, 0.0], "context": ["", "Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"among": 1, "shown": 1, "word": 1, "regard": 1, "agreement": 1, "nativ": 1, "speaker": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [14, 0.08489433674026116, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["A high-level relation is agent, which relates an animate nominal to a predicate."], "label": "Non-Prov", "citing": "W10-3708", "vector": [0, 0, 0, 0.0], "context": ["", "Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"among": 1, "shown": 1, "word": 1, "regard": 1, "agreement": 1, "nativ": 1, "speaker": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [14, 0.08489433674026116, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["Figure 3 shows a small fragment of the WFST encoding the dictionary, containing both entries forjust discussed, g:t zhonglhua2 min2guo2 (China Republic) 'Republic of China,' and iinl."], "label": "Non-Prov", "citing": "W10-3708", "vector": [1, 0, 0, 0.0], "context": ["", "Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).", ""], "marker": "Sproat et al., 1996", "vector_1": {"among": 1, "shown": 1, "word": 1, "regard": 1, "agreement": 1, "nativ": 1, "speaker": 1, "experi": 1, "segment": 1, "correct": 1}, "vector_2": [14, 0.08489433674026116, 1, 1, 0, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["The transitive closure of the dictionary in (a) is composed with Id(input) (b) to form the WFST (c).", "The segmentation chosen is the best path through the WFST, shown in (d).", "(In this figure eps is c) be implemented, though, such as a maximal-grouping strategy (as suggested by one reviewer of this paper); or a pairwise-grouping strategy, whereby long sequences of unattached hanzi are grouped into two-hanzi words (which may have some prosodic motivation)."], "label": "Non-Prov", "citing": "W11-0823", "vector": [5, 0, 1, 0.0], "context": ["", "There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.", ""], "marker": "1996", "vector_1": {"altern": 1, "cha": 1, "mutual": 1, "juman": 1, "sproat": 1, "dictionarybas": 1, "solut": 2, "number": 1, "al": 1, "inform": 1, "base": 1, "statist": 1, "et": 1, "popular": 1, "sen": 1, "distribut": 1, "propos": 1}, "vector_2": [15, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["(In this figure eps is c) be implemented, though, such as a maximal-grouping strategy (as suggested by one reviewer of this paper); or a pairwise-grouping strategy, whereby long sequences of unattached hanzi are grouped into two-hanzi words (which may have some prosodic motivation).", "We have not to date explored these various options.", "Word frequencies are estimated by a re-estimation procedure that involves apply ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of."], "label": "Non-Prov", "citing": "W11-0823", "vector": [5, 0, 1, 0.0], "context": ["", "There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.", ""], "marker": "1996", "vector_1": {"altern": 1, "cha": 1, "mutual": 1, "juman": 1, "sproat": 1, "dictionarybas": 1, "solut": 2, "number": 1, "al": 1, "inform": 1, "base": 1, "statist": 1, "et": 1, "popular": 1, "sen": 1, "distribut": 1, "propos": 1}, "vector_2": [15, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "J96-3004", "provenance": ["2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..", "3 Throughout this paper we shall give Chinese examples in traditional orthography, followed.", "immediately by a Romanization into the pinyin transliteration scheme; numerals following each pinyin syllable represent tones."], "label": "Non-Prov", "citing": "W11-0823", "vector": [2, 0, 0, 0.0], "context": ["", "There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.", ""], "marker": "1996", "vector_1": {"altern": 1, "cha": 1, "mutual": 1, "juman": 1, "sproat": 1, "dictionarybas": 1, "solut": 2, "number": 1, "al": 1, "inform": 1, "base": 1, "statist": 1, "et": 1, "popular": 1, "sen": 1, "distribut": 1, "propos": 1}, "vector_2": [15, 0.5, 1, 2, 2, 0]}, {"function": "CoCo", "cited": "N06-2049", "provenance": ["R P FR oo vR iv A S 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0."], "label": "Non-Prov", "citing": "J11-1005", "vector": [0, 0, 0, 0.0], "context": ["", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", ""], "marker": "2006", "vector_1": {"sumita": 1, "chose": 1, "zhang": 1, "emerson": 1, "well": 1, "three": 1, "least": 1, "one": 1, "kikui": 1, "achiev": 1, "score": 1, "subwordbas": 1, "test": 1, "close": 1, "model": 2, "comparison": 1, "best": 1}, "vector_2": [5, 0.28068530583135004, 2, 0, 2, 1]}, {"function": "CoCo", "cited": "N06-2049", "provenance": ["Another advantage of the word-based IOB tagging over the character-based is its speed."], "label": "Non-Prov", "citing": "J11-1005", "vector": [2, 0, 0, 0.0], "context": ["", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", ""], "marker": "2006", "vector_1": {"sumita": 1, "chose": 1, "zhang": 1, "emerson": 1, "well": 1, "three": 1, "least": 1, "one": 1, "kikui": 1, "achiev": 1, "score": 1, "subwordbas": 1, "test": 1, "close": 1, "model": 2, "comparison": 1, "best": 1}, "vector_2": [5, 0.28068530583135004, 2, 0, 2, 1]}, {"function": "CoCo", "cited": "N06-2049", "provenance": ["A forward-backward algorithm was used in the training and viterbi algorithm was used in the decoding."], "label": "Non-Prov", "citing": "J11-1005", "vector": [3, 0, 1, 0.0], "context": ["", "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.", ""], "marker": "2006", "vector_1": {"sumita": 1, "chose": 1, "zhang": 1, "emerson": 1, "well": 1, "three": 1, "least": 1, "one": 1, "kikui": 1, "achiev": 1, "score": 1, "subwordbas": 1, "test": 1, "close": 1, "model": 2, "comparison": 1, "best": 1}, "vector_2": [5, 0.28068530583135004, 2, 0, 2, 1]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data."], "label": "Non-Prov", "citing": "W06-0118", "vector": [1, 0, 0, 0.0], "context": ["", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 1, "maximum": 1, "subwordbas": 1, "also": 1, "tag": 1, "crf": 1, "model": 1}, "vector_2": [0, 0.1024783777890835, 1, 5, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["Tri- gram LMs were generated using the SRI LM toolkit for disambiguation."], "label": "Non-Prov", "citing": "W06-0118", "vector": [2, 0, 0, 0.1259881576697424], "context": ["", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 1, "maximum": 1, "subwordbas": 1, "also": 1, "tag": 1, "crf": 1, "model": 1}, "vector_2": [0, 0.1024783777890835, 1, 5, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["71 6 0.9 64 0.9 72 Table 2: Segmentation results by a pure subword-based IOB tagging."], "label": "Non-Prov", "citing": "W06-0118", "vector": [2, 0, 0, 0.26726124191242434], "context": ["", "Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 1, "maximum": 1, "subwordbas": 1, "also": 1, "tag": 1, "crf": 1, "model": 1}, "vector_2": [0, 0.1024783777890835, 1, 5, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["We think our proposed subword- based tagging played an important role for the good results.", "Since it was a closed test, some information such as Arabic and Chinese number and alphabetical letters cannot be used.", "We could yield a better results than those shown in Table 4 using such information."], "label": "Non-Prov", "citing": "W06-0118", "vector": [7, 0, 0, 0.07597371763975863], "context": ["", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", ""], "marker": "Zhang et al., 2006", "vector_1": {"competit": 1, "task": 1, "word": 1, "accuraci": 1, "appli": 1, "chines": 1, "previou": 1, "obtain": 1, "maximum": 1, "subwordbas": 1, "system": 1, "high": 1, "approach": 1, "tagger": 1, "data": 1, "sighan": 1, "share": 1, "recent": 1, "segment": 1, "iob": 1, "propos": 1}, "vector_2": [0, 0.24304091867141508, 1, 5, 0, 0]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.", "An example exhibiting each steps results is also given in the figure.", "Since the dictionary-based approach is a well-known method, we skip its technical descriptions."], "label": "Non-Prov", "citing": "W06-0118", "vector": [8, 0, 1, 0.1781741612749496], "context": ["", "Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.", ""], "marker": "Zhang et al., 2006", "vector_1": {"competit": 1, "task": 1, "word": 1, "accuraci": 1, "appli": 1, "chines": 1, "previou": 1, "obtain": 1, "maximum": 1, "subwordbas": 1, "system": 1, "high": 1, "approach": 1, "tagger": 1, "data": 1, "sighan": 1, "share": 1, "recent": 1, "segment": 1, "iob": 1, "propos": 1}, "vector_2": [0, 0.24304091867141508, 1, 5, 0, 0]}, {"function": "CoCo", "cited": "N06-2049", "provenance": ["A forward-backward algorithm was used in the training and viterbi algorithm was used in the decoding."], "label": "Non-Prov", "citing": "W10-4128", "vector": [1, 0, 0, 0.0], "context": ["", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 1, "previou": 1, "word": 1, "illustr": 1, "work": 1, "effect": 1, "lexic": 1, "charact": 1, "employ": 1, "tag": 2, "subword": 1, "focu": 1, "literatur": 1, "unit": 2}, "vector_2": [4, 0.10215770890545312, 6, 11, 0, 1]}, {"function": "CoCo", "cited": "N06-2049", "provenance": ["The types of unigram features used in our experiments included the following types: w0 , w1 , w1 , w2 , w2 , w0 w1 , w0 w1 , w1 w1 , w2 w1 , w2 w0 where w stands for word."], "label": "Non-Prov", "citing": "W10-4128", "vector": [2, 0, 0, 0.0], "context": ["", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 1, "previou": 1, "word": 1, "illustr": 1, "work": 1, "effect": 1, "lexic": 1, "charact": 1, "employ": 1, "tag": 2, "subword": 1, "focu": 1, "literatur": 1, "unit": 2}, "vector_2": [4, 0.10215770890545312, 6, 11, 0, 1]}, {"function": "CoCo", "cited": "N06-2049", "provenance": ["The subword-based approach is faster because fewer words than characters were labeled."], "label": "Non-Prov", "citing": "W10-4128", "vector": [3, 0, 0, 0.16903085094570328], "context": ["", "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.", ""], "marker": "Zhang et al., 2006", "vector_1": {"use": 1, "previou": 1, "word": 1, "illustr": 1, "work": 1, "effect": 1, "lexic": 1, "charact": 1, "employ": 1, "tag": 2, "subword": 1, "focu": 1, "literatur": 1, "unit": 2}, "vector_2": [4, 0.10215770890545312, 6, 11, 0, 1]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.", "By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005."], "label": "Non-Prov", "citing": "W10-4138", "vector": [4, 0, 0, 0.06659271582120269], "context": ["", "Thus, the bigram RAIL ENQUIRIES gives a misleading probability that RAIL is followed by ENQUIRIES irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al (2006) were proposed.", ""], "marker": "2006", "vector_1": {"bigram": 1, "give": 1, "al": 1, "tag": 1, "et": 1, "follow": 1, "happen": 1, "still": 1, "unit": 1, "probabl": 1, "mislead": 1, "appear": 1, "solut": 1, "overlap": 1, "also": 1, "enquiri": 2, "therefor": 1, "zhang": 1, "rail": 2, "compound": 1, "sinc": 1, "wordtoken": 1, "thu": 1, "corpora": 2, "correspond": 1, "n": 1, "irrespect": 1, "preced": 1, "gram": 1, "problem": 1, "propos": 1}, "vector_2": [4, 0.4131716542440761, 1, 0, 0, 1]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["The IOB tagging approach adopted in this work is not a new idea.", "It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used."], "label": "Non-Prov", "citing": "W10-4138", "vector": [9, 0, 0, 0.0], "context": ["", "Thus, the bigram RAIL ENQUIRIES gives a misleading probability that RAIL is followed by ENQUIRIES irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al (2006) were proposed.", ""], "marker": "2006", "vector_1": {"bigram": 1, "give": 1, "al": 1, "tag": 1, "et": 1, "follow": 1, "happen": 1, "still": 1, "unit": 1, "probabl": 1, "mislead": 1, "appear": 1, "solut": 1, "overlap": 1, "also": 1, "enquiri": 2, "therefor": 1, "zhang": 1, "rail": 2, "compound": 1, "sinc": 1, "wordtoken": 1, "thu": 1, "corpora": 2, "correspond": 1, "n": 1, "irrespect": 1, "preced": 1, "gram": 1, "problem": 1, "propos": 1}, "vector_2": [4, 0.4131716542440761, 1, 0, 0, 1]}, {"function": "Neut", "cited": "N06-2049", "provenance": ["Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).", "For detailed info."], "label": "Non-Prov", "citing": "W10-4138", "vector": [1, 0, 0, 0.0], "context": ["", "Thus, the bigram RAIL ENQUIRIES gives a misleading probability that RAIL is followed by ENQUIRIES irrespective of what precedes it. This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al (2006) were proposed.", ""], "marker": "2006", "vector_1": {"bigram": 1, "give": 1, "al": 1, "tag": 1, "et": 1, "follow": 1, "happen": 1, "still": 1, "unit": 1, "probabl": 1, "mislead": 1, "appear": 1, "solut": 1, "overlap": 1, "also": 1, "enquiri": 2, "therefor": 1, "zhang": 1, "rail": 2, "compound": 1, "sinc": 1, "wordtoken": 1, "thu": 1, "corpora": 2, "correspond": 1, "n": 1, "irrespect": 1, "preced": 1, "gram": 1, "problem": 1, "propos": 1}, "vector_2": [4, 0.4131716542440761, 1, 0, 0, 1]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["The weight could also be divided by the rank (RANK) to penalise supersenses further down the list."], "label": "Non-Prov", "citing": "J07-4005", "vector": [4, 0, 1, 0.10341753799900383], "context": ["", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", ""], "marker": "2005", "vector_1": {"text": 1, "curran": 1, "automat": 2, "one": 1, "find": 1, "wordnet": 2, "use": 2, "inventori": 1, "unknown": 1, "also": 1, "adapt": 1, "synset": 1, "sens": 1, "might": 1, "method": 2, "induc": 1, "relat": 1, "although": 1, "new": 1, "noun": 1, "johnson": 1, "could": 1, "combin": 1, "ciaramita": 1}, "vector_2": [2, 0.3230273921992928, 2, 0, 1, 1]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering."], "label": "Non-Prov", "citing": "J07-4005", "vector": [5, 0, 0, 0.09166984970282112], "context": ["", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", ""], "marker": "2005", "vector_1": {"text": 1, "curran": 1, "automat": 2, "one": 1, "find": 1, "wordnet": 2, "use": 2, "inventori": 1, "unknown": 1, "also": 1, "adapt": 1, "synset": 1, "sens": 1, "might": 1, "method": 2, "induc": 1, "relat": 1, "although": 1, "new": 1, "noun": 1, "johnson": 1, "could": 1, "combin": 1, "ciaramita": 1}, "vector_2": [2, 0.3230273921992928, 2, 0, 1, 1]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["The problem now becomes how to convert the ranked list of extracted synonyms for each unknown noun into a single supersense selection."], "label": "Non-Prov", "citing": "J07-4005", "vector": [3, 0, 0, 0.04950737714883372], "context": ["", "Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.", ""], "marker": "2005", "vector_1": {"text": 1, "curran": 1, "automat": 2, "one": 1, "find": 1, "wordnet": 2, "use": 2, "inventori": 1, "unknown": 1, "also": 1, "adapt": 1, "synset": 1, "sens": 1, "might": 1, "method": 2, "induc": 1, "relat": 1, "although": 1, "new": 1, "noun": 1, "johnson": 1, "could": 1, "combin": 1, "ciaramita": 1}, "vector_2": [2, 0.3230273921992928, 2, 0, 1, 1]}, {"function": "Weak", "cited": "P05-1004", "provenance": ["Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETs hierarchical structure to create many annotated training instances from the synset glosses.", "This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences."], "label": "Non-Prov", "citing": "J09-3004", "vector": [7, 0, 0, 0.0], "context": ["", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", ""], "marker": "Curran 2005", "vector_1": {"curran": 1, "automat": 2, "acquisit": 1, "seem": 1, "suffer": 1, "sever": 1, "acquir": 1, "rather": 1, "overlap": 1, "wordnet": 1, "method": 1, "relationship": 1, "extent": 1, "found": 1, "addit": 1, "seriou": 1, "inform": 1, "limit": 2, "integr": 1, "potenti": 1, "output": 1, "typic": 1}, "vector_2": [4, 0.9903861370470011, 0, 0, 1, 1]}, {"function": "Weak", "cited": "P05-1004", "provenance": ["The corpus consists of the British National Corpus (BNC), the Reuters Corpus Volume 1 (RCV1), and most of the Linguistic Data Consortiums news text collected since 1987: Continuous Speech Recognition III (CSRIII); North American News Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus.", "The components and their sizes including punctuation are given in Table 3."], "label": "Non-Prov", "citing": "J09-3004", "vector": [4, 0, 0, 0.0], "context": ["", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", ""], "marker": "Curran 2005", "vector_1": {"curran": 1, "automat": 2, "acquisit": 1, "seem": 1, "suffer": 1, "sever": 1, "acquir": 1, "rather": 1, "overlap": 1, "wordnet": 1, "method": 1, "relationship": 1, "extent": 1, "found": 1, "addit": 1, "seriou": 1, "inform": 1, "limit": 2, "integr": 1, "potenti": 1, "output": 1, "typic": 1}, "vector_2": [4, 0.9903861370470011, 0, 0, 1, 1]}, {"function": "Weak", "cited": "P05-1004", "provenance": ["The interaction between the head verb and the preposition determine whether the noun is an indirect object of a ditransitive verb or alternatively the head of a PP that is modifying the verb.", "However, SEXTANT always attaches the PP to the previous phrase."], "label": "Non-Prov", "citing": "J09-3004", "vector": [7, 0, 0, 0.0], "context": ["", "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.", ""], "marker": "Curran 2005", "vector_1": {"curran": 1, "automat": 2, "acquisit": 1, "seem": 1, "suffer": 1, "sever": 1, "acquir": 1, "rather": 1, "overlap": 1, "wordnet": 1, "method": 1, "relationship": 1, "extent": 1, "found": 1, "addit": 1, "seriou": 1, "inform": 1, "limit": 2, "integr": 1, "potenti": 1, "output": 1, "typic": 1}, "vector_2": [4, 0.9903861370470011, 0, 0, 1, 1]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Finally, SEXTANT collapses the nn, nnprep and adj relations together into a single broad noun-modifier grammatical relation."], "label": "Non-Prov", "citing": "S10-1090", "vector": [1, 0, 0, 0.0], "context": ["", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"use": 1, "set": 1, "wsd": 1, "classbas": 1, "sensegroup": 1, "focus": 1, "classifi": 1, "research": 1, "learn": 1, "predefin": 1, "contrast": 1}, "vector_2": [5, 0.17027141645462257, 6, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["SEXTANT extracts relation tuples (w, r, wt ) for each noun, where w is the headword, r is the relation type and wt is the other word."], "label": "Non-Prov", "citing": "S10-1090", "vector": [2, 0, 0, 0.0], "context": ["", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"use": 1, "set": 1, "wsd": 1, "classbas": 1, "sensegroup": 1, "focus": 1, "classifi": 1, "research": 1, "learn": 1, "predefin": 1, "contrast": 1}, "vector_2": [5, 0.17027141645462257, 6, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1004", "provenance": ["Our evaluation will use exactly the same test sets as Ciaramita and Johnson (2003)."], "label": "Non-Prov", "citing": "S10-1090", "vector": [2, 1, 0, 0.11396057645963795], "context": ["", "In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).", ""], "marker": "Curran, 2005", "vector_1": {"use": 1, "set": 1, "wsd": 1, "classbas": 1, "sensegroup": 1, "focus": 1, "classifi": 1, "research": 1, "learn": 1, "predefin": 1, "contrast": 1}, "vector_2": [5, 0.17027141645462257, 6, 1, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1."], "label": "Non-Prov", "citing": "D09-1149", "vector": [1, 0, 0, 0.0], "context": ["", "This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "ace": 1, "semisupervis": 1, "unsupervis": 1, "focus": 1, "subtask": 1, "paper": 1, "includ": 1, "rdc": 1, "learn": 1, "mani": 1, "method": 4, "propos": 1}, "vector_2": [4, 0.055550295110942775, 14, 4, 6, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It also shows that our system performs best on the subtype SOCIAL.Parent and ROLE."], "label": "Non-Prov", "citing": "D09-1149", "vector": [3, 0, 1, 0.0], "context": ["", "This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "ace": 1, "semisupervis": 1, "unsupervis": 1, "focus": 1, "subtask": 1, "paper": 1, "includ": 1, "rdc": 1, "learn": 1, "mani": 1, "method": 4, "propos": 1}, "vector_2": [4, 0.055550295110942775, 14, 4, 6, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["The reason why we choose SVMs for this purpose is that SVMs represent the state-ofthe-art in the machine learning research community, and there are good implementations of the algorithm available."], "label": "Non-Prov", "citing": "D09-1149", "vector": [5, 0, 1, 0.090075469822209], "context": ["", "This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "ace": 1, "semisupervis": 1, "unsupervis": 1, "focus": 1, "subtask": 1, "paper": 1, "includ": 1, "rdc": 1, "learn": 1, "mani": 1, "method": 4, "propos": 1}, "vector_2": [4, 0.055550295110942775, 14, 4, 6, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.", "This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other."], "label": "Non-Prov", "citing": "D09-1149", "vector": [8, 0, 0, 0.07142857142857142], "context": ["", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "comput": 1, "syntact": 1, "variou": 1, "relat": 1, "system": 1, "lexic": 1, "employ": 1, "mention": 1, "stateoftheart": 1, "pair": 1, "extract": 2, "entiti": 1}, "vector_2": [4, 0.2651895338193984, 1, 4, 6, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.", "Table 4 separately measures the performance of different relation types and major subtypes."], "label": "Non-Prov", "citing": "D09-1149", "vector": [3, 0, 0, 0.0944911182523068], "context": ["", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "comput": 1, "syntact": 1, "variou": 1, "relat": 1, "system": 1, "lexic": 1, "employ": 1, "mention": 1, "stateoftheart": 1, "pair": 1, "extract": 2, "entiti": 1}, "vector_2": [4, 0.2651895338193984, 1, 4, 6, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.", "While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations."], "label": "Non-Prov", "citing": "D09-1149", "vector": [6, 0, 0, 0.130066495428618], "context": ["", "For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "comput": 1, "syntact": 1, "variou": 1, "relat": 1, "system": 1, "lexic": 1, "employ": 1, "mention": 1, "stateoftheart": 1, "pair": 1, "extract": 2, "entiti": 1}, "vector_2": [4, 0.2651895338193984, 1, 4, 6, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": [" ET1DW1: combination of the entity type and the dependent word for M1  H1DW1: combination of the head word and the dependent word for M1  ET2DW2: combination of the entity type and the dependent word for M2  H2DW2: combination of the head word and the dependent word for M2  ET12SameNP: combination of ET12 and whether M1 and M2 included in the same NP  ET12SamePP: combination of ET12 and whether M1 and M2 exist in the same PP  ET12SameVP: combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree.", "This category of features concerns about the information inherent only in the full parse tree."], "label": "Non-Prov", "citing": "D09-1149", "vector": [5, 1, 2, 0.02752409412815901], "context": ["", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", ""], "marker": "Zhou et al., 2005", "vector_1": {"corpu": 1, "unbalanc": 1, "corpora": 1, "greatli": 1, "ace": 2, "relat": 1, "well": 1, "number": 1, "instanc": 1, "tabl": 1, "rdc": 2, "shown": 1, "known": 1, "type": 1}, "vector_2": [4, 0.4024240128775684, 1, 4, 6, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Table 4 also indicates the low performance on the relation type AT although it frequently occurs in both the training and testing data.", "This suggests the difficulty of detecting and classifying the relation type AT and its subtypes."], "label": "Non-Prov", "citing": "D09-1149", "vector": [7, 0, 1, 0.22821773229381923], "context": ["", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", ""], "marker": "Zhou et al., 2005", "vector_1": {"corpu": 1, "unbalanc": 1, "corpora": 1, "greatli": 1, "ace": 2, "relat": 1, "well": 1, "number": 1, "instanc": 1, "tabl": 1, "rdc": 2, "shown": 1, "known": 1, "type": 1}, "vector_2": [4, 0.4024240128775684, 1, 4, 6, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Others like PART.Subsidary and SOCIAL.", "Other-Professional also suffer from their low occurrences."], "label": "Non-Prov", "citing": "D09-1149", "vector": [0, 0, 0, 0.0], "context": ["", "It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.", ""], "marker": "Zhou et al., 2005", "vector_1": {"corpu": 1, "unbalanc": 1, "corpora": 1, "greatli": 1, "ace": 2, "relat": 1, "well": 1, "number": 1, "instanc": 1, "tabl": 1, "rdc": 2, "shown": 1, "known": 1, "type": 1}, "vector_2": [4, 0.4024240128775684, 1, 4, 6, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance."], "label": "Non-Prov", "citing": "P06-1017", "vector": [1, 0, 0, 0.0], "context": ["", "Many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithms (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "algorithm": 3, "semisupervis": 1, "eg": 1, "learn": 4, "unsupervis": 1, "address": 1, "mani": 1, "problem": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.0491425646275915, 9, 18, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include:  WM1: bag-of-words in M1  HM1: head word of M1 3 In ACE, each mention has a head annotation and an."], "label": "Non-Prov", "citing": "P06-1017", "vector": [2, 0, 0, 0.0], "context": ["", "Many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithms (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "algorithm": 3, "semisupervis": 1, "eg": 1, "learn": 4, "unsupervis": 1, "address": 1, "mani": 1, "problem": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.0491425646275915, 9, 18, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998)."], "label": "Non-Prov", "citing": "P06-1017", "vector": [3, 1, 1, 0.4364357804719847], "context": ["", "Many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithms (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"machin": 1, "supervis": 1, "algorithm": 3, "semisupervis": 1, "eg": 1, "learn": 4, "unsupervis": 1, "address": 1, "mani": 1, "problem": 1, "method": 1, "propos": 1}, "vector_2": [1, 0.0491425646275915, 9, 18, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Section 3 and Section 4 describe our approach and various features employed respectively."], "label": "Non-Prov", "citing": "P06-1017", "vector": [1, 0, 0, 0.0], "context": ["", "Bootstrapping Currently most of works on the RDC task of ACE focused on supervised learning methods Culotta and Soresen (2004; Kambhatla (2004; Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"supervis": 1, "work": 1, "zhou": 1, "ace": 1, "soresen": 1, "bootstrap": 1, "focus": 1, "al": 1, "current": 1, "task": 1, "kambhatla": 1, "rdc": 1, "learn": 1, "et": 1, "culotta": 1, "method": 1}, "vector_2": [1, 0.9191621875266616, 1, 0, 18, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["(2002)."], "label": "Non-Prov", "citing": "P06-1017", "vector": [0, 0, 0, 0.0], "context": ["", "Bootstrapping Currently most of works on the RDC task of ACE focused on supervised learning methods Culotta and Soresen (2004; Kambhatla (2004; Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"supervis": 1, "work": 1, "zhou": 1, "ace": 1, "soresen": 1, "bootstrap": 1, "focus": 1, "al": 1, "current": 1, "task": 1, "kambhatla": 1, "rdc": 1, "learn": 1, "et": 1, "culotta": 1, "method": 1}, "vector_2": [1, 0.9191621875266616, 1, 0, 18, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list."], "label": "Non-Prov", "citing": "P06-1017", "vector": [3, 0, 0, 0.0], "context": ["", "Bootstrapping Currently most of works on the RDC task of ACE focused on supervised learning methods Culotta and Soresen (2004; Kambhatla (2004; Zhou et al (2005).", ""], "marker": "2005", "vector_1": {"supervis": 1, "work": 1, "zhou": 1, "ace": 1, "soresen": 1, "bootstrap": 1, "focus": 1, "al": 1, "current": 1, "task": 1, "kambhatla": 1, "rdc": 1, "learn": 1, "et": 1, "culotta": 1, "method": 1}, "vector_2": [1, 0.9191621875266616, 1, 0, 18, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities."], "label": "Non-Prov", "citing": "P06-1017", "vector": [3, 0, 0, 0.0], "context": ["", "In the future, we would like to use more effective feature sets Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "like": 1, "would": 1, "effect": 1, "al": 1, "set": 1, "futur": 1, "zhou": 1, "et": 1}, "vector_2": [1, 0.948895145465404, 1, 0, 18, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Two features are defined to include this information:  ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype."], "label": "Non-Prov", "citing": "P06-1017", "vector": [2, 0, 0, 0.0], "context": ["", "In the future, we would like to use more effective feature sets Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "like": 1, "would": 1, "effect": 1, "al": 1, "set": 1, "futur": 1, "zhou": 1, "et": 1}, "vector_2": [1, 0.948895145465404, 1, 0, 18, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text."], "label": "Non-Prov", "citing": "P06-1017", "vector": [2, 0, 1, 0.0], "context": ["", "In the future, we would like to use more effective feature sets Zhou et al (2005)", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "like": 1, "would": 1, "effect": 1, "al": 1, "set": 1, "futur": 1, "zhou": 1, "et": 1}, "vector_2": [1, 0.948895145465404, 1, 0, 18, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["In Dietterich T.G., Becker S. and Ghahramani Z. editors.", "Advances in Neural Information Processing Systems 14.", "Cambridge, MA."], "label": "Non-Prov", "citing": "P08-2023", "vector": [2, 0, 0, 0.05698028822981897], "context": ["", "Based on his work, Zhou et al (2005) further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", ""], "marker": "2005", "vector_1": {"person": 1, "word": 1, "name": 1, "zhou": 1, "collect": 1, "semiautomat": 1, "trigger": 1, "chunk": 1, "work": 1, "list": 2, "al": 1, "rel": 1, "inform": 1, "base": 2, "incorpor": 1, "phrase": 1, "et": 1, "countri": 1}, "vector_2": [3, 0.2484637201607185, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Note that only 6 of these 24 relation subtypes are symmetric: Relative- Location, Associate, Other-Relative, Other- Professional, Sibling, and Spouse.", "In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a NONE class for the case where the two mentions are not related.", "5.2 Experimental Results."], "label": "Non-Prov", "citing": "P08-2023", "vector": [3, 0, 0, 0.03143473067309657], "context": ["", "Based on his work, Zhou et al (2005) further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", ""], "marker": "2005", "vector_1": {"person": 1, "word": 1, "name": 1, "zhou": 1, "collect": 1, "semiautomat": 1, "trigger": 1, "chunk": 1, "work": 1, "list": 2, "al": 1, "rel": 1, "inform": 1, "base": 2, "incorpor": 1, "phrase": 1, "et": 1, "countri": 1}, "vector_2": [3, 0.2484637201607185, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["For example, in the case where the noun phrase the former CEO of McDonald has the head annotation of CEO and the extent annotation of the former CEO of McDonald, we only consider the former CEO in this paper.", "4 In this paper, the head word of a mention is normally.", "set as the last word of the mention."], "label": "Non-Prov", "citing": "P08-2023", "vector": [4, 0, 0, 0.08398387664337813], "context": ["", "Based on his work, Zhou et al (2005) further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.", ""], "marker": "2005", "vector_1": {"person": 1, "word": 1, "name": 1, "zhou": 1, "collect": 1, "semiautomat": 1, "trigger": 1, "chunk": 1, "work": 1, "list": 2, "al": 1, "rel": 1, "inform": 1, "base": 2, "incorpor": 1, "phrase": 1, "et": 1, "countri": 1}, "vector_2": [3, 0.2484637201607185, 1, 0, 0, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering."], "label": "Non-Prov", "citing": "P11-3012", "vector": [3, 0, 0, 0.10690449676496974], "context": ["", "We used Zhou et al.s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", ""], "marker": "Zhou et al., 2005", "vector_1": {"use": 1, "featur": 2, "zhou": 1, "al": 1, "lexic": 1, "system": 1, "done": 1, "et": 1, "research": 1, "similar": 1, "basi": 1}, "vector_2": [6, 0.31043752940385777, 2, 2, 0, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus."], "label": "Non-Prov", "citing": "P11-3012", "vector": [4, 0, 1, 0.0668153104781061], "context": ["", "We used Zhou et al.s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", ""], "marker": "Zhou et al., 2005", "vector_1": {"use": 1, "featur": 2, "zhou": 1, "al": 1, "lexic": 1, "system": 1, "done": 1, "et": 1, "research": 1, "similar": 1, "basi": 1}, "vector_2": [6, 0.31043752940385777, 2, 2, 0, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["This trigger word list is first gathered from WordNet by checking whether a word has the semantic class person||relative."], "label": "Non-Prov", "citing": "P11-3012", "vector": [1, 0, 0, 0.0], "context": ["", "We used Zhou et al.s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).", ""], "marker": "Zhou et al., 2005", "vector_1": {"use": 1, "featur": 2, "zhou": 1, "al": 1, "lexic": 1, "system": 1, "done": 1, "et": 1, "research": 1, "similar": 1, "basi": 1}, "vector_2": [6, 0.31043752940385777, 2, 2, 0, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": ["It shows that the"], "label": "Non-Prov", "citing": "P11-3012", "vector": [1, 0, 0, 0.0], "context": ["", "Although a bit lower than Zhou et al.s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", ""], "marker": "Zhou et al., 2005", "vector_1": {"lower": 1, "parser": 1, "zhou": 1, "semant": 1, "use": 2, "al": 1, "inform": 1, "token": 1, "featur": 1, "result": 1, "although": 1, "et": 1, "bit": 1, "differ": 3, "attribut": 1}, "vector_2": [6, 0.41313260299751325, 1, 2, 0, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": ["(2004)."], "label": "Non-Prov", "citing": "P11-3012", "vector": [0, 0, 0, 0.0], "context": ["", "Although a bit lower than Zhou et al.s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", ""], "marker": "Zhou et al., 2005", "vector_1": {"lower": 1, "parser": 1, "zhou": 1, "semant": 1, "use": 2, "al": 1, "inform": 1, "token": 1, "featur": 1, "result": 1, "although": 1, "et": 1, "bit": 1, "differ": 3, "attribut": 1}, "vector_2": [6, 0.41313260299751325, 1, 2, 0, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": [" SC1ET2: combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype."], "label": "Non-Prov", "citing": "P11-3012", "vector": [5, 0, 1, 0.05590169943749474], "context": ["", "Although a bit lower than Zhou et al.s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", ""], "marker": "Zhou et al., 2005", "vector_1": {"lower": 1, "parser": 1, "zhou": 1, "semant": 1, "use": 2, "al": 1, "inform": 1, "token": 1, "featur": 1, "result": 1, "although": 1, "et": 1, "bit": 1, "differ": 3, "attribut": 1}, "vector_2": [6, 0.41313260299751325, 1, 2, 0, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": ["However, when a preposition exists in the mention, its head word is set as the last word before the preposition.", "For example, the head word of the name mention University of Michigan is University."], "label": "Non-Prov", "citing": "P13-1147", "vector": [4, 0, 0, 0.0], "context": ["", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "featur": 1, "featuresinst": 1, "parser": 1, "ii": 1, "phrase": 1, "ie": 1, "differ": 2, "depend": 1, "iii": 1, "behind": 1, "threefold": 1, "preprocess": 1, "might": 1, "sourc": 1, "resourc": 1, "partit": 1, "zhang": 1, "use": 1, "reason": 1, "data": 1, "averag": 1, "addit": 1, "slightli": 1, "chunker": 1, "incorpor": 1}, "vector_2": [8, 0.6765932650432436, 2, 3, 11, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": ["It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall.", "It also shows that feature-based methods dramatically outperform kernel methods."], "label": "Non-Prov", "citing": "P13-1147", "vector": [1, 0, 0, 0.0], "context": ["", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "featur": 1, "featuresinst": 1, "parser": 1, "ii": 1, "phrase": 1, "ie": 1, "differ": 2, "depend": 1, "iii": 1, "behind": 1, "threefold": 1, "preprocess": 1, "might": 1, "sourc": 1, "resourc": 1, "partit": 1, "zhang": 1, "use": 1, "reason": 1, "data": 1, "averag": 1, "addit": 1, "slightli": 1, "chunker": 1, "incorpor": 1}, "vector_2": [8, 0.6765932650432436, 2, 3, 11, 0]}, {"function": "CoCo", "cited": "P05-1053", "provenance": ["(1999).", "Head-driven statistical models for natural language parsing."], "label": "Non-Prov", "citing": "P13-1147", "vector": [0, 0, 0, 0.0], "context": ["", "This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).", ""], "marker": "Zhou et al., 2005", "vector_1": {"semant": 1, "featur": 1, "featuresinst": 1, "parser": 1, "ii": 1, "phrase": 1, "ie": 1, "differ": 2, "depend": 1, "iii": 1, "behind": 1, "threefold": 1, "preprocess": 1, "might": 1, "sourc": 1, "resourc": 1, "partit": 1, "zhang": 1, "use": 1, "reason": 1, "data": 1, "averag": 1, "addit": 1, "slightli": 1, "chunker": 1, "incorpor": 1}, "vector_2": [8, 0.6765932650432436, 2, 3, 11, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["This may be due to the fact that most of relations in the ACE corpus are quite local."], "label": "Non-Prov", "citing": "W06-1667", "vector": [1, 0, 0, 0.0], "context": ["", "Prior work on automatic relation extraction come in three kinds: supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "kind": 1, "work": 1, "algorithm": 3, "semisupervis": 1, "extract": 1, "unsupervis": 1, "relat": 1, "three": 1, "automat": 1, "prior": 1, "learn": 3, "come": 1}, "vector_2": [1, 0.04478182722092623, 9, 23, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["For details about SVMLight, please see http://svmlight.joachims.org/"], "label": "Non-Prov", "citing": "W06-1667", "vector": [0, 0, 0, 0.0], "context": ["", "Prior work on automatic relation extraction come in three kinds: supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "kind": 1, "work": 1, "algorithm": 3, "semisupervis": 1, "extract": 1, "unsupervis": 1, "relat": 1, "three": 1, "automat": 1, "prior": 1, "learn": 3, "come": 1}, "vector_2": [1, 0.04478182722092623, 9, 23, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998)."], "label": "Non-Prov", "citing": "W06-1667", "vector": [2, 1, 0, 0.3614784456460256], "context": ["", "Prior work on automatic relation extraction come in three kinds: supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004).", ""], "marker": "Zhou et al., 2005", "vector_1": {"supervis": 1, "kind": 1, "work": 1, "algorithm": 3, "semisupervis": 1, "extract": 1, "unsupervis": 1, "relat": 1, "three": 1, "automat": 1, "prior": 1, "learn": 3, "come": 1}, "vector_2": [1, 0.04478182722092623, 9, 23, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Then, all the trigger words are semi-automatically6 classified into different categories according to their related personal social relation subtypes."], "label": "Non-Prov", "citing": "W06-1667", "vector": [2, 0, 0, 0.048280454958526765], "context": ["", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "captur": 1, "chunk": 1, "still": 1, "entiti": 1, "supervis": 1, "depend": 1, "concern": 1, "simpl": 1, "method": 1, "full": 1, "especi": 1, "complement": 1, "pars": 1, "although": 1, "pair": 1, "characterist": 1, "word": 1, "provid": 1, "tree": 2, "inform": 3, "incorpor": 1}, "vector_2": [1, 0.8953688477570844, 4, 23, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion."], "label": "Non-Prov", "citing": "W06-1667", "vector": [4, 0, 0, 0.037986858819879316], "context": ["", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "captur": 1, "chunk": 1, "still": 1, "entiti": 1, "supervis": 1, "depend": 1, "concern": 1, "simpl": 1, "method": 1, "full": 1, "especi": 1, "complement": 1, "pars": 1, "although": 1, "pair": 1, "characterist": 1, "word": 1, "provid": 1, "tree": 2, "inform": 3, "incorpor": 1}, "vector_2": [1, 0.8953688477570844, 4, 23, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["4 5 5 . 9 6 2 . 3 Subsid iary 2 7 1 4 2 3 37."], "label": "Non-Prov", "citing": "W06-1667", "vector": [0, 0, 0, 0.0], "context": ["", "Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.", ""], "marker": "Zhou et al., 2005", "vector_1": {"featur": 1, "captur": 1, "chunk": 1, "still": 1, "entiti": 1, "supervis": 1, "depend": 1, "concern": 1, "simpl": 1, "method": 1, "full": 1, "especi": 1, "complement": 1, "pars": 1, "although": 1, "pair": 1, "characterist": 1, "word": 1, "provid": 1, "tree": 2, "inform": 3, "incorpor": 1}, "vector_2": [1, 0.8953688477570844, 4, 23, 0, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall."], "label": "Non-Prov", "citing": "W08-0602", "vector": [1, 0, 0, 0.0], "context": ["", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", ""], "marker": "2005", "vector_1": {"nlp": 1, "zhou": 1, "success": 1, "gener": 1, "al": 1, "see": 1, "exampl": 1, "et": 1, "follow": 1, "method": 1}, "vector_2": [3, 0.15162721893491124, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document."], "label": "Non-Prov", "citing": "W08-0602", "vector": [4, 0, 0, 0.0], "context": ["", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", ""], "marker": "2005", "vector_1": {"nlp": 1, "zhou": 1, "success": 1, "gener": 1, "al": 1, "see": 1, "exampl": 1, "et": 1, "follow": 1, "method": 1}, "vector_2": [3, 0.15162721893491124, 1, 0, 1, 0]}, {"function": "Neut", "cited": "P05-1053", "provenance": ["Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted."], "label": "Non-Prov", "citing": "W08-0602", "vector": [4, 0, 0, 0.0], "context": ["", "This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).", ""], "marker": "2005", "vector_1": {"nlp": 1, "zhou": 1, "success": 1, "gener": 1, "al": 1, "see": 1, "exampl": 1, "et": 1, "follow": 1, "method": 1}, "vector_2": [3, 0.15162721893491124, 1, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["During development, 155 of 674 documents in the training set are set aside for fine-tuning the system."], "label": "Non-Prov", "citing": "W08-0602", "vector": [1, 0, 0, 0.0], "context": ["", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "develop": 1, "describ": 1, "zhou": 1, "al": 2, "part": 1, "et": 2, "wang": 1}, "vector_2": [3, 0.5425778235142784, 2, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": ["While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations."], "label": "Non-Prov", "citing": "W08-0602", "vector": [3, 0, 0, 0.10540925533894598], "context": ["", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "develop": 1, "describ": 1, "zhou": 1, "al": 2, "part": 1, "et": 2, "wang": 1}, "vector_2": [3, 0.5425778235142784, 2, 0, 1, 0]}, {"function": "Pos", "cited": "P05-1053", "provenance": [" Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase."], "label": "Non-Prov", "citing": "W08-0602", "vector": [2, 0, 0, 0.0816496580927726], "context": ["", "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).", ""], "marker": "2005", "vector_1": {"use": 1, "featur": 1, "develop": 1, "describ": 1, "zhou": 1, "al": 2, "part": 1, "et": 2, "wang": 1}, "vector_2": [3, 0.5425778235142784, 2, 0, 1, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["cor e."], "label": "Non-Prov", "citing": "J05-1004", "vector": [0, 0, 0, 0.0], "context": ["", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levins original classes, adding an additional level to the hierarchy (Dang et al 1998).", ""], "marker": "Dang et al. 1998", "vector_1": {"case": 1, "origin": 1, "use": 1, "ad": 1, "level": 1, "provid": 1, "levin": 1, "al": 1, "caus": 1, "inform": 1, "dang": 1, "subdivid": 1, "intersect": 1, "et": 1, "hierarchi": 1, "mani": 1, "addit": 2, "class": 2, "verbnet": 1}, "vector_2": [7, 0.5, 0, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["2."], "label": "Non-Prov", "citing": "J05-1004", "vector": [0, 0, 0, 0.0], "context": ["", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levins original classes, adding an additional level to the hierarchy (Dang et al 1998).", ""], "marker": "Dang et al. 1998", "vector_1": {"case": 1, "origin": 1, "use": 1, "ad": 1, "level": 1, "provid": 1, "levin": 1, "al": 1, "caus": 1, "inform": 1, "dang": 1, "subdivid": 1, "intersect": 1, "et": 1, "hierarchi": 1, "mani": 1, "addit": 2, "class": 2, "verbnet": 1}, "vector_2": [7, 0.5, 0, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Nora pushed at/against the package.."], "label": "Non-Prov", "citing": "J05-1004", "vector": [1, 0, 0, 0.0], "context": ["", "In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levins original classes, adding an additional level to the hierarchy (Dang et al 1998).", ""], "marker": "Dang et al. 1998", "vector_1": {"case": 1, "origin": 1, "use": 1, "ad": 1, "level": 1, "provid": 1, "levin": 1, "al": 1, "caus": 1, "inform": 1, "dang": 1, "subdivid": 1, "intersect": 1, "et": 1, "hierarchi": 1, "mani": 1, "addit": 2, "class": 2, "verbnet": 1}, "vector_2": [7, 0.5, 0, 2, 2, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["For example, flutuar does not take a direct object, so some of the alterna tions that are related to its transitive meaning are not present."], "label": "Non-Prov", "citing": "W02-1108", "vector": [2, 0, 0, 0.0], "context": ["", "Manual classification of verbs to semanticclasses yields accurate results but is time con suming (Levin, 1993; Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"classif": 1, "accur": 1, "manual": 1, "yield": 1, "verb": 1, "result": 1, "time": 1, "semanticclass": 1, "sume": 1, "con": 1}, "vector_2": [4, 0.5, 2, 2, 2, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["This is an obvious shortcoming of the current implementation of intersec tive classes, and might affect the choice of 3 as a relevance cutoff in later implementations."], "label": "Non-Prov", "citing": "W02-1108", "vector": [2, 0, 0, 0.0], "context": ["", "Manual classification of verbs to semanticclasses yields accurate results but is time con suming (Levin, 1993; Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"classif": 1, "accur": 1, "manual": 1, "yield": 1, "verb": 1, "result": 1, "time": 1, "semanticclass": 1, "sume": 1, "con": 1}, "vector_2": [4, 0.5, 2, 2, 2, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["(carry verb implies causation of accompa nied motion, no separation) 2."], "label": "Non-Prov", "citing": "W02-1108", "vector": [1, 0, 0, 0.0], "context": ["", "Manual classification of verbs to semanticclasses yields accurate results but is time con suming (Levin, 1993; Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"classif": 1, "accur": 1, "manual": 1, "yield": 1, "verb": 1, "result": 1, "time": 1, "semanticclass": 1, "sume": 1, "con": 1}, "vector_2": [4, 0.5, 2, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["*Nora pushed at the package to Pamela."], "label": "Non-Prov", "citing": "W02-1108", "vector": [1, 0, 0, 0.0], "context": ["", "Dang et al (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", ""], "marker": "1998", "vector_1": {"classif": 1, "creat": 1, "levin": 1, "share": 1, "al": 1, "one": 1, "current": 1, "verb": 1, "membership": 1, "exampl": 1, "intersect": 1, "refin": 1, "dang": 1, "et": 1, "class": 2}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Whereas high level semantic relations (syn onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class."], "label": "Non-Prov", "citing": "W02-1108", "vector": [4, 0, 0, 0.1720618004029213], "context": ["", "Dang et al (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", ""], "marker": "1998", "vector_1": {"classif": 1, "creat": 1, "levin": 1, "share": 1, "al": 1, "one": 1, "current": 1, "verb": 1, "membership": 1, "exampl": 1, "intersect": 1, "refin": 1, "dang": 1, "et": 1, "class": 2}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["completely (scrape, scratch), having cut into, incise as an immediate hypernym, which in turn has cut, separate with an in strument as an immediate hypernym."], "label": "Non-Prov", "citing": "W02-1108", "vector": [1, 0, 0, 0.0], "context": ["", "Dang et al (1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.", ""], "marker": "1998", "vector_1": {"classif": 1, "creat": 1, "levin": 1, "share": 1, "al": 1, "one": 1, "current": 1, "verb": 1, "membership": 1, "exampl": 1, "intersect": 1, "refin": 1, "dang": 1, "et": 1, "class": 2}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un derlying semantic components that constrain al lowable arguments."], "label": "Non-Prov", "citing": "W04-2606", "vector": [4, 0, 0, 0.0], "context": ["", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "comput": 1, "consider": 1, "relationship": 1, "captur": 1, "eg": 1, "lexicalsemant": 1, "syntax": 1, "aim": 1, "verb": 1, "interest": 1, "close": 1, "linguist": 2, "attract": 1, "class": 1}, "vector_2": [6, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["There are still many questions that require further investigation."], "label": "Non-Prov", "citing": "W04-2606", "vector": [0, 0, 0, 0.0], "context": ["", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "comput": 1, "consider": 1, "relationship": 1, "captur": 1, "eg": 1, "lexicalsemant": 1, "syntax": 1, "aim": 1, "verb": 1, "interest": 1, "close": 1, "linguist": 2, "attract": 1, "class": 1}, "vector_2": [6, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["When a roll verb occurs in the tran sitive (Bill moved the box across the room), the subject physically causes the object to move, whereas the subject of a transitive run verb merely induces the object to move (the coach ran the athlete around the track)."], "label": "Non-Prov", "citing": "W04-2606", "vector": [4, 0, 0, 0.0], "context": ["", "Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).", ""], "marker": "Dang et al., 1998", "vector_1": {"semant": 1, "comput": 1, "consider": 1, "relationship": 1, "captur": 1, "eg": 1, "lexicalsemant": 1, "syntax": 1, "aim": 1, "verb": 1, "interest": 1, "close": 1, "linguist": 2, "attract": 1, "class": 1}, "vector_2": [6, 0.5, 6, 2, 2, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the mid dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily."], "label": "Non-Prov", "citing": "W11-0910", "vector": [2, 0, 0, 0.0], "context": ["", "Complicating the issue is the phenomenon of regular sense extensions (Dang et al., 1998), where what once may have been coercion has become entrenched and is now seen as a different sense of the verb.", ""], "marker": "Dang et al., 1998", "vector_1": {"differ": 1, "entrench": 1, "phenomenon": 1, "may": 1, "coercion": 1, "verb": 1, "issu": 1, "extens": 1, "regular": 1, "seen": 1, "sens": 2, "becom": 1, "complic": 1}, "vector_2": [13, 0.270500710524116, 1, 2, 4, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Current approaches to English classifica tion, Levin classes and WordNet, have limita tions in their applicability that impede their utility as general classification schemes."], "label": "Non-Prov", "citing": "W11-0910", "vector": [3, 0, 0, 0.0], "context": ["", "Complicating the issue is the phenomenon of regular sense extensions (Dang et al., 1998), where what once may have been coercion has become entrenched and is now seen as a different sense of the verb.", ""], "marker": "Dang et al., 1998", "vector_1": {"differ": 1, "entrench": 1, "phenomenon": 1, "may": 1, "coercion": 1, "verb": 1, "issu": 1, "extens": 1, "regular": 1, "seen": 1, "sens": 2, "becom": 1, "complic": 1}, "vector_2": [13, 0.270500710524116, 1, 2, 4, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig inal Levin classes."], "label": "Non-Prov", "citing": "W11-0910", "vector": [3, 0, 0, 0.0], "context": ["", "Complicating the issue is the phenomenon of regular sense extensions (Dang et al., 1998), where what once may have been coercion has become entrenched and is now seen as a different sense of the verb.", ""], "marker": "Dang et al., 1998", "vector_1": {"differ": 1, "entrench": 1, "phenomenon": 1, "may": 1, "coercion": 1, "verb": 1, "issu": 1, "extens": 1, "regular": 1, "seen": 1, "sens": 2, "becom": 1, "complic": 1}, "vector_2": [13, 0.270500710524116, 1, 2, 4, 0]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["The only escape from this lim itation will be through the use of automated or semi-automated methods of lexical acquisi tion."], "label": "Non-Prov", "citing": "W99-0632", "vector": [3, 0, 2, 0.07453559924999299], "context": ["", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim plify the definition of different verb senses.", ""], "marker": "1998", "vector_1": {"use": 1, "syntact": 1, "frame": 1, "palmer": 1, "definit": 1, "argu": 1, "verb": 2, "sens": 1, "dang": 1, "et": 1, "differ": 1, "class": 1, "sim": 1, "plifi": 1}, "vector_2": [1, 0.16959409014106644, 2, 0, 4, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Levin verb classes are based on an underlying lat tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations."], "label": "Non-Prov", "citing": "W99-0632", "vector": [3, 0, 1, 0.18898223650461363], "context": ["", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim plify the definition of different verb senses.", ""], "marker": "1998", "vector_1": {"use": 1, "syntact": 1, "frame": 1, "palmer": 1, "definit": 1, "argu": 1, "verb": 2, "sens": 1, "dang": 1, "et": 1, "differ": 1, "class": 1, "sim": 1, "plifi": 1}, "vector_2": [1, 0.16959409014106644, 2, 0, 4, 1]}, {"function": "Neut", "cited": "P98-1046", "provenance": ["Whereas high level semantic relations (syn onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class."], "label": "Non-Prov", "citing": "W99-0632", "vector": [4, 0, 1, 0.16222142113076254], "context": ["", "Palmer (1999) and Dang et a!. (1998) argue that the use of syntactic frames and verb classes can sim plify the definition of different verb senses.", ""], "marker": "1998", "vector_1": {"use": 1, "syntact": 1, "frame": 1, "palmer": 1, "definit": 1, "argu": 1, "verb": 2, "sens": 1, "dang": 1, "et": 1, "differ": 1, "class": 1, "sim": 1, "plifi": 1}, "vector_2": [1, 0.16959409014106644, 2, 0, 4, 1]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["*Nora pushed at the package to Pamela."], "label": "Non-Prov", "citing": "W99-0632", "vector": [1, 0, 0, 0.0], "context": ["", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"differ": 1, "scheme": 1, "levin": 1, "semant": 1, "classif": 1, "also": 1, "verb": 1, "plan": 1, "intersect": 1, "experi": 1, "class": 1, "wordnet": 1}, "vector_2": [1, 0.9864434751856654, 2, 2, 1, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["Thus they cannot take the conative al ternation."], "label": "Non-Prov", "citing": "W99-0632", "vector": [0, 0, 0, 0.0], "context": ["", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"differ": 1, "scheme": 1, "levin": 1, "semant": 1, "classif": 1, "also": 1, "verb": 1, "plan": 1, "intersect": 1, "experi": 1, "class": 1, "wordnet": 1}, "vector_2": [1, 0.9864434751856654, 2, 2, 1, 0]}, {"function": "Pos", "cited": "P98-1046", "provenance": ["isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses."], "label": "Non-Prov", "citing": "W99-0632", "vector": [3, 0, 1, 0.12309149097933272], "context": ["", "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).", ""], "marker": "Dang et al., 1998", "vector_1": {"differ": 1, "scheme": 1, "levin": 1, "semant": 1, "classif": 1, "also": 1, "verb": 1, "plan": 1, "intersect": 1, "experi": 1, "class": 1, "wordnet": 1}, "vector_2": [1, 0.9864434751856654, 2, 2, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["We used the robust approach as a basis for devel oping a genre-specific reference resolution approach in Polish."], "label": "Non-Prov", "citing": "P00-1022", "vector": [2, 0, 0, 0.062017367294604234], "context": ["", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", ""], "marker": "1998", "vector_1": {"origin": 1, "restrict": 1, "semant": 1, "shallow": 1, "mitkov": 1, "see": 1, "inform": 1, "resolut": 1, "prefer": 1, "employ": 1, "current": 1, "consequ": 1, "reli": 1, "exampl": 1, "heurist": 1, "morphosyntact": 1, "analysi": 1, "mainli": 1, "method": 1, "anaphora": 1}, "vector_2": [2, 0.134280882589789, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["If this indicator does not hold again, go for the most recent candidate."], "label": "Non-Prov", "citing": "P00-1022", "vector": [1, 0, 0, 0.0], "context": ["", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", ""], "marker": "1998", "vector_1": {"origin": 1, "restrict": 1, "semant": 1, "shallow": 1, "mitkov": 1, "see": 1, "inform": 1, "resolut": 1, "prefer": 1, "employ": 1, "current": 1, "consequ": 1, "reli": 1, "exampl": 1, "heurist": 1, "morphosyntact": 1, "analysi": 1, "mainli": 1, "method": 1, "anaphora": 1}, "vector_2": [2, 0.134280882589789, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["With a view to avoiding complex syntactic, seman tic and discourse analysis (which is vital for real world applications), we developed a robust, knowl edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors."], "label": "Non-Prov", "citing": "P00-1022", "vector": [5, 0, 0, 0.08606629658238703], "context": ["", "Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).", ""], "marker": "1998", "vector_1": {"origin": 1, "restrict": 1, "semant": 1, "shallow": 1, "mitkov": 1, "see": 1, "inform": 1, "resolut": 1, "prefer": 1, "employ": 1, "current": 1, "consequ": 1, "reli": 1, "exampl": 1, "heurist": 1, "morphosyntact": 1, "analysi": 1, "mainli": 1, "method": 1, "anaphora": 1}, "vector_2": [2, 0.134280882589789, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Also, a sequence of noun phrases with the same head counts as lexical reiteration (e.g. \"toner bottle\", \"bottle of toner\", \"the bottle\").", "Section heading preference If a noun phrase occurs in the heading of the section, part of which is the current sentence, then we con sider it as the preferred candidate (1, 0)."], "label": "Non-Prov", "citing": "P00-1022", "vector": [3, 0, 0, 0.0], "context": ["", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", ""], "marker": "1998", "vector_1": {"mitkov": 1, "success": 1, "work": 1, "manual": 1, "obtain": 1, "rate": 1, "english": 1, "pronomin": 1, "technic": 1, "refer": 1}, "vector_2": [2, 0.9156466419713319, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The first Base line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%.", "There fore, the 93.3% success rate (see above) demon strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences."], "label": "Non-Prov", "citing": "P00-1022", "vector": [4, 0, 3, 0.21380899352993948], "context": ["", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", ""], "marker": "1998", "vector_1": {"mitkov": 1, "success": 1, "work": 1, "manual": 1, "obtain": 1, "rate": 1, "english": 1, "pronomin": 1, "technic": 1, "refer": 1}, "vector_2": [2, 0.9156466419713319, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["This preference can be explained in terms of sali ence from the point of view of the centering theory.", "The latter proposes the ranking \"subject, direct ob ject, indirect object\" (Brennan et al. 1987) and noun phrases which are parts of prepositional phrases are usually indirect objects."], "label": "Non-Prov", "citing": "P00-1022", "vector": [1, 0, 0, 0.0], "context": ["", "Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.", ""], "marker": "1998", "vector_1": {"mitkov": 1, "success": 1, "work": 1, "manual": 1, "obtain": 1, "rate": 1, "english": 1, "pronomin": 1, "technic": 1, "refer": 1}, "vector_2": [2, 0.9156466419713319, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["We have recently adapted the approach for Ara bic as well (Mitkov & Belguith 1998)."], "label": "Non-Prov", "citing": "P00-1022", "vector": [2, 1, 0, 0.08574929257125441], "context": ["", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", ""], "marker": "1998", "vector_1": {"knowledg": 1, "mitkov": 1, "evalu": 1, "pronoun": 1, "resolut": 2, "limit": 1, "anaphora": 1, "th": 1, "robust": 1, "implement": 1, "pronomin": 1, "ruslan": 1, "sever": 1, "baselin": 1}, "vector_2": [2, 0.9758817845063618, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Out of 223 pro nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\")."], "label": "Non-Prov", "citing": "P00-1022", "vector": [1, 0, 0, 0.0], "context": ["", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", ""], "marker": "1998", "vector_1": {"knowledg": 1, "mitkov": 1, "evalu": 1, "pronoun": 1, "resolut": 2, "limit": 1, "anaphora": 1, "th": 1, "robust": 1, "implement": 1, "pronomin": 1, "ruslan": 1, "sever": 1, "baselin": 1}, "vector_2": [2, 0.9758817845063618, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["55 % I 48 .5 5 % 6 5 . 9 5 % The lower figure in \"Baseline subject\" corresponds to \"recall\" and the higher figure- to \"precision\"."], "label": "Non-Prov", "citing": "P00-1022", "vector": [1, 0, 0, 0.0], "context": ["", "Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.", ""], "marker": "1998", "vector_1": {"knowledg": 1, "mitkov": 1, "evalu": 1, "pronoun": 1, "resolut": 2, "limit": 1, "anaphora": 1, "th": 1, "robust": 1, "implement": 1, "pronomin": 1, "ruslan": 1, "sever": 1, "baselin": 1}, "vector_2": [2, 0.9758817845063618, 1, 0, 3, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The \"Baseline subject\" model tested on the same data scored 33.9% recall and 67.9% precision, whereas \"Baseline most recent\" scored 66.7%.", "Note that \"Baseline subject\" can be assessed both in terms of recall and precision because this \"version\" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many im perative zero-subject sentences)."], "label": "Non-Prov", "citing": "P03-1023", "vector": [4, 0, 0, 0.03673591791853225], "context": ["", "Mitkovs knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "set": 1, "mitkov": 1, "score": 1, "pronoun": 1, "resolut": 1, "indic": 1, "method": 1, "candid": 1, "exampl": 1, "anteced": 1, "rank": 1, "knowledgepoor": 1}, "vector_2": [5, 0.055885805156942185, 1, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The first Base line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%.", "There fore, the 93.3% success rate (see above) demon strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences."], "label": "Non-Prov", "citing": "P03-1023", "vector": [5, 0, 1, 0.04688072309384954], "context": ["", "Mitkovs knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 1, "set": 1, "mitkov": 1, "score": 1, "pronoun": 1, "resolut": 1, "indic": 1, "method": 1, "candid": 1, "exampl": 1, "anteced": 1, "rank": 1, "knowledgepoor": 1}, "vector_2": [5, 0.055885805156942185, 1, 1, 1, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Our evalua tion, based on 63 examples (anaphors) from a tech nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %)."], "label": "Non-Prov", "citing": "P04-1018", "vector": [2, 0, 0, 0.0], "context": ["", "Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)", ""], "marker": "Mitkov, 1998", "vector_1": {"earli": 1, "work": 1, "pronoun": 1, "resolut": 1, "focus": 1, "anteced": 1, "ing": 1, "find": 1, "anaphora": 1}, "vector_2": [6, 0.04756803802765039, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%."], "label": "Non-Prov", "citing": "P04-1018", "vector": [1, 0, 0, 0.0], "context": ["", "Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)", ""], "marker": "Mitkov, 1998", "vector_1": {"earli": 1, "work": 1, "pronoun": 1, "resolut": 1, "focus": 1, "anteced": 1, "ing": 1, "find": 1, "anaphora": 1}, "vector_2": [6, 0.04756803802765039, 3, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj."], "label": "Non-Prov", "citing": "P04-1018", "vector": [2, 0, 0, 0.0], "context": ["", "Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)", ""], "marker": "Mitkov, 1998", "vector_1": {"earli": 1, "work": 1, "pronoun": 1, "resolut": 1, "focus": 1, "anteced": 1, "ing": 1, "find": 1, "anaphora": 1}, "vector_2": [6, 0.04756803802765039, 3, 1, 0, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["Example: Identify the draweq by the lit paper port LED and add paper to itj."], "label": "Non-Prov", "citing": "P05-1021", "vector": [2, 0, 0, 0.0], "context": ["", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al", ""], "marker": "1998", "vector_1": {"use": 1, "learningbas": 1, "knowledg": 1, "perform": 1, "shallow": 1, "resolut": 1, "well": 1, "soon": 1, "al": 1, "inde": 1, "reason": 1, "exist": 1, "et": 1, "limit": 1, "anaphor": 1, "mitkov": 1, "approach": 1, "eg": 1}, "vector_2": [7, 0.0890668178275349, 1, 0, 0, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante cedent."], "label": "Non-Prov", "citing": "P05-1021", "vector": [0, 0, 0, 0.0], "context": ["", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al", ""], "marker": "1998", "vector_1": {"use": 1, "learningbas": 1, "knowledg": 1, "perform": 1, "shallow": 1, "resolut": 1, "well": 1, "soon": 1, "al": 1, "inde": 1, "reason": 1, "exist": 1, "et": 1, "limit": 1, "anaphor": 1, "mitkov": 1, "approach": 1, "eg": 1}, "vector_2": [7, 0.0890668178275349, 1, 0, 0, 0]}, {"function": "Pos", "cited": "P98-2143", "provenance": ["We regard a noun phrase as definite if the head noun is modified by a definite article, or by demonstrative or posses sive pronouns."], "label": "Non-Prov", "citing": "P05-1021", "vector": [0, 0, 0, 0.0], "context": ["", "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al", ""], "marker": "1998", "vector_1": {"use": 1, "learningbas": 1, "knowledg": 1, "perform": 1, "shallow": 1, "resolut": 1, "well": 1, "soon": 1, "al": 1, "inde": 1, "reason": 1, "exist": 1, "et": 1, "limit": 1, "anaphor": 1, "mitkov": 1, "approach": 1, "eg": 1}, "vector_2": [7, 0.0890668178275349, 1, 0, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im mediate reference."], "label": "Non-Prov", "citing": "P05-1021", "vector": [5, 0, 1, 0.08032193289024989], "context": ["", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"c": 2, "word": 1, "ana": 3, "evalu": 1, "whether": 1, "max": 1, "surround": 1, "ilar": 1, "also": 1, "candid": 2, "salienc": 1, "factor": 1, "anaphor": 1, "statsemn": 1, "paralstuctmark": 1, "statsemci": 1, "sim": 1}, "vector_2": [7, 0.4518883299085949, 1, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re maining candidates (see next section)."], "label": "Non-Prov", "citing": "P05-1021", "vector": [7, 0, 0, 0.06160411036336975], "context": ["", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"c": 2, "word": 1, "ana": 3, "evalu": 1, "whether": 1, "max": 1, "surround": 1, "ilar": 1, "also": 1, "candid": 2, "salienc": 1, "factor": 1, "anaphor": 1, "statsemn": 1, "paralstuctmark": 1, "statsemci": 1, "sim": 1}, "vector_2": [7, 0.4518883299085949, 1, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["If we regard as \"discriminative power\" of each antecedent indicator the ratio \"number of successful antecedent identifications when this indicator was applied\"/\"number of applications of this indicator\" (for the non-prepositional noun phrase and definite ness being penalising indicators, this figure is calcu lated as the ratio \"number of unsuccessful antece dent identifications\"/\"number of applications\"), the immediate reference emerges as the most discrimi native indicator (100%), followed by non prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%)."], "label": "Non-Prov", "citing": "P05-1021", "vector": [4, 0, 1, 0.0], "context": ["", "ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).", ""], "marker": "Mitkov, 1998", "vector_1": {"c": 2, "word": 1, "ana": 3, "evalu": 1, "whether": 1, "max": 1, "surround": 1, "ilar": 1, "also": 1, "candid": 2, "salienc": 1, "factor": 1, "anaphor": 1, "statsemn": 1, "paralstuctmark": 1, "statsemci": 1, "sim": 1}, "vector_2": [7, 0.4518883299085949, 1, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["If we regard as \"discriminative power\" of each antecedent indicator the ratio \"number of successful antecedent identifications when this indicator was applied\"/\"number of applications of this indicator\" (for the non-prepositional noun phrase and definite ness being penalising indicators, this figure is calcu lated as the ratio \"number of unsuccessful antece dent identifications\"/\"number of applications\"), the immediate reference emerges as the most discrimi native indicator (100%), followed by non prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%)."], "label": "Non-Prov", "citing": "P06-1006", "vector": [4, 0, 0, 0.0], "context": ["", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 3, "featur": 1, "machinelearn": 1, "manual": 1, "could": 1, "resolut": 1, "tree": 1, "mine": 1, "rule": 1, "calcul": 1, "pars": 1, "design": 1, "method": 1}, "vector_2": [8, 0.05915543575920935, 6, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Our evalua tion, based on 63 examples (anaphors) from a tech nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %)."], "label": "Non-Prov", "citing": "P06-1006", "vector": [1, 0, 0, 0.0], "context": ["", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 3, "featur": 1, "machinelearn": 1, "manual": 1, "could": 1, "resolut": 1, "tree": 1, "mine": 1, "rule": 1, "calcul": 1, "pars": 1, "design": 1, "method": 1}, "vector_2": [8, 0.05915543575920935, 6, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Our preference-based approach showed clear su periority over both baseline models."], "label": "Non-Prov", "citing": "P06-1006", "vector": [0, 0, 0, 0.0], "context": ["", "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).", ""], "marker": "Mitkov, 1998", "vector_1": {"use": 3, "featur": 1, "machinelearn": 1, "manual": 1, "could": 1, "resolut": 1, "tree": 1, "mine": 1, "rule": 1, "calcul": 1, "pars": 1, "design": 1, "method": 1}, "vector_2": [8, 0.05915543575920935, 6, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["In fact, our evaluation shows that the re sults are comparable to syntax-based methods (Lappin & Leass I994).", "We believe that the good success rate is due to the fact that a number of ante cedent indicators are taken into account and no fac tor is given absolute preference.", "In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer ences (see below)."], "label": "Non-Prov", "citing": "P13-3012", "vector": [4, 0, 0, 0.0], "context": ["", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", ""], "marker": "Mitkov, 1998", "vector_1": {"graphbas": 1, "resolut": 2, "develop": 1, "edg": 1, "pronoun": 2, "cluster": 1, "neg": 1, "regard": 1, "within": 1, "factorbas": 1, "weight": 1, "repres": 1, "multigraph": 1, "framework": 1, "factor": 1, "posit": 1, "approach": 1, "greedi": 1}, "vector_2": [15, 0.09670611982750711, 1, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in (Dagan & ltai 1990).", "Example: Press the keyi down and turn the volume up...", "Press iti again."], "label": "Non-Prov", "citing": "P13-3012", "vector": [1, 0, 0, 0.0], "context": ["", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", ""], "marker": "Mitkov, 1998", "vector_1": {"graphbas": 1, "resolut": 2, "develop": 1, "edg": 1, "pronoun": 2, "cluster": 1, "neg": 1, "regard": 1, "within": 1, "factorbas": 1, "weight": 1, "repres": 1, "multigraph": 1, "framework": 1, "factor": 1, "posit": 1, "approach": 1, "greedi": 1}, "vector_2": [15, 0.09670611982750711, 1, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["\"Non-prepositional\" noun phrases A \"pure\", \"non-prepositional\" noun phrase is given a higher preference than a noun phrase which is part of a prepositional phrase (0, -1 ).", "Example: Insert the cassettei into the VCR making sure iti is suitable for the length of recording.", "Here \"the VCR\" is penalised (-1) for being part of the prepositional phrase \"into the VCR\"."], "label": "Non-Prov", "citing": "P13-3012", "vector": [2, 0, 0, 0.0], "context": ["", "While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.", ""], "marker": "Mitkov, 1998", "vector_1": {"graphbas": 1, "resolut": 2, "develop": 1, "edg": 1, "pronoun": 2, "cluster": 1, "neg": 1, "regard": 1, "within": 1, "factorbas": 1, "weight": 1, "repres": 1, "multigraph": 1, "framework": 1, "factor": 1, "posit": 1, "approach": 1, "greedi": 1}, "vector_2": [15, 0.09670611982750711, 1, 1, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["Usually knowledge-based ap proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\".", "Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn tactic function/semantic role of each individual word."], "label": "Non-Prov", "citing": "W99-0207", "vector": [6, 0, 1, 0.0], "context": ["", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro nouns in English technical manuals to the most re cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an tecedent (cf.", ""], "marker": "Mitkov, 1998", "vector_1": {"pro": 1, "cf": 1, "rate": 1, "result": 1, "baselin": 1, "correctli": 1, "cent": 1, "verifi": 1, "experi": 2, "difficulti": 1, "accord": 1, "wherea": 1, "candid": 2, "teced": 1, "report": 1, "technic": 1, "recent": 1, "task": 1, "noun": 1, "success": 1, "howev": 1, "manual": 1, "resolv": 2, "achiev": 1, "english": 1}, "vector_2": [1, 0.8277052445231246, 1, 2, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolv ing anaphors (with critical success rate of 86.2%).", "Similarly to the evaluation for English, we com pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor."], "label": "Non-Prov", "citing": "W99-0207", "vector": [14, 0, 6, 0.22677868380553634], "context": ["", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro nouns in English technical manuals to the most re cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an tecedent (cf.", ""], "marker": "Mitkov, 1998", "vector_1": {"pro": 1, "cf": 1, "rate": 1, "result": 1, "baselin": 1, "correctli": 1, "cent": 1, "verifi": 1, "experi": 2, "difficulti": 1, "accord": 1, "wherea": 1, "candid": 2, "teced": 1, "report": 1, "technic": 1, "recent": 1, "task": 1, "noun": 1, "success": 1, "howev": 1, "manual": 1, "resolv": 2, "achiev": 1, "english": 1}, "vector_2": [1, 0.8277052445231246, 1, 2, 0, 0]}, {"function": "Neut", "cited": "P98-2143", "provenance": ["The new information, or rheme, provides some information about the theme.", "1We use the simple heuristics that the given information is the first noun phrase in a non-imperative sentence."], "label": "Non-Prov", "citing": "W99-0207", "vector": [3, 0, 0, 0.0], "context": ["", "However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998). Resolving pro nouns in English technical manuals to the most re cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an tecedent (cf.", ""], "marker": "Mitkov, 1998", "vector_1": {"pro": 1, "cf": 1, "rate": 1, "result": 1, "baselin": 1, "correctli": 1, "cent": 1, "verifi": 1, "experi": 2, "difficulti": 1, "accord": 1, "wherea": 1, "candid": 2, "teced": 1, "report": 1, "technic": 1, "recent": 1, "task": 1, "noun": 1, "success": 1, "howev": 1, "manual": 1, "resolv": 2, "achiev": 1, "english": 1}, "vector_2": [1, 0.8277052445231246, 1, 2, 0, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["In a coherent text (Firbas 1992), the given or known information, or theme, usually appears first, and thus forms a co referential link with the preceding text.", "The new information, or rheme, provides some information about the theme."], "label": "Non-Prov", "citing": "W99-0207", "vector": [4, 0, 0, 0.03077287274483318], "context": ["", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen sive in the cost of human effort at development time and limited ability to scale to new domains, more re cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", ""], "marker": "Mitkov, 1998", "vector_1": {"domain": 1, "expen": 1, "knowledg": 1, "cost": 1, "human": 1, "limit": 1, "abil": 1, "scale": 1, "multipl": 1, "system": 1, "cent": 1, "without": 1, "new": 1, "approach": 1, "strategi": 1, "knowledgebas": 1, "resolut": 1, "wherea": 1, "sophist": 1, "address": 1, "develop": 1, "effort": 1, "knowledgepoor": 1, "like": 2, "sive": 1, "combin": 1, "time": 1, "problem": 1, "linguist": 1}, "vector_2": [1, 0.8375746846647488, 4, 2, 0, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3).", "The evaluation indicated 83.6% success rate."], "label": "Non-Prov", "citing": "W99-0207", "vector": [2, 0, 0, 0.0], "context": ["", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen sive in the cost of human effort at development time and limited ability to scale to new domains, more re cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", ""], "marker": "Mitkov, 1998", "vector_1": {"domain": 1, "expen": 1, "knowledg": 1, "cost": 1, "human": 1, "limit": 1, "abil": 1, "scale": 1, "multipl": 1, "system": 1, "cent": 1, "without": 1, "new": 1, "approach": 1, "strategi": 1, "knowledgebas": 1, "resolut": 1, "wherea": 1, "sophist": 1, "address": 1, "develop": 1, "effort": 1, "knowledgepoor": 1, "like": 2, "sive": 1, "combin": 1, "time": 1, "problem": 1, "linguist": 1}, "vector_2": [1, 0.8375746846647488, 4, 2, 0, 0]}, {"function": "CoCo", "cited": "P98-2143", "provenance": ["CogNIAC successfully resolved the pronouns in 75% of the cases.", "This result is comparable with the results described in (Baldwin 1997)."], "label": "Non-Prov", "citing": "W99-0207", "vector": [3, 0, 0, 0.0], "context": ["", "Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen sive in the cost of human effort at development time and limited ability to scale to new domains, more re cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.", ""], "marker": "Mitkov, 1998", "vector_1": {"domain": 1, "expen": 1, "knowledg": 1, "cost": 1, "human": 1, "limit": 1, "abil": 1, "scale": 1, "multipl": 1, "system": 1, "cent": 1, "without": 1, "new": 1, "approach": 1, "strategi": 1, "knowledgebas": 1, "resolut": 1, "wherea": 1, "sophist": 1, "address": 1, "develop": 1, "effort": 1, "knowledgepoor": 1, "like": 2, "sive": 1, "combin": 1, "time": 1, "problem": 1, "linguist": 1}, "vector_2": [1, 0.8375746846647488, 4, 2, 0, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Mary baked... a cake for Joan/Joan a cake."], "label": "Non-Prov", "citing": "D07-1018", "vector": [1, 0, 0, 0.0], "context": ["", "Stevenson and Joanis, 2003 for English semantic verb classes", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"semant": 1, "verb": 1, "class": 1, "english": 1}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2)."], "label": "Non-Prov", "citing": "D07-1018", "vector": [2, 0, 1, 0.23570226039551587], "context": ["", "Stevenson and Joanis, 2003 for English semantic verb classes", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"semant": 1, "verb": 1, "class": 1, "english": 1}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["3.2 Verb Selection."], "label": "Non-Prov", "citing": "D07-1018", "vector": [1, 0, 0, 0.35355339059327373], "context": ["", "Stevenson and Joanis, 2003 for English semantic verb classes", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"semant": 1, "verb": 1, "class": 1, "english": 1}, "vector_2": [4, 0.5, 1, 2, 2, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect (Levin, 1993; Merlo and Stevenson, 2001)."], "label": "Non-Prov", "citing": "D11-1095", "vector": [0, 0, 0, 0.0], "context": ["", "We adopt as our baseline method a well-known hierarchical method  agglomerative clustering (AGG)  which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"hierarch": 1, "flat": 1, "use": 1, "levinstyl": 1, "acquir": 1, "agg": 1, "adopt": 1, "classif": 1, "cluster": 1, "agglom": 1, "previous": 1, "wellknown": 1, "method": 2, "baselin": 1}, "vector_2": [8, 0.07637231503579953, 1, 10, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks."], "label": "Non-Prov", "citing": "D11-1095", "vector": [2, 0, 0, 0.0], "context": ["", "We adopt as our baseline method a well-known hierarchical method  agglomerative clustering (AGG)  which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"hierarch": 1, "flat": 1, "use": 1, "levinstyl": 1, "acquir": 1, "agg": 1, "adopt": 1, "classif": 1, "cluster": 1, "agglom": 1, "previous": 1, "wellknown": 1, "method": 2, "baselin": 1}, "vector_2": [8, 0.07637231503579953, 1, 10, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["In the remainder of the paper, we first briefly review our feature space and present our experimental classes and verbs."], "label": "Non-Prov", "citing": "D11-1095", "vector": [2, 0, 0, 0.0], "context": ["", "We adopt as our baseline method a well-known hierarchical method  agglomerative clustering (AGG)  which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"hierarch": 1, "flat": 1, "use": 1, "levinstyl": 1, "acquir": 1, "agg": 1, "adopt": 1, "classif": 1, "cluster": 1, "agglom": 1, "previous": 1, "wellknown": 1, "method": 2, "baselin": 1}, "vector_2": [8, 0.07637231503579953, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder."], "label": "Non-Prov", "citing": "D11-1095", "vector": [3, 0, 0, 0.0], "context": ["", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levins original taxonomy (Stevenson and Joanis, 2003).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"origin": 1, "flat": 1, "use": 1, "set": 1, "resourc": 1, "includ": 1, "gold": 3, "levin": 1, "correspond": 1, "first": 1, "three": 1, "appear": 1, "3": 1, "experi": 1, "test": 1, "taxonomi": 1, "standard": 3, "extract": 1, "class": 1, "t": 1}, "vector_2": [8, 0.1730461315368116, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["It is important to emphasize, however, that our features are extracted from part-of-speech (POS) tagged and chunked text only: there are no semantic tags of any kind."], "label": "Non-Prov", "citing": "D11-1095", "vector": [5, 0, 1, 0.046524210519923545], "context": ["", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levins original taxonomy (Stevenson and Joanis, 2003).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"origin": 1, "flat": 1, "use": 1, "set": 1, "resourc": 1, "includ": 1, "gold": 3, "levin": 1, "correspond": 1, "first": 1, "three": 1, "appear": 1, "3": 1, "experi": 1, "test": 1, "taxonomi": 1, "standard": 3, "extract": 1, "class": 1, "t": 1}, "vector_2": [8, 0.1730461315368116, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["In supervised experiments, the learner uses class labels during the training stage to determine which features are relevant to the task at hand."], "label": "Non-Prov", "citing": "D11-1095", "vector": [4, 0, 0, 0.048280454958526765], "context": ["", "We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levins original taxonomy (Stevenson and Joanis, 2003).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"origin": 1, "flat": 1, "use": 1, "set": 1, "resourc": 1, "includ": 1, "gold": 3, "levin": 1, "correspond": 1, "first": 1, "three": 1, "appear": 1, "3": 1, "experi": 1, "test": 1, "taxonomi": 1, "standard": 3, "extract": 1, "class": 1, "t": 1}, "vector_2": [8, 0.1730461315368116, 1, 10, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["It is important to emphasize, however, that our features are extracted from part-of-speech (POS) tagged and chunked text only: there are no semantic tags of any kind."], "label": "Non-Prov", "citing": "D11-1095", "vector": [3, 0, 0, 0.0], "context": ["", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"corpu": 1, "joani": 1, "stevenson": 1, "least": 1, "occur": 1, "verb": 1, "time": 1, "follow": 1, "class": 1, "select": 1}, "vector_2": [8, 0.17996435153016524, 1, 10, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["Mary gave... a cake to Joan/Joan a cake."], "label": "Non-Prov", "citing": "D11-1095", "vector": [0, 0, 0, 0.0], "context": ["", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"corpu": 1, "joani": 1, "stevenson": 1, "least": 1, "occur": 1, "verb": 1, "time": 1, "follow": 1, "class": 1, "select": 1}, "vector_2": [8, 0.17996435153016524, 1, 10, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["3 3 5 Re ci pi en t 13."], "label": "Non-Prov", "citing": "D11-1095", "vector": [0, 0, 0, 0.0], "context": ["", "Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"corpu": 1, "joani": 1, "stevenson": 1, "least": 1, "occur": 1, "verb": 1, "time": 1, "follow": 1, "class": 1, "select": 1}, "vector_2": [8, 0.17996435153016524, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["This captures what is reflected in the dendrogram, in that very short lines connect verbs low in the tree, and longer lines connect the two main clusters."], "label": "Non-Prov", "citing": "D11-1095", "vector": [1, 0, 0, 0.0], "context": ["", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"style": 1, "previou": 1, "optim": 1, "levin": 1, "work": 1, "classif": 1, "task": 1, "verb": 1, "investig": 1, "featur": 1}, "vector_2": [8, 0.2505060270082475, 3, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery."], "label": "Non-Prov", "citing": "D11-1095", "vector": [1, 0, 0, 0.09999999999999998], "context": ["", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"style": 1, "previou": 1, "optim": 1, "levin": 1, "work": 1, "classif": 1, "task": 1, "verb": 1, "investig": 1, "featur": 1}, "vector_2": [8, 0.2505060270082475, 3, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Our next step is to explore these issues, and investigate other methods appropriate to the practical problem of grouping verbs in a new language."], "label": "Non-Prov", "citing": "D11-1095", "vector": [0, 0, 0, 0.0], "context": ["", "Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"style": 1, "previou": 1, "optim": 1, "levin": 1, "work": 1, "classif": 1, "task": 1, "verb": 1, "investig": 1, "featur": 1}, "vector_2": [8, 0.2505060270082475, 3, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["We are indebted to Allan Jepson for helpful discussions and suggestions."], "label": "Non-Prov", "citing": "D11-1095", "vector": [1, 0, 0, 0.0], "context": ["", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"cutbas": 1, "cutoff": 1, "use": 1, "set": 1, "lost": 1, "cluster": 1, "remov": 1, "amount": 1, "predefin": 1, "inform": 1, "signific": 1, "pairwis": 1, "valu": 1, "although": 1, "addit": 1, "method": 1, "requir": 1, "difficult": 1}, "vector_2": [8, 0.31213558502764266, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["4.1 Clustering Parameters."], "label": "Non-Prov", "citing": "D11-1095", "vector": [1, 0, 0, 0.16666666666666666], "context": ["", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"cutbas": 1, "cutoff": 1, "use": 1, "set": 1, "lost": 1, "cluster": 1, "remov": 1, "amount": 1, "predefin": 1, "inform": 1, "signific": 1, "pairwis": 1, "valu": 1, "although": 1, "addit": 1, "method": 1, "requir": 1, "difficult": 1}, "vector_2": [8, 0.31213558502764266, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["In performing hierarchical clustering, both a vector distance measure and a cluster distance (linkage) measure are specified."], "label": "Non-Prov", "citing": "D11-1095", "vector": [3, 0, 0, 0.06085806194501846], "context": ["", "Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003). In addition, a significant amount of information is lost in pairwise clustering.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"cutbas": 1, "cutoff": 1, "use": 1, "set": 1, "lost": 1, "cluster": 1, "remov": 1, "amount": 1, "predefin": 1, "inform": 1, "signific": 1, "pairwis": 1, "valu": 1, "although": 1, "addit": 1, "method": 1, "requir": 1, "difficult": 1}, "vector_2": [8, 0.31213558502764266, 1, 10, 3, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["Admire versus Amuse verbs."], "label": "Non-Prov", "citing": "D11-1095", "vector": [0, 0, 0, 0.0], "context": ["", "Table 1: Comparison against Stevenson and Joanis (2003)s result on T1 (using similar features).", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"comparison": 1, "use": 1, "featur": 1, "joani": 1, "stevenson": 1, "t": 1, "tabl": 1, "similar": 1, "result": 1}, "vector_2": [8, 0.6423975106492251, 1, 10, 3, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["Performance differences may be due to the types of features (ours are noisier, but capture information beyond subcat), or due to the number or size of classes."], "label": "Non-Prov", "citing": "D11-1095", "vector": [1, 0, 0, 0.07453559924999299], "context": ["", "Table 1: Comparison against Stevenson and Joanis (2003)s result on T1 (using similar features).", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"comparison": 1, "use": 1, "featur": 1, "joani": 1, "stevenson": 1, "t": 1, "tabl": 1, "similar": 1, "result": 1}, "vector_2": [8, 0.6423975106492251, 1, 10, 3, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["Another striking result is the difference in values, which are very much higher than those for Ling (which are in turn much higher than for Full)."], "label": "Non-Prov", "citing": "D11-1095", "vector": [1, 0, 0, 0.07669649888473704], "context": ["", "Table 1: Comparison against Stevenson and Joanis (2003)s result on T1 (using similar features).", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"comparison": 1, "use": 1, "featur": 1, "joani": 1, "stevenson": 1, "t": 1, "tabl": 1, "similar": 1, "result": 1}, "vector_2": [8, 0.6423975106492251, 1, 10, 3, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["We tentatively conclude that, yes, any subset of verbs of the appropriate class may be sufficient as a seed set, although some sets are better than others."], "label": "Non-Prov", "citing": "D11-1095", "vector": [3, 0, 0, 0.0], "context": ["", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"use": 1, "joani": 1, "show": 1, "agg": 1, "stevenson": 1, "employ": 1, "linkag": 1, "criterion": 1, "t": 1, "tabl": 1, "ward": 1, "result": 2}, "vector_2": [8, 0.6455394096855079, 1, 10, 3, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["This captures what is reflected in the dendrogram, in that very short lines connect verbs low in the tree, and longer lines connect the two main clusters."], "label": "Non-Prov", "citing": "D11-1095", "vector": [2, 0, 0, 0.0], "context": ["", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"use": 1, "joani": 1, "show": 1, "agg": 1, "stevenson": 1, "employ": 1, "linkag": 1, "criterion": 1, "t": 1, "tabl": 1, "ward": 1, "result": 2}, "vector_2": [8, 0.6455394096855079, 1, 10, 3, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["Mean over these clusterings was much lower than for the Seed sets, and was extremely low (below .08 in all cases)."], "label": "Non-Prov", "citing": "D11-1095", "vector": [2, 0, 0, 0.0], "context": ["", "Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"use": 1, "joani": 1, "show": 1, "agg": 1, "stevenson": 1, "employ": 1, "linkag": 1, "criterion": 1, "t": 1, "tabl": 1, "ward": 1, "result": 2}, "vector_2": [8, 0.6455394096855079, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work."], "label": "Non-Prov", "citing": "D11-1095", "vector": [7, 1, 0, 0.0936585811581694], "context": ["", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"therefor": 1, "use": 1, "featur": 1, "joani": 1, "agg": 1, "reproduc": 1, "stevenson": 1, "abl": 1, "differ": 1, "see": 1, "set": 2, "result": 1, "experi": 1, "b": 1, "section": 1, "smaller": 1}, "vector_2": [8, 0.6489229932630434, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["This confirms that appropriate feature selection, and not just a small number of features, is important for the task of verb class discovery."], "label": "Non-Prov", "citing": "D11-1095", "vector": [5, 0, 0, 0.06622661785325219], "context": ["", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"therefor": 1, "use": 1, "featur": 1, "joani": 1, "agg": 1, "reproduc": 1, "stevenson": 1, "abl": 1, "differ": 1, "see": 1, "set": 2, "result": 1, "experi": 1, "b": 1, "section": 1, "smaller": 1}, "vector_2": [8, 0.6489229932630434, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["A plausible scenario is that researchers would have examples of verbs which they believe fall into different classes of interest, and they want to separate other verbs along the same lines."], "label": "Non-Prov", "citing": "D11-1095", "vector": [5, 0, 1, 0.0], "context": ["", "In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"therefor": 1, "use": 1, "featur": 1, "joani": 1, "agg": 1, "reproduc": 1, "stevenson": 1, "abl": 1, "differ": 1, "see": 1, "set": 2, "result": 1, "experi": 1, "b": 1, "section": 1, "smaller": 1}, "vector_2": [8, 0.6489229932630434, 1, 10, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["The unsupervised feature selection method, on the other hand, was not usable for our data."], "label": "Non-Prov", "citing": "J06-2001", "vector": [2, 0, 0, 0.0], "context": ["", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", ""], "marker": "Stevenson and Joanis 2003", "vector_1": {"class": 1, "measur": 1, "korhonen": 1, "joani": 1, "gold": 1, "evalu": 1, "accuracypur": 1, "whether": 1, "stevenson": 1, "krymolowski": 1, "correct": 1, "member": 1, "cluster": 2, "exampl": 1, "verb": 1, "respect": 1, "major": 1, "standard": 1, "marx": 1, "assign": 1}, "vector_2": [3, 0.49879795850000985, 0, 0, 5, 1]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["I filled..."], "label": "Non-Prov", "citing": "J06-2001", "vector": [0, 0, 0, 0.0], "context": ["", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", ""], "marker": "Stevenson and Joanis 2003", "vector_1": {"class": 1, "measur": 1, "korhonen": 1, "joani": 1, "gold": 1, "evalu": 1, "accuracypur": 1, "whether": 1, "stevenson": 1, "krymolowski": 1, "correct": 1, "member": 1, "cluster": 2, "exampl": 1, "verb": 1, "respect": 1, "major": 1, "standard": 1, "marx": 1, "assign": 1}, "vector_2": [3, 0.49879795850000985, 0, 0, 5, 1]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["5.3 Unsupervised Feature Selection."], "label": "Non-Prov", "citing": "J06-2001", "vector": [0, 0, 0, 0.0], "context": ["", "For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.", ""], "marker": "Stevenson and Joanis 2003", "vector_1": {"class": 1, "measur": 1, "korhonen": 1, "joani": 1, "gold": 1, "evalu": 1, "accuracypur": 1, "whether": 1, "stevenson": 1, "krymolowski": 1, "correct": 1, "member": 1, "cluster": 2, "exampl": 1, "verb": 1, "respect": 1, "major": 1, "standard": 1, "marx": 1, "assign": 1}, "vector_2": [3, 0.49879795850000985, 0, 0, 5, 1]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["We began with this same set of 20 verbs per class for our current work.", "We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs).", "All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments)."], "label": "Non-Prov", "citing": "J06-2001", "vector": [19, 0, 2, 0.40451991747794525], "context": ["", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"classif": 1, "enlarg": 1, "entropi": 1, "gold": 1, "number": 1, "dash": 1, "seed": 1, "exclud": 1, "total": 1, "featur": 1, "select": 1, "supervis": 1, "use": 1, "joani": 1, "techniqu": 1, "compar": 1, "semisupervis": 2, "frequenc": 1, "classifi": 1, "liu": 1, "yao": 1, "low": 1, "outperform": 1, "experi": 1, "approach": 2, "method": 1, "relat": 1, "standard": 1, "verb": 6, "five": 1, "multidimension": 1, "data": 1, "class": 5, "recent": 1, "measur": 1, "organ": 1, "levin": 1, "stevenson": 1, "work": 1, "manual": 1, "space": 1, "ambigu": 1, "train": 1, "english": 1, "found": 1, "unsupervis": 2}, "vector_2": [3, 0.8416852227717895, 2, 1, 10, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["We also count fixed elements in certain slots (it and there, as in It rains or There appeared a ship), since these are part of the syntactic frame specifications for a verb.", "In addition to approximating the syntactic frames themselves, we also want to capture regularities in the mapping of arguments to particular slots.", "For example, the location argument, the truck, is direct object in I loaded the truck with hay, and object of a preposition in I loaded hay onto the truck."], "label": "Non-Prov", "citing": "J06-2001", "vector": [10, 0, 0, 0.040291148201269014], "context": ["", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"classif": 1, "enlarg": 1, "entropi": 1, "gold": 1, "number": 1, "dash": 1, "seed": 1, "exclud": 1, "total": 1, "featur": 1, "select": 1, "supervis": 1, "use": 1, "joani": 1, "techniqu": 1, "compar": 1, "semisupervis": 2, "frequenc": 1, "classifi": 1, "liu": 1, "yao": 1, "low": 1, "outperform": 1, "experi": 1, "approach": 2, "method": 1, "relat": 1, "standard": 1, "verb": 6, "five": 1, "multidimension": 1, "data": 1, "class": 5, "recent": 1, "measur": 1, "organ": 1, "levin": 1, "stevenson": 1, "work": 1, "manual": 1, "space": 1, "ambigu": 1, "train": 1, "english": 1, "found": 1, "unsupervis": 2}, "vector_2": [3, 0.8416852227717895, 2, 1, 10, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Run versus Sound Emission verbs.", "The kids ran in the room./*The room ran with kids.", "The birds sang in the trees./The trees sang with birds.These activity verbs both have an Agent subject in the in transitive, but differ in the prepositional alternations they allow."], "label": "Non-Prov", "citing": "J06-2001", "vector": [7, 0, 0, 0.10969086361906959], "context": ["", "In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques. In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs. Low- frequency and ambiguous verbs were excluded from the classes. They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"classif": 1, "enlarg": 1, "entropi": 1, "gold": 1, "number": 1, "dash": 1, "seed": 1, "exclud": 1, "total": 1, "featur": 1, "select": 1, "supervis": 1, "use": 1, "joani": 1, "techniqu": 1, "compar": 1, "semisupervis": 2, "frequenc": 1, "classifi": 1, "liu": 1, "yao": 1, "low": 1, "outperform": 1, "experi": 1, "approach": 2, "method": 1, "relat": 1, "standard": 1, "verb": 6, "five": 1, "multidimension": 1, "data": 1, "class": 5, "recent": 1, "measur": 1, "organ": 1, "levin": 1, "stevenson": 1, "work": 1, "manual": 1, "space": 1, "ambigu": 1, "train": 1, "english": 1, "found": 1, "unsupervis": 2}, "vector_2": [3, 0.8416852227717895, 2, 1, 10, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["On the 2-way tasks, the performance on average is very close to that of the full feature set for the and measures."], "label": "Non-Prov", "citing": "P03-1009", "vector": [4, 0, 1, 0.0], "context": ["", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", ""], "marker": "(Stevenson and Joanis, 2003)", "vector_1": {"size": 1, "measur": 2, "accord": 1, "deriv": 1, "evalu": 1, "global": 1, "weight": 1, "precis": 1, "cluster": 2, "second": 1, "mean": 1, "puriti": 1}, "vector_2": [0, 0.5941138544392612, 1, 2, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs)."], "label": "Non-Prov", "citing": "P03-1009", "vector": [5, 0, 1, 0.0], "context": ["", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", ""], "marker": "(Stevenson and Joanis, 2003)", "vector_1": {"size": 1, "measur": 2, "accord": 1, "deriv": 1, "evalu": 1, "global": 1, "weight": 1, "precis": 1, "cluster": 2, "second": 1, "mean": 1, "puriti": 1}, "vector_2": [0, 0.5941138544392612, 1, 2, 3, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values."], "label": "Non-Prov", "citing": "P03-1009", "vector": [2, 0, 1, 0.0], "context": ["", "Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).", ""], "marker": "(Stevenson and Joanis, 2003)", "vector_1": {"size": 1, "measur": 2, "accord": 1, "deriv": 1, "evalu": 1, "global": 1, "weight": 1, "precis": 1, "cluster": 2, "second": 1, "mean": 1, "puriti": 1}, "vector_2": [0, 0.5941138544392612, 1, 2, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["These three classes also assign the same semantic roles but differ in prepositional alternants."], "label": "Non-Prov", "citing": "P03-1009", "vector": [4, 0, 0, 0.07647191129018724], "context": ["", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R  29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"involv": 1, "impli": 1, "task": 1, "joani": 1, "structur": 1, "stevenson": 1, "predicateargu": 1, "differ": 1, "classifi": 1, "accuraci": 1, "exampl": 1, "u": 1, "mp": 1, "base": 1, "verb": 1, "report": 1, "r": 1, "class": 1}, "vector_2": [0, 0.698524670990253, 1, 2, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["In other ways, however, it is too difficult: the learner has to distinguish multiple classes, rather than focus on the important properties of a single class."], "label": "Non-Prov", "citing": "P03-1009", "vector": [6, 0, 0, 0.06362847629757777], "context": ["", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R  29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"involv": 1, "impli": 1, "task": 1, "joani": 1, "structur": 1, "stevenson": 1, "predicateargu": 1, "differ": 1, "classifi": 1, "accuraci": 1, "exampl": 1, "u": 1, "mp": 1, "base": 1, "verb": 1, "report": 1, "r": 1, "class": 1}, "vector_2": [0, 0.698524670990253, 1, 2, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Levins classes form a hierarchy of verb groupings with shared meaning and syntax."], "label": "Non-Prov", "citing": "P03-1009", "vector": [3, 0, 0, 0.07647191129018724], "context": ["", "For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R  29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.", ""], "marker": "Stevenson and Joanis (2003)", "vector_1": {"involv": 1, "impli": 1, "task": 1, "joani": 1, "structur": 1, "stevenson": 1, "predicateargu": 1, "differ": 1, "classifi": 1, "accuraci": 1, "exampl": 1, "u": 1, "mp": 1, "base": 1, "verb": 1, "report": 1, "r": 1, "class": 1}, "vector_2": [0, 0.698524670990253, 1, 2, 3, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["For example, the location argument, the truck, is direct object in I loaded the truck with hay, and object of a preposition in I loaded hay onto the truck."], "label": "Non-Prov", "citing": "P04-2007", "vector": [1, 0, 0, 0.0], "context": ["", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"machin": 1, "use": 1, "attempt": 1, "techniqu": 1, "variou": 1, "automat": 1, "reason": 1, "verb": 1, "classifi": 1, "learn": 1, "method": 1}, "vector_2": [1, 0.07063896151552682, 3, 3, 1, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["Many feature sets performed very well, and some far outperformed our best results using other feature selection methods."], "label": "Non-Prov", "citing": "P04-2007", "vector": [2, 0, 0, 0.1556997888323046], "context": ["", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"machin": 1, "use": 1, "attempt": 1, "techniqu": 1, "variou": 1, "automat": 1, "reason": 1, "verb": 1, "classifi": 1, "learn": 1, "method": 1}, "vector_2": [1, 0.07063896151552682, 3, 3, 1, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89"], "label": "Non-Prov", "citing": "P04-2007", "vector": [0, 0, 0, 0.0], "context": ["", "For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"machin": 1, "use": 1, "attempt": 1, "techniqu": 1, "variou": 1, "automat": 1, "reason": 1, "verb": 1, "classifi": 1, "learn": 1, "method": 1}, "vector_2": [1, 0.07063896151552682, 3, 3, 1, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["Note, however, that the options for Spray/Load verbs overlap with those of the other two types of verbs."], "label": "Non-Prov", "citing": "P04-2007", "vector": [2, 0, 1, 0.0], "context": ["", "Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"work": 1, "choic": 1, "set": 1, "motiv": 1, "paramet": 1}, "vector_2": [1, 0.5300674280996285, 1, 3, 1, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["More importantly, the Seed set shows a mean overall reduction in error rate (over Base ) of 28%, compared to 17% for the Ling set."], "label": "Non-Prov", "citing": "P04-2007", "vector": [2, 0, 0, 0.0], "context": ["", "Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"work": 1, "choic": 1, "set": 1, "motiv": 1, "paramet": 1}, "vector_2": [1, 0.5300674280996285, 1, 3, 1, 0]}, {"function": "Pos", "cited": "W03-0410", "provenance": ["To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features."], "label": "Non-Prov", "citing": "P04-2007", "vector": [3, 0, 0, 0.0], "context": ["", "Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"work": 1, "choic": 1, "set": 1, "motiv": 1, "paramet": 1}, "vector_2": [1, 0.5300674280996285, 1, 3, 1, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments).", "3.3 Feature Extraction."], "label": "Non-Prov", "citing": "P04-2007", "vector": [7, 0, 2, 0.33954987505086615], "context": ["", "Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"subset": 1, "rand": 1, "adjust": 1, "compar": 1, "cluster": 1, "nevertheless": 1, "gener": 1, "measur": 1, "descript": 1, "obtain": 1, "perform": 1, "verb": 2, "result": 1, "base": 1, "english": 1, "report": 1, "experi": 2, "similar": 1, "averag": 1}, "vector_2": [1, 0.8445025457547819, 1, 3, 1, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure).", "Many feature sets performed very well, and some far outperformed our best results using other feature selection methods."], "label": "Non-Prov", "citing": "P04-2007", "vector": [9, 0, 1, 0.0762000762001143], "context": ["", "Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"subset": 1, "rand": 1, "adjust": 1, "compar": 1, "cluster": 1, "nevertheless": 1, "gener": 1, "measur": 1, "descript": 1, "obtain": 1, "perform": 1, "verb": 2, "result": 1, "base": 1, "english": 1, "report": 1, "experi": 2, "similar": 1, "averag": 1}, "vector_2": [1, 0.8445025457547819, 1, 3, 1, 0]}, {"function": "CoCo", "cited": "W03-0410", "provenance": ["Mean over these clusterings was much lower than for the Seed sets, and was extremely low (below .08 in all cases).", "Interestingly, was generally very high, indicating that there is structure in the data, but not what matches our classification."], "label": "Non-Prov", "citing": "P04-2007", "vector": [3, 0, 0, 0.0], "context": ["", "Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.", ""], "marker": "Stevenson and Joanis, 2003", "vector_1": {"subset": 1, "rand": 1, "adjust": 1, "compar": 1, "cluster": 1, "nevertheless": 1, "gener": 1, "measur": 1, "descript": 1, "obtain": 1, "perform": 1, "verb": 2, "result": 1, "base": 1, "english": 1, "report": 1, "experi": 2, "similar": 1, "averag": 1}, "vector_2": [1, 0.8445025457547819, 1, 3, 1, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["The Seed set has slightly lower values for and , but a much higher value (.89) for , indicating a better separation of the data."], "label": "Non-Prov", "citing": "P07-3016", "vector": [2, 0, 0, 0.0], "context": ["", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", ""], "marker": "(Stevenson and Joanis, 2003)", "vector_1": {"applic": 1, "verb": 1, "investig": 1, "space": 1, "gener": 1, "task": 1, "cluster": 1, "featur": 1, "unsupervis": 1}, "vector_2": [4, 0.25432923012726893, 1, 1, 4, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["However, our features do not give us such perfect knowledge, since PP arguments and adjuncts cannot be distinguished with high accuracy."], "label": "Non-Prov", "citing": "P07-3016", "vector": [0, 0, 0, 0.0], "context": ["", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", ""], "marker": "(Stevenson and Joanis, 2003)", "vector_1": {"applic": 1, "verb": 1, "investig": 1, "space": 1, "gener": 1, "task": 1, "cluster": 1, "featur": 1, "unsupervis": 1}, "vector_2": [4, 0.25432923012726893, 1, 1, 4, 0]}, {"function": "Neut", "cited": "W03-0410", "provenance": ["3 In our experiments for estimating the baseline, we in-."], "label": "Non-Prov", "citing": "P07-3016", "vector": [1, 0, 0, 0.0], "context": ["", "(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.", ""], "marker": "(Stevenson and Joanis, 2003)", "vector_1": {"applic": 1, "verb": 1, "investig": 1, "space": 1, "gener": 1, "task": 1, "cluster": 1, "featur": 1, "unsupervis": 1}, "vector_2": [4, 0.25432923012726893, 1, 1, 4, 0]}]