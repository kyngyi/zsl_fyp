{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9opfHSipt4G",
        "colab_type": "text"
      },
      "source": [
        "https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=JhUZO9vc_l6T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERWmLrIFhPbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73tee5iRiT5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install json-lines\n",
        "!pip install transformers\n",
        "\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NButH-G-iU9A",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Config\n",
        "\"\"\"\n",
        "Configuration file for the project.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Base directory.\n",
        "\"\"\"\n",
        "PWD = '/content/drive/My Drive/KY, FYP/Code/'\n",
        "\n",
        "\"\"\"\n",
        "File directories.\n",
        "\"\"\"\n",
        "# Directory for the word embeddings\n",
        "GLOVE_DIR = PWD + '/glove.6B'\n",
        "\n",
        "# Directory for storing citation function data\n",
        "DATA_DIR = PWD + '/data/data'\n",
        "\n",
        "\"\"\"\n",
        "Data files: the citation and provenance dataset.\n",
        "MTL refers to the aligned dataset.\n",
        "\"\"\"\n",
        "DATA_FILES = {\n",
        "    'func': {\n",
        "        'golden_train': 'processed/golden_train.func.json',\n",
        "        'golden_test': 'processed/golden_test.func.json',\n",
        "    },\n",
        "    'scicite': {\n",
        "        'train': 'scicite/train.jsonl',\n",
        "        'test': 'scicite/test.jsonl',\n",
        "        'dev': 'scicite/dev.jsonl'\n",
        "    },\n",
        "    'acl-arc': {\n",
        "        'train': 'acl-arc/train.jsonl',\n",
        "        'test': 'acl-arc/test.jsonl',\n",
        "        'dev': 'acl-arc/dev.jsonl'\n",
        "    },\n",
        "    'prov': {\n",
        "        'golden_train': 'processed/golden_train.prov.json',\n",
        "        'golden_test': 'processed/golden_test.prov.json',\n",
        "    },\n",
        "    'mtl': {\n",
        "        'golden_train': 'processed/golden_train.mtl.json',\n",
        "        'golden_test': 'processed/golden_test.mtl.json'\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWV9AXG_ign5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Data\n",
        "\"\"\"\n",
        "Common data operations.\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "import json_lines\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def read_json_data(filename):\n",
        "    \"\"\"\n",
        "    Read the given JSON file.\n",
        "    \"\"\"\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    with open(path, 'rb') as fp:\n",
        "        content = json.load(fp)\n",
        "        return content\n",
        "\n",
        "def read_jsonl_data(filename):\n",
        "    \"\"\"\n",
        "    Read the given JSONL file.\n",
        "    \"\"\"\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    content = []\n",
        "    print (type(content))\n",
        "    with open(path, 'rb') as fp:\n",
        "        for item in json_lines.reader(fp):\n",
        "            content.append(item)\n",
        "        return content\n",
        "\n",
        "\"\"\"\n",
        "Custom cross validation.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def compress_y(ys):\n",
        "    \"\"\"\n",
        "    For each y in ys, if y is of the form [0 0 ... 1 ... 0], compress it to a\n",
        "    single integer.\n",
        "    \"\"\"\n",
        "    if len(ys) < 1:\n",
        "        return ys\n",
        "\n",
        "    if isinstance(ys[0], np.ndarray):\n",
        "        # A hack >.<\n",
        "        return map(lambda x: x.tolist().index(1), ys)\n",
        "    else:\n",
        "        return ys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1x6nerZikWj",
        "colab_type": "code",
        "outputId": "9f9a724b-8792-40ce-82ed-df38145270bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALoTAHpei60V",
        "colab_type": "code",
        "outputId": "4cf48d5c-d188-408b-c4cf-0cdff37fb07f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rbf5l-8jCqJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACL dataset (4 Classes) Initialization\n",
        "\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['func']\n",
        "test = read_json_data(datafiles['golden_test'])\n",
        "# train = data.read_jsonl_data((datafiles['train']))\n",
        "train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_train + dataset_test\n",
        "\n",
        "texts = list(map(lambda d: d['context'][0], dataset))\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['label']], dataset))\n",
        "\n",
        "seed = 2\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sample in texts:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sample,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 200\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train(80%) and test(20%) sets\n",
        "x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                random_state=seed, test_size=0.2)\n",
        "# Do the same for the masks.\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                             random_state=seed, test_size=0.2)\n",
        "\n",
        "\n",
        "\n",
        "# Futher split train data into train(80%) and validation(20%) sets\n",
        "train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "x_train = torch.tensor(x_train)\n",
        "x_test = torch.tensor(x_test)\n",
        "x_val = torch.tensor(x_val)\n",
        "\n",
        "y_train = torch.tensor(y_train)\n",
        "y_test = torch.tensor(y_test)\n",
        "y_val = torch.tensor(y_val)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgDyUbjYxP6d",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Model Structure\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "# model = BertForSequenceClassification.from_pretrained(\n",
        "#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "#     num_labels = 4, # The number of output labels--2 for binary classification.\n",
        "#                     # You can increase this for multi-class tasks.   \n",
        "#     output_attentions = False, # Whether the model returns attentions weights.\n",
        "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "# )\n",
        "\n",
        "#### Import from scicite(3 Classes) ####\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "transfered_model = PWD + 'Sciicite.pt'\n",
        "model.load_state_dict(torch.load(transfered_model))\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "# model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "R9QtI10GN7Yt",
        "colab": {}
      },
      "source": [
        "#@title Training + Validation\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs =  4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 40\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask)\n",
        "                    # labels=b_labels)\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        # loss = outputs[0]\n",
        "        loss = criterion(outputs[0], b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0QBDiEYxUhf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test\n",
        "# Create the DataLoader for our validation set.\n",
        "test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# ========================================\n",
        "#               Testing\n",
        "# ========================================\n",
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('DONE.')\n",
        "\n",
        "y_test = y_test.tolist()\n",
        "\n",
        "y_pred = []\n",
        "for i in predictions:\n",
        "  y_pred += i.tolist()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(metrics.classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRYHIgZHoRc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'Model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YvOSOO26EeX",
        "colab_type": "text"
      },
      "source": [
        "###################################### END ##################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0nvEpt886KdV",
        "colab": {}
      },
      "source": [
        "model2 = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "model2.load_state_dict(torch.load(path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wdmG0xos6KJT",
        "colab": {}
      },
      "source": [
        "model2.classifier = torch.nn.Linear(768,6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaiQX3khffQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AGT3vK5W6Jnt",
        "colab": {}
      },
      "source": [
        "model2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FloXs9jKrwZ4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title SciCite\n",
        "\n",
        "# Function dataset start\n",
        "funcs_index = {'background': 0, 'method': 1, 'result': 2}\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['scicite']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data(datafiles['train'])\n",
        "\n",
        "dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_train + dataset_test\n",
        "\n",
        "texts = list(map(lambda d: d['string'], dataset))\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['label']], dataset))\n",
        "\n",
        "seed = 3\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sample in texts:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sample,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "# Filter out the instances with >512 tokens\n",
        "for i, sample in enumerate(input_ids):\n",
        "  if len(sample) > 511:\n",
        "    texts.pop(i)\n",
        "    input_ids.pop(i)\n",
        "    ys.pop(i)\n",
        "    print(\"removed Index: \" +  str(i))\n",
        "\n",
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 500\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                random_state=seed, test_size=0.2)\n",
        "# Do the same for the masks.\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                             random_state=seed, test_size=0.2)\n",
        "\n",
        "train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "x_train = torch.tensor(x_train)\n",
        "x_test = torch.tensor(x_test)\n",
        "x_val = torch.tensor(x_val)\n",
        "\n",
        "y_train = torch.tensor(y_train)\n",
        "y_test = torch.tensor(y_test)\n",
        "y_val = torch.tensor(y_val)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs =  0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 40\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('DONE.')\n",
        "\n",
        "y_test = y_test.tolist()\n",
        "\n",
        "y_pred = []\n",
        "for i in predictions:\n",
        "  y_pred += i.tolist()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(metrics.classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk3BaG2SkpkD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for Training scicite scicite(seed,filepath,text_path)\n",
        "\n",
        "def scicite(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "  import random\n",
        "  \n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "  funcs_index = {'background': 0, 'method': 1, 'result': 2}\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['scicite']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data(datafiles['train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n",
        "\n",
        "  texts = list(map(lambda d: d['string'], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['label']], dataset))\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  # Filter out the instances with >512 tokens\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 500\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Use 90% for training and 10% for validation.\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  # Tell pytorch to run this model on the GPU.\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  2\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "  # Set the seed value all over the place to make this reproducible.\n",
        "  seed_val = seed\n",
        "\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Scicite \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n",
        "\n",
        "  torch.save(model.state_dict(), filepath)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii3T6b2WU0fk",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclarc dataset (6 Classes) aclarc(seed,filepath,text_path) -- 0 Shot\n",
        "\n",
        "\n",
        "\n",
        "def aclarc(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      fl\n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  new_train_mask = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "          new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  new_train_mask = new_train_mask[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "  train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  0\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclarc test zero shot \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcyjSWjnZRvA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclarc dataset (6 Classes) aclarcf(seed,filepath,text_path) -- 5 Shot\n",
        "\n",
        "\n",
        "def aclarcf(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  new_train_mask = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "          new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  new_train_mask = new_train_mask[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "  train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  15\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclarc test Few shot \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHXJUHAba7qJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclarc dataset (6 Classes) aclarcfull(seed,filepath,text_path) -- full Shot\n",
        "\n",
        "\n",
        "def aclarcfull(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  # new_x_train = []\n",
        "  # new_y_train = []\n",
        "  # new_train_mask = []\n",
        "  # arr = {}\n",
        "  # for index in range(len(funcs_index)):\n",
        "  #     arr[index] = []\n",
        "  #     for i, value in enumerate(y_train):\n",
        "  #         if (value == index):\n",
        "  #             arr[index].append(i)\n",
        "  #     # print(index, \":\", len(arr[index]))\n",
        "  #     # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "  #     # sample_length = int(sample_length)\n",
        "  #     sample_length = 5\n",
        "  #     for j in range(sample_length):\n",
        "  #         new_x_train.append(x_train[arr[index][j]])\n",
        "  #         new_y_train.append(y_train[arr[index][j]])\n",
        "  #         new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  # new_x_train = np.asarray(new_x_train)\n",
        "  # new_y_train = np.asarray(new_y_train)\n",
        "  # new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  # indices = np.arange(new_x_train.shape[0])\n",
        "  # np.random.shuffle(indices)\n",
        "\n",
        "  # new_x_train = new_x_train[indices]\n",
        "  # new_y_train = new_y_train[indices]\n",
        "  # new_train_mask = new_train_mask[indices]\n",
        "  # x_train = new_x_train\n",
        "  # y_train = new_y_train\n",
        "  # train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  2\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclarc test full \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo4_p81NR7qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "seeds = [663,883,544,201,356,648,898,88,997,788]\n",
        "# seeds = [1,2]\n",
        "\n",
        "for seed in seeds:\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_'\n",
        "  text_path = '/content/drive/My Drive/KY, FYP/Code/results/output_'\n",
        "  path = path + str(seed) + \".pt\"\n",
        "  text_path = text_path + str(seed) + \".txt\"\n",
        "  aclarc(seed,path,text_path)\n",
        "  aclarcf(seed,path,text_path)\n",
        "  aclarcfull(seed,path,text_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyK_y7qDWahW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "text_path = '/content/drive/My Drive/KY, FYP/Code/results/output_663.txt'\n",
        "\n",
        "aclarcf(0,path,text_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQr4sgBQcDgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aclarcfull(0,path,text_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NmUQXOScbR8",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title aclarc dataset (6 Classes) Initialization Transfer test\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "               'Extends': 5}\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['acl-arc']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "\n",
        "dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "\n",
        "texts = list(map(lambda d: d['text'], dataset))\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['intent']], dataset))\n",
        "\n",
        "seed = 2\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sample in texts:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sample,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "for i, sample in enumerate(input_ids):\n",
        "  if len(sample) > 511:\n",
        "    texts.pop(i)\n",
        "    input_ids.pop(i)\n",
        "    ys.pop(i)\n",
        "    print(\"removed Index: \" +  str(i))\n",
        "\n",
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 200\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train(80%) and test(20%) sets\n",
        "x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                random_state=seed, test_size=0.2)\n",
        "# Do the same for the masks.\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                             random_state=seed, test_size=0.2)\n",
        "\n",
        "y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "# Proportional Reduction\n",
        "# ------------------------------------\n",
        "\n",
        "new_x_train = []\n",
        "new_y_train = []\n",
        "new_train_mask = []\n",
        "arr = {}\n",
        "for index in range(len(funcs_index)):\n",
        "    arr[index] = []\n",
        "    for i, value in enumerate(y_train):\n",
        "        if (value == index):\n",
        "            arr[index].append(i)\n",
        "    # print(index, \":\", len(arr[index]))\n",
        "    # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "    # sample_length = int(sample_length)\n",
        "    sample_length = 5\n",
        "    for j in range(sample_length):\n",
        "        new_x_train.append(x_train[arr[index][j]])\n",
        "        new_y_train.append(y_train[arr[index][j]])\n",
        "        new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "new_x_train = np.asarray(new_x_train)\n",
        "new_y_train = np.asarray(new_y_train)\n",
        "new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "indices = np.arange(new_x_train.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "new_x_train = new_x_train[indices]\n",
        "new_y_train = new_y_train[indices]\n",
        "new_train_mask = new_train_mask[indices]\n",
        "x_train = new_x_train\n",
        "y_train = new_y_train\n",
        "train_masks = new_train_mask\n",
        "#----------------------------------------------\n",
        "\n",
        "# Futher split train data into train(80%) and validation(20%) sets\n",
        "train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "x_train = torch.tensor(x_train)\n",
        "x_test = torch.tensor(x_test)\n",
        "x_val = torch.tensor(x_val)\n",
        "\n",
        "y_train = torch.tensor(y_train)\n",
        "y_test = torch.tensor(y_test)\n",
        "y_val = torch.tensor(y_val)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "model.classifier = torch.nn.Linear(768,3)\n",
        "path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "model.load_state_dict(torch.load(path))\n",
        "\n",
        "model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "# # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(\n",
        "#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "#     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "#                     # You can increase this for multi-class tasks.   \n",
        "#     output_attentions = False, # Whether the model returns attentions weights.\n",
        "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "# )\n",
        "# path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "# # model.load_state_dict(torch.load(path))\n",
        "\n",
        "# model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs =  2\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 16\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('DONE.')\n",
        "\n",
        "y_test = y_test.tolist()\n",
        "\n",
        "y_pred = []\n",
        "for i in predictions:\n",
        "  y_pred += i.tolist()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(metrics.classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RACEB7HWJCQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La_ClDkSJHKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(new_x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}