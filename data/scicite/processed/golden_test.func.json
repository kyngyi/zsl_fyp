[{"label": "Neut", "current": "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "context": ["Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).", "Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).", "WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations."], "vector_1": {"among": 1, "semant": 1, "wup": 1, "synonymi": 1, "past": 1, "packag": 1, "set": 1, "wordnet": 2, "use": 1, "instrument": 1, "perform": 1, "databas": 1, "lin": 1, "re": 1, "other": 1, "languag": 1, "test": 1, "import": 1, "jcn": 1, "method": 2, "variou": 1, "relat": 1, "path": 1, "becom": 1, "lch": 1, "measur": 2, "word": 2, "princeton": 1, "also": 1, "wordnetsimilar": 2, "implement": 1, "similar": 2, "propos": 1}, "marker": "(Lin, 1998)", "article": "W14-0118", "vector_2": [16, 0.04608496916189224, 8, 1, 0, 0]}, {"label": "Neut", "current": "The toolkit was used to evaluate the different measures against a gold standard of English words created by Rubenstein and Goodenough (1965) and Miller and Charles (1991).", "context": ["WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.", "The toolkit was used to evaluate the different measures against a gold standard of English words created by Rubenstein and Goodenough (1965) and Miller and Charles (1991).", "The evaluation results tell us something about the capacity of WordNet to mimic human judgements of similarity but also about the different methods in relation to each other."], "vector_1": {"semant": 1, "set": 1, "evalu": 2, "synonymi": 1, "goodenough": 1, "rubenstein": 1, "toolkit": 1, "result": 1, "human": 1, "wordnet": 2, "use": 1, "gold": 1, "instrument": 1, "perform": 1, "databas": 1, "someth": 1, "capac": 1, "also": 2, "languag": 1, "creat": 1, "miller": 1, "test": 1, "import": 1, "tell": 1, "relat": 2, "differ": 2, "standard": 1, "judgement": 1, "charl": 1, "becom": 1, "measur": 2, "word": 2, "us": 1, "method": 1, "mimic": 1, "wordnetsimilar": 1, "english": 1, "similar": 2}, "marker": "(1965)", "article": "W14-0118", "vector_2": [49, 0.05381797689489997, 2, 1, 0, 0]}, {"label": "Neut", "current": "Unfortunately, WordNet::Similarity only works for the Princeton WordNet released in its proprietary format and not wordnets in other languages in other formats, such as Wordnet-LMF (Vossen, Soria and Monachini, 2013).", "context": ["The evaluation results tell us something about the capacity of WordNet to mimic human judgements of similarity but also about the different methods in relation to each other.", "Unfortunately, WordNet::Similarity only works for the Princeton WordNet released in its proprietary format and not wordnets in other languages in other formats, such as Wordnet-LMF (Vossen, Soria and Monachini, 2013).", "Furthermore, no gold standard exists for Dutch, the language that we study."], "vector_1": {"evalu": 1, "proprietari": 1, "result": 1, "human": 1, "wordnet": 3, "differ": 1, "wordnetlmf": 1, "gold": 1, "someth": 1, "capac": 1, "also": 1, "tell": 1, "dutch": 1, "languag": 2, "method": 1, "format": 2, "relat": 1, "standard": 1, "judgement": 1, "releas": 1, "furthermor": 1, "princeton": 1, "work": 1, "us": 1, "unfortun": 1, "mimic": 1, "wordnetsimilar": 1, "exist": 1, "studi": 1, "similar": 1}, "marker": "(Vossen, Soria and Monachini, 2013)", "article": "W14-0118", "vector_2": [1, 0.06577752731598885, 1, 3, 9, 0]}, {"label": "Neut", "current": "This difference is dubbed the 'tennisphenomenon' in Fellbaum (1998) : where tennis ball, player, racket and game are closely related but all very different things.", "context": ["part-whole or causal relations, are most likely not similar but strongly related.", "This difference is dubbed the 'tennisphenomenon' in Fellbaum (1998) : where tennis ball, player, racket and game are closely related but all very different things.", "Since WordNet dominantly consists of synonymy and hyponymy relations, it more naturally reflects similarity than relatedness."], "vector_1": {"synonymi": 1, "natur": 1, "related": 1, "strongli": 1, "player": 1, "racket": 1, "close": 1, "partwhol": 1, "causal": 1, "wordnet": 1, "differ": 2, "dub": 1, "tennisphenomenon": 1, "fellbaum": 1, "ball": 1, "relat": 4, "tenni": 1, "reflect": 1, "game": 1, "hyponymi": 1, "sinc": 1, "like": 1, "consist": 1, "thing": 1, "dominantli": 1, "similar": 2}, "marker": "(1998)", "article": "W14-0118", "vector_2": [16, 0.15037099652484268, 1, 3, 0, 0]}, {"label": "Neut", "current": "Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011).", "context": ["Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.", "Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011).", "The aim of their paper is to show that it might be possible to use the scores from the English gold standards in other languages, hence making it unnecessary to create gold standards with human-assigned judgements in every single language."], "vector_1": {"gold": 2, "show": 1, "synonymi": 1, "joubarn": 1, "henc": 1, "paper": 1, "languag": 3, "lesk": 1, "differ": 1, "humanassign": 1, "describ": 1, "anoth": 1, "make": 1, "inkpen": 1, "except": 1, "score": 1, "creat": 1, "might": 1, "across": 1, "singl": 1, "everi": 1, "use": 1, "hyponymi": 1, "judgement": 1, "base": 1, "pair": 1, "standard": 2, "measur": 2, "algorithm": 1, "possibl": 1, "approach": 1, "aim": 1, "vector": 2, "english": 1, "similar": 1, "unnecessari": 1}, "marker": "(2011)", "article": "W14-0118", "vector_2": [3, 0.16345762499608654, 4, 2, 0, 0]}, {"label": "Neut", "current": "Inspired by Hassan and Mihalcea (2009), the following general procedure is followed in the translation of the 49 words: 2", "context": ["Whenever Rubenstein & Goodenough used the word cord, Miller & Charles uses the word chord.", "Inspired by Hassan and Mihalcea (2009), the following general procedure is followed in the translation of the 49 words: 2", "1."], "vector_1": {"cord": 1, "use": 2, "chord": 1, "word": 3, "whenev": 1, "goodenough": 1, "gener": 1, "rubenstein": 1, "procedur": 1, "hassan": 1, "charl": 1, "miller": 1, "inspir": 1, "follow": 2, "mihalcea": 1, "translat": 1}, "marker": "(2009)", "article": "W14-0118", "vector_2": [5, 0.26398672552518704, 1, 3, 0, 0]}, {"label": "Neut", "current": "We are aware of the fact that the Dutch sense-tagged corpus DutchSemCor (Vossen et al., 2012) exists.", "context": ["For Dutch, such a resource was not available.", "We are aware of the fact that the Dutch sense-tagged corpus DutchSemCor (Vossen et al., 2012) exists.", "However, an effort was made to provide an equal number of examples for each meaning in this corpus."], "vector_1": {"corpu": 2, "awar": 1, "made": 1, "resourc": 1, "provid": 1, "howev": 1, "equal": 1, "number": 1, "avail": 1, "exampl": 1, "exist": 1, "dutch": 2, "sensetag": 1, "effort": 1, "dutchsemcor": 1, "fact": 1, "mean": 1}, "marker": "(Vossen et al., 2012)", "article": "W14-0118", "vector_2": [2, 0.34241257318180396, 1, 1, 3, 0]}, {"label": "Neut", "current": "Therefore the frequencies of the lemmas in the Dutch corpus called SoNaR (Oostdijk et al., 2008) were used.", "context": ["Although this is very useful for WSD-experiments, this makes this corpus less useful for Information Content calculations.", "Therefore the frequencies of the lemmas in the Dutch corpus called SoNaR (Oostdijk et al., 2008) were used.", "It was checked whether or not the English word and its Dutch counterpart were located in the same class of relative frequency."], "vector_1": {"corpu": 2, "less": 1, "check": 1, "use": 3, "locat": 1, "make": 1, "content": 1, "lemma": 1, "call": 1, "rel": 1, "dutch": 2, "therefor": 1, "wsdexperi": 1, "although": 1, "class": 1, "word": 1, "whether": 1, "frequenc": 2, "counterpart": 1, "inform": 1, "calcul": 1, "english": 1, "sonar": 1}, "marker": "(Oostdijk et al., 2008)", "article": "W14-0118", "vector_2": [6, 0.34930027237719546, 1, 1, 0, 0]}, {"label": "Pos", "current": "The WordSimilarity-353 Test Collection (Finkelstein et al., 2002) was used to obtain example word pairs for each value that could be assigned to a word pair.", "context": ["These instructions were explained to the participants by an example of each value that could be assigned to a word pair and a general description.", "The WordSimilarity-353 Test Collection (Finkelstein et al., 2002) was used to obtain example word pairs for each value that could be assigned to a word pair.", "This dataset contains two sets of English word pairs with similarity scores assigned by humans."], "vector_1": {"set": 1, "obtain": 1, "dataset": 1, "instruct": 1, "human": 1, "use": 1, "explain": 1, "particip": 1, "two": 1, "score": 1, "test": 1, "gener": 1, "contain": 1, "wordsimilar": 1, "pair": 4, "valu": 2, "word": 4, "could": 2, "descript": 1, "collect": 1, "exampl": 2, "english": 1, "similar": 1, "assign": 3}, "marker": "(Finkelstein et al., 2002)", "article": "W14-0118", "vector_2": [12, 0.38765223380607994, 1, 3, 0, 0]}, {"label": "Neut", "current": "Finally, we try to replicate the English experiment by Pedersen (2010) using English Wordnet-LMF and WordnetTools.", "context": ["We start by comparing the Dutch to the English gold standards, followed by an evaluation of the comparison between the Dutch gold standards and the similarity measures.", "Finally, we try to replicate the English experiment by Pedersen (2010) using English Wordnet-LMF and WordnetTools.", "5"], "vector_1": {"tri": 1, "comparison": 1, "use": 1, "experi": 1, "wordnetlmf": 1, "compar": 1, "gold": 2, "measur": 1, "wordnettool": 1, "dutch": 2, "pedersen": 1, "standard": 2, "start": 1, "evalu": 1, "english": 3, "follow": 1, "replic": 1, "similar": 1, "final": 1}, "marker": "(2010)", "article": "W14-0118", "vector_2": [4, 0.616636924329232, 1, 5, 6, 0]}, {"label": "Pos", "current": "One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).", "context": ["Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.", "One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).", "Our phrasal semantic relatedness approach is inspired from those methods."], "vector_1": {"semant": 6, "featur": 1, "entail": 1, "knowledg": 1, "direct": 1, "related": 4, "one": 1, "set": 1, "path": 1, "wordnet": 1, "use": 2, "group": 1, "network": 2, "reli": 1, "call": 1, "includ": 2, "synset": 2, "approach": 3, "method": 1, "inspir": 1, "knowledgebas": 1, "relat": 1, "hyponymi": 1, "base": 1, "link": 1, "synonym": 1, "measur": 2, "word": 1, "princeton": 1, "type": 1, "meronymi": 1, "structur": 1, "depth": 1, "hypernymi": 1, "phrasal": 1, "frequent": 1}, "marker": "(Fellbaum, 1998)", "article": "S13-2019", "vector_2": [15, 0.14110836398721582, 5, 1, 0, 0]}, {"label": "Neut", "current": "One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).", "context": ["Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.", "One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).", "Our phrasal semantic relatedness approach is inspired from those methods."], "vector_1": {"semant": 6, "featur": 1, "entail": 1, "knowledg": 1, "direct": 1, "related": 4, "one": 1, "set": 1, "path": 1, "wordnet": 1, "use": 2, "group": 1, "network": 2, "reli": 1, "call": 1, "includ": 2, "synset": 2, "approach": 3, "method": 1, "inspir": 1, "knowledgebas": 1, "relat": 1, "hyponymi": 1, "base": 1, "link": 1, "synonym": 1, "measur": 2, "word": 1, "princeton": 1, "type": 1, "meronymi": 1, "structur": 1, "depth": 1, "hypernymi": 1, "phrasal": 1, "frequent": 1}, "marker": "(Leacock and Chodorow, 1998", "article": "S13-2019", "vector_2": [15, 0.14110836398721582, 5, 1, 0, 0]}, {"label": "Neut", "current": "One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).", "context": ["Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.", "One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).", "Our phrasal semantic relatedness approach is inspired from those methods."], "vector_1": {"semant": 6, "featur": 1, "entail": 1, "knowledg": 1, "direct": 1, "related": 4, "one": 1, "set": 1, "path": 1, "wordnet": 1, "use": 2, "group": 1, "network": 2, "reli": 1, "call": 1, "includ": 2, "synset": 2, "approach": 3, "method": 1, "inspir": 1, "knowledgebas": 1, "relat": 1, "hyponymi": 1, "base": 1, "link": 1, "synonym": 1, "measur": 2, "word": 1, "princeton": 1, "type": 1, "meronymi": 1, "structur": 1, "depth": 1, "hypernymi": 1, "phrasal": 1, "frequent": 1}, "marker": "(Tsatsaronis et al., 2010)", "article": "S13-2019", "vector_2": [3, 0.14110836398721582, 5, 1, 0, 0]}, {"label": "Neut", "current": "Distributional similarity models rely on the distributional hypothesis (Harris, 1954) to represent a word by its context in order to compare word semantics.", "context": ["2.2 Distributional Similarity Model", "Distributional similarity models rely on the distributional hypothesis (Harris, 1954) to represent a word by its context in order to compare word semantics.", "There are various approach for the selection, representation, and comparison of contextual data."], "vector_1": {"variou": 1, "represent": 1, "semant": 1, "word": 2, "distribut": 3, "hypothesi": 1, "comparison": 1, "approach": 1, "contextu": 1, "repres": 1, "reli": 1, "data": 1, "context": 1, "model": 2, "similar": 2, "order": 1, "select": 1, "compar": 1}, "marker": "(Harris, 1954)", "article": "S13-2019", "vector_2": [59, 0.5011156003135742, 1, 1, 0, 0]}, {"label": "Neut", "current": "One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005).", "context": ["There have been two approaches to finding such parenthetical translations.", "One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005).", "Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4)."], "vector_1": {"corpu": 1, "text": 1, "one": 1, "nonenglish": 2, "search": 1, "go": 1, "assum": 1, "find": 1, "web": 1, "use": 1, "engin": 1, "anoth": 1, "two": 1, "snippet": 1, "pattern": 1, "approach": 1, "method": 1, "match": 1, "english": 1, "given": 1, "translat": 1, "predomin": 1, "eeen": 2, "term": 1, "retriev": 1, "parenthet": 2, "collect": 1, "instanc": 1, "contain": 1, "page": 1}, "marker": "(Nagata et al, 2001, ", "article": "P08-1113", "vector_2": [7, 0.16758800242519348, 3, 2, 0, 0]}, {"label": "Neut", "current": "One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005).", "context": ["There have been two approaches to finding such parenthetical translations.", "One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005).", "Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4)."], "vector_1": {"corpu": 1, "text": 1, "one": 1, "nonenglish": 2, "search": 1, "go": 1, "assum": 1, "find": 1, "web": 1, "use": 1, "engin": 1, "anoth": 1, "two": 1, "snippet": 1, "pattern": 1, "approach": 1, "method": 1, "match": 1, "english": 1, "given": 1, "translat": 1, "predomin": 1, "eeen": 2, "term": 1, "retriev": 1, "parenthet": 2, "collect": 1, "instanc": 1, "contain": 1, "page": 1}, "marker": "Kwok et al, 2005)", "article": "P08-1113", "vector_2": [3, 0.16758800242519348, 3, 3, 0, 0]}, {"label": "Neut", "current": "Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4).", "context": ["One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005).", "Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4).", "We followed the second approach since it does not require a predefined list of English terms and is amendable for extraction at large scale."], "vector_1": {"corpu": 1, "extract": 1, "text": 1, "one": 1, "nonenglish": 2, "second": 1, "go": 1, "follow": 1, "assum": 1, "predefin": 1, "web": 1, "use": 1, "eeen": 2, "engin": 1, "anoth": 1, "snippet": 1, "larg": 1, "pattern": 1, "approach": 1, "method": 1, "match": 1, "english": 2, "search": 1, "scale": 1, "predomin": 1, "given": 1, "sinc": 1, "requir": 1, "term": 2, "retriev": 1, "amend": 1, "parenthet": 1, "list": 1, "collect": 1, "instanc": 1, "contain": 1, "page": 1}, "marker": "(Cao et al, 2007)", "article": "P08-1113", "vector_2": [1, 0.16944256214558293, 3, 6, 0, 0]}, {"label": "Neut", "current": "The lengths and frequency counts of the suffixes have been used to determine what is the translation of the in-parenthesis term (Kwok et al, 2005).", "context": ["In both cases, one can obtain a list of candidate pairs, where the translation of the in-parenthesis terms is a suffix of the pre-parenthesis text.", "The lengths and frequency counts of the suffixes have been used to determine what is the translation of the in-parenthesis term (Kwok et al, 2005).", "For example, Table 1 lists a set of Chinese segments (with word-to-word translation underneath) that precede the English term Lower Egypt."], "vector_1": {"set": 1, "suffix": 2, "egypt": 1, "text": 1, "inparenthesi": 2, "obtain": 1, "one": 1, "tabl": 1, "use": 1, "preced": 1, "count": 1, "chines": 1, "english": 1, "lower": 1, "underneath": 1, "candid": 1, "translat": 3, "pair": 1, "wordtoword": 1, "segment": 1, "case": 1, "term": 3, "frequenc": 1, "list": 2, "length": 1, "exampl": 1, "determin": 1, "preparenthesi": 1}, "marker": "(Kwok et al, 2005)", "article": "P08-1113", "vector_2": [3, 0.184600021398766, 1, 3, 0, 0]}, {"label": "Neut", "current": "1.4~3.0 (MacArthur, 1967) b cJILS/ (VN901 flight information 15:20-22:30) Vietnam Airlines Beijing/Ho Chi Minh (VN901 15:20-22:30) c ' Af*#(255-8FT) product Id.", "context": ["Examples with translations in Function of the initalic parenthesis text a ARTi3l9t1.4~3.0ZN to provide citation (MacArthur, 1967) The range of its values is within", "1.4~3.0 (MacArthur, 1967) b cJILS/ (VN901 flight information 15:20-22:30) Vietnam Airlines Beijing/Ho Chi Minh (VN901 15:20-22:30) c ' Af*#(255-8FT) product Id.", "sale of pool table (255-8FT) d // _tV1T_- // void main ( void ) function declaration // main program // void main (void ) e *,*,?"], "vector_1": {"minh": 1, "program": 1, "rang": 1, "citat": 1, "beijingho": 1, "text": 1, "within": 1, "520220": 2, "tabl": 1, "vn0": 2, "ft": 1, "artilt40zn": 1, "airlin": 1, "id": 1, "declar": 1, "chi": 1, "40": 1, "cjil": 1, "vietnam": 1, "pool": 1, "tvt": 1, "main": 3, "function": 2, "product": 1, "flight": 1, "void": 4, "afft": 1, "parenthesi": 1, "b": 1, "translat": 1, "valu": 1, "c": 1, "inital": 1, "e": 1, "provid": 1, "sale": 1, "inform": 1, "exampl": 1}, "marker": "(MacArthur, 1967)", "article": "P08-1113", "vector_2": [41, 0.3517600485038696, 2, 4, 0, 0]}, {"label": "Pos", "current": "We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000).", "context": ["Word alignment is a well-studied topic in Machine Translation with many algorithms having been proposed (Brown et al, 1993; Och and Ney 2003).", "We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000).", "The algorithm assumes that there is a score associated with each pair of words in a bi-text."], "vector_1": {"version": 1, "one": 1, "topic": 1, "assum": 1, "och": 1, "use": 1, "bitext": 1, "associ": 1, "score": 1, "call": 1, "simplest": 1, "machin": 1, "competit": 1, "ney": 1, "wellstudi": 1, "link": 1, "translat": 1, "pair": 1, "modifi": 1, "word": 3, "algorithm": 3, "align": 2, "mani": 1, "propos": 1}, "marker": "(Melamed, 2000)", "article": "P08-1113", "vector_2": [8, 0.4501230429045258, 2, 1, 0, 0]}, {"label": "Neut", "current": "Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest precision scores.", "context": ["The algorithm terminates when there are no more links to make.", "Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest precision scores.", "A disadvantage of Competitive Linking, however, is that the alignments are restricted word-to-word alignments, which implies that multi-word expressions can only be partially linked at best."], "vector_1": {"impli": 1, "partial": 1, "one": 1, "restrict": 1, "disadvantag": 1, "best": 1, "compar": 1, "make": 1, "score": 1, "tiedemann": 1, "multiword": 1, "varieti": 1, "termin": 1, "competit": 2, "express": 1, "link": 4, "wordtoword": 1, "highest": 1, "algorithm": 2, "howev": 1, "precis": 1, "found": 1, "align": 3}, "marker": "(2004)", "article": "P08-1113", "vector_2": [4, 0.4687043047184279, 1, 1, 0, 0]}, {"label": "Neut", "current": "Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).", "context": ["For example, the word happens to have a similar 2 score with Shapiro as the word  (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.", "Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).", "They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model."], "vector_1": {"corpu": 1, "shapiro": 2, "partial": 1, "dictionari": 1, "deal": 1, "represent": 1, "fluenci": 1, "phonet": 1, "happen": 2, "total": 1, "special": 1, "use": 1, "cooccurr": 1, "reli": 1, "score": 1, "approach": 1, "pinyin": 1, "chines": 1, "previou": 1, "train": 1, "translat": 1, "transliter": 3, "known": 1, "parallel": 1, "convert": 1, "word": 3, "algorithm": 1, "parenthet": 1, "exampl": 1, "statist": 1, "bilingu": 1, "model": 1, "unrel": 1, "similar": 1}, "marker": "Jiang et al, 2007", "article": "P08-1113", "vector_2": [1, 0.5502692678055565, 3, 1, 0, 0]}, {"label": "Neut", "current": "To evaluate the coverage of output produced by their method, Cao et al (2007) extracted English queries from the query log of a Chinese search engine.", "context": ["5.2 Evaluation with term translation requests", "To evaluate the coverage of output produced by their method, Cao et al (2007) extracted English queries from the query log of a Chinese search engine.", "They assume that the reason why users typed the English queries in a Chinese search box is mostly to find out their Chinese translations."], "vector_1": {"evalu": 2, "queri": 3, "al": 1, "et": 1, "assum": 1, "find": 1, "engin": 1, "log": 1, "extract": 1, "coverag": 1, "type": 1, "method": 1, "chines": 3, "search": 2, "reason": 1, "translat": 2, "mostli": 1, "box": 1, "term": 1, "request": 1, "cao": 1, "english": 2, "output": 1, "produc": 1, "user": 1}, "marker": "(2007)", "article": "P08-1113", "vector_2": [1, 0.8440029958272406, 1, 6, 0, 0]}, {"label": "Neut", "current": "Furthermore, we are able to extract two orders of magnitude more translations from than (Cao et al., 2007).", "context": ["Our work relies on unsupervised learning and does not make a distinction between translations and transliterations.", "Furthermore, we are able to extract two orders of magnitude more translations from than (Cao et al., 2007).", "7 Conclusion"], "vector_1": {"furthermor": 1, "distinct": 1, "make": 1, "work": 1, "abl": 1, "two": 1, "order": 1, "reli": 1, "magnitud": 1, "translat": 2, "transliter": 1, "learn": 1, "extract": 1, "unsupervis": 1, "conclus": 1}, "marker": "(Cao et al., 2007)", "article": "P08-1113", "vector_2": [1, 0.9664752665929598, 1, 6, 0, 0]}, {"label": "Neut", "current": "[Cohen and Levesque, 1990; Grosz and Sidner, 1986; Lochbaum; hughes and McCoy]) is centered: intuitively, all that is needed for successful communication is that the hearer understand the speaker's end intentions, not that the act types themselves be recognized.", "context": ["It is on the following point that the main criticism of bounded sets of speech acts or rhetorical relations (e.g.", "[Cohen and Levesque, 1990; Grosz and Sidner, 1986; Lochbaum; hughes and McCoy]) is centered: intuitively, all that is needed for successful communication is that the hearer understand the speaker's end intentions, not that the act types themselves be recognized.", "This intuition, along with the lack of general agreement on the precise set of acts or relations lead some to reject the utility of relations altogether and concentrate only On intentions."], "vector_1": {"set": 2, "point": 1, "rhetor": 1, "eg": 1, "hearer": 1, "bound": 1, "critic": 1, "need": 1, "follow": 1, "concentr": 1, "end": 1, "lead": 1, "commun": 1, "hugh": 1, "speaker": 1, "reject": 1, "speech": 1, "mccoy": 1, "main": 1, "type": 1, "lochbaum": 1, "lack": 1, "recogn": 1, "altogeth": 1, "gener": 1, "relat": 3, "intuit": 2, "agreement": 1, "util": 1, "understand": 1, "along": 1, "center": 1, "success": 1, "precis": 1, "act": 3, "intent": 2}, "marker": "Cohen and Levesque, 1990", "article": "W93-0235", "vector_2": [3, 0.543358350664617, 2, 1, 0, 0]}, {"label": "Neut", "current": "by clue words such as \"so\", \"no\", \"okay\", purpose clauses) are hypothesized by the Speech Act Interpreter [Heeman, 1993] and used by the Dialogue Manager [Traum, 1993] to guide further interpretation.", "context": ["Those relations that are conventionally signalled by surface features (e.g.", "by clue words such as \"so\", \"no\", \"okay\", purpose clauses) are hypothesized by the Speech Act Interpreter [Heeman, 1993] and used by the Dialogue Manager [Traum, 1993] to guide further interpretation.", "In the case of more implicit relationships we often do not identify the precise relation, merely operating on the speech act level forms."], "vector_1": {"oper": 1, "featur": 1, "identifi": 1, "often": 1, "eg": 1, "guid": 1, "mere": 1, "surfac": 1, "form": 1, "convent": 1, "speech": 2, "use": 1, "claus": 1, "okay": 1, "relationship": 1, "relat": 2, "dialogu": 1, "clue": 1, "implicit": 1, "interpret": 2, "case": 1, "word": 1, "level": 1, "hypothes": 1, "manag": 1, "precis": 1, "act": 2, "purpos": 1, "signal": 1}, "marker": "Heeman, 1993]", "article": "W93-0235", "vector_2": [0, 0.7326159688941134, 2, 1, 0, 1]}, {"label": "Neut", "current": "Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011).", "context": ["One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).", "Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011).", "In our work, we extend the task to predicting the learner's level based on the errors, focusing on Hebrew."], "vector_1": {"rang": 2, "help": 1, "predict": 1, "challeng": 1, "hebrew": 1, "focus": 1, "one": 1, "topic": 1, "see": 1, "morphologicallycomplex": 1, "increasingli": 1, "languag": 2, "detect": 2, "hoo": 1, "learner": 1, "write": 1, "type": 1, "extend": 1, "assist": 1, "autom": 1, "base": 1, "becom": 1, "recent": 1, "task": 2, "rare": 1, "level": 1, "work": 2, "exampl": 1, "error": 4, "popular": 1}, "marker": "(Dickinson et al., 2011)", "article": "W12-2011", "vector_2": [1, 0.03294662160104747, 4, 2, 0, 0]}, {"label": "Weak", "current": "learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.", "context": ["The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95-104, Montreal, Canada, June 3-8, 2012. c2012 Association for Computational Linguistics", "learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.", "For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses."], "vector_1": {"nlp": 2, "comput": 1, "deal": 1, "obtain": 1, "exist": 1, "canada": 1, "use": 1, "illform": 1, "gold": 1, "innov": 1, "perform": 1, "associ": 1, "learner": 2, "workshop": 1, "build": 1, "th": 1, "answer": 1, "analys": 1, "adapt": 1, "analysi": 1, "applic": 1, "product": 1, "resourc": 1, "optim": 1, "tool": 1, "june": 1, "standard": 1, "reason": 1, "educ": 1, "data": 1, "c": 1, "potenti": 1, "linguist": 3, "page": 1, "montreal": 1}, "marker": "(Goldberg and Elhadad, 2011", "article": "W12-2011", "vector_2": [1, 0.09415060442739259, 3, 1, 0, 0]}, {"label": "Neut", "current": "learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.", "context": ["The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95-104, Montreal, Canada, June 3-8, 2012. c2012 Association for Computational Linguistics", "learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.", "For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses."], "vector_1": {"nlp": 2, "comput": 1, "deal": 1, "obtain": 1, "exist": 1, "canada": 1, "use": 1, "illform": 1, "gold": 1, "innov": 1, "perform": 1, "associ": 1, "learner": 2, "workshop": 1, "build": 1, "th": 1, "answer": 1, "analys": 1, "adapt": 1, "analysi": 1, "applic": 1, "product": 1, "resourc": 1, "optim": 1, "tool": 1, "june": 1, "standard": 1, "reason": 1, "educ": 1, "data": 1, "c": 1, "potenti": 1, "linguist": 3, "page": 1, "montreal": 1}, "marker": "Yona and Wintner, 2008", "article": "W12-2011", "vector_2": [4, 0.09415060442739259, 3, 1, 1, 0]}, {"label": "Neut", "current": "(Dickinson, 2011)).", "context": ["e.g.", "(Dickinson, 2011)).", "An error could feature, for instance, a letter inserted in an irregular verb stem, or between two nouns; any of these properties may be relevant to describing the error (cf."], "vector_1": {"insert": 1, "verb": 1, "featur": 1, "noun": 1, "describ": 1, "irregular": 1, "may": 1, "eg": 1, "properti": 1, "two": 1, "relev": 1, "stem": 1, "instanc": 1, "cf": 1, "letter": 1, "error": 2, "could": 1}, "marker": "(Dickinson, 2011)", "article": "W12-2011", "vector_2": [1, 0.05605797631010018, 1, 2, 0, 0]}, {"label": "Pos", "current": "Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.", "context": ["The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.", "Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.", "In this paper, we introduce two novel machine learning algorithms specialized for the MT task."], "vector_1": {"entropi": 1, "ten": 1, "appli": 1, "foundat": 1, "socal": 1, "signific": 1, "paper": 1, "year": 1, "special": 1, "techniqu": 1, "gradient": 1, "smt": 1, "two": 1, "noisychannel": 1, "method": 1, "introduc": 1, "machin": 3, "rerank": 1, "task": 1, "translat": 2, "recent": 1, "novel": 1, "algorithm": 1, "provid": 1, "maximum": 1, "mt": 2, "statist": 1, "learn": 1, "improv": 1, "model": 2}, "marker": "(Och and Ney, 2002)", "article": "N04-1023", "vector_2": [2, 0.044060341834175704, 3, 3, 8, 1]}, {"label": "Pos", "current": "Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.", "context": ["The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.", "Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.", "In this paper, we introduce two novel machine learning algorithms specialized for the MT task."], "vector_1": {"entropi": 1, "ten": 1, "appli": 1, "foundat": 1, "socal": 1, "signific": 1, "paper": 1, "year": 1, "special": 1, "techniqu": 1, "gradient": 1, "smt": 1, "two": 1, "noisychannel": 1, "method": 1, "introduc": 1, "machin": 3, "rerank": 1, "task": 1, "translat": 2, "recent": 1, "novel": 1, "algorithm": 1, "provid": 1, "maximum": 1, "mt": 2, "statist": 1, "learn": 1, "improv": 1, "model": 2}, "marker": "(Och, 2003)", "article": "N04-1023", "vector_2": [1, 0.044060341834175704, 3, 6, 5, 1]}, {"label": "Pos", "current": "The seminal IBM models (Brown et al., 1990) were the first to introduce generative models to the MT task.", "context": ["1.1 Generative Models for MT", "The seminal IBM models (Brown et al., 1990) were the first to introduce generative models to the MT task.", "The IBM models applied the sequence learning paradigm well-known from Hidden Markov Models in speech recognition to the problem of MT."], "vector_1": {"task": 1, "ibm": 2, "recognit": 1, "appli": 1, "gener": 2, "learn": 1, "semin": 1, "mt": 3, "speech": 1, "markov": 1, "sequenc": 1, "problem": 1, "model": 5, "paradigm": 1, "wellknown": 1, "hidden": 1, "introduc": 1, "first": 1}, "marker": "(Brown et al., 1990)", "article": "N04-1023", "vector_2": [14, 0.09691731934102396, 1, 2, 0, 0]}, {"label": "Neut", "current": "In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation.", "context": ["Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders.", "In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation.", "However, phrase level alignment cannot handle long distance reordering effectively."], "vector_1": {"distanc": 1, "directli": 1, "within": 1, "templat": 2, "phrase": 6, "languag": 1, "differ": 1, "long": 1, "accuraci": 1, "reorder": 3, "higher": 1, "handl": 3, "shallow": 1, "use": 2, "effect": 1, "util": 1, "cannot": 1, "translat": 3, "sinc": 1, "twolevel": 1, "word": 2, "howev": 1, "align": 4, "level": 1, "structur": 1, "employ": 1, "achiev": 1, "model": 2, "order": 1}, "marker": "(Och and Weber, 1998", "article": "N04-1023", "vector_2": [6, 0.1502758594081562, 2, 1, 10, 1]}, {"label": "Neut", "current": "Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model estimated using a maximum entropy model.", "context": ["1.2 Discriminative Models for MT", "Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model estimated using a maximum entropy model.", "A small number of feature functions defined on the source and target sentence were used to rerank the translations generated by a baseline MT system."], "vector_1": {"featur": 1, "entropi": 1, "direct": 1, "baselin": 1, "och": 1, "use": 3, "discrimin": 1, "system": 1, "defin": 1, "estim": 1, "ney": 1, "condit": 1, "function": 1, "sourc": 1, "rerank": 1, "sentenc": 1, "gener": 1, "number": 1, "framework": 1, "base": 1, "translat": 2, "target": 1, "maximum": 1, "mt": 3, "small": 1, "model": 3, "propos": 1}, "marker": "(2002)", "article": "N04-1023", "vector_2": [2, 0.19360314826960917, 1, 3, 8, 1]}, {"label": "Pos", "current": "This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).", "context": ["The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation.", "This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).", "SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions."], "vector_1": {"set": 1, "show": 1, "obtain": 1, "templat": 1, "minimum": 1, "significantli": 1, "featur": 2, "paramet": 1, "och": 1, "use": 4, "smt": 1, "better": 1, "also": 1, "estim": 1, "criterion": 1, "larg": 1, "experi": 1, "approach": 3, "function": 1, "error": 1, "mutual": 1, "number": 1, "train": 1, "align": 1, "maximum": 1, "inform": 1, "team": 1, "result": 1}, "marker": "(Och and Ney, 2002)", "article": "N04-1023", "vector_2": [2, 0.21729233380917473, 3, 3, 8, 1]}, {"label": "Neut", "current": "One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003).", "context": ["Two large margin approaches have been used.", "One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003).", "However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks."], "vector_1": {"show": 1, "global": 1, "rank": 2, "one": 1, "bias": 1, "perceptron": 1, "prank": 2, "use": 2, "multipl": 1, "boundari": 1, "two": 2, "larg": 1, "consecut": 1, "approach": 1, "everi": 1, "rerank": 1, "variant": 1, "repres": 1, "introduct": 1, "task": 1, "algorithm": 3, "howev": 1, "work": 1, "due": 1, "margin": 1, "section": 1}, "marker": "Harrington, 2003)", "article": "N04-1023", "vector_2": [1, 0.3057216713607778, 2, 1, 0, 0]}, {"label": "Neut", "current": "In (SMT Team, 2003), 450 features were generated.", "context": ["The test set is used to assess the quality of the reranking output.", "In (SMT Team, 2003), 450 features were generated.", "Six features from (Och, 2003) were used as baseline features."], "vector_1": {"use": 2, "set": 1, "rerank": 1, "gener": 1, "six": 1, "qualiti": 1, "assess": 1, "featur": 3, "test": 1, "output": 1, "baselin": 1}, "marker": "(SMT Team, 2003)", "article": "N04-1023", "vector_2": [1, 0.7975230525869054, 2, 8, 0, 0]}, {"label": "Neut", "current": "Our response has been to design and implement a software environment called GATE (Cunninham et al., 1997), which we will demonstrate at ANLP.", "context": ["But the pressure towards theoretical diversity means that there is no point attempting to gain agreement, in the short term, on what set of component technologies should be developed or on the informational content or syntax of representations that these components should require or produce.", "Our response has been to design and implement a software environment called GATE (Cunninham et al., 1997), which we will demonstrate at ANLP.", "GATE attempts to meet the following objectives:"], "vector_1": {"represent": 1, "set": 1, "point": 1, "syntax": 1, "pressur": 1, "design": 1, "anlp": 1, "respons": 1, "follow": 1, "compon": 2, "develop": 1, "theoret": 1, "content": 1, "call": 1, "demonstr": 1, "gate": 2, "divers": 1, "object": 1, "agreement": 1, "gain": 1, "requir": 1, "term": 1, "short": 1, "attempt": 2, "technolog": 1, "inform": 1, "environ": 1, "meet": 1, "implement": 1, "toward": 1, "softwar": 1, "produc": 1, "mean": 1}, "marker": "(Cunninham et al., 1997)", "article": "A97-2017", "vector_2": [0, 0.24920479569366283, 1, 1, 1, 1]}, {"label": "Neut", "current": "The basic concepts of the data model underlying the GDM are those of the TIPSTER architecture, which is specified (Grishman, 1996).", "context": ["All communication between the components of an LE system goes through GDM, which insulates these components from direct contact with each other and provides them with a uniform API for manipulating the data they produce and consume.", "The basic concepts of the data model underlying the GDM are those of the TIPSTER architecture, which is specified (Grishman, 1996).", "All the real work of analysing texts in a GATEbased LE system is done by CREOLE modules or objects (we use the terms module and object rather loosely to mean interfaces to resources which may be predominantly algorithmic or predominantly data, or a mixture of both)."], "vector_1": {"underli": 1, "concept": 1, "text": 1, "modul": 2, "consum": 1, "direct": 1, "api": 1, "done": 1, "tipster": 1, "gdm": 2, "creol": 1, "use": 1, "le": 2, "commun": 1, "goe": 1, "system": 2, "gatebas": 1, "uniform": 1, "compon": 2, "interfac": 1, "basic": 1, "analys": 1, "architectur": 1, "real": 1, "object": 2, "resourc": 1, "may": 1, "manipul": 1, "mixtur": 1, "specifi": 1, "insul": 1, "data": 3, "term": 1, "algorithm": 1, "provid": 1, "work": 1, "loos": 1, "rather": 1, "contact": 1, "model": 1, "predominantli": 2, "produc": 1, "mean": 1}, "marker": "(Grishman, 1996)", "article": "A97-2017", "vector_2": [1, 0.4291656471739662, 1, 1, 0, 0]}, {"label": "Pos", "current": "The C-test is a variant of the cloze test which contains more gaps but provides part of the solution as a hint and has been found to be a good estimate for language proficiency (Eckes and Grotjahn, 2006).", "context": ["For language testing, redundancy can be reduced by eliminating words from a text and asking the learner to fill in the gap, also known as the cloze test.", "The C-test is a variant of the cloze test which contains more gaps but provides part of the solution as a hint and has been found to be a good estimate for language proficiency (Eckes and Grotjahn, 2006).", "We present an approach for determining the difficulty of C-tests that overcomes the mentioned drawbacks of subjective evaluation by teachers."], "vector_1": {"profici": 1, "evalu": 1, "text": 1, "known": 1, "languag": 2, "fill": 1, "hint": 1, "learner": 1, "drawback": 1, "cloze": 2, "also": 1, "estim": 1, "present": 1, "overcom": 1, "test": 3, "subject": 1, "approach": 1, "difficulti": 1, "elimin": 1, "good": 1, "variant": 1, "gap": 2, "solut": 1, "part": 1, "ask": 1, "redund": 1, "teacher": 1, "ctest": 2, "reduc": 1, "word": 1, "mention": 1, "provid": 1, "determin": 1, "contain": 1, "found": 1}, "marker": "(Eckes and Grotjahn, 2006)", "article": "Q14-1040", "vector_2": [8, 0.046091900532140906, 1, 2, 3, 0]}, {"label": "Weak", "current": "Unfortunately, this scoring method turned out to be quite subjective and time-consuming as it is not possible to anticipate all tolerable solutions (Raatz and Klein-Braley, 2002).", "context": ["In relaxed scoring, teachers accept all tolerable candidates for a gap and not only the intended solution as in exact scoring.", "Unfortunately, this scoring method turned out to be quite subjective and time-consuming as it is not possible to anticipate all tolerable solutions (Raatz and Klein-Braley, 2002).", "The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked."], "vector_1": {"set": 1, "anticip": 1, "circumv": 1, "accept": 1, "need": 1, "close": 1, "open": 1, "subject": 1, "quit": 1, "use": 1, "space": 1, "timeconsum": 1, "solut": 4, "distractor": 1, "score": 3, "method": 1, "intend": 1, "relax": 1, "gap": 1, "candid": 2, "toler": 2, "exact": 1, "teacher": 1, "possibl": 1, "provid": 1, "unfortun": 1, "turn": 1, "pick": 1}, "marker": "(Raatz and Klein-Braley, 2002)", "article": "Q14-1040", "vector_2": [12, 0.10652934868381118, 1, 2, 9, 0]}, {"label": "Neut", "current": "Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves.", "context": ["The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked.", "Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves.", "However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results."], "vector_1": {"set": 2, "circumv": 1, "hard": 1, "automat": 1, "bias": 1, "result": 1, "need": 1, "close": 1, "open": 1, "select": 1, "use": 1, "easi": 1, "space": 1, "make": 1, "solut": 4, "distractor": 4, "valid": 1, "sever": 1, "approach": 1, "correct": 1, "sure": 1, "option": 1, "presenc": 1, "guess": 1, "candid": 1, "enabl": 1, "provid": 1, "howev": 1, "lead": 1, "random": 1, "pick": 1, "propos": 1}, "marker": "(Sakaguchi et al., 2013", "article": "Q14-1040", "vector_2": [1, 0.11231612811848735, 2, 1, 0, 0]}, {"label": "Neut", "current": "In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.", "context": ["However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results.", "In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.", "Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995)."], "vector_1": {"set": 1, "advantag": 1, "thorough": 1, "kleinbraley": 1, "random": 1, "indic": 1, "bias": 1, "result": 1, "empir": 1, "raatz": 1, "stabl": 1, "languag": 1, "guess": 1, "lead": 1, "solut": 1, "cloze": 2, "distractor": 1, "valid": 1, "overcom": 1, "test": 4, "analys": 1, "correct": 1, "altern": 1, "option": 1, "presenc": 1, "regard": 1, "weak": 1, "theori": 1, "ctest": 2, "enabl": 1, "howev": 1, "reliabl": 1, "correl": 1, "follow": 1, "principl": 1, "order": 1, "propos": 1}, "marker": "(1984)", "article": "Q14-1040", "vector_2": [30, 0.11834570948748559, 4, 4, 7, 0]}, {"label": "Neut", "current": "As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition.", "context": ["In addition, the prefix hint allows for a narrower deletion pattern (every second gap) providing more empirical evidence for the students' abilities on less text.", "As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition.", "However, Jakschik et al (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps."], "vector_1": {"extent": 1, "less": 1, "text": 1, "cohen": 1, "al": 1, "second": 1, "significantli": 1, "empir": 1, "et": 1, "abil": 2, "open": 1, "skill": 1, "given": 1, "multipl": 1, "hint": 1, "pattern": 1, "transform": 1, "find": 1, "delet": 1, "ctest": 3, "test": 2, "easier": 1, "jakschik": 1, "product": 1, "everi": 1, "option": 1, "evid": 1, "read": 1, "variant": 1, "gap": 2, "student": 1, "consid": 1, "true": 1, "requir": 1, "addit": 1, "reduc": 1, "choic": 1, "examin": 1, "provid": 2, "howev": 1, "prefix": 2, "allow": 1, "recognit": 2, "narrow": 1}, "marker": "(1984)", "article": "Q14-1040", "vector_2": [30, 0.13267102361248811, 2, 1, 0, 0]}, {"label": "Neut", "current": "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "context": ["Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.", "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates."], "vector_1": {"set": 1, "profici": 1, "process": 1, "natur": 1, "close": 1, "languag": 2, "multipl": 1, "student": 1, "cloze": 1, "distractor": 1, "field": 1, "exercis": 3, "test": 2, "perspect": 1, "correct": 1, "difficulti": 1, "format": 1, "gener": 2, "previou": 1, "answer": 1, "candid": 1, "vocabulari": 1, "educ": 1, "choic": 2, "grammar": 1, "provid": 1, "approach": 1, "work": 1, "focu": 1, "discrimin": 1, "determin": 1, "usual": 1}, "marker": "(Mostow and Jang, 2012", "article": "Q14-1040", "vector_2": [2, 0.14527649070270926, 7, 1, 0, 0]}, {"label": "Neut", "current": "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "context": ["Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.", "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates."], "vector_1": {"set": 1, "profici": 1, "process": 1, "natur": 1, "close": 1, "languag": 2, "multipl": 1, "student": 1, "cloze": 1, "distractor": 1, "field": 1, "exercis": 3, "test": 2, "perspect": 1, "correct": 1, "difficulti": 1, "format": 1, "gener": 2, "previou": 1, "answer": 1, "candid": 1, "vocabulari": 1, "educ": 1, "choic": 2, "grammar": 1, "provid": 1, "approach": 1, "work": 1, "focu": 1, "discrimin": 1, "determin": 1, "usual": 1}, "marker": "(Skory and Eskenazi, 2010", "article": "Q14-1040", "vector_2": [4, 0.14527649070270926, 7, 1, 2, 0]}, {"label": "Neut", "current": "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "context": ["Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.", "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates."], "vector_1": {"set": 1, "profici": 1, "process": 1, "natur": 1, "close": 1, "languag": 2, "multipl": 1, "student": 1, "cloze": 1, "distractor": 1, "field": 1, "exercis": 3, "test": 2, "perspect": 1, "correct": 1, "difficulti": 1, "format": 1, "gener": 2, "previou": 1, "answer": 1, "candid": 1, "vocabulari": 1, "educ": 1, "choic": 2, "grammar": 1, "provid": 1, "approach": 1, "work": 1, "focu": 1, "discrimin": 1, "determin": 1, "usual": 1}, "marker": "Heilman et al., 2007", "article": "Q14-1040", "vector_2": [7, 0.14527649070270926, 7, 1, 2, 0]}, {"label": "Neut", "current": "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "context": ["Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.", "The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).", "The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates."], "vector_1": {"set": 1, "profici": 1, "process": 1, "natur": 1, "close": 1, "languag": 2, "multipl": 1, "student": 1, "cloze": 1, "distractor": 1, "field": 1, "exercis": 3, "test": 2, "perspect": 1, "correct": 1, "difficulti": 1, "format": 1, "gener": 2, "previou": 1, "answer": 1, "candid": 1, "vocabulari": 1, "educ": 1, "choic": 2, "grammar": 1, "provid": 1, "approach": 1, "work": 1, "focu": 1, "discrimin": 1, "determin": 1, "usual": 1}, "marker": "Brown et al., 2005)", "article": "Q14-1040", "vector_2": [9, 0.14527649070270926, 7, 1, 2, 0]}, {"label": "CoCo", "current": "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "context": ["The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\"", "While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.", "The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level."], "vector_1": {"featur": 1, "profici": 1, "focus": 1, "connect": 1, "touch": 1, "skill": 1, "languag": 3, "involv": 1, "rather": 1, "question": 1, "construct": 1, "valid": 1, "advoc": 1, "factor": 1, "test": 2, "instead": 1, "analys": 1, "tightli": 1, "difficulti": 2, "gener": 1, "argu": 1, "gap": 1, "vocabulari": 1, "earliest": 1, "ctest": 4, "reduc": 1, "measur": 2, "search": 1, "grammar": 1, "level": 3, "aim": 1, "paragraph": 1, "combin": 1, "determin": 1, "model": 1, "other": 1}, "marker": "(Babaii and Ansary, 2001)", "article": "Q14-1040", "vector_2": [13, 0.1619488901928252, 6, 3, 0, 0]}, {"label": "Neut", "current": "Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators - average sentence length and type-token ratio - obtaining good results for her target group.", "context": ["The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.", "Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators - average sentence length and type-token ratio - obtaining good results for her target group.", "Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level."], "vector_1": {"calibr": 1, "classic": 1, "pool": 1, "kleinbraley": 1, "obtain": 1, "indic": 1, "result": 1, "use": 1, "earliest": 1, "ratio": 1, "compar": 1, "perform": 2, "two": 1, "ctest": 3, "test": 1, "instead": 1, "analys": 1, "build": 1, "analysi": 2, "difficulti": 3, "good": 1, "intend": 1, "linear": 1, "sentenc": 1, "eck": 1, "differ": 1, "rasch": 1, "focus": 1, "group": 1, "gap": 2, "averag": 1, "target": 1, "typetoken": 1, "level": 2, "kamimoto": 1, "item": 1, "length": 1, "paragraph": 1, "regress": 1, "model": 1, "order": 1, "first": 1}, "marker": "(1984)", "article": "Q14-1040", "vector_2": [30, 0.1695971511239706, 3, 4, 7, 0]}, {"label": "Pos", "current": "As the generous time limit allows the students to revise their solutions for typos, we consider them as normal errors in line with Raatz and Klein-Braley (2002).", "context": ["of vs. off or then vs. than and we cannot decide whether it is a spelling error or a wrong word choice.", "As the generous time limit allows the students to revise their solutions for typos, we consider them as normal errors in line with Raatz and Klein-Braley (2002).", "4 C-Test Difficulty Model"], "vector_1": {"consid": 1, "kleinbraley": 1, "vs": 2, "raatz": 1, "revis": 1, "decid": 1, "error": 2, "solut": 1, "typo": 1, "difficulti": 1, "normal": 1, "gener": 1, "spell": 1, "wrong": 1, "cannot": 1, "student": 1, "line": 1, "ctest": 1, "choic": 1, "word": 1, "whether": 1, "limit": 1, "allow": 1, "time": 1, "model": 1}, "marker": "(2002)", "article": "Q14-1040", "vector_2": [12, 0.31068530845962405, 1, 2, 9, 0]}, {"label": "Neut", "current": "In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001).", "context": ["We find that the difficulty of C-tests is determined by a combination of many factors.", "In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001).", "Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving."], "vector_1": {"process": 1, "share": 1, "terminolog": 1, "establish": 1, "find": 1, "learner": 1, "detail": 1, "microlevel": 1, "requir": 1, "factor": 1, "macrolevel": 1, "analys": 1, "strategi": 3, "difficulti": 1, "solv": 2, "categor": 1, "discuss": 1, "ctest": 3, "success": 1, "psycholinguist": 1, "combin": 1, "determin": 1, "mani": 1, "order": 1}, "marker": "(Babaii and Ansary, 2001)", "article": "Q14-1040", "vector_2": [13, 0.328996621006414, 3, 3, 0, 0]}, {"label": "Neut", "current": "Sigott (1995) calculates the frequency of the solution word using counts from the SUSANNE corpus.7 For our calculations, we use the larger Web1T corpus (Brants and Franz, 2006) and extract normalized probabilities instead of absolute frequencies for better comparison.", "context": ["This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty.", "Sigott (1995) calculates the frequency of the solution word using counts from the SUSANNE corpus.7 For our calculations, we use the larger Web1T corpus (Brants and Franz, 2006) and extract normalized probabilities instead of absolute frequencies for better comparison.", "Furthermore, a gap is easier to solve, if the solution occurs in a very typical context, e.g."], "vector_1": {"corpu": 2, "identifi": 1, "eg": 1, "clearli": 1, "indic": 1, "bias": 1, "webt": 1, "still": 1, "extract": 1, "occur": 1, "probabl": 1, "absolut": 1, "use": 2, "solut": 2, "cloze": 1, "better": 1, "estim": 1, "instead": 1, "easier": 1, "difficulti": 1, "good": 1, "normal": 1, "sigott": 1, "susann": 1, "gap": 2, "solv": 1, "count": 1, "comparison": 1, "word": 1, "furthermor": 1, "frequenc": 3, "larger": 1, "calcul": 2, "context": 1, "typic": 1}, "marker": "(1995)", "article": "Q14-1040", "vector_2": [19, 0.3798029257633086, 2, 7, 1, 0]}, {"label": "Neut", "current": "Sigott (1995) calculates the frequency of the solution word using counts from the SUSANNE corpus.7 For our calculations, we use the larger Web1T corpus (Brants and Franz, 2006) and extract normalized probabilities instead of absolute frequencies for better comparison.", "context": ["This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty.", "Sigott (1995) calculates the frequency of the solution word using counts from the SUSANNE corpus.7 For our calculations, we use the larger Web1T corpus (Brants and Franz, 2006) and extract normalized probabilities instead of absolute frequencies for better comparison.", "Furthermore, a gap is easier to solve, if the solution occurs in a very typical context, e.g."], "vector_1": {"corpu": 2, "identifi": 1, "eg": 1, "clearli": 1, "indic": 1, "bias": 1, "webt": 1, "still": 1, "extract": 1, "occur": 1, "probabl": 1, "absolut": 1, "use": 2, "solut": 2, "cloze": 1, "better": 1, "estim": 1, "instead": 1, "easier": 1, "difficulti": 1, "good": 1, "normal": 1, "sigott": 1, "susann": 1, "gap": 2, "solv": 1, "count": 1, "comparison": 1, "word": 1, "furthermor": 1, "frequenc": 3, "larger": 1, "calcul": 2, "context": 1, "typic": 1}, "marker": "(Brants and Franz, 2006)", "article": "Q14-1040", "vector_2": [8, 0.3798029257633086, 2, 1, 0, 0]}, {"label": "Weak", "current": "Sigott (1995) could not confirm any effect of the word class on C-test difficulty.", "context": ["Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners.", "Sigott (1995) could not confirm any effect of the word class on C-test difficulty.", "6In all examples, we only highlight a single gap to illustrate a certain phenomenon."], "vector_1": {"preposit": 1, "claim": 1, "often": 1, "difficulti": 1, "illustr": 1, "kleinbraley": 1, "in": 1, "find": 1, "phenomenon": 1, "confirm": 1, "learner": 1, "certain": 1, "easier": 1, "singl": 1, "function": 1, "brown": 1, "sigott": 1, "effect": 1, "gap": 1, "solv": 1, "class": 1, "ctest": 1, "word": 2, "could": 1, "harder": 1, "exampl": 1, "highlight": 1}, "marker": "(1995)", "article": "Q14-1040", "vector_2": [19, 0.4090807923436457, 3, 7, 1, 0]}, {"label": "Pos", "current": "We retrieve cognates from UBY using string similarity and from a cognate production algorithm (Beinborn et al., 2013).", "context": ["for skeletons we find *skellets, *skelleton(s), *skelets, *skelletts, *skeletton(s), *skeltons, *skeletes, and *skelette(s).11 In order to account for this phenomenon, we estimate the cognateness of words by gathering data from four different lists.", "We retrieve cognates from UBY using string similarity and from a cognate production algorithm (Beinborn et al., 2013).", "In addition, we consult the COCA list of academic words12 and a list of words with latin roots.13 Inflection Many errors are caused by wrong morphological inflection as in this example: And in har times like these, ... [harder] The base form hard (72) is provided more often than the correct comparative harder (48), although it is too short."], "vector_1": {"skeleton": 1, "harder": 2, "hard": 1, "four": 1, "find": 1, "phenomenon": 1, "differ": 1, "skeletton": 1, "compar": 1, "data": 1, "skelleton": 1, "short": 1, "morpholog": 1, "estim": 1, "har": 1, "order": 1, "skellett": 1, "correct": 1, "product": 1, "latin": 1, "string": 1, "form": 1, "skellet": 1, "inflect": 2, "time": 1, "use": 1, "often": 1, "wrong": 1, "base": 1, "although": 1, "consult": 1, "word": 3, "skelet": 2, "ubi": 1, "addit": 1, "account": 1, "skelton": 1, "retriev": 1, "like": 1, "algorithm": 1, "provid": 1, "root": 1, "gather": 1, "list": 3, "cognat": 3, "caus": 1, "exampl": 1, "coca": 1, "error": 1, "mani": 1, "academ": 1, "similar": 1, "skelett": 1}, "marker": "(Beinborn et al., 2013)", "article": "Q14-1040", "vector_2": [1, 0.44206138842239445, 1, 1, 5, 0]}, {"label": "Neut", "current": "We build a character-based language model that indicates the probability of a character sequence using BerkeleyLM (Pauls and Klein, 2011).", "context": ["In addition, a spelling error is more likely to occur, in words with rare grapheme-phoneme mapping as in Wednesday.", "We build a character-based language model that indicates the probability of a character sequence using BerkeleyLM (Pauls and Klein, 2011).", "In addition, we build a phonetic model using phonetisaurus, a statistical alignment algorithm that maps characters onto phonemes.14 Both models are trained only on words from the Basic English list in order to reflect the knowledge of a language learner.15 Based on this scarce data, the phonetic model only learns the most frequent character-tophoneme mappings and assigns higher phonetic scores to less general letter sequences."], "vector_1": {"learn": 1, "charactertophonem": 1, "less": 1, "knowledg": 1, "phonet": 3, "phonem": 1, "occur": 1, "languag": 2, "probabl": 1, "basic": 1, "use": 2, "rare": 1, "learner": 1, "characterbas": 1, "score": 1, "berkeleylm": 1, "build": 2, "phonetisauru": 1, "higher": 1, "map": 3, "assign": 1, "sequenc": 2, "gener": 1, "spell": 1, "reflect": 1, "train": 1, "indic": 1, "letter": 1, "graphemephonem": 1, "data": 1, "addit": 2, "word": 2, "like": 1, "algorithm": 1, "english": 1, "align": 1, "list": 1, "wednesday": 1, "charact": 2, "base": 1, "statist": 1, "error": 1, "model": 4, "scarc": 1, "onto": 1, "order": 1, "frequent": 1}, "marker": "(Pauls and Klein, 2011)", "article": "Q14-1040", "vector_2": [3, 0.4785626125488133, 1, 1, 0, 0]}, {"label": "Neut", "current": "Klein-Braley (1984) already determined the type-token ratio as useful cue for paragraph difficulty prediction.", "context": ["The type-token ratio, the verb variation, and the pronoun ratio are used as indicators for lexical diversity and referentiality.", "Klein-Braley (1984) already determined the type-token ratio as useful cue for paragraph difficulty prediction.", "We also use syntactic readability features such as the number of entity mentions, the number of certain POS types (e.g."], "vector_1": {"featur": 1, "pronoun": 1, "predict": 1, "kleinbraley": 1, "eg": 1, "number": 2, "indic": 1, "alreadi": 1, "entiti": 1, "use": 3, "ratio": 3, "referenti": 1, "readabl": 1, "also": 1, "cue": 1, "certain": 1, "type": 1, "divers": 1, "difficulti": 1, "syntact": 1, "variat": 1, "lexic": 1, "mention": 1, "verb": 1, "typetoken": 2, "po": 1, "paragraph": 1, "determin": 1}, "marker": "(1984)", "article": "Q14-1040", "vector_2": [30, 0.6655201019768124, 1, 4, 7, 0]}, {"label": "Neut", "current": "Classification Regression P R Fl Pearson's r RMSE Majority Baseline .19 .43 .26 .00 .25 Sigott (1995) .23 .40 .28 .34 .24 Our Approach .46 .48 .46 .64 .20 Human Median .56 .53 .54 -", "context": ["Thus, it is possible to outperform human performance with automatic methods and provide a very helpful tool.", "Classification Regression P R Fl Pearson's r RMSE Majority Baseline .19 .43 .26 .00 .25 Sigott (1995) .23 .40 .28 .34 .24 Our Approach .46 .48 .46 .64 .20 Human Median .56 .53 .54 -", "Table 3: Results for leave-one-out crossvalidation on the training set for regression and classification prediction (both trained on support vector machines)."], "vector_1": {"classif": 2, "set": 1, "crossvalid": 1, "help": 1, "predict": 1, "major": 1, "automat": 1, "result": 1, "human": 2, "tabl": 1, "baselin": 1, "perform": 1, "support": 1, "vector": 1, "approach": 1, "method": 1, "machin": 1, "tool": 1, "sigott": 1, "train": 2, "leaveoneout": 1, "fl": 1, "pearson": 1, "possibl": 1, "provid": 1, "outperform": 1, "thu": 1, "rmse": 1, "median": 1, "p": 1, "r": 2, "regress": 2}, "marker": "(1995)", "article": "Q14-1040", "vector_2": [19, 0.7294781781761528, 1, 7, 1, 0]}, {"label": "Neut", "current": "We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18", "context": ["We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014).", "We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18", "6.1 Classification vs Regression"], "vector_1": {"core": 1, "use": 2, "featur": 1, "weka": 1, "tc": 1, "framework": 1, "process": 1, "natur": 1, "tool": 1, "differ": 1, "classifi": 1, "dataset": 1, "provid": 1, "vs": 1, "regress": 1, "perform": 1, "experi": 1, "extract": 1, "dkpro": 2, "languag": 1, "classif": 1}, "marker": "(Daxenberger et al., 2014)", "article": "Q14-1040", "vector_2": [0, 0.7420634117718471, 3, 1, 5, 0]}, {"label": "Neut", "current": "We compare our model against the human performance and two baselines: A naive one that predicts the majority class for classification and the mean value for regression and one that only uses the features proposed by Sigott (1995) (solution probability, word class of solution, and constituent type of gap).", "context": ["We perform leave-one-out testing on the training set in order to determine the best approach.", "We compare our model against the human performance and two baselines: A naive one that predicts the majority class for classification and the mean value for regression and one that only uses the features proposed by Sigott (1995) (solution probability, word class of solution, and constituent type of gap).", "In Table 3, we report weighted precision, recall and Fl-measure over all classes for classification and Pearson correlation and root mean squared error for regression."], "vector_1": {"major": 1, "set": 1, "leaveoneout": 1, "weight": 1, "predict": 1, "naiv": 1, "classif": 2, "one": 2, "featur": 1, "human": 1, "tabl": 1, "best": 1, "baselin": 1, "precis": 1, "use": 1, "squar": 1, "compar": 1, "perform": 2, "solut": 2, "two": 1, "flmeasur": 1, "test": 1, "probabl": 1, "approach": 1, "recal": 1, "sigott": 1, "gap": 1, "train": 1, "report": 1, "class": 3, "valu": 1, "pearson": 1, "word": 1, "type": 1, "constitu": 1, "correl": 1, "determin": 1, "error": 1, "regress": 2, "model": 1, "propos": 1, "root": 1, "order": 1, "mean": 2}, "marker": "(1995)", "article": "Q14-1040", "vector_2": [19, 0.7575420350848795, 1, 7, 1, 0]}, {"label": "Neut", "current": "Useful features for this approach include TFIDF and its variations, position of a phrase, POS information, and relative length of a phrase (Turney, 2000).", "context": ["In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.", "Useful features for this approach include TFIDF and its variations, position of a phrase, POS information, and relative length of a phrase (Turney, 2000).", "Some of these features may not work well for meeting transcripts."], "vector_1": {"set": 1, "featur": 3, "need": 1, "phrase": 2, "use": 2, "neg": 1, "classifi": 1, "includ": 1, "rel": 1, "document": 1, "approach": 2, "po": 1, "may": 1, "variat": 1, "candid": 1, "transcript": 1, "word": 1, "algorithm": 1, "work": 1, "well": 1, "tfidf": 1, "inform": 1, "length": 1, "exampl": 1, "learn": 2, "posit": 2, "meet": 1}, "marker": "(Turney, 2000)", "article": "N09-1070", "vector_2": [9, 0.16724154604945038, 1, 2, 3, 0]}, {"label": "Neut", "current": "The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus.", "context": ["Not many studies have been performed on speech transcripts for keyword extraction.", "The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus.", "They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF)."], "vector_1": {"corpu": 1, "semant": 1, "show": 1, "signific": 1, "extract": 2, "leverag": 1, "ratio": 1, "compar": 1, "perform": 2, "speech": 1, "rel": 1, "multiparti": 1, "approach": 1, "resourc": 1, "base": 1, "mani": 1, "transcript": 1, "relev": 1, "task": 1, "keyword": 2, "frequenc": 1, "work": 1, "yield": 1, "idf": 1, "improv": 1, "studi": 2, "similar": 1, "meet": 1}, "marker": "(Plas et al., 2004)", "article": "N09-1070", "vector_2": [5, 0.216788627059089, 1, 1, 0, 0]}, {"label": "CoCo", "current": "There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).", "context": ["They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).", "There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).", "(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data."], "vector_1": {"semant": 2, "show": 2, "process": 1, "eg": 1, "signific": 1, "extract": 1, "leverag": 1, "use": 2, "ratio": 1, "compar": 1, "perform": 2, "also": 1, "speech": 2, "rel": 1, "approach": 1, "resourc": 1, "broadcast": 1, "base": 1, "news": 1, "data": 1, "task": 1, "retriev": 1, "keyword": 2, "frequenc": 1, "work": 1, "yield": 1, "combin": 1, "verif": 1, "idf": 1, "improv": 2, "similar": 1}, "marker": "Bulyko et al., 2007", "article": "N09-1070", "vector_2": [2, 0.2224622030237581, 6, 1, 2, 0]}, {"label": "Neut", "current": "There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).", "context": ["They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).", "There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).", "(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data."], "vector_1": {"semant": 2, "show": 2, "process": 1, "eg": 1, "signific": 1, "extract": 1, "leverag": 1, "use": 2, "ratio": 1, "compar": 1, "perform": 2, "also": 1, "speech": 2, "rel": 1, "approach": 1, "resourc": 1, "broadcast": 1, "base": 1, "news": 1, "data": 1, "task": 1, "retriev": 1, "keyword": 2, "frequenc": 1, "work": 1, "yield": 1, "combin": 1, "verif": 1, "idf": 1, "improv": 2, "similar": 1}, "marker": "Wu et al., 2007", "article": "N09-1070", "vector_2": [2, 0.2224622030237581, 6, 2, 0, 0]}, {"label": "Neut", "current": "All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).", "context": ["We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings.", "All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).", "The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus."], "vector_1": {"corpu": 2, "telephon": 1, "natur": 1, "obtain": 1, "system": 1, "topic": 1, "rate": 1, "extract": 1, "occur": 1, "use": 1, "icsi": 1, "transcrib": 1, "speech": 1, "asr": 1, "error": 1, "da": 1, "entir": 1, "meet": 4, "data": 1, "word": 1, "convers": 1, "sri": 1, "annot": 1, "record": 1, "stateoftheart": 1, "dialog": 1, "act": 1, "output": 1, "summari": 1}, "marker": "(Murray et al., 2005)", "article": "N09-1070", "vector_2": [4, 0.25337674478579025, 4, 3, 0, 0]}, {"label": "Pos", "current": "We used the TnT POS tagger (Brants, 2000) trained from the Switchboard data to tag the meeting transcripts.", "context": ["Our hypothesis is that verb, noun and adjective words are more likely to be keywords, so we restrict our selection to words with these POS tags only.", "We used the TnT POS tagger (Brants, 2000) trained from the Switchboard data to tag the meeting transcripts.", "(C) Integrating word clustering One weakness of the baseline TFIDF is that it counts the frequency for a particular word, without considering any words that are similar to it in terms of semantic meaning."], "vector_1": {"semant": 1, "particular": 1, "one": 1, "restrict": 1, "cluster": 1, "tag": 2, "tagger": 1, "select": 1, "baselin": 1, "use": 1, "data": 1, "adject": 1, "po": 2, "hypothesi": 1, "weak": 1, "term": 1, "verb": 1, "consid": 1, "switchboard": 1, "noun": 1, "transcript": 1, "count": 1, "tnt": 1, "c": 1, "word": 5, "like": 1, "keyword": 1, "frequenc": 1, "without": 1, "tfidf": 1, "train": 1, "integr": 1, "meet": 1, "similar": 1, "mean": 1}, "marker": "(Brants, 2000)", "article": "N09-1070", "vector_2": [9, 0.43625286096515264, 1, 1, 0, 0]}, {"label": "Neut", "current": "The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation.", "context": ["Since that is highly correlated with the results using exact word match, we report results based on strict matching in the following experiments.", "The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation.", "Instead of comparing the system output with each individual human annotation, the method creates a \"pyramid\" using all the human annotated keywords, and then compares system output to this pyramid."], "vector_1": {"pyramid": 3, "evalu": 1, "metric": 1, "second": 1, "correl": 1, "result": 2, "human": 2, "follow": 1, "summar": 1, "highli": 1, "use": 4, "creat": 1, "compar": 2, "system": 2, "strict": 1, "instead": 1, "experi": 1, "individu": 1, "match": 2, "base": 1, "report": 1, "exact": 1, "sinc": 1, "word": 1, "keyword": 1, "annot": 2, "method": 1, "output": 2, "similar": 1}, "marker": "(Nenkova and Passonneau, 2004)", "article": "N09-1070", "vector_2": [5, 0.6302182392572774, 1, 1, 0, 0]}, {"label": "Neut", "current": "(Liu et al., 2008) investigated adding bigram key phrases, which we expect to be independent of these unigram-based approaches and adding bigram phrases will yield further performance gain for the unsupervised approach.", "context": ["Again note that these results used wordbased selection.", "(Liu et al., 2008) investigated adding bigram key phrases, which we expect to be independent of these unigram-based approaches and adding bigram phrases will yield further performance gain for the unsupervised approach.", "Finally, we analyzed if the system's keyword extraction performance is correlated with human annotation disagreement using the unsupervised approach (TFIDF+POS+Sent weight)."], "vector_1": {"bigram": 2, "ad": 2, "weight": 1, "independ": 1, "correl": 1, "result": 1, "human": 1, "phrase": 2, "extract": 1, "select": 1, "use": 2, "perform": 2, "system": 1, "note": 1, "analyz": 1, "approach": 3, "final": 1, "wordbas": 1, "investig": 1, "tfidfposs": 1, "gain": 1, "key": 1, "keyword": 1, "disagr": 1, "unsupervis": 2, "annot": 1, "yield": 1, "unigrambas": 1, "expect": 1}, "marker": "(Liu et al., 2008)", "article": "N09-1070", "vector_2": [1, 0.775345733535347, 1, 4, 0, 1]}, {"label": "Neut", "current": "We focus on distributional paraphrasing (Pasca and Dienes 2005; Marton et al., 2009; Marton, to appear 2012).", "context": ["\"phrases\" -- and doing so overcoming previous working memory and representation limitations.", "We focus on distributional paraphrasing (Pasca and Dienes 2005; Marton et al., 2009; Marton, to appear 2012).", "We will also cover pivot paraphrasing (Bannard and Callison-Burch, 2005)."], "vector_1": {"represent": 1, "previou": 1, "distribut": 1, "memori": 1, "also": 1, "work": 1, "cover": 1, "focu": 1, "pasca": 1, "marton": 1, "limit": 1, "paraphras": 2, "overcom": 1, "pivot": 1, "phrase": 1, "dien": 1, "appear": 1}, "marker": "Marton et al., 2009", "article": "N12-4007", "vector_2": [3, 0.4788774894387447, 2, 6, 0, 0]}, {"label": "Neut", "current": "(Mohammad et al., EMNLP 2008; Hovy, 2010; Marton et al., WMT 2011).", "context": ["What else can be done to ameliorate this problem?", "(Mohammad et al., EMNLP 2008; Hovy, 2010; Marton et al., WMT 2011).", "Another potential weakness is the difficulty in detecting and generating longer-thanword (phrasal) paraphrases, because pre-calculating a collocation matrix for phrases becomes prohibitive in the matrix size with longer phrases, even with sparse representation."], "vector_1": {"prohibit": 1, "represent": 1, "emnlp": 1, "al": 2, "done": 1, "paraphras": 1, "phrase": 2, "et": 2, "size": 1, "even": 1, "colloc": 1, "detect": 1, "matrix": 2, "anoth": 1, "wmt": 1, "els": 1, "precalcul": 1, "difficulti": 1, "spars": 1, "amelior": 1, "gener": 1, "weak": 1, "becom": 1, "phrasal": 1, "mohammad": 1, "longer": 1, "longerthanword": 1, "marton": 1, "potenti": 1, "problem": 1}, "marker": "Hovy, 2010", "article": "N12-4007", "vector_2": [2, 0.6427278213639107, 1, 6, 0, 0]}, {"label": "Neut", "current": "There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).", "context": ["We will present an alternative to pre-calculation: on-demand paraphrasing, as described in Marton (to appear 2012).", "There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).", "This enables constructing large vector representation, since there is no longer a need to compute a whole matrix."], "vector_1": {"represent": 1, "comput": 1, "suffix": 2, "ondemand": 2, "text": 1, "prefix": 1, "monolingu": 1, "done": 1, "paraphras": 1, "need": 1, "array": 1, "longer": 1, "describ": 1, "construct": 1, "matrix": 1, "larg": 1, "altern": 1, "resourc": 1, "precalcul": 1, "link": 1, "sinc": 1, "present": 1, "appear": 1, "search": 1, "enabl": 1, "tree": 1, "marton": 1, "vector": 1, "whole": 1}, "marker": "Lopez, 2007)", "article": "N12-4007", "vector_2": [5, 0.7857573928786964, 3, 6, 0, 0]}, {"label": "Neut", "current": "Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system (Marino et al., 2006).", "context": ["1 Introduction", "Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system (Marino et al., 2006).", "In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al (2007)."], "vector_1": {"campaign": 1, "ngrambas": 2, "al": 1, "past": 1, "year": 1, "et": 1, "shown": 1, "group": 1, "evalu": 1, "compar": 1, "prove": 1, "smt": 2, "system": 2, "approach": 1, "statist": 1, "machin": 1, "koehn": 1, "previou": 1, "translat": 1, "develop": 1, "introduct": 1, "callisonburch": 1, "phrasebas": 1, "talpupc": 1, "stateoftheart": 1, "monz": 1}, "marker": "(Marino et al., 2006)", "article": "W08-0315", "vector_2": [2, 0.08759800427655025, 3, 1, 0, 0]}, {"label": "Neut", "current": "Similar to the work in (Lita et al., 2003), we tag each word in a sentence with one of the tags:", "context": ["Automatic capitalization can be seen as a sequence tagging problem: each lower-case word receives a tag that describes its capitalization form.", "Similar to the work in (Lita et al., 2003), we tag each word in a sentence with one of the tags:", " LOC lowercase  CAP capitalized  MXC mixed case; no further guess is made as to the capitalization of such words."], "vector_1": {"work": 1, "automat": 1, "one": 1, "tag": 4, "seen": 1, "loc": 1, "guess": 1, "describ": 1, "receiv": 1, "mix": 1, "sentenc": 1, "capit": 4, "form": 1, "sequenc": 1, "lowercas": 2, "mxc": 1, "case": 1, "made": 1, "word": 3, "cap": 1, "problem": 1, "similar": 1}, "marker": "(Lita et al., 2003)", "article": "W04-3237", "vector_2": [1, 0.16807653129013686, 1, 3, 0, 0]}, {"label": "Neut", "current": "The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well.", "context": ["Due to its popularity, both our work and that of (Lita et al., 2003) uses the 1-gram capitalizer as a baseline.", "The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well.", "2.2 Previous Work"], "vector_1": {"use": 3, "previou": 1, "word": 1, "evalu": 1, "algorithm": 2, "perform": 1, "work": 3, "well": 1, "capit": 1, "indic": 1, "due": 1, "consequ": 1, "gram": 2, "popular": 1, "microsoft": 1, "baselin": 2}, "marker": "(Kim and Woodland, 2004)", "article": "W04-3237", "vector_2": [0, 0.25319987599096505, 2, 2, 0, 0]}, {"label": "Neut", "current": "A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors.", "context": ["The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output.", "A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors.", "Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996)."], "vector_1": {"larger": 1, "depart": 1, "partofspeech": 1, "show": 1, "appli": 1, "idea": 1, "automat": 1, "second": 1, "tag": 2, "tagger": 1, "explor": 1, "work": 1, "languag": 1, "quit": 1, "use": 1, "describ": 1, "sensit": 1, "recognit": 2, "speech": 2, "build": 1, "approach": 3, "capit": 2, "memm": 1, "sequenc": 1, "gener": 2, "standard": 1, "punctuat": 2, "consid": 1, "robust": 1, "techniqu": 1, "case": 1, "name": 1, "success": 1, "outperform": 1, "rulebas": 1, "context": 1, "error": 2, "output": 1, "model": 1}, "marker": "(Brill, 1994)", "article": "W04-3237", "vector_2": [10, 0.27853315027237696, 3, 2, 0, 0]}, {"label": "Neut", "current": "Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996).", "context": ["A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors.", "Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996).", "The MEMM approach models the tag sequence T conditionally on the word sequence W, which has a few substantial advantages over the 1-gram tagging approach:"], "vector_1": {"work": 1, "depart": 1, "partofspeech": 1, "advantag": 1, "show": 1, "appli": 1, "second": 1, "tag": 4, "tagger": 1, "substanti": 1, "languag": 1, "quit": 1, "use": 1, "describ": 1, "sensit": 1, "recognit": 1, "speech": 1, "build": 1, "condit": 1, "approach": 5, "capit": 1, "memm": 2, "sequenc": 3, "gener": 1, "standard": 1, "punctuat": 1, "consid": 1, "robust": 1, "techniqu": 1, "case": 1, "word": 1, "name": 1, "success": 1, "outperform": 1, "rulebas": 1, "gram": 1, "w": 1, "error": 2, "model": 2}, "marker": "(Ratnaparkhi, 1996)", "article": "W04-3237", "vector_2": [8, 0.28256344390805616, 2, 3, 0, 0]}, {"label": "CoCo", "current": "More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001) which slightly outperforms MEMMs on a standard partof-speech tagging task.", "context": [" discriminative training of probability model P(T|W) using conditional maximum likelihood is well correlated with tagging accuracy  ability to use a rich set of word-level features in a parsimonious way: sub-word features such as prefixes and suffixes, as well as future words3 are easily incorporated in the probability model  no concept of \"out-of-vocabulary\" word: subword features are very useful in dealing with words not seen in the training data  ability to integrate rich contextual features into the model", "More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001) which slightly outperforms MEMMs on a standard partof-speech tagging task.", "In a similar vein, the work"], "vector_1": {"set": 1, "suffix": 1, "deal": 1, "outofvocabulari": 1, "parsimoni": 1, "prefix": 1, "correl": 1, "abil": 2, "ptw": 1, "seen": 1, "featur": 4, "probabl": 2, "concept": 1, "use": 3, "easili": 1, "data": 1, "accuraci": 1, "field": 1, "drawback": 1, "futur": 1, "rich": 2, "certain": 1, "condit": 2, "approach": 1, "random": 1, "way": 1, "memm": 2, "wordlevel": 1, "standard": 1, "train": 2, "subword": 2, "partofspeech": 1, "address": 1, "likelihood": 1, "vein": 1, "recent": 1, "task": 1, "word": 3, "slightli": 1, "outperform": 1, "work": 1, "well": 2, "maximum": 1, "contextu": 1, "discrimin": 1, "tag": 2, "crf": 1, "integr": 1, "incorpor": 1, "model": 4, "similar": 1}, "marker": "(Lafferty et al., 2001)", "article": "W04-3237", "vector_2": [3, 0.30829531865893084, 1, 1, 0, 0]}, {"label": "Neut", "current": "The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).", "context": ["of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs.", "The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).", "We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004)."], "vector_1": {"often": 1, "fair": 1, "gaussian": 2, "one": 1, "explor": 1, "exponenti": 1, "paramet": 1, "awar": 1, "use": 4, "prior": 3, "overlook": 1, "label": 1, "favor": 1, "adapt": 2, "baselin": 1, "build": 1, "map": 2, "memm": 1, "sequenc": 1, "previou": 1, "differ": 1, "hmm": 2, "train": 1, "likelihood": 1, "model": 4, "present": 1, "case": 1, "work": 2, "smooth": 2, "maximum": 1, "maxent": 2, "discrimin": 1, "inadequ": 1, "problem": 1}, "marker": "(Chen and Rosenfeld, 2000)", "article": "W04-3237", "vector_2": [4, 0.34279640373798664, 3, 5, 0, 0]}, {"label": "Neut", "current": "The next section briefly describes the training procedure; for details the reader is referred to (Berger et al., 1996).", "context": ["The probability P(ti|xi(W,T i1 1 )) is modeled using a maximum entropy model.", "The next section briefly describes the training procedure; for details the reader is referred to (Berger et al., 1996).", "3.1 Maximum Entropy State Transition Model"], "vector_1": {"state": 1, "use": 1, "entropi": 2, "describ": 1, "transit": 1, "3": 1, "i": 1, "detail": 1, "ptixiwt": 1, "maximum": 2, "procedur": 1, "next": 1, "briefli": 1, "train": 1, "reader": 1, "model": 3, "refer": 1, "section": 1, "probabl": 1}, "marker": "(Berger et al., 1996)", "article": "W04-3237", "vector_2": [8, 0.420656362106382, 1, 1, 0, 0]}, {"label": "Neut", "current": "As shown in (Chen and Rosenfeld, 2000) - and rederived in Appendix A for the non-zero mean case - the update equations are:", "context": ["L(A) = E p(x, y)log pA(y|x) (1) x,y n P(T|W) = i=1 E Z(x, A) = y i 2' + const(A)i", "As shown in (Chen and Rosenfeld, 2000) - and rederived in Appendix A for the non-zero mean case - the update equations are:", "(t+1) i= (t) i+ i, where i satisfies: i p(x, y)fi(x, y)  = 2 i p(x)p(y|x)fi(x, y)exp(if#(x, y))"], "vector_1": {"payx": 1, "ylog": 1, "xy": 1, "zx": 1, "yfix": 1, "ptw": 1, "nonzero": 1, "shown": 1, "la": 1, "constai": 1, "px": 2, "t": 1, "satisfi": 1, "appendix": 1, "rederiv": 1, "case": 1, "e": 2, "i": 1, "n": 1, "equat": 1, "pxpyxfix": 1, "yexpifx": 1, "updat": 1, "mean": 1}, "marker": "(Chen and Rosenfeld, 2000)", "article": "W04-3237", "vector_2": [4, 0.4943088710748926, 1, 5, 0, 0]}, {"label": "Neut", "current": "A Gaussian prior for the model parameters A has been previously used in (Chen and Rosenfeld, 2000) for smoothing MaxEnt models.", "context": ["A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters.", "A Gaussian prior for the model parameters A has been previously used in (Chen and Rosenfeld, 2000) for smoothing MaxEnt models.", "The prior has 0 mean and diagonal covariance: A  N(0, diag(2 i))."], "vector_1": {"map": 1, "use": 3, "accomplish": 1, "distribut": 1, "diag": 1, "smooth": 1, "gaussian": 1, "maxent": 1, "diagon": 1, "prior": 3, "n": 1, "covari": 1, "adapt": 1, "way": 1, "mean": 1, "model": 3, "previous": 1, "simpl": 1, "paramet": 2}, "marker": "(Chen and Rosenfeld, 2000)", "article": "W04-3237", "vector_2": [4, 0.5198193011205102, 1, 5, 0, 0]}, {"label": "Neut", "current": "ing (Pietra et al., 1995) between the background exponential model B - assumed fixed - and an exponential model A built using the Fbackground U Fadapt feature set.", "context": ["F  i=1 E x,y i + (2) 2i E x,y F  i=1", "ing (Pietra et al., 1995) between the background exponential model B - assumed fixed - and an exponential model A built using the Fbackground U Fadapt feature set.", "It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 - following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models - then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5."], "vector_1": {"set": 1, "fadapt": 1, "weight": 1, "gaussian": 1, "procedur": 1, "maximum": 1, "xy": 2, "follow": 1, "ing": 1, "assum": 1, "use": 1, "built": 1, "exponenti": 2, "fix": 1, "estim": 1, "adapt": 2, "approach": 1, "updat": 1, "map": 1, "ident": 1, "shown": 1, "mindiv": 1, "background": 1, "data": 1, "featur": 2, "b": 1, "e": 2, "center": 1, "f": 2, "i": 3, "smooth": 2, "fbackground": 1, "prior": 1, "u": 1, "equat": 1, "model": 4, "entropi": 1, "propos": 1}, "marker": "(Pietra et al., 1995)", "article": "W04-3237", "vector_2": [9, 0.6498959209885292, 2, 1, 0, 0]}, {"label": "Neut", "current": "The baseline 1-gram and the background MEMM capitalizer were trained on various amounts of WSJ (Paul and Baker, 1992) data from 1987 - files WS87_{001-126}.", "context": ["5 Experiments", "The baseline 1-gram and the background MEMM capitalizer were trained on various amounts of WSJ (Paul and Baker, 1992) data from 1987 - files WS87_{001-126}.", "The in-domain test data used was file WS94_000 (8.7kwds)."], "vector_1": {"capit": 1, "kwd": 1, "file": 2, "variou": 1, "memm": 1, "indomain": 1, "wsj": 1, "use": 1, "ws0026": 1, "test": 1, "amount": 1, "train": 1, "ws": 1, "gram": 1, "background": 1, "9": 1, "experi": 1, "data": 2, "baselin": 1}, "marker": "(Paul and Baker, 1992)", "article": "W04-3237", "vector_2": [12, 0.6991452234377076, 1, 1, 0, 0]}, {"label": "Neut", "current": "For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.", "context": ["Researchers have already performed related work on reconstructing language family trees.", "For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.", "More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering."], "vector_1": {"kita": 1, "metric": 1, "al": 1, "cluster": 1, "chrietien": 1, "alreadi": 1, "famili": 2, "batagelj": 1, "et": 1, "languag": 3, "use": 1, "perform": 1, "research": 1, "method": 2, "statist": 1, "relat": 1, "reconstruct": 2, "recent": 1, "measur": 1, "work": 1, "tree": 2, "ellegard": 1, "instanc": 1, "kroeber": 1, "similar": 1, "propos": 2}, "marker": "(1937)", "article": "P13-1112", "vector_2": [76, 0.16172366201502836, 4, 1, 0, 0]}, {"label": "Neut", "current": "For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.", "context": ["Researchers have already performed related work on reconstructing language family trees.", "For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.", "More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering."], "vector_1": {"kita": 1, "metric": 1, "al": 1, "cluster": 1, "chrietien": 1, "alreadi": 1, "famili": 2, "batagelj": 1, "et": 1, "languag": 3, "use": 1, "perform": 1, "research": 1, "method": 2, "statist": 1, "relat": 1, "reconstruct": 2, "recent": 1, "measur": 1, "work": 1, "tree": 2, "ellegard": 1, "instanc": 1, "kroeber": 1, "similar": 1, "propos": 2}, "marker": "(1959)", "article": "P13-1112", "vector_2": [54, 0.16172366201502836, 4, 1, 0, 0]}, {"label": "Neut", "current": "The similarity used for clustering is based on a divergence-like distance between two language models that was originally proposed by Juang and Rabiner (1985).", "context": ["Then, agglomerative hierarchical clustering is applied to the language models to reconstruct a language family tree.", "The similarity used for clustering is based on a divergence-like distance between two language models that was originally proposed by Juang and Rabiner (1985).", "This method is purely data-driven and does not require human expert knowledge for the selection of linguistic features."], "vector_1": {"origin": 1, "distanc": 1, "datadriven": 1, "human": 1, "appli": 1, "knowledg": 1, "cluster": 2, "featur": 1, "famili": 1, "languag": 3, "hierarch": 1, "use": 1, "expert": 1, "two": 1, "select": 1, "pure": 1, "divergencelik": 1, "method": 1, "agglom": 1, "reconstruct": 1, "requir": 1, "rabin": 1, "juang": 1, "tree": 1, "base": 1, "model": 2, "linguist": 1, "similar": 1, "propos": 1}, "marker": "(1985)", "article": "P13-1112", "vector_2": [28, 0.18285539027756478, 1, 1, 0, 0]}, {"label": "Neut", "current": "For reference, we also used native English (British and American university students' essays in the LOCNESS corpus5) and two sets of Japanese English (ICLE and the NICE corpus (Sugiura et al., 2007)).", "context": ["Also, the symbols ' and ' were unified into '4.", "For reference, we also used native English (British and American university students' essays in the LOCNESS corpus5) and two sets of Japanese English (ICLE and the NICE corpus (Sugiura et al., 2007)).", "Table 1 shows the statistics on the corpus data."], "vector_1": {"essay": 1, "corpu": 3, "set": 1, "show": 1, "tabl": 1, "loc": 1, "use": 1, "japanes": 1, "univers": 1, "two": 1, "also": 2, "nice": 1, "symbol": 1, "nativ": 1, "student": 1, "data": 1, "icl": 1, "british": 1, "american": 1, "unifi": 1, "statist": 1, "english": 2, "refer": 1}, "marker": "(Sugiura et al., 2007)", "article": "P13-1112", "vector_2": [6, 0.3830087409906456, 1, 1, 0, 0]}, {"label": "Weak", "current": "Considering this, we tested CRFTagger6 on non-native English texts containing various grammatical errors before the experiments (Nagata et al., 2011).", "context": ["Existing POS taggers might not perform well on non-native English texts because they are normally developed to analyze native English texts.", "Considering this, we tested CRFTagger6 on non-native English texts containing various grammatical errors before the experiments (Nagata et al., 2011).", "It turned out that CRFTagger achieved an accuracy of 0.932 (compared to 0.970 on native texts)."], "vector_1": {"text": 4, "tagger": 1, "develop": 1, "compar": 1, "perform": 1, "accuraci": 1, "crftagger": 2, "analyz": 1, "test": 1, "experi": 1, "might": 1, "po": 1, "variou": 1, "normal": 1, "contain": 1, "nativ": 2, "consid": 1, "grammat": 1, "english": 3, "nonn": 2, "well": 1, "turn": 1, "achiev": 1, "error": 1, "exist": 1}, "marker": "(Nagata et al., 2011)", "article": "P13-1112", "vector_2": [2, 0.39362061033583806, 1, 1, 0, 0]}, {"label": "Neut", "current": "The tree at the top is the Indo-European family tree drawn based on the figure shown in Crystal (1997).", "context": ["Fig.1 shows the experimental results.", "The tree at the top is the Indo-European family tree drawn based on the figure shown in Crystal (1997).", "It shows that the 11 languages are divided into three groups: Italic, Germanic, and Slavic branches."], "vector_1": {"shown": 1, "group": 1, "branch": 1, "show": 2, "german": 1, "ital": 1, "top": 1, "divid": 1, "tree": 2, "indoeuropean": 1, "three": 1, "crystal": 1, "figur": 1, "experiment": 1, "result": 1, "famili": 1, "slavic": 1, "drawn": 1, "base": 1, "languag": 1, "fig": 1}, "marker": "(1997)", "article": "P13-1112", "vector_2": [16, 0.447937432909063, 1, 1, 0, 0]}, {"label": "Neut", "current": "5 shows that the average length roughly distinguishes the Italic Englishes from the other nonnative Englishes; French-English is the shortest, which is explained by the discussion above, while Dutch- and German-Englishes are longest, which may correspond to the fact that they have a preference for noun-noun compounds as Snyder (1996) argues.", "context": ["Fig.", "5 shows that the average length roughly distinguishes the Italic Englishes from the other nonnative Englishes; French-English is the shortest, which is explained by the discussion above, while Dutch- and German-Englishes are longest, which may correspond to the fact that they have a preference for noun-noun compounds as Snyder (1996) argues.", "For instance, German allows the concatenated form as in Orangensaft (equivalently orangejuice)."], "vector_1": {"show": 1, "german": 1, "prefer": 1, "averag": 1, "orangejuic": 1, "equival": 1, "explain": 1, "snyder": 1, "nounnoun": 1, "fig": 1, "dutch": 1, "orangensaft": 1, "germanenglish": 1, "form": 1, "may": 1, "ital": 1, "argu": 1, "longest": 1, "compound": 1, "distinguish": 1, "discuss": 1, "shortest": 1, "concaten": 1, "frenchenglish": 1, "nonn": 1, "correspond": 1, "length": 1, "instanc": 1, "allow": 1, "english": 2, "roughli": 1, "fact": 1}, "marker": "(1996)", "article": "P13-1112", "vector_2": [17, 0.772550222358534, 1, 1, 0, 0]}, {"label": "Neut", "current": "Researchers including Wong and Dras (2009), Wong et al (2011; 2012), and Koppel et al (2005) work on native language identification and show that machine learning-based methods are effective.", "context": ["6 Implications for Work in Related Domains", "Researchers including Wong and Dras (2009), Wong et al (2011; 2012), and Koppel et al (2005) work on native language identification and show that machine learning-based methods are effective.", "Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation."], "vector_1": {"domain": 1, "show": 2, "al": 2, "expect": 1, "et": 2, "languag": 1, "implic": 1, "use": 2, "perform": 2, "research": 1, "identif": 1, "includ": 1, "better": 1, "method": 1, "machin": 1, "relat": 1, "effect": 1, "nativ": 1, "koppel": 1, "grammat": 1, "learningbas": 1, "contrari": 1, "work": 2, "inform": 1, "achiev": 1, "wong": 3, "determin": 1, "error": 2, "dra": 2, "improv": 1, "propos": 1}, "marker": "(2005)", "article": "P13-1112", "vector_2": [8, 0.8258549302254256, 3, 1, 1, 0]}, {"label": "Neut", "current": "7 shows that the observed values in the FrenchEnglish data very closely fit the theoretical proba\"For comparison, we conducted a pilot study where we reconstructed a language family tree from English texts in European Parliament Proceedings Parallel Corpus (Europarl) (Koehn, 2011).", "context": ["Fig.", "7 shows that the observed values in the FrenchEnglish data very closely fit the theoretical proba\"For comparison, we conducted a pilot study where we reconstructed a language family tree from English texts in European Parliament Proceedings Parallel Corpus (Europarl) (Koehn, 2011).", "It turned out that the reconstructed tree was different from the canonical tree (available at http: //web.hyogo-u.ac.jp/nagata/acl/)."], "vector_1": {"corpu": 1, "show": 1, "text": 1, "canon": 1, "famili": 1, "webhyogouacjpnagataacl": 1, "close": 1, "languag": 1, "differ": 1, "proceed": 1, "fit": 1, "theoret": 1, "probafor": 1, "avail": 1, "fig": 1, "conduct": 1, "european": 1, "parliament": 1, "http": 1, "reconstruct": 2, "data": 1, "parallel": 1, "valu": 1, "pilot": 1, "comparison": 1, "frenchenglish": 1, "tree": 3, "turn": 1, "english": 1, "europarl": 1, "studi": 1, "observ": 1}, "marker": "(Koehn, 2011)", "article": "P13-1112", "vector_2": [2, 0.8896488268670449, 1, 1, 0, 0]}, {"label": "Neut", "current": "Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "context": ["In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.", "Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose."], "vector_1": {"corpu": 1, "domain": 1, "task": 1, "comput": 1, "techniqu": 1, "appli": 1, "use": 2, "tree": 2, "histor": 1, "research": 1, "other": 1, "cluster": 1, "corpusbas": 1, "statist": 1, "famili": 2, "reconstruct": 2, "linguist": 1, "method": 1, "languag": 2, "purpos": 1}, "marker": "Nakhleh et al., 2005)", "article": "P13-1112", "vector_2": [8, 0.9278944947094004, 7, 2, 2, 0]}, {"label": "Neut", "current": "Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.", "context": ["Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.", "Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.", "These methods reconstruct language family trees based on linguistic features that exist within words including lexical, phonological, and morphological features."], "vector_1": {"corpu": 1, "featur": 2, "phonolog": 1, "appli": 1, "within": 1, "cluster": 1, "linguist": 1, "exist": 1, "famili": 2, "languag": 2, "use": 1, "techniqu": 1, "morpholog": 1, "other": 1, "includ": 1, "method": 1, "lexic": 1, "base": 1, "reconstruct": 2, "task": 1, "word": 1, "tree": 2, "statist": 1, "purpos": 1}, "marker": "(Kita, 1999", "article": "P13-1112", "vector_2": [14, 0.9316055819659561, 7, 3, 0, 0]}, {"label": "Neut", "current": "Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.", "context": ["Parallelement  l'evolution des ressources lexicales, on a pu observer une explosion de travaux portant sur les graphes (graphes complexes, phenomene 'petit monde', etc.).", "Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.", "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al."], "vector_1": {"phrase": 1, "associ": 1, "domain": 1, "portant": 1, "aspect": 1, "modelis": 2, "petit": 1, "celui": 1, "al": 1, "ce": 1, "en": 1, "travaux": 2, "il": 1, "semblent": 1, "explos": 1, "eu": 1, "sen": 1, "diver": 1, "le": 2, "pu": 1, "graph": 2, "lexical": 1, "complex": 1, "sur": 1, "capter": 1, "leur": 1, "dictionnair": 1, "dernier": 1, "preter": 1, "phenomen": 1, "structur": 2, "effet": 1, "et": 2, "de": 6, "lexic": 1, "mot": 1, "levolut": 1, "du": 2, "langu": 1, "parallel": 1, "compri": 1, "mond": 2, "merveil": 1, "nombreux": 2, "montrant": 1, "la": 2, "pour": 2, "etc": 1, "gaum": 1, "ressourc": 1, "une": 1, "pertin": 1, "ou": 1, "observ": 1, "se": 1}, "marker": "(Barrat, 2008, ", "article": "W14-6700", "vector_2": [6, 0.24098810612991767, 8, 1, 0, 0]}, {"label": "Neut", "current": "Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.", "context": ["Parallelement  l'evolution des ressources lexicales, on a pu observer une explosion de travaux portant sur les graphes (graphes complexes, phenomene 'petit monde', etc.).", "Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.", "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al."], "vector_1": {"phrase": 1, "associ": 1, "domain": 1, "portant": 1, "aspect": 1, "modelis": 2, "petit": 1, "celui": 1, "al": 1, "ce": 1, "en": 1, "travaux": 2, "il": 1, "semblent": 1, "explos": 1, "eu": 1, "sen": 1, "diver": 1, "le": 2, "pu": 1, "graph": 2, "lexical": 1, "complex": 1, "sur": 1, "capter": 1, "leur": 1, "dictionnair": 1, "dernier": 1, "preter": 1, "phenomen": 1, "structur": 2, "effet": 1, "et": 2, "de": 6, "lexic": 1, "mot": 1, "levolut": 1, "du": 2, "langu": 1, "parallel": 1, "compri": 1, "mond": 2, "merveil": 1, "nombreux": 2, "montrant": 1, "la": 2, "pour": 2, "etc": 1, "gaum": 1, "ressourc": 1, "une": 1, "pertin": 1, "ou": 1, "observ": 1, "se": 1}, "marker": "Barabsi, 2003)", "article": "W14-6700", "vector_2": [11, 0.24098810612991767, 8, 1, 0, 0]}, {"label": "Neut", "current": "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "context": ["Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.", "En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc."], "vector_1": {"phrase": 1, "domain": 1, "en": 1, "aspect": 2, "modelis": 2, "celui": 1, "al": 1, "ce": 1, "accessibilitd": 1, "distanc": 1, "travaux": 1, "il": 1, "semblent": 1, "et": 2, "eu": 1, "sen": 1, "diver": 1, "le": 2, "la": 2, "associ": 1, "montrant": 1, "capter": 1, "densitd": 1, "dictionnair": 1, "dernier": 1, "preter": 1, "structur": 2, "effet": 1, "de": 5, "lexic": 1, "mot": 2, "compri": 1, "du": 2, "langu": 1, "mond": 1, "merveil": 1, "nombreux": 2, "pour": 2, "etc": 1, "gaum": 1, "entr": 1, "leur": 1, "lexical": 1, "dynamiqu": 1, "graph": 1, "pertin": 1, "ou": 1, "moyenn": 1, "se": 1}, "marker": "(Bieman, 2012", "article": "W14-6700", "vector_2": [2, 0.2616651418115279, 11, 4, 0, 1]}, {"label": "Neut", "current": "2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.", "context": ["En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.", "Les graphes sont essentiellement une forme de representation mathematique et visuelle des relations entre des objets/entites."], "vector_1": {"represent": 1, "en": 1, "modelis": 1, "al": 1, "accessibilitd": 1, "distanc": 1, "travaux": 1, "il": 1, "aspect": 2, "et": 3, "eu": 1, "phrase": 1, "diver": 1, "sen": 1, "le": 3, "graph": 2, "celui": 1, "visuel": 1, "capter": 1, "densitd": 1, "dictionnair": 1, "essentiel": 1, "relat": 1, "structur": 2, "form": 1, "effet": 1, "de": 7, "lexic": 1, "mot": 2, "sont": 1, "objetsentit": 1, "du": 2, "associ": 1, "mond": 1, "une": 1, "nombreux": 1, "montrant": 1, "pour": 2, "etc": 1, "gaum": 1, "entr": 2, "leur": 1, "lexical": 1, "dynamiqu": 1, "pertin": 1, "ou": 1, "moyenn": 1, "mathematiqu": 1}, "marker": "(Vitevitch, 2008)", "article": "W14-6700", "vector_2": [6, 0.25580969807868253, 9, 2, 0, 0]}, {"label": "Neut", "current": "2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.", "context": ["En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.", "2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.", "Les graphes sont essentiellement une forme de representation mathematique et visuelle des relations entre des objets/entites."], "vector_1": {"represent": 1, "en": 1, "modelis": 1, "al": 1, "accessibilitd": 1, "distanc": 1, "travaux": 1, "il": 1, "aspect": 2, "et": 3, "eu": 1, "phrase": 1, "diver": 1, "sen": 1, "le": 3, "graph": 2, "celui": 1, "visuel": 1, "capter": 1, "densitd": 1, "dictionnair": 1, "essentiel": 1, "relat": 1, "structur": 2, "form": 1, "effet": 1, "de": 7, "lexic": 1, "mot": 2, "sont": 1, "objetsentit": 1, "du": 2, "associ": 1, "mond": 1, "une": 1, "nombreux": 1, "montrant": 1, "pour": 2, "etc": 1, "gaum": 1, "entr": 2, "leur": 1, "lexical": 1, "dynamiqu": 1, "pertin": 1, "ou": 1, "moyenn": 1, "mathematiqu": 1}, "marker": "(Ferrer i Cancho & Sole, 2001)", "article": "W14-6700", "vector_2": [13, 0.25580969807868253, 9, 1, 0, 0]}, {"label": "Neut", "current": "Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.", "context": ["4 Conclusion", "Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.", "Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html)."], "vector_1": {"domain": 1, "lexical": 1, "nouvel": 1, "ce": 2, "vu": 3, "et": 3, "nombr": 1, "facil": 1, "ont": 1, "causer": 1, "le": 6, "jour": 1, "autr": 1, "semantiqu": 1, "la": 2, "avec": 1, "gnralis": 1, "cogalex": 1, "vectoriel": 1, "silenc": 1, "pa": 1, "soumiss": 1, "sur": 1, "francophon": 1, "distributionel": 1, "ressourc": 1, "normment": 1, "memoir": 1, "contrast": 1, "pu": 1, "travail": 1, "consacr": 1, "nest": 1, "de": 4, "succ": 1, "comm": 1, "voisin": 1, "savoir": 1, "vivier": 1, "qui": 1, "nou": 1, "vnement": 1, "du": 3, "problem": 1, "conclus": 1, "faibl": 1, "distributionnel": 1, "mond": 1, "surpri": 1, "dun": 1, "httppagepersolifunivmrsfrmichaelzockcogalexivcogalexwebpageindexhtml": 1, "car": 1, "od": 1, "tre": 2, "thori": 1, "method": 1, "etc": 1, "dynamism": 1, "sont": 1, "tion": 1, "se": 1, "il": 2}, "marker": "Vitevitch, 2008)", "article": "W14-6700", "vector_2": [6, 0.9209515096065873, 4, 2, 0, 0]}, {"label": "Neut", "current": "Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.", "context": ["4 Conclusion", "Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.", "Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html)."], "vector_1": {"domain": 1, "lexical": 1, "nouvel": 1, "ce": 2, "vu": 3, "et": 3, "nombr": 1, "facil": 1, "ont": 1, "causer": 1, "le": 6, "jour": 1, "autr": 1, "semantiqu": 1, "la": 2, "avec": 1, "gnralis": 1, "cogalex": 1, "vectoriel": 1, "silenc": 1, "pa": 1, "soumiss": 1, "sur": 1, "francophon": 1, "distributionel": 1, "ressourc": 1, "normment": 1, "memoir": 1, "contrast": 1, "pu": 1, "travail": 1, "consacr": 1, "nest": 1, "de": 4, "succ": 1, "comm": 1, "voisin": 1, "savoir": 1, "vivier": 1, "qui": 1, "nou": 1, "vnement": 1, "du": 3, "problem": 1, "conclus": 1, "faibl": 1, "distributionnel": 1, "mond": 1, "surpri": 1, "dun": 1, "httppagepersolifunivmrsfrmichaelzockcogalexivcogalexwebpageindexhtml": 1, "car": 1, "od": 1, "tre": 2, "thori": 1, "method": 1, "etc": 1, "dynamism": 1, "sont": 1, "tion": 1, "se": 1, "il": 2}, "marker": "(Baroni et Lenci, 2010)", "article": "W14-6700", "vector_2": [4, 0.9209515096065873, 4, 1, 0, 0]}, {"label": "Neut", "current": "This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.", "context": ["If a system has seen only positive examples, how does it recognize a negative example?", "This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.", "Outlier detection approaches typically derive some model of \"normal\" objects from the training set and use a distance measure and a threshold to detect abnormal items."], "vector_1": {"set": 1, "distanc": 1, "threshold": 1, "seen": 2, "differ": 1, "detect": 5, "unknown": 1, "normal": 1, "system": 1, "also": 1, "call": 1, "approach": 1, "model": 1, "recogn": 1, "deriv": 1, "outlier": 2, "object": 1, "use": 1, "train": 2, "address": 1, "data": 1, "typic": 1, "measur": 1, "novel": 1, "novelti": 1, "item": 2, "exampl": 2, "abnorm": 1, "neg": 1, "posit": 1, "problem": 1}, "marker": "Markou and Singh, 2003b", "article": "N06-1017", "vector_2": [3, 0.026821842845581718, 3, 3, 3, 0]}, {"label": "Neut", "current": "Figure 1 shows an example, a sentence from the Hound of the Baskervilles, analyzed by the SHALMANESER (Erk and Pado, 2006) shallow semantic parser.", "context": ["In cases where a sense is missing from the inventory, WSD will wrongly assign one of the existing senses.", "Figure 1 shows an example, a sentence from the Hound of the Baskervilles, analyzed by the SHALMANESER (Erk and Pado, 2006) shallow semantic parser.", "The analysis is based on FrameNet (Baker et al., 1998), a resource that lists senses and semantic roles for English expressions."], "vector_1": {"semant": 2, "show": 1, "parser": 1, "one": 1, "exist": 1, "hound": 1, "miss": 1, "inventori": 1, "figur": 1, "role": 1, "analyz": 1, "sens": 3, "baskervil": 1, "analysi": 1, "express": 1, "resourc": 1, "sentenc": 1, "shallow": 1, "base": 1, "wrongli": 1, "shalmanes": 1, "case": 1, "wsd": 1, "list": 1, "exampl": 1, "english": 1, "framenet": 1, "assign": 1}, "marker": "(Erk and Pado, 2006)", "article": "N06-1017", "vector_2": [0, 0.09996531658252726, 2, 1, 0, 0]}, {"label": "Pos", "current": "In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method (Tax and Duin, 2000) that compares the local probability density at each test item with that of its nearest training item.", "context": ["Our study will be evaluated on FrameNet because of our main aim of improving shallow semantic parsing, but the method we propose is applicable to any sense inventory that has annotated data; in particular, it is also applicable to WordNet.", "In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method (Tax and Duin, 2000) that compares the local probability density at each test item with that of its nearest training item.", "To our knowledge, there exists no other approach to date to the problem of detecting unknown senses."], "vector_1": {"nearest": 2, "semant": 1, "evalu": 1, "knowledg": 1, "paper": 1, "exist": 1, "wordnet": 1, "probabl": 1, "use": 1, "detect": 3, "inventori": 1, "compar": 1, "unknown": 2, "also": 1, "approach": 1, "densiti": 1, "test": 1, "sens": 3, "main": 1, "simpl": 1, "method": 2, "applic": 2, "neighborbas": 1, "outlier": 1, "shallow": 1, "problem": 1, "train": 1, "pars": 1, "particular": 1, "date": 1, "data": 1, "studi": 1, "local": 1, "annot": 1, "aim": 1, "item": 2, "framenet": 1, "improv": 1, "model": 1, "propos": 1}, "marker": "(Tax and Duin, 2000)", "article": "N06-1017", "vector_2": [6, 0.13580484797102008, 1, 2, 1, 0]}, {"label": "Neut", "current": "Frame Semantics (Fillmore, 1982) models the meanings of a word or expression by reference to frames which describe the background and situational knowledge necessary for understanding what the predicate is \"about\".", "context": ["2 FrameNet", "Frame Semantics (Fillmore, 1982) models the meanings of a word or expression by reference to frames which describe the background and situational knowledge necessary for understanding what the predicate is \"about\".", "Each frame provides its specific set of semantic roles."], "vector_1": {"semant": 2, "knowledg": 1, "word": 1, "describ": 1, "specif": 1, "provid": 1, "role": 1, "frame": 3, "express": 1, "situat": 1, "predic": 1, "set": 1, "necessari": 1, "background": 1, "framenet": 1, "understand": 1, "model": 1, "refer": 1, "mean": 1}, "marker": "(Fillmore, 1982)", "article": "N06-1017", "vector_2": [24, 0.19831207368299356, 1, 1, 0, 0]}, {"label": "Pos", "current": "They were parsed with Minipar (Lin, 1993); named entities were computed using Heart of Gold (Callmeier et al., 2004).", "context": ["After removal of instances that were annotated with more than one sense, we obtain 26,496 annotated sentences for the 1,031 ambiguous lemmas.", "They were parsed with Minipar (Lin, 1993); named entities were computed using Heart of Gold (Callmeier et al., 2004).", "4 Experiment 1: WSD confidence scores"], "vector_1": {"score": 1, "minipar": 1, "use": 1, "experi": 1, "comput": 1, "name": 1, "gold": 1, "heart": 1, "sentenc": 1, "remov": 1, "obtain": 1, "annot": 2, "wsd": 1, "lemma": 1, "instanc": 1, "pars": 1, "confid": 1, "ambigu": 1, "sens": 1, "one": 1, "entiti": 1}, "marker": "(Lin, 1993)", "article": "N06-1017", "vector_2": [13, 0.30124474931596595, 2, 1, 0, 0]}, {"label": "Neut", "current": "They were parsed with Minipar (Lin, 1993); named entities were computed using Heart of Gold (Callmeier et al., 2004).", "context": ["After removal of instances that were annotated with more than one sense, we obtain 26,496 annotated sentences for the 1,031 ambiguous lemmas.", "They were parsed with Minipar (Lin, 1993); named entities were computed using Heart of Gold (Callmeier et al., 2004).", "4 Experiment 1: WSD confidence scores"], "vector_1": {"score": 1, "minipar": 1, "use": 1, "experi": 1, "comput": 1, "name": 1, "gold": 1, "heart": 1, "sentenc": 1, "remov": 1, "obtain": 1, "annot": 2, "wsd": 1, "lemma": 1, "instanc": 1, "pars": 1, "confid": 1, "ambigu": 1, "sens": 1, "one": 1, "entiti": 1}, "marker": "(Callmeier et al., 2004)", "article": "N06-1017", "vector_2": [2, 0.30124474931596595, 2, 1, 0, 0]}, {"label": "Neut", "current": "Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of \"normality\" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).", "context": ["In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not.", "Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of \"normality\" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).", "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999)."], "vector_1": {"set": 3, "around": 1, "decid": 1, "detect": 5, "network": 1, "compar": 1, "boundari": 1, "normal": 1, "write": 1, "includ": 1, "deciph": 1, "new": 2, "approach": 1, "belong": 1, "applic": 1, "deriv": 2, "outlier": 3, "gener": 1, "object": 2, "given": 1, "hand": 1, "intrus": 1, "train": 2, "typic": 1, "task": 1, "whether": 1, "fault": 1, "model": 1}, "marker": "(Markou and Singh, 2003a", "article": "N06-1017", "vector_2": [3, 0.47932482947319743, 8, 3, 3, 0]}, {"label": "Neut", "current": "Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of \"normality\" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).", "context": ["In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not.", "Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of \"normality\" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).", "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999)."], "vector_1": {"set": 3, "around": 1, "decid": 1, "detect": 5, "network": 1, "compar": 1, "boundari": 1, "normal": 1, "write": 1, "includ": 1, "deciph": 1, "new": 2, "approach": 1, "belong": 1, "applic": 1, "deriv": 2, "outlier": 3, "gener": 1, "object": 2, "given": 1, "hand": 1, "intrus": 1, "train": 2, "typic": 1, "task": 1, "whether": 1, "fault": 1, "model": 1}, "marker": "Markou and Singh, 2003b", "article": "N06-1017", "vector_2": [3, 0.47932482947319743, 8, 3, 3, 0]}, {"label": "Neut", "current": "Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of \"normality\" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).", "context": ["In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not.", "Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of \"normality\" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).", "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999)."], "vector_1": {"set": 3, "around": 1, "decid": 1, "detect": 5, "network": 1, "compar": 1, "boundari": 1, "normal": 1, "write": 1, "includ": 1, "deciph": 1, "new": 2, "approach": 1, "belong": 1, "applic": 1, "deriv": 2, "outlier": 3, "gener": 1, "object": 2, "given": 1, "hand": 1, "intrus": 1, "train": 2, "typic": 1, "task": 1, "whether": 1, "fault": 1, "model": 1}, "marker": "Marsland, 2003)", "article": "N06-1017", "vector_2": [3, 0.47932482947319743, 8, 3, 0, 0]}, {"label": "Pos", "current": "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).", "context": ["Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of \"normality\" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).", "Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).", "One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set."], "vector_1": {"set": 4, "one": 1, "densiti": 1, "around": 1, "detect": 5, "network": 1, "compar": 1, "boundari": 1, "normal": 1, "classifi": 1, "write": 1, "estim": 1, "includ": 1, "deciph": 1, "test": 1, "new": 1, "probabl": 2, "approach": 2, "belong": 1, "applic": 1, "accord": 1, "deriv": 2, "outlier": 4, "object": 2, "hand": 1, "intrus": 1, "train": 2, "standard": 1, "model": 1, "fault": 1, "typic": 1, "nonoutli": 1}, "marker": "Dasgupta and Forrest, 1999)", "article": "N06-1017", "vector_2": [7, 0.4826390226983699, 8, 1, 0, 0]}, {"label": "Neut", "current": "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", "context": ["Once items have been identified as unknown, they are available for further processing: If possible one would like to assign some measure of sense information even to these items.", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", ""], "vector_1": {"even": 1, "measur": 1, "identifi": 1, "like": 1, "would": 1, "possibl": 2, "process": 1, "unknown": 1, "approxim": 1, "one": 1, "avail": 1, "item": 3, "inform": 1, "exist": 1, "includ": 1, "cluster": 1, "sens": 3, "similar": 1, "associ": 1, "assign": 1}, "marker": "Curran, 2005", "article": "N06-1017", "vector_2": [1, 0.9910593857181395, 3, 2, 0, 0]}, {"label": "Neut", "current": "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", "context": ["Once items have been identified as unknown, they are available for further processing: If possible one would like to assign some measure of sense information even to these items.", "Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.", ""], "vector_1": {"even": 1, "measur": 1, "identifi": 1, "like": 1, "would": 1, "possibl": 2, "process": 1, "unknown": 1, "approxim": 1, "one": 1, "avail": 1, "item": 3, "inform": 1, "exist": 1, "includ": 1, "cluster": 1, "sens": 3, "similar": 1, "associ": 1, "assign": 1}, "marker": "Burchardt et al., 2005)", "article": "N06-1017", "vector_2": [1, 0.9910593857181395, 3, 3, 0, 1]}, {"label": "Pos", "current": "For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).", "context": ["Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model.", "For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).", "Following the learning stage, each vector in the test data set is labeled with a predicted word and sense."], "vector_1": {"semant": 1, "set": 1, "predict": 2, "follow": 1, "label": 1, "previous": 1, "use": 2, "memori": 1, "next": 1, "process": 1, "vector": 1, "disambigu": 1, "timbl": 1, "test": 2, "sens": 3, "run": 1, "made": 1, "base": 1, "data": 1, "stage": 1, "task": 1, "word": 3, "algorithm": 1, "separ": 1, "exampl": 1, "learn": 4, "found": 1, "model": 1}, "marker": "(Daelemans et al., 2001)", "article": "P05-3014", "vector_2": [4, 0.7244623655913979, 3, 1, 4, 0]}, {"label": "Neut", "current": "and 65.2% on SENSEVAL-3 data (Decadt et al., 2004).", "context": ["binations of contextual (model*1/2) and collocational (model*Coll) models are also included.", "and 65.2% on SENSEVAL-3 data (Decadt et al., 2004).", "Note however that both these systems rely on significantly larger training data sets, and thus the results are not directly comparable."], "vector_1": {"result": 1, "colloc": 1, "set": 1, "directli": 1, "sensev": 1, "binat": 1, "howev": 1, "larger": 1, "system": 1, "contextu": 1, "note": 1, "also": 1, "reli": 1, "train": 1, "includ": 1, "compar": 1, "model": 2, "modelcol": 1, "data": 2, "thu": 1, "significantli": 1}, "marker": "(Decadt et al., 2004)", "article": "P05-3014", "vector_2": [1, 0.8723118279569892, 1, 3, 2, 0]}, {"label": "Neut", "current": "In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993).", "context": ["In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses.", "In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993).", "Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic."], "vector_1": {"represent": 2, "semant": 1, "consider": 1, "product": 1, "annot": 1, "unit": 1, "despit": 1, "ultim": 1, "prove": 1, "research": 1, "content": 1, "call": 1, "thompson": 1, "proposit": 1, "simpl": 1, "discours": 4, "relat": 3, "sentenc": 1, "claus": 1, "entir": 1, "lascarid": 1, "rst": 1, "marcu": 1, "well": 1, "correspond": 1, "structur": 1, "element": 1, "asher": 1, "amount": 1, "problemat": 1, "mann": 1, "sdrt": 1}, "marker": "(Kamp and Reyle, 1993)", "article": "W04-0208", "vector_2": [11, 0.16858930491739174, 1, 1, 0, 0]}, {"label": "Neut", "current": "Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective.", "context": ["Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima.", "Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective.", "Since then, there has been a large body of work addressing the flaws of the EM-based approach."], "vector_1": {"em": 2, "encount": 1, "via": 1, "obstacl": 1, "inappropri": 1, "ineffect": 1, "rewrit": 1, "two": 1, "stuck": 1, "bracket": 1, "larg": 1, "local": 1, "earli": 1, "flaw": 1, "get": 1, "induc": 1, "object": 1, "bodi": 1, "embas": 1, "address": 1, "approach": 1, "likelihood": 1, "sinc": 1, "addit": 1, "tendenc": 1, "grammar": 2, "constraint": 1, "seriou": 1, "unsupervis": 1, "work": 2, "rule": 1, "without": 1, "allow": 1, "learn": 1, "optima": 1}, "marker": "(Pereira and Shabes, 1992)", "article": "P08-1100", "vector_2": [16, 0.03462149643303918, 2, 1, 0, 0]}, {"label": "Neut", "current": "Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).", "context": ["Since then, there has been a large body of work addressing the flaws of the EM-based approach.", "Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).", "Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure."], "vector_1": {"procedur": 1, "empir": 1, "pcfg": 1, "develop": 1, "smith": 2, "learnabl": 1, "larg": 1, "new": 2, "approach": 1, "introduc": 1, "function": 1, "flaw": 1, "syntact": 1, "object": 1, "bodi": 1, "embas": 1, "train": 1, "address": 1, "sinc": 1, "work": 1, "eisner": 2, "model": 1, "propos": 1}, "marker": "Klein and Manning, 2004)", "article": "P08-1100", "vector_2": [4, 0.04385164147817561, 4, 5, 2, 1]}, {"label": "Neut", "current": "We present a metamodel of the changes that EM makes and show how this tool can shed some light on the undesired biases of the HMM, the PCFG, and the dependency model with valence (Klein and Manning, 2004).", "context": ["Our key idea for understanding this mis-match is to \"cheat\" and initialize EM with the true relationship and then study the ways in which EM repurposes our desired syntactic structures to increase likelihood.", "We present a metamodel of the changes that EM makes and show how this tool can shed some light on the undesired biases of the HMM, the PCFG, and the dependency model with valence (Klein and Manning, 2004).", "Identifiability error can be incurred when two distinct parameter settings yield the same probability distribution over sentences."], "vector_1": {"em": 3, "shed": 1, "set": 1, "identifi": 1, "show": 1, "cheat": 1, "idea": 1, "bias": 1, "repurpos": 1, "pcfg": 1, "paramet": 1, "depend": 1, "distinct": 1, "true": 1, "make": 1, "two": 1, "way": 1, "metamodel": 1, "distribut": 1, "probabl": 1, "model": 1, "desir": 1, "syntact": 1, "relationship": 1, "sentenc": 1, "tool": 1, "mismatch": 1, "yield": 1, "initi": 1, "hmm": 1, "understand": 1, "key": 1, "increas": 1, "likelihood": 1, "chang": 1, "present": 1, "undesir": 1, "light": 1, "structur": 1, "valenc": 1, "error": 1, "incur": 1, "studi": 1}, "marker": "(Klein and Manning, 2004)", "article": "P08-1100", "vector_2": [4, 0.08516752882307198, 1, 5, 2, 1]}, {"label": "Neut", "current": "See Klein and Manning (2004) for a formal description.", "context": ["At each point, we stop with a probability parametrized by the direction and whether any arguments have already been generated in that direction.", "See Klein and Manning (2004) for a formal description.", "In all our experiments, we used the Wall Street Journal (WSJ) portion of the Penn Treebank."], "vector_1": {"point": 1, "wall": 1, "treebank": 1, "direct": 2, "see": 1, "street": 1, "probabl": 1, "use": 1, "parametr": 1, "penn": 1, "experi": 1, "journal": 1, "klein": 1, "gener": 1, "stop": 1, "argument": 1, "formal": 1, "whether": 1, "wsj": 1, "descript": 1, "portion": 1, "alreadi": 1, "man": 1}, "marker": "(2004)", "article": "P08-1100", "vector_2": [4, 0.17165364979544917, 1, 5, 2, 1]}, {"label": "Pos", "current": "Grenager et al (2005) augments an HMM to allow emission from a generic stopword distribution at any position with probability q.", "context": ["While seemingly benign, this symmetry actually presents a serious challenge in measuring discrepancy (Section 5.1).", "Grenager et al (2005) augments an HMM to allow emission from a generic stopword distribution at any position with probability q.", "Their model would definitely not be identifiable if q were a free parameter, since we can set q to 0 and just mix in the stopword distribution with each of the other emission distributions to obtain a different parameter setting yielding the same overall distribution."], "vector_1": {"set": 2, "identifi": 1, "challeng": 1, "emiss": 2, "al": 1, "symmetri": 1, "et": 1, "probabl": 1, "q": 3, "differ": 1, "would": 1, "mix": 1, "section": 1, "definit": 1, "benign": 1, "present": 1, "discrep": 1, "paramet": 2, "distribut": 4, "gener": 1, "overal": 1, "free": 1, "obtain": 1, "stopword": 2, "hmm": 1, "sinc": 1, "seemingli": 1, "measur": 1, "actual": 1, "augment": 1, "seriou": 1, "yield": 1, "grenag": 1, "allow": 1, "posit": 1, "model": 1}, "marker": "(2005)", "article": "P08-1100", "vector_2": [3, 0.6633194712107381, 1, 1, 6, 1]}, {"label": "CoCo", "current": "Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).", "context": ["There is also a rich body of theoretical work on learning latent-variable models.", "Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).", "But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV."], "vector_1": {"em": 1, "certain": 1, "except": 1, "littl": 1, "famili": 1, "complex": 1, "special": 1, "provabl": 1, "capac": 2, "theoret": 2, "also": 1, "discret": 1, "pcfg": 1, "rich": 1, "dmv": 1, "dasgupta": 1, "gener": 2, "weak": 1, "bodi": 1, "hmm": 1, "understand": 1, "strong": 1, "constrain": 1, "schulman": 1, "latentvari": 1, "term": 2, "algorithm": 1, "work": 1, "alon": 1, "hiddenvari": 1, "learn": 2, "let": 1, "model": 3, "other": 1}, "marker": "(Ron et al., 1998", "article": "P08-1100", "vector_2": [10, 0.9644994421340907, 6, 1, 0, 0]}, {"label": "Neut", "current": "Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).", "context": ["There is also a rich body of theoretical work on learning latent-variable models.", "Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).", "But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV."], "vector_1": {"em": 1, "certain": 1, "except": 1, "littl": 1, "famili": 1, "complex": 1, "special": 1, "provabl": 1, "capac": 2, "theoret": 2, "also": 1, "discret": 1, "pcfg": 1, "rich": 1, "dmv": 1, "dasgupta": 1, "gener": 2, "weak": 1, "bodi": 1, "hmm": 1, "understand": 1, "strong": 1, "constrain": 1, "schulman": 1, "latentvari": 1, "term": 2, "algorithm": 1, "work": 1, "alon": 1, "hiddenvari": 1, "learn": 2, "let": 1, "model": 3, "other": 1}, "marker": "(Dasgupta, 1999", "article": "P08-1100", "vector_2": [9, 0.9644994421340907, 6, 1, 1, 0]}, {"label": "Neut", "current": "But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.", "context": ["Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).", "But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.", "8 Conclusion"], "vector_1": {"em": 1, "certain": 1, "except": 1, "littl": 1, "famili": 1, "complex": 1, "special": 1, "provabl": 1, "capac": 2, "theoret": 1, "other": 1, "pcfg": 1, "model": 2, "dmv": 1, "dasgupta": 1, "gener": 2, "weak": 1, "hmm": 1, "understand": 1, "strong": 1, "discret": 1, "conclus": 1, "schulman": 1, "term": 2, "algorithm": 1, "alon": 1, "hiddenvari": 1, "learn": 1, "let": 1, "constrain": 1}, "marker": "(2007)", "article": "P08-1100", "vector_2": [1, 0.9667647158264868, 6, 1, 1, 0]}, {"label": "Neut", "current": "The volume of information available to users on the World Wide Web is growing at an exponential rate (Lyman and Varian, 2003).", "context": ["1 Introduction", "The volume of information available to users on the World Wide Web is growing at an exponential rate (Lyman and Varian, 2003).", "Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006)."], "vector_1": {"synonymi": 1, "natur": 1, "ir": 1, "one": 1, "rate": 1, "suffer": 1, "sever": 1, "web": 1, "differ": 2, "accur": 1, "multipl": 1, "appear": 1, "exponenti": 1, "system": 1, "avail": 1, "larg": 1, "languag": 1, "notabl": 1, "polysemi": 1, "inabl": 1, "word": 3, "volum": 1, "user": 1, "govern": 1, "world": 1, "grow": 1, "introduct": 1, "wide": 1, "retriev": 1, "current": 1, "ambigu": 1, "keywordmatch": 1, "inform": 2, "limit": 1, "context": 1, "model": 1, "mean": 2}, "marker": "(Lyman and Varian, 2003)", "article": "W10-3506", "vector_2": [7, 0.02828656671324446, 2, 1, 0, 0]}, {"label": "Neut", "current": "Such systems allow for sophisticated semantic search, however, require the use of a more difficult-to-understand querysyntax (Tran et al., 2008).", "context": ["In recent years, much research attention has therefore been given to semantic techniques of information retrieval.", "Such systems allow for sophisticated semantic search, however, require the use of a more difficult-to-understand querysyntax (Tran et al., 2008).", "Furthermore, these methods require specially encoded (and thus costly) ontologies to describe the particular domain knowledge in which the system operates, and the specific interrelations of concepts within that domain."], "vector_1": {"oper": 1, "semant": 2, "concept": 1, "ontolog": 1, "knowledg": 1, "encod": 1, "domain": 2, "costli": 1, "year": 1, "difficulttounderstand": 1, "special": 1, "given": 1, "specif": 1, "techniqu": 1, "system": 2, "querysyntax": 1, "research": 1, "much": 1, "therefor": 1, "method": 1, "within": 1, "use": 1, "sophist": 1, "interrel": 1, "attent": 1, "particular": 1, "requir": 2, "recent": 1, "search": 1, "retriev": 1, "describ": 1, "furthermor": 1, "howev": 1, "inform": 1, "allow": 1, "thu": 1}, "marker": "(Tran et al., 2008)", "article": "W10-3506", "vector_2": [2, 0.046225310623512976, 1, 1, 0, 0]}, {"label": "Neut", "current": "Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.", "context": ["Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).", "Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.", "WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them."], "vector_1": {"wikipedia": 4, "semant": 3, "concept": 1, "comput": 2, "creat": 1, "appli": 1, "ir": 1, "outlink": 1, "repres": 1, "cost": 1, "exist": 1, "overlap": 1, "cognit": 1, "network": 1, "compar": 1, "memori": 1, "much": 1, "activ": 1, "due": 1, "coverag": 2, "articl": 3, "linkbas": 1, "spread": 1, "treat": 2, "esa": 1, "text": 2, "hurdl": 1, "method": 2, "analysi": 1, "knowledg": 1, "variou": 1, "regard": 1, "wordnetbas": 1, "foremost": 1, "knowledgebas": 1, "biggest": 1, "base": 3, "link": 2, "theori": 1, "although": 1, "arguabl": 1, "associ": 1, "model": 1, "earlier": 1, "recent": 1, "measur": 1, "success": 1, "level": 1, "outperform": 1, "larger": 1, "explicit": 1, "equal": 1, "conceptu": 2, "adequ": 1, "wlm": 2, "found": 1, "sa": 1, "similar": 1, "anchor": 1}, "marker": "(Witten and Milne, 2008)", "article": "W10-3506", "vector_2": [2, 0.11926432267079573, 6, 7, 1, 0]}, {"label": "Weak", "current": "This pure model of SA has several significant problems, the most notable being that activation can saturate the entire network unless certain constraints are imposed, namely limiting how far activation can spread from the initially activated nodes (distance constraint), and limiting the effect of very highly-connected nodes (fanout constraint) (Crestani, 1997).", "context": ["(1) vjN(vi)", "This pure model of SA has several significant problems, the most notable being that activation can saturate the entire network unless certain constraints are imposed, namely limiting how far activation can spread from the initially activated nodes (distance constraint), and limiting the effect of very highly-connected nodes (fanout constraint) (Crestani, 1997).", "In the following three sections we discuss how these constraints were implemented in our model for SA."], "vector_1": {"distanc": 1, "certain": 1, "signific": 1, "fanout": 1, "implement": 1, "follow": 1, "sever": 1, "sa": 2, "network": 1, "section": 1, "activ": 3, "three": 1, "discuss": 1, "spread": 1, "pure": 1, "notabl": 1, "node": 2, "model": 2, "unless": 1, "far": 1, "effect": 1, "initi": 1, "vjnvi": 1, "entir": 1, "satur": 1, "highlyconnect": 1, "impos": 1, "name": 1, "constraint": 4, "limit": 2, "problem": 1}, "marker": "(Crestani, 1997)", "article": "W10-3506", "vector_2": [13, 0.3127761622417765, 1, 3, 0, 0]}, {"label": "Neut", "current": "After spreading has terminated, relatedness is computed as the amount of overlap between the individual nodes' activation vectors, using either the cosine similarity (AA-cos), or an adapted version of the information theory based WLM (Witten and Milne, 2008) measure.", "context": ["The second approach is called the Agglomerative Approach since we agglomerate all activations into one score resembling relatedness.", "After spreading has terminated, relatedness is computed as the amount of overlap between the individual nodes' activation vectors, using either the cosine similarity (AA-cos), or an adapted version of the information theory based WLM (Witten and Milne, 2008) measure.", "Assume the same set of initial nodes vi and vj."], "vector_1": {"set": 1, "comput": 1, "version": 1, "vi": 1, "individu": 1, "related": 2, "one": 1, "second": 1, "aaco": 1, "assum": 1, "use": 1, "resembl": 1, "activ": 2, "overlap": 1, "spread": 1, "score": 1, "call": 1, "adapt": 1, "termin": 1, "approach": 2, "vj": 1, "either": 1, "node": 2, "wlm": 1, "initi": 1, "agglom": 1, "theori": 1, "base": 1, "sinc": 1, "measur": 1, "inform": 1, "amount": 1, "vector": 1, "agglomer": 1, "cosin": 1, "similar": 1}, "marker": "(Witten and Milne, 2008)", "article": "W10-3506", "vector_2": [2, 0.47769930888628725, 1, 7, 1, 0]}, {"label": "Neut", "current": "The results obtained on the Lee (2005) document similarity dataset using the three document similarity metrics (MAXSIM, WIKISPREAD, and COMBINED) are summarised in Table 4.", "context": ["Results", "The results obtained on the Lee (2005) document similarity dataset using the three document similarity metrics (MAXSIM, WIKISPREAD, and COMBINED) are summarised in Table 4.", "Of the two Wikipedia-only methods, the MaxSim method achieves the best correlation score of p = 0.68."], "vector_1": {"metric": 1, "obtain": 1, "dataset": 1, "correl": 1, "result": 2, "tabl": 1, "best": 1, "use": 1, "three": 1, "score": 1, "document": 2, "method": 2, "lee": 1, "two": 1, "wikipediaonli": 1, "maxsim": 2, "wikispread": 1, "p": 1, "achiev": 1, "summaris": 1, "combin": 1, "similar": 2}, "marker": "(2005)", "article": "W10-3506", "vector_2": [5, 0.9252615280033234, 1, 4, 0, 0]}, {"label": "Neut", "current": "When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997).", "context": ["In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.", "When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997).", "However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available."], "vector_1": {"corpu": 3, "directli": 1, "satisfactori": 1, "exist": 1, "result": 1, "largescal": 3, "use": 1, "research": 1, "avail": 2, "domainspecif": 3, "neither": 1, "method": 1, "train": 1, "translat": 1, "dictionari": 2, "address": 1, "problem": 1, "requir": 1, "word": 2, "howev": 1, "order": 1, "achiev": 1, "bilingu": 3, "improv": 1, "studi": 1, "align": 2}, "marker": "(Ker and Chang, 1997)", "article": "P05-1058", "vector_2": [8, 0.0476264289866305, 2, 1, 0, 0]}, {"label": "Neut", "current": "In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999), which is shown in Equation (2).", "context": ["f e , ) = | a '  |e)  p(a' ,  |) f e (1) , a ( f p ( a p", "In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999), which is shown in Equation (2).", "This simplified version does not take word classes into account as described in (Brown et al., 1993)."], "vector_1": {"simplifi": 2, "use": 1, "e": 3, "ibm": 1, "f": 3, "pa": 1, "shown": 1, "p": 2, "describ": 1, "paper": 1, "equat": 1, "version": 1, "account": 1, "word": 1, "model": 1, "class": 1, "take": 1}, "marker": "(Al-Onaizan et al., 1999)", "article": "P05-1058", "vector_2": [6, 0.21832978105018408, 2, 1, 3, 0]}, {"label": "Neut", "current": "In order to deal with this problem, we perform word alignment in two directions (source to target, and target to source) as described in (Och and Ney, 2000).", "context": ["Thus, some multi-word units in the domain-specific corpus cannot be correctly aligned.", "In order to deal with this problem, we perform word alignment in two directions (source to target, and target to source) as described in (Och and Ney, 2000).", "The GIZA++ toolkit2 is used to perform statistical word alignment."], "vector_1": {"corpu": 1, "deal": 1, "direct": 1, "unit": 1, "use": 1, "describ": 1, "correctli": 1, "perform": 2, "two": 1, "multiword": 1, "domainspecif": 1, "sourc": 2, "toolkit": 1, "cannot": 1, "align": 3, "word": 2, "target": 2, "thu": 1, "giza": 1, "statist": 1, "problem": 1, "order": 1}, "marker": "(Och and Ney, 2000)", "article": "P05-1058", "vector_2": [5, 0.2473551637279597, 1, 3, 1, 0]}, {"label": "Pos", "current": "In order to filter the noise caused by the error alignment links, we only retain those translation pairs whose log-likelihood ratio scores (Dunning, 1993) are above a threshold.", "context": ["Based on the extended alignment links, we build a translation dictionary.", "In order to filter the noise caused by the error alignment links, we only retain those translation pairs whose log-likelihood ratio scores (Dunning, 1993) are above a threshold.", "Based on the alignment results on the d one-to-many alignments."], "vector_1": {"result": 1, "caus": 1, "ratio": 1, "nois": 1, "extend": 1, "whose": 1, "align": 4, "onetomani": 1, "loglikelihood": 1, "filter": 1, "threshold": 1, "base": 2, "link": 2, "build": 1, "dictionari": 1, "error": 1, "pair": 1, "retain": 1, "score": 1, "order": 1, "translat": 2}, "marker": "(Dunning, 1993)", "article": "P05-1058", "vector_2": [12, 0.47638054640573535, 1, 1, 0, 0]}, {"label": "Neut", "current": "In order to further compare our method with the method described in (Wu and Wang, 2004).", "context": ["Results", "In order to further compare our method with the method described in (Wu and Wang, 2004).", "We do another experiment using almost the same-scale in-domain training corpus as described in (Wu and Wang, 2004)."], "vector_1": {"corpu": 1, "use": 1, "samescal": 1, "describ": 2, "compar": 1, "almost": 1, "anoth": 1, "order": 1, "train": 1, "result": 1, "indomain": 1, "experi": 1, "method": 2}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.7599689982561519, 2, 13, 0, 1]}, {"label": "Neut", "current": "while the method \"ResAdapt\" described in (Wu and Wang, 2004) only achieves an error rate reduction of 8.59%.", "context": ["472", "while the method \"ResAdapt\" described in (Wu and Wang, 2004) only achieves an error rate reduction of 8.59%.", "Compared with the method \"ResAdapt\", our method achieves an error rate reduction of 10.15%."], "vector_1": {"describ": 1, "compar": 1, "achiev": 2, "reduct": 2, "rate": 2, "resadapt": 2, "error": 2, "method": 3}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.7876380546405736, 1, 13, 0, 1]}, {"label": "Neut", "current": "The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004).", "context": ["This result is different from that in (Wu and Wang, 2004), where their method achieved an error rate reduction of 21.96% as compared with the method \"Gen+Spec\".", "The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004).", "The training data and the testing data described in (Wu and Wang, 2004) are from a single manual."], "vector_1": {"corpu": 2, "differ": 2, "describ": 1, "compar": 1, "genspec": 1, "indomain": 1, "manual": 1, "achiev": 1, "reduct": 1, "paper": 1, "test": 2, "rate": 1, "train": 2, "result": 1, "error": 1, "reason": 1, "main": 1, "data": 2, "method": 2, "singl": 1}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.794884712265065, 3, 13, 0, 1]}, {"label": "Neut", "current": "In addition to the above evaluations, we also evaluate our model adaptation method using the \"refined\" combination in Och and Ney (2000) instead of the translation dictionary.", "context": ["The data in our corpus are from several manuals describing how to use the diagnostic ultrasound systems.", "In addition to the above evaluations, we also evaluate our model adaptation method using the \"refined\" combination in Och and Ney (2000) instead of the translation dictionary.", "Using the \"refined\" method to select the alignments produced by our model adaptation method (AER: 0.2371) still yields better result than directly combining out-of-domain and in-domain corpora as training data of the \"refined\" method (AER: 0.2290)."], "vector_1": {"corpu": 1, "evalu": 2, "ultrasound": 1, "select": 1, "result": 1, "still": 1, "sever": 1, "och": 1, "use": 3, "describ": 1, "system": 1, "better": 1, "also": 1, "adapt": 2, "ney": 1, "instead": 1, "method": 4, "outofdomain": 1, "directli": 1, "train": 1, "translat": 1, "dictionari": 1, "refin": 3, "data": 2, "addit": 1, "aer": 2, "diagnost": 1, "indomain": 1, "align": 1, "corpora": 1, "manual": 1, "yield": 1, "combin": 2, "model": 2, "produc": 1}, "marker": "(2000)", "article": "P05-1058", "vector_2": [5, 0.8084479751986049, 1, 3, 1, 0]}, {"label": "CoCo", "current": "In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004), our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004).", "context": ["It also achieves a relative error rate reduction of 6.56% as compared with the previous work in (Wu and Wang, 2004).", "In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004), our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004).", "We also use in-domain corpora and out-of-domain corpora of different sizes to perform adaptation experiments."], "vector_1": {"corpu": 1, "corpora": 2, "rate": 2, "reduct": 2, "size": 1, "use": 1, "describ": 1, "compar": 2, "perform": 1, "outofdomain": 1, "also": 2, "adapt": 1, "rel": 1, "experi": 1, "method": 2, "previou": 1, "differ": 1, "train": 1, "smallerscal": 1, "addit": 1, "indomain": 2, "work": 1, "achiev": 2, "error": 2, "model": 1}, "marker": "(Wu and Wang, 2004)", "article": "P05-1058", "vector_2": [1, 0.9774462313505135, 3, 13, 0, 1]}, {"label": "CoCo", "current": "Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space.", "context": ["Zadrozny (1995) proposes Q-Complexity, which roughly corresponds to vocabulary size, and is essentially the number of questions that can be asked about a database.", "Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space.", "Gorin et al (2000) distinguish between semantic and linguistic complexity of calls to a spoken DS."], "vector_1": {"semant": 2, "vocabulari": 1, "spoken": 1, "number": 2, "ask": 1, "qcomplex": 1, "zadrozni": 1, "et": 1, "size": 1, "bierman": 1, "describ": 1, "space": 1, "databas": 1, "question": 1, "complex": 1, "call": 1, "gorin": 1, "attribut": 1, "everi": 1, "relationship": 1, "object": 1, "al": 1, "consid": 1, "distinguish": 2, "bit": 1, "ds": 1, "requir": 1, "measur": 1, "essenti": 1, "correspond": 1, "pollard": 1, "linguist": 1, "similar": 1, "roughli": 1, "propos": 1}, "marker": "(2000)", "article": "W12-1635", "vector_2": [12, 0.28116086708499716, 3, 1, 0, 0]}, {"label": "Neut", "current": "To avoid addressing information presentation issues such as those explored in (Polifroni and Walker, 2008), CheckItOut followed a simple strategy of offering each next candidate book in a query return, and user studies with CheckItOut restricted query return size to a maximum of three books.", "context": ["When the system cannot uniquely identify a requested book, it begins a disambiguation subdialogue, an example of which is shown in Figure 1.", "To avoid addressing information presentation issues such as those explored in (Polifroni and Walker, 2008), CheckItOut followed a simple strategy of offering each next candidate book in a query return, and user studies with CheckItOut restricted query return size to a maximum of three books.", "For the simulations, we expect an inverse relationship between specificity and dialogue length."], "vector_1": {"identifi": 1, "maximum": 1, "queri": 2, "system": 1, "restrict": 1, "expect": 1, "explor": 1, "follow": 1, "simul": 1, "checkitout": 2, "shown": 1, "avoid": 1, "subdialogu": 1, "next": 1, "figur": 1, "book": 3, "disambigu": 1, "invers": 1, "size": 1, "simpl": 1, "strategi": 1, "begin": 1, "return": 2, "relationship": 1, "offer": 1, "dialogu": 1, "cannot": 1, "user": 1, "address": 1, "present": 1, "specif": 1, "request": 1, "uniqu": 1, "candid": 1, "issu": 1, "inform": 1, "length": 1, "exampl": 1, "three": 1, "studi": 1}, "marker": "(Polifroni and Walker, 2008)", "article": "W12-1635", "vector_2": [4, 0.7485738733599544, 1, 2, 0, 0]}, {"label": "Neut", "current": "The system is based to a large extent on Burchardt and Frank's system (2006) used in the second RTE challenge (Bar-Haim et al., 2006); it relies on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap.", "context": ["This paper reports on the system we used in the third PASCAL challenge on Recognizing Textual Entailment.", "The system is based to a large extent on Burchardt and Frank's system (2006) used in the second RTE challenge (Bar-Haim et al., 2006); it relies on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap.", "As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements."], "vector_1": {"rte": 1, "entail": 1, "challeng": 2, "deep": 1, "pascal": 1, "paper": 1, "overlap": 1, "systemat": 1, "concentr": 1, "use": 2, "describ": 1, "system": 5, "textual": 1, "compon": 1, "reli": 1, "larg": 1, "futur": 1, "rel": 1, "complement": 1, "analysi": 1, "earlier": 1, "recogn": 1, "frank": 1, "burchardt": 1, "shallow": 1, "spot": 1, "behaviour": 1, "extens": 1, "base": 2, "promis": 1, "extent": 1, "report": 1, "discuss": 1, "word": 1, "third": 1, "second": 1, "aim": 1, "improv": 1, "linguist": 1, "anchor": 1}, "marker": "(2006)", "article": "W07-1402", "vector_2": [1, 0.04272160192902448, 2, 2, 1, 0]}, {"label": "Neut", "current": "The system is based to a large extent on Burchardt and Frank's system (2006) used in the second RTE challenge (Bar-Haim et al., 2006); it relies on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap.", "context": ["This paper reports on the system we used in the third PASCAL challenge on Recognizing Textual Entailment.", "The system is based to a large extent on Burchardt and Frank's system (2006) used in the second RTE challenge (Bar-Haim et al., 2006); it relies on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap.", "As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements."], "vector_1": {"rte": 1, "entail": 1, "challeng": 2, "deep": 1, "pascal": 1, "paper": 1, "overlap": 1, "systemat": 1, "concentr": 1, "use": 2, "describ": 1, "system": 5, "textual": 1, "compon": 1, "reli": 1, "larg": 1, "futur": 1, "rel": 1, "complement": 1, "analysi": 1, "earlier": 1, "recogn": 1, "frank": 1, "burchardt": 1, "shallow": 1, "spot": 1, "behaviour": 1, "extens": 1, "base": 2, "promis": 1, "extent": 1, "report": 1, "discuss": 1, "word": 1, "third": 1, "second": 1, "aim": 1, "improv": 1, "linguist": 1, "anchor": 1}, "marker": "(Bar-Haim et al., 2006)", "article": "W07-1402", "vector_2": [1, 0.04272160192902448, 2, 1, 0, 0]}, {"label": "Neut", "current": "It has been observed for related systems that a combination of separately trained features in the machine learning component can lead to an overall improvement in system performance, in particular if features from a more \"informed\" component and shallow ones are combined (Hickl et al., 2006; Bos and Markert, 2006).", "context": ["As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements.", "It has been observed for related systems that a combination of separately trained features in the machine learning component can lead to an overall improvement in system performance, in particular if features from a more \"informed\" component and shallow ones are combined (Hickl et al., 2006; Bos and Markert, 2006).", "We provide a detailed analysis of our system's behaviour on different training and test sets."], "vector_1": {"analysi": 1, "featur": 2, "one": 1, "set": 1, "systemat": 1, "concentr": 1, "separ": 1, "overal": 1, "describ": 1, "lead": 1, "perform": 1, "shallow": 1, "system": 5, "compon": 2, "futur": 1, "test": 1, "machin": 1, "earlier": 1, "detail": 1, "relat": 1, "spot": 1, "differ": 1, "behaviour": 2, "extens": 1, "train": 2, "promis": 1, "particular": 1, "discuss": 1, "provid": 1, "observ": 1, "aim": 1, "inform": 1, "combin": 2, "learn": 1, "improv": 2, "anchor": 1}, "marker": "Bos and Markert, 2006)", "article": "W07-1402", "vector_2": [1, 0.06106830214394297, 2, 1, 0, 0]}, {"label": "Neut", "current": "More details can be found in (Burchardt and Frank, 2006).", "context": ["In this Section, we review the basic architecture of the SALSA RTE system, and report on some improvements and extensions.", "More details can be found in (Burchardt and Frank, 2006).", "2.1 Architecture"], "vector_1": {"rte": 1, "found": 1, "section": 1, "system": 1, "detail": 1, "extens": 1, "basic": 1, "report": 1, "salsa": 1, "improv": 1, "review": 1, "architectur": 2}, "marker": "(Burchardt and Frank, 2006)", "article": "W07-1402", "vector_2": [1, 0.14016878964197724, 1, 2, 1, 0]}, {"label": "Neut", "current": "The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).", "context": ["Linguistic analysis.", "The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).", "Frame semantic analysis is especially interesting for the task of recognising textual entailment as it offers a robust yet relatively precise measure for semantic overlap."], "vector_1": {"lfg": 1, "entail": 1, "frame": 4, "semant": 3, "shalmanes": 1, "overlap": 1, "primari": 1, "rel": 1, "develop": 1, "offer": 1, "system": 3, "textual": 1, "yet": 1, "compon": 1, "role": 1, "interest": 1, "analysi": 3, "parc": 1, "recognis": 1, "especi": 1, "detour": 1, "probabilist": 2, "robust": 1, "measur": 1, "task": 1, "grammar": 1, "rulebas": 1, "annot": 2, "precis": 1, "combin": 1, "english": 1, "linguist": 2, "assign": 1}, "marker": "(Erk and Pado, 2006)", "article": "W07-1402", "vector_2": [1, 0.18257587671017456, 3, 1, 1, 0]}, {"label": "Neut", "current": "Semantic nodes are further projected into an ontological analysis layer containing WordNet (Fellbaum, 1997) senses and SUMO (Niles and Pease, 2001) classes.", "context": ["The predicate discover is associated with the frame ACxIEVING_FIRST, the semantic role COGNIZER points to the pseudo-predicate Leloir and NEW_IDEA points to the PROCESS frame evoked by metabolism.", "Semantic nodes are further projected into an ontological analysis layer containing WordNet (Fellbaum, 1997) senses and SUMO (Niles and Pease, 2001) classes.", "Semantic phenomena not treated by FrameNet like anaphora, negation or modality are (approximately) encoded with special operators."], "vector_1": {"oper": 1, "analysi": 1, "semant": 3, "newidea": 1, "point": 2, "process": 1, "ontolog": 1, "frame": 2, "phenomena": 1, "layer": 1, "cogniz": 1, "wordnet": 1, "metabol": 1, "pseudopred": 1, "role": 1, "treat": 1, "leloir": 1, "approxim": 1, "node": 1, "anaphora": 1, "evok": 1, "encod": 1, "discov": 1, "sumo": 1, "sens": 1, "acxievingfirst": 1, "associ": 1, "class": 1, "special": 1, "like": 1, "negat": 1, "predic": 1, "project": 1, "modal": 1, "contain": 1, "framenet": 1}, "marker": "(Fellbaum, 1997)", "article": "W07-1402", "vector_2": [10, 0.24888609320123709, 2, 1, 0, 0]}, {"label": "Neut", "current": "Semantic nodes are further projected into an ontological analysis layer containing WordNet (Fellbaum, 1997) senses and SUMO (Niles and Pease, 2001) classes.", "context": ["The predicate discover is associated with the frame ACxIEVING_FIRST, the semantic role COGNIZER points to the pseudo-predicate Leloir and NEW_IDEA points to the PROCESS frame evoked by metabolism.", "Semantic nodes are further projected into an ontological analysis layer containing WordNet (Fellbaum, 1997) senses and SUMO (Niles and Pease, 2001) classes.", "Semantic phenomena not treated by FrameNet like anaphora, negation or modality are (approximately) encoded with special operators."], "vector_1": {"oper": 1, "analysi": 1, "semant": 3, "newidea": 1, "point": 2, "process": 1, "ontolog": 1, "frame": 2, "phenomena": 1, "layer": 1, "cogniz": 1, "wordnet": 1, "metabol": 1, "pseudopred": 1, "role": 1, "treat": 1, "leloir": 1, "approxim": 1, "node": 1, "anaphora": 1, "evok": 1, "encod": 1, "discov": 1, "sumo": 1, "sens": 1, "acxievingfirst": 1, "associ": 1, "class": 1, "special": 1, "like": 1, "negat": 1, "predic": 1, "project": 1, "modal": 1, "contain": 1, "framenet": 1}, "marker": "(Niles and Pease, 2001)", "article": "W07-1402", "vector_2": [6, 0.24888609320123709, 2, 1, 0, 0]}, {"label": "Pos", "current": "To cope with longer texts, we integrated the sentence splitter of the JTok tokeniser (Schafer, 2005) into the system.", "context": ["Sentence splitter.", "To cope with longer texts, we integrated the sentence splitter of the JTok tokeniser (Schafer, 2005) into the system.", "WordNet interface."], "vector_1": {"splitter": 2, "longer": 1, "cope": 1, "sentenc": 2, "system": 1, "tokenis": 1, "interfac": 1, "integr": 1, "text": 1, "jtok": 1, "wordnet": 1}, "marker": "(Schafer, 2005)", "article": "W07-1402", "vector_2": [2, 0.3519421292656078, 1, 1, 0, 0]}, {"label": "Neut", "current": "Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b).", "context": ["An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al (2009).", "Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b).", "Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task."], "vector_1": {"reasonablywel": 1, "evalu": 1, "predict": 1, "challeng": 1, "al": 1, "msr": 2, "belz": 1, "result": 1, "year": 1, "et": 1, "subject": 1, "develop": 1, "compar": 1, "perform": 2, "neg": 1, "system": 3, "accuraci": 1, "score": 1, "compet": 1, "submiss": 1, "main": 1, "refer": 1, "string": 1, "especi": 1, "gener": 1, "previou": 1, "extens": 1, "although": 1, "along": 1, "regtyp": 1, "task": 3, "provid": 1, "descript": 1, "grec": 1, "indepth": 1, "disappointinglylow": 1}, "marker": "(Greenbacker and McCoy, 2009a)", "article": "W10-4231", "vector_2": [1, 0.16411181244364292, 3, 1, 1, 0]}, {"label": "CoCo", "current": "We remain several points below the bestperforming team from 2009 (ICSI-Berkeley), possibly due to the inclusion of additional items in their feature set, or the use of Conditional Random Fields as their learning technique (Favre and Bohnet, 2009).", "context": ["Our efforts during this iteration of the NEG task were primarily focused on enhancing our methods of choosing the best RE once the reference type was selected.", "We remain several points below the bestperforming team from 2009 (ICSI-Berkeley), possibly due to the inclusion of additional items in their feature set, or the use of Conditional Random Fields as their learning technique (Favre and Bohnet, 2009).", "5 Future Work"], "vector_1": {"featur": 1, "primarili": 1, "point": 1, "icsiberkeley": 1, "focus": 1, "bestperform": 1, "set": 1, "learn": 1, "select": 1, "inclus": 1, "techniqu": 1, "neg": 1, "due": 1, "best": 1, "field": 1, "sever": 1, "condit": 1, "type": 1, "method": 1, "refer": 1, "random": 1, "choos": 1, "use": 1, "enhanc": 1, "effort": 1, "futur": 1, "addit": 1, "task": 1, "possibl": 1, "work": 1, "iter": 1, "item": 1, "remain": 1, "team": 1}, "marker": "(Favre and Bohnet, 2009)", "article": "W10-4231", "vector_2": [1, 0.860595130748422, 1, 2, 0, 0]}, {"label": "Neut", "current": "Certains travaux ont rendu le LVF plus accessible en termes d'encodage et de format de donn&es : Denis Le Pesant en a cr&& une version sous format Excel pour faciliter sa consultation manuelle, mais ce mode d'acc&s ne s'est pas av&r& pratique pour les applications informatiques ; Guy Lapalme en a alors propos& une version XML qui en facilite l'exploitation par les applications de traitement automatique de la langue et Hadouche et Lapalme (2010) l'ont compar&  d'autres ressources lexicales.", "context": ["E cause de problemes de diffusion et de distribution, le LVF n'a malheureusement pas pu etre exploit& par les chercheurs et les linguistes qui, pour plusieurs, en ignoraient meme l'existence.", "Certains travaux ont rendu le LVF plus accessible en termes d'encodage et de format de donn&es : Denis Le Pesant en a cr&& une version sous format Excel pour faciliter sa consultation manuelle, mais ce mode d'acc&s ne s'est pas av&r& pratique pour les applications informatiques ; Guy Lapalme en a alors propos& une version XML qui en facilite l'exploitation par les applications de traitement automatique de la langue et Hadouche et Lapalme (2010) l'ont compar&  d'autres ressources lexicales.", "Ces derni&res ann&es, il y a eu un regain d'int&ret pour la notion d'ontologie, sous l'impulsion du web s&mantique."], "vector_1": {"ann": 1, "en": 5, "version": 2, "dontologi": 1, "certain": 1, "automatiqu": 1, "traitement": 1, "ce": 2, "web": 1, "mai": 1, "exploit": 1, "travaux": 1, "dencodag": 1, "avr": 1, "et": 5, "eu": 1, "malheureus": 1, "smantiqu": 1, "limpuls": 1, "ont": 1, "donn": 1, "xml": 1, "alor": 1, "par": 2, "le": 7, "rendu": 1, "compar": 1, "pu": 1, "informatiqu": 1, "na": 1, "dacc": 1, "excel": 1, "ne": 1, "chercheur": 1, "access": 1, "pesant": 1, "pa": 2, "lexploit": 1, "propo": 1, "dernir": 1, "ignorai": 1, "ressourc": 1, "pratiqu": 1, "un": 1, "lont": 1, "mode": 1, "meme": 1, "applic": 2, "plu": 1, "distribut": 1, "format": 2, "de": 7, "lexical": 1, "diffus": 1, "facilit": 2, "consult": 1, "qui": 2, "hadouch": 1, "du": 1, "cr": 1, "sa": 1, "sest": 1, "etr": 1, "lexist": 1, "term": 1, "lapalm": 2, "e": 1, "dintret": 1, "la": 2, "plusieur": 1, "langu": 1, "pour": 4, "caus": 1, "dautr": 1, "deni": 1, "lvf": 2, "guy": 1, "une": 2, "manuel": 1, "sou": 2, "problem": 1, "linguist": 1, "regain": 1, "notion": 1, "il": 1}, "marker": "(2010)", "article": "W14-6402", "vector_2": [4, 0.0953872849086615, 1, 1, 0, 0]}, {"label": "Pos", "current": "Les ontologies repr&sentent des ressources de mod&lisation et de conceptualisation tr&s importantes (Noy et McGuinness, 2000).", "context": ["On fournit ainsi des ontologies qui sont des ressources conceptuelles repr&sent&es par ces langages mod&lisant les domaines des connaissances et on facilite leur acc&s et leur partage.", "Les ontologies repr&sentent des ressources de mod&lisation et de conceptualisation tr&s importantes (Noy et McGuinness, 2000).", "Elles constituent en soi un modele de donn&es repr&sentatif d'un ensemble de concepts dans un domaine, ainsi que des relations entre ces concepts."], "vector_1": {"domain": 2, "constitu": 1, "soi": 1, "ontolog": 2, "conceptuel": 1, "partag": 1, "ce": 2, "reprsentatif": 1, "en": 1, "reprsent": 2, "et": 3, "donn": 1, "concept": 2, "par": 1, "le": 2, "ell": 1, "tr": 1, "modlis": 2, "ressourc": 2, "conceptualis": 1, "relat": 1, "entr": 1, "ainsi": 2, "langag": 1, "dan": 1, "de": 9, "important": 1, "sont": 1, "facilit": 1, "qui": 1, "acc": 1, "dun": 1, "connaiss": 1, "fournit": 1, "un": 2, "leur": 2, "ensembl": 1, "que": 1, "model": 1}, "marker": "(Noy et McGuinness, 2000)", "article": "W14-6402", "vector_2": [14, 0.3090085313539307, 1, 1, 0, 0]}, {"label": "Neut", "current": "Contrairement  cela, d'autres approches consid&rent qu'il y a une s&mantique dans les documents XML qui peut etre d&couverte  partir de la structure des documents, en l'occurrence l'approche de Melnik (1999).", "context": ["Certaines approches proposent une m&thode g&n&rique de transformation XML en modele OWL  partir d'un sch&ma XML et des donn&es du fichier XML, d'autres pensent qu'il est impossible de proposer une approche automatique convenable pour une transformation automatique complete de XML vers OWL, car XML ne d&finit aucune contrainte s&mantique.", "Contrairement  cela, d'autres approches consid&rent qu'il y a une s&mantique dans les documents XML qui peut etre d&couverte  partir de la structure des documents, en l'occurrence l'approche de Melnik (1999).", "Meme si XML n'est pas cens& repr&senter d'informations s&mantiques ou de s&mantique entre les donn&es, les balises imbriqu&es peuvent repr&senter une relation is-a ou part-of ou subType-of."], "vector_1": {"owl": 2, "en": 2, "considr": 1, "conven": 1, "subtypeof": 1, "certain": 1, "automatiqu": 2, "peut": 1, "ne": 1, "entr": 1, "structur": 1, "reprsent": 2, "et": 1, "smantiqu": 4, "partof": 1, "donn": 2, "xml": 7, "fichier": 1, "est": 1, "ver": 1, "dun": 1, "la": 1, "transform": 2, "balis": 1, "si": 1, "pa": 1, "cen": 1, "peuvent": 1, "quil": 2, "document": 2, "cela": 1, "isa": 1, "complet": 1, "meme": 1, "etr": 1, "relat": 1, "melnik": 1, "contraint": 1, "dan": 1, "de": 8, "pensent": 1, "mthode": 1, "qui": 1, "schma": 1, "du": 1, "dinform": 1, "ou": 3, "contrair": 1, "gnriqu": 1, "loccurr": 1, "dcouvert": 1, "car": 1, "dfinit": 1, "le": 3, "pour": 1, "partir": 2, "dautr": 2, "approch": 3, "une": 5, "imbriqu": 1, "aucun": 1, "lapproch": 1, "model": 1, "nest": 1, "imposs": 1, "propos": 2}, "marker": "(1999)", "article": "W14-6402", "vector_2": [15, 0.47312864510531644, 1, 1, 0, 0]}, {"label": "Pos", "current": "Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).", "context": ["national project on item generation for testing student competencies in solving probability problems.", "Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).", "The goal of our item generation project is to develop a model to support optimal problem and test construction."], "vector_1": {"control": 1, "set": 1, "automat": 1, "predefin": 1, "paramet": 1, "develop": 1, "goal": 1, "support": 1, "construct": 2, "compet": 1, "way": 1, "test": 2, "probabl": 1, "difficulti": 1, "optim": 1, "gener": 3, "effect": 1, "nation": 1, "base": 1, "student": 1, "solv": 1, "model": 1, "task": 1, "project": 2, "item": 4, "mani": 1, "problem": 2}, "marker": "Deane and Sheehan, 2003", "article": "W11-1403", "vector_2": [8, 0.05554905516878254, 4, 4, 1, 0]}, {"label": "Neut", "current": "iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).", "context": ["(Left: German original, right: English translation.)", "iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).", "A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005)."], "vector_1": {"origin": 1, "genpex": 1, "use": 1, "right": 1, "principl": 1, "german": 1, "similar": 1, "thu": 1, "system": 1, "sophist": 1, "text": 1, "translat": 1, "allow": 1, "english": 1, "modelcr": 1, "variat": 1, "linguist": 1, "approach": 1, "left": 1}, "marker": "iron and Williamson, 2002", "article": "W11-1403", "vector_2": [9, 0.1438600596735506, 5, 3, 0, 0]}, {"label": "Neut", "current": "iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).", "context": ["(Left: German original, right: English translation.)", "iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).", "A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005)."], "vector_1": {"origin": 1, "genpex": 1, "use": 1, "right": 1, "principl": 1, "german": 1, "similar": 1, "thu": 1, "system": 1, "sophist": 1, "text": 1, "translat": 1, "allow": 1, "english": 1, "modelcr": 1, "variat": 1, "linguist": 1, "approach": 1, "left": 1}, "marker": "Arendasy et al., 2006", "article": "W11-1403", "vector_2": [5, 0.1438600596735506, 5, 3, 0, 0]}, {"label": "Neut", "current": "A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).", "context": ["iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).", "A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).", "However, this system focuses on semantic factors influencing the expression of events with different participants (e.g., different types of vehicles) rather than on generating linguistic variations."], "vector_1": {"vehicl": 1, "semant": 1, "text": 1, "eg": 1, "focus": 1, "differ": 2, "event": 1, "use": 1, "influenc": 1, "particip": 1, "system": 2, "genpex": 1, "factor": 1, "approach": 1, "variat": 2, "gener": 1, "express": 1, "sophist": 1, "linguist": 2, "type": 1, "thu": 1, "modelcr": 1, "rather": 1, "allow": 1, "principl": 1, "similar": 1, "howev": 1}, "marker": "(Deane and Sheehan, 2003", "article": "W11-1403", "vector_2": [8, 0.1472240098285848, 5, 4, 1, 0]}, {"label": "Pos", "current": "Its architecture reflects the language generation pipeline of Reiter and Dale (2000), with three modules: Document Planner, Microplanner and Surface Realizer.", "context": ["An overview of the NLG component of Genpex is given in Figure 3.", "Its architecture reflects the language generation pipeline of Reiter and Dale (2000), with three modules: Document Planner, Microplanner and Surface Realizer.", "Information between the modules is exchanged in the form of a list of sentence trees, each defining the content and grammatical structure of a sentence."], "vector_1": {"overview": 1, "modul": 2, "nlg": 1, "languag": 1, "content": 1, "given": 1, "defin": 1, "three": 1, "tree": 1, "genpex": 1, "compon": 1, "figur": 1, "exchang": 1, "document": 1, "architectur": 1, "dale": 1, "pipelin": 1, "form": 1, "sentenc": 2, "gener": 1, "microplann": 1, "surfac": 1, "reflect": 1, "realiz": 1, "grammat": 1, "planner": 1, "list": 1, "structur": 1, "reiter": 1, "inform": 1}, "marker": "(2000)", "article": "W11-1403", "vector_2": [11, 0.3830807933072018, 1, 2, 0, 0]}, {"label": "Neut", "current": "This is the removal of duplicate words from sentences, which typically applies to aggregated sentences (Harbusch and Kempen, 2009).", "context": ["Ellipsis.", "This is the removal of duplicate words from sentences, which typically applies to aggregated sentences (Harbusch and Kempen, 2009).", "Genpex can apply different types of ellipsis, such as Gapping and Conjunction Reduction."], "vector_1": {"differ": 1, "word": 1, "sentenc": 2, "remov": 1, "conjunct": 1, "genpex": 1, "ellipsi": 2, "aggreg": 1, "appli": 2, "typic": 1, "gap": 1, "type": 1, "reduct": 1, "duplic": 1}, "marker": "(Harbusch and Kempen, 2009)", "article": "W11-1403", "vector_2": [2, 0.653308371848125, 1, 1, 0, 0]}, {"label": "Pos", "current": "This makes it relatively easy to test the influence of those factors on the difficulty of the exercise, for example by carrying out the kind of statistical and cognitive analysis advocated by Graf et al (2005).", "context": ["The underlying probability problem is saved together with the text as well, so all factors that certainly or potentially influence item difficulty are known.", "This makes it relatively easy to test the influence of those factors on the difficulty of the exercise, for example by carrying out the kind of statistical and cognitive analysis advocated by Graf et al (2005).", "The effect of the main parameters of the probability problems in Genpex (i.e., the type of question being asked) was already statistically analyzed by Holling et al (2009) and Zeuch (In preparation)."], "vector_1": {"underli": 1, "carri": 1, "text": 1, "al": 2, "ask": 1, "alreadi": 1, "et": 2, "cognit": 1, "probabl": 2, "easi": 1, "influenc": 2, "ie": 1, "make": 1, "question": 1, "genpex": 1, "zeuch": 1, "certainli": 1, "advoc": 1, "analyz": 1, "rel": 1, "factor": 2, "test": 1, "save": 1, "type": 1, "analysi": 1, "difficulti": 2, "exercis": 1, "effect": 1, "holl": 1, "togeth": 1, "known": 1, "kind": 1, "main": 1, "paramet": 1, "prepar": 1, "well": 1, "item": 1, "exampl": 1, "statist": 2, "potenti": 1, "problem": 2, "graf": 1}, "marker": "(2005)", "article": "W11-1403", "vector_2": [6, 0.9161352600479729, 2, 1, 0, 0]}, {"label": "Neut", "current": "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "context": ["Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.", "Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).", "The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort."], "vector_1": {"control": 1, "creat": 1, "al": 1, "assess": 1, "exist": 1, "expect": 1, "need": 1, "et": 1, "probabl": 1, "use": 1, "develop": 2, "pay": 1, "system": 1, "textual": 1, "genpex": 1, "also": 1, "larg": 2, "statement": 1, "test": 3, "instead": 1, "difficulti": 1, "exercis": 1, "variat": 1, "given": 1, "initi": 1, "effect": 1, "mention": 1, "holl": 1, "know": 1, "onetim": 1, "littl": 1, "effort": 1, "afterward": 1, "valu": 1, "count": 1, "great": 2, "level": 1, "invest": 1, "materi": 1, "item": 1, "amount": 2, "learn": 1}, "marker": "(Enright et al., 2002", "article": "W11-1403", "vector_2": [9, 0.9378107997425846, 7, 2, 0, 0]}, {"label": "Neut", "current": "As the basic mechanism for parsing text into a shallow semantic representation, we choose a shiftreduce type parser (Marcus, 1980).", "context": ["2 Basic Parsing Paradigm", "As the basic mechanism for parsing text into a shallow semantic representation, we choose a shiftreduce type parser (Marcus, 1980).", "It breaks parsing into an ordered sequence of small and manageable parse actions such as shift and reduce."], "vector_1": {"reduc": 1, "represent": 1, "semant": 1, "shiftreduc": 1, "choos": 1, "text": 1, "shallow": 1, "parser": 1, "manag": 1, "break": 1, "action": 1, "sequenc": 1, "mechan": 1, "basic": 2, "small": 1, "pars": 4, "shift": 1, "paradigm": 1, "type": 1, "order": 1}, "marker": "(Marcus, 1980)", "article": "P97-1062", "vector_2": [17, 0.09680669932937211, 1, 1, 1, 0]}, {"label": "Neut", "current": "We believe that an extensive collection of complex translation pairs in the bilingual dictionary is critical for translation quality and we are confident that its acquisition can be at least partially automated by using techniques like those described in (Smadja et al., 1996).", "context": ["ambiguous with respect to German, but the English compound conclusively maps to the German compound \"Zinssatz\".", "We believe that an extensive collection of complex translation pairs in the bilingual dictionary is critical for translation quality and we are confident that its acquisition can be at least partially automated by using techniques like those described in (Smadja et al., 1996).", "Complex translation entries are preprocessed using the same parser as for normal text."], "vector_1": {"partial": 1, "german": 2, "text": 1, "parser": 1, "qualiti": 1, "zinssatz": 1, "critic": 1, "respect": 1, "use": 2, "techniqu": 1, "least": 1, "complex": 2, "preprocess": 1, "confid": 1, "map": 1, "normal": 1, "bilingu": 1, "autom": 1, "extens": 1, "believ": 1, "dictionari": 1, "compound": 2, "pair": 1, "conclus": 1, "describ": 1, "like": 1, "acquisit": 1, "entri": 1, "ambigu": 1, "collect": 1, "english": 1, "translat": 3}, "marker": "(Smadja et al., 1996)", "article": "P97-1062", "vector_2": [1, 0.5427221237708051, 1, 1, 0, 0]}, {"label": "Neut", "current": "Tadayoshi et al (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format.", "context": ["For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.", "Tadayoshi et al (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format.", "To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain."], "vector_1": {"corpu": 2, "domain": 3, "creat": 1, "applic": 1, "parser": 2, "ptb": 1, "al": 1, "biomed": 1, "cost": 1, "need": 1, "et": 1, "flexibl": 1, "extant": 1, "select": 2, "paramet": 1, "use": 1, "gildea": 1, "minim": 1, "collin": 1, "make": 2, "system": 1, "accuraci": 1, "adapt": 1, "test": 2, "new": 2, "correct": 1, "brown": 1, "contribut": 1, "format": 1, "effect": 1, "bilex": 1, "retrain": 1, "train": 2, "pars": 2, "tadayoshi": 1, "report": 1, "rang": 1, "viabl": 1, "data": 4, "tune": 1, "wsjderiv": 1, "deriv": 1, "augment": 1, "wsj": 1, "manual": 1, "annot": 1, "yield": 1, "instanc": 1, "statist": 2, "improv": 1, "model": 1, "genia": 1}, "marker": "(2005)", "article": "W07-2203", "vector_2": [2, 0.0698806147813605, 3, 1, 1, 0]}, {"label": "Weak", "current": "Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001).", "context": ["To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain.", "Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001).", "However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB."], "vector_1": {"em": 1, "rang": 1, "creat": 1, "appli": 1, "parser": 1, "ptb": 1, "domain": 1, "cost": 1, "expect": 1, "need": 1, "flexibl": 1, "extract": 1, "pcfg": 1, "use": 1, "unsuccess": 1, "minim": 1, "make": 2, "system": 1, "insideoutsid": 1, "includ": 1, "rel": 1, "new": 2, "method": 1, "unlabel": 1, "adapt": 1, "maxim": 1, "applic": 1, "effect": 1, "viabl": 1, "schabe": 1, "train": 2, "pars": 1, "date": 1, "extant": 1, "data": 3, "tune": 1, "algorithm": 1, "unsupervis": 1, "semisupervis": 1, "annot": 1, "bracket": 1, "statist": 1, "ioa": 2, "pereira": 1, "howev": 1}, "marker": "(Baker, 1979", "article": "W07-2203", "vector_2": [28, 0.08486503518537969, 3, 1, 0, 0]}, {"label": "Weak", "current": "Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001).", "context": ["To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain.", "Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001).", "However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB."], "vector_1": {"em": 1, "rang": 1, "creat": 1, "appli": 1, "parser": 1, "ptb": 1, "domain": 1, "cost": 1, "expect": 1, "need": 1, "flexibl": 1, "extract": 1, "pcfg": 1, "use": 1, "unsuccess": 1, "minim": 1, "make": 2, "system": 1, "insideoutsid": 1, "includ": 1, "rel": 1, "new": 2, "method": 1, "unlabel": 1, "adapt": 1, "maxim": 1, "applic": 1, "effect": 1, "viabl": 1, "schabe": 1, "train": 2, "pars": 1, "date": 1, "extant": 1, "data": 3, "tune": 1, "algorithm": 1, "unsupervis": 1, "semisupervis": 1, "annot": 1, "bracket": 1, "statist": 1, "ioa": 2, "pereira": 1, "howev": 1}, "marker": "Prescher, 2001)", "article": "W07-2203", "vector_2": [6, 0.08486503518537969, 3, 1, 0, 0]}, {"label": "Pos", "current": "More recently, both Riezler et al (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation.", "context": ["Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank.", "More recently, both Riezler et al (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation.", "In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing."], "vector_1": {"represent": 1, "entropi": 1, "clark": 1, "treebank": 1, "curran": 1, "al": 1, "annot": 2, "cost": 1, "riezler": 1, "paper": 1, "et": 1, "partial": 1, "probabl": 1, "develop": 1, "weight": 1, "richer": 1, "anoth": 1, "unlabel": 2, "investig": 1, "deriv": 2, "extend": 1, "normal": 1, "ptb": 1, "util": 2, "train": 1, "pars": 1, "line": 1, "associ": 1, "recent": 1, "count": 1, "lower": 1, "consist": 1, "success": 1, "wsj": 1, "maximum": 1, "bracket": 2, "model": 2}, "marker": "(2002)", "article": "W07-2203", "vector_2": [5, 0.12582711900010504, 2, 1, 1, 0]}, {"label": "Neut", "current": "More recently, both Riezler et al (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation.", "context": ["Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank.", "More recently, both Riezler et al (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation.", "In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing."], "vector_1": {"represent": 1, "entropi": 1, "clark": 1, "treebank": 1, "curran": 1, "al": 1, "annot": 2, "cost": 1, "riezler": 1, "paper": 1, "et": 1, "partial": 1, "probabl": 1, "develop": 1, "weight": 1, "richer": 1, "anoth": 1, "unlabel": 2, "investig": 1, "deriv": 2, "extend": 1, "normal": 1, "ptb": 1, "util": 2, "train": 1, "pars": 1, "line": 1, "associ": 1, "recent": 1, "count": 1, "lower": 1, "consist": 1, "success": 1, "wsj": 1, "maximum": 1, "bracket": 2, "model": 2}, "marker": "(2004)", "article": "W07-2203", "vector_2": [3, 0.12582711900010504, 2, 1, 0, 0]}, {"label": "Neut", "current": "Following Pereira and Schabes (1992) given t = (s, U), a node's span in the parse forest is valid if it does not overlap with any span outlined in U, and hence, a derivation is correct if the span of every node in the derivation is valid in U.", "context": ["In this case, equality between the derivation tree and the treebank annotation A identifies the correct derivation.", "Following Pereira and Schabes (1992) given t = (s, U), a node's span in the parse forest is valid if it does not overlap with any span outlined in U, and hence, a derivation is correct if the span of every node in the derivation is valid in U.", "That is, if no crossing brackets are present in the derivation."], "vector_1": {"identifi": 1, "treebank": 1, "henc": 1, "annot": 1, "follow": 1, "given": 1, "span": 3, "cross": 1, "overlap": 1, "valid": 2, "forest": 1, "correct": 2, "node": 2, "everi": 1, "deriv": 5, "schabe": 1, "pars": 1, "present": 1, "case": 1, "outlin": 1, "tree": 1, "equal": 1, "bracket": 1, "u": 3, "pereira": 1}, "marker": "(1992)", "article": "W07-2203", "vector_2": [15, 0.3615166474109862, 1, 3, 0, 0]}, {"label": "Neut", "current": "The parser's output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1.", "context": ["4 The Evaluation Scheme", "The parser's output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1.", "Relations are organized into a hierarchy with the root node specifying an unlabeled dependency."], "vector_1": {"node": 1, "measur": 1, "use": 1, "relat": 2, "depend": 2, "organ": 1, "evalu": 3, "f": 1, "parser": 1, "specifi": 1, "standard": 1, "precis": 1, "hierarchi": 1, "output": 1, "scheme": 2, "root": 1, "recal": 1, "unlabel": 1}, "marker": "Lin, 1998)", "article": "W07-2203", "vector_2": [9, 0.4953261212057557, 2, 1, 0, 0]}, {"label": "Neut", "current": "The steep drop in performance (down to 69.93% F1) after the first iteration is probably due to loss of information from S. However, this run also eventually converges to similar performance, suggesting that the information in S is effectively disregarded as it forms only a small portion of SW, and that these runs effectively converge to a local maximum over W. Bacchiani et al (2006), working in a similar framework, explore weighting the contribution (frequency counts) of the in-domain and out-ofdomain training datasets and demonstrate that this can have beneficial effects.", "context": ["In this case, the graph illustrates a combination of Elworthy's 'initial' and 'classical' patterns.", "The steep drop in performance (down to 69.93% F1) after the first iteration is probably due to loss of information from S. However, this run also eventually converges to similar performance, suggesting that the information in S is effectively disregarded as it forms only a small portion of SW, and that these runs effectively converge to a local maximum over W. Bacchiani et al (2006), working in a similar framework, explore weighting the contribution (frequency counts) of the in-domain and out-ofdomain training datasets and demonstrate that this can have beneficial effects.", "Furthermore, they also tried unsupervised tuning to the indomain corpus by weighting parses for it by their normalized probability."], "vector_1": {"corpu": 1, "suggest": 1, "work": 1, "explor": 1, "weight": 2, "classic": 1, "illustr": 1, "tri": 1, "al": 1, "bacchiani": 1, "framework": 1, "et": 1, "probabl": 2, "converg": 2, "perform": 2, "graph": 1, "frequenc": 1, "due": 1, "indomain": 2, "outofdomain": 1, "also": 2, "tune": 1, "normal": 1, "eventu": 1, "local": 1, "count": 1, "run": 2, "contribut": 1, "form": 1, "benefici": 1, "effect": 3, "initi": 1, "small": 1, "elworthi": 1, "train": 1, "pars": 1, "dataset": 1, "demonstr": 1, "case": 1, "loss": 1, "furthermor": 1, "f": 1, "similar": 2, "howev": 1, "drop": 1, "sw": 1, "maximum": 1, "iter": 1, "inform": 2, "portion": 1, "combin": 1, "w": 1, "pattern": 1, "disregard": 1, "steep": 1, "unsupervis": 1, "first": 1}, "marker": "(2006)", "article": "W07-2203", "vector_2": [1, 0.8766586142912159, 1, 2, 0, 0]}, {"label": "Neut", "current": "For instance, Bacchiani et al (2006) demonstrate imrpovements in parsing accuracy with unsupervised adaptation from unannotated data and explore the effect of different weighting of counts derived from the supervised and unsupervised data.", "context": ["Finally, further experiments on weighting the contribution of each dataset might be beneficial.", "For instance, Bacchiani et al (2006) demonstrate imrpovements in parsing accuracy with unsupervised adaptation from unannotated data and explore the effect of different weighting of counts derived from the supervised and unsupervised data.", "Acknowledgements"], "vector_1": {"unannot": 1, "weight": 2, "acknowledg": 1, "al": 1, "bacchiani": 1, "explor": 1, "et": 1, "deriv": 1, "supervis": 1, "differ": 1, "accuraci": 1, "adapt": 1, "experi": 1, "might": 1, "final": 1, "contribut": 1, "benefici": 1, "effect": 1, "pars": 1, "dataset": 1, "data": 2, "imrpov": 1, "demonstr": 1, "count": 1, "unsupervis": 2, "instanc": 1}, "marker": "(2006)", "article": "W07-2203", "vector_2": [1, 0.9816195777754437, 1, 2, 0, 0]}, {"label": "Neut", "current": "Ontology is defined as 'Explicit specification of conceptualization' (Gruber, 1993).", "context": ["1 Introduction", "Ontology is defined as 'Explicit specification of conceptualization' (Gruber, 1993).", "As a knowledge representation formalism, ontologies have found a wide range of applications in the areas like knowledge management, information retrieval and information extraction."], "vector_1": {"applic": 1, "represent": 1, "rang": 1, "retriev": 1, "like": 1, "specif": 1, "defin": 1, "ontolog": 2, "knowledg": 2, "explicit": 1, "area": 1, "conceptu": 1, "inform": 2, "wide": 1, "found": 1, "formal": 1, "extract": 1, "manag": 1, "introduct": 1}, "marker": "(Gruber, 1993)", "article": "W12-5209", "vector_2": [19, 0.07066812453242792, 1, 1, 0, 0]}, {"label": "Neut", "current": "Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.", "context": ["The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.", "Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.", "Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks."], "vector_1": {"domain": 1, "concept": 4, "identifi": 2, "creat": 1, "process": 1, "ontolog": 3, "challeng": 1, "major": 1, "exist": 1, "construct": 1, "hierarchi": 2, "extract": 2, "involv": 2, "use": 1, "top": 1, "subsumpt": 1, "two": 1, "construt": 1, "identif": 1, "basic": 1, "document": 1, "method": 1, "good": 1, "variou": 1, "relat": 1, "relev": 1, "term": 3, "task": 2, "specif": 1, "algorithm": 1, "level": 1, "learn": 2}, "marker": "Kozakov et al., 2004", "article": "W12-5209", "vector_2": [8, 0.09967198020371756, 5, 1, 0, 0]}, {"label": "Neut", "current": "Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.", "context": ["The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.", "Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.", "Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks."], "vector_1": {"domain": 1, "concept": 4, "identifi": 2, "creat": 1, "process": 1, "ontolog": 3, "challeng": 1, "major": 1, "exist": 1, "construct": 1, "hierarchi": 2, "extract": 2, "involv": 2, "use": 1, "top": 1, "subsumpt": 1, "two": 1, "construt": 1, "identif": 1, "basic": 1, "document": 1, "method": 1, "good": 1, "variou": 1, "relat": 1, "relev": 1, "term": 3, "task": 2, "specif": 1, "algorithm": 1, "level": 1, "learn": 2}, "marker": "Sclano and Velardi, 2007", "article": "W12-5209", "vector_2": [5, 0.09967198020371756, 5, 1, 0, 0]}, {"label": "Neut", "current": "Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.", "context": ["The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.", "Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.", "Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks."], "vector_1": {"domain": 1, "concept": 4, "identifi": 2, "creat": 1, "process": 1, "ontolog": 3, "challeng": 1, "major": 1, "exist": 1, "construct": 1, "hierarchi": 2, "extract": 2, "involv": 2, "use": 1, "top": 1, "subsumpt": 1, "two": 1, "construt": 1, "identif": 1, "basic": 1, "document": 1, "method": 1, "good": 1, "variou": 1, "relat": 1, "relev": 1, "term": 3, "task": 2, "specif": 1, "algorithm": 1, "level": 1, "learn": 2}, "marker": "Frantzi et al., 1998", "article": "W12-5209", "vector_2": [14, 0.09967198020371756, 5, 1, 0, 0]}, {"label": "Neut", "current": "Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.", "context": ["The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.", "Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.", "Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks."], "vector_1": {"domain": 1, "concept": 4, "identifi": 2, "creat": 1, "process": 1, "ontolog": 3, "challeng": 1, "major": 1, "exist": 1, "construct": 1, "hierarchi": 2, "extract": 2, "involv": 2, "use": 1, "top": 1, "subsumpt": 1, "two": 1, "construt": 1, "identif": 1, "basic": 1, "document": 1, "method": 1, "good": 1, "variou": 1, "relat": 1, "relev": 1, "term": 3, "task": 2, "specif": 1, "algorithm": 1, "level": 1, "learn": 2}, "marker": "Gacitua et al., 2011)", "article": "W12-5209", "vector_2": [1, 0.09967198020371756, 5, 1, 0, 0]}, {"label": "Neut", "current": "Also, the automatically constructed ontologies often create false association between terms and result in erroneous concept hierarchy (Zhou, 2007).", "context": ["As noted by Fountain and Lapata (2012), 'Most of the existing approaches construct flat structure rather than a taxonomy.", "Also, the automatically constructed ontologies often create false association between terms and result in erroneous concept hierarchy (Zhou, 2007).", "In order to handle the above mentioned issues, we propose a graph-based ontology learning algorithm."], "vector_1": {"concept": 1, "often": 1, "creat": 1, "ontolog": 2, "automat": 1, "exist": 1, "result": 1, "hierarchi": 1, "taxonomi": 1, "lapata": 1, "rather": 1, "construct": 2, "note": 1, "also": 1, "fountain": 1, "handl": 1, "approach": 1, "flat": 1, "graphbas": 1, "fals": 1, "mention": 1, "associ": 1, "term": 1, "algorithm": 1, "erron": 1, "structur": 1, "issu": 1, "learn": 1, "order": 1, "propos": 1}, "marker": "(Zhou, 2007)", "article": "W12-5209", "vector_2": [5, 0.12424469125856016, 2, 1, 0, 0]}, {"label": "Neut", "current": "'Terms with high information content remain lower in the concept hierarchy and terms with low information content remain higher in the concept hierarchy' (Resnik, 1999).", "context": ["Our approach is based on the information content of the term.", "'Terms with high information content remain lower in the concept hierarchy and terms with low information content remain higher in the concept hierarchy' (Resnik, 1999).", "Caraballo and Charniak (1999) have shown that the term frequency is a good indicator of determining specificity of a term."], "vector_1": {"high": 1, "term": 5, "concept": 2, "specif": 1, "frequenc": 1, "caraballo": 1, "lower": 1, "indic": 1, "good": 1, "content": 3, "inform": 3, "remain": 2, "base": 1, "low": 1, "determin": 1, "charniak": 1, "hierarchi": 2, "approach": 1, "shown": 1, "higher": 1}, "marker": "(Resnik, 1999)", "article": "W12-5209", "vector_2": [13, 0.144501352362318, 2, 1, 0, 0]}, {"label": "Neut", "current": "The proposed approach combines evidences from linguistic patterns and WordNet (Fellbaum, 1998) to detect subsumption relation.", "context": ["This early identification of hierarchy creates a better taxonomic structure and avoids false association between the terms.", "The proposed approach combines evidences from linguistic patterns and WordNet (Fellbaum, 1998) to detect subsumption relation.", "The patterns used in the system are generic and can be used across languages."], "vector_1": {"creat": 1, "hierarchi": 1, "wordnet": 1, "identif": 1, "use": 2, "detect": 1, "pattern": 2, "avoid": 1, "subsumpt": 1, "system": 1, "taxonom": 1, "better": 1, "languag": 1, "approach": 1, "across": 1, "earli": 1, "evid": 1, "fals": 1, "gener": 1, "relat": 1, "associ": 1, "term": 1, "structur": 1, "combin": 1, "linguist": 1, "propos": 1}, "marker": "(Fellbaum, 1998)", "article": "W12-5209", "vector_2": [14, 0.1817920239396904, 1, 2, 0, 0]}, {"label": "Neut", "current": "Statistical methods relate concepts based on distributional hypothesis (Harris, 1968), that is 'similar terms appear in the similar context.'", "context": ["Statistical approaches model ontology learning as a classification or clustering problem.", "Statistical methods relate concepts based on distributional hypothesis (Harris, 1968), that is 'similar terms appear in the similar context.'", "Hindle (1990) performed semantic clustering to find semantically similar nouns."], "vector_1": {"classif": 1, "concept": 1, "ontolog": 1, "semant": 2, "cluster": 2, "find": 1, "appear": 1, "perform": 1, "hindl": 1, "approach": 1, "method": 1, "distribut": 1, "hypothesi": 1, "relat": 1, "base": 1, "problem": 1, "term": 1, "noun": 1, "statist": 2, "context": 1, "learn": 1, "model": 1, "similar": 3}, "marker": "(Harris, 1968)", "article": "W12-5209", "vector_2": [44, 0.32335846233527077, 2, 1, 0, 0]}, {"label": "Neut", "current": "Hindle (1990) performed semantic clustering to find semantically similar nouns.", "context": ["Statistical methods relate concepts based on distributional hypothesis (Harris, 1968), that is 'similar terms appear in the similar context.'", "Hindle (1990) performed semantic clustering to find semantically similar nouns.", "They calculated the co-occurrence weight for each verb-subject and verb-object pair."], "vector_1": {"semant": 2, "concept": 1, "weight": 1, "cluster": 1, "find": 1, "cooccurr": 1, "appear": 1, "perform": 1, "hindl": 1, "method": 1, "verbobject": 1, "distribut": 1, "hypothesi": 1, "relat": 1, "base": 1, "pair": 1, "verbsubject": 1, "term": 1, "noun": 1, "calcul": 1, "statist": 1, "context": 1, "similar": 3}, "marker": "(1990)", "article": "W12-5209", "vector_2": [22, 0.326983944294182, 2, 1, 0, 0]}, {"label": "Neut", "current": "In a similar approach, Cimiano et al (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy.", "context": ["Hearst-patterns are used to detect hypernymy relation between similar nouns.", "In a similar approach, Cimiano et al (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy.", "Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy."], "vector_1": {"directli": 1, "al": 1, "cluster": 2, "unlik": 1, "hierarchi": 2, "et": 1, "wordnet": 1, "web": 1, "use": 2, "detect": 1, "cimiano": 1, "pattern": 1, "construct": 1, "hearstpattern": 2, "approach": 1, "sourc": 1, "distribut": 1, "relat": 1, "oracl": 1, "decid": 1, "base": 1, "pair": 1, "noun": 3, "integr": 1, "arrang": 1, "similar": 3, "hypernymi": 3}, "marker": "(2005)", "article": "W12-5209", "vector_2": [7, 0.38539448696552914, 3, 2, 1, 0]}, {"label": "Neut", "current": "In a similar approach, Cimiano et al (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy.", "context": ["Hearst-patterns are used to detect hypernymy relation between similar nouns.", "In a similar approach, Cimiano et al (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy.", "Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy."], "vector_1": {"directli": 1, "al": 1, "cluster": 2, "unlik": 1, "hierarchi": 2, "et": 1, "wordnet": 1, "web": 1, "use": 2, "detect": 1, "cimiano": 1, "pattern": 1, "construct": 1, "hearstpattern": 2, "approach": 1, "sourc": 1, "distribut": 1, "relat": 1, "oracl": 1, "decid": 1, "base": 1, "pair": 1, "noun": 3, "integr": 1, "arrang": 1, "similar": 3, "hypernymi": 3}, "marker": "(Fellbaum, 1998)", "article": "W12-5209", "vector_2": [14, 0.38539448696552914, 3, 2, 0, 0]}, {"label": "Neut", "current": "Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy.", "context": ["In a similar approach, Cimiano et al (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy.", "Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy.", "Dominguez Garcia et al (2012) used wikipedia to extract ontology for different languages."], "vector_1": {"directli": 1, "ontolog": 1, "al": 2, "cluster": 2, "unlik": 1, "dominguez": 1, "hierarchi": 2, "et": 2, "differ": 1, "extract": 1, "wordnet": 1, "web": 1, "use": 2, "cimiano": 1, "pattern": 1, "wikipedia": 1, "construct": 1, "hearstpattern": 1, "languag": 1, "approach": 1, "sourc": 1, "distribut": 1, "oracl": 1, "decid": 1, "base": 1, "pair": 1, "noun": 2, "garcia": 1, "integr": 1, "arrang": 1, "similar": 2, "hypernymi": 2}, "marker": "(Caraballo, 1999)", "article": "W12-5209", "vector_2": [13, 0.38844449559762906, 4, 2, 3, 0]}, {"label": "Neut", "current": "Dominguez Garcia et al (2012) used wikipedia to extract ontology for different languages.", "context": ["Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy.", "Dominguez Garcia et al (2012) used wikipedia to extract ontology for different languages.", "Like Cimiano et al (2005), we follow a hybrid approach and construct a concept hierarchy using distributional similarity, patterns and WordNet."], "vector_1": {"concept": 1, "directli": 1, "ontolog": 1, "al": 2, "cluster": 1, "unlik": 1, "dominguez": 1, "hierarchi": 2, "et": 2, "differ": 1, "extract": 1, "languag": 1, "decid": 1, "cimiano": 1, "pattern": 1, "hybrid": 1, "wikipedia": 1, "construct": 1, "wordnet": 1, "approach": 1, "sourc": 1, "distribut": 1, "use": 2, "pair": 1, "noun": 1, "like": 1, "garcia": 1, "follow": 1, "integr": 1, "arrang": 1, "similar": 1, "hypernymi": 1}, "marker": "(2012)", "article": "W12-5209", "vector_2": [0, 0.39765206882660986, 3, 3, 0, 0]}, {"label": "Neut", "current": "Fountain and Lapata (2012) proposed a graph based approach that does not require a separate term extraction step.", "context": ["Other method similar to our work is proposed in Fountain and Lapata (2012).", "Fountain and Lapata (2012) proposed a graph based approach that does not require a separate term extraction step.", "However, their approach works with a predefined set of seed terms."], "vector_1": {"term": 2, "set": 1, "lapata": 2, "graph": 1, "howev": 1, "work": 2, "predefin": 1, "separ": 1, "step": 1, "base": 1, "fountain": 2, "seed": 1, "approach": 2, "extract": 1, "similar": 1, "method": 1, "requir": 1, "propos": 2}, "marker": "(2012)", "article": "W12-5209", "vector_2": [0, 0.4416757783276745, 2, 3, 0, 0]}, {"label": "Neut", "current": "2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just \"regular\" readers (Graesser et al., 2004).", "context": ["The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.", "2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just \"regular\" readers (Graesser et al., 2004).", "Text simplification is most often performed on the sentence level."], "vector_1": {"often": 1, "text": 3, "al": 1, "impair": 1, "mild": 1, "need": 1, "et": 1, "cognit": 1, "special": 1, "differ": 1, "develop": 1, "sentenc": 1, "perform": 1, "learner": 1, "elderli": 1, "reader": 3, "languag": 1, "type": 1, "gener": 1, "poor": 1, "mainstream": 1, "peopl": 2, "methodolog": 1, "tool": 1, "dell": 1, "literaci": 1, "regular": 1, "address": 1, "aluisio": 1, "simplif": 2, "level": 2}, "marker": "Orletta et al., 2011)", "article": "W14-5605", "vector_2": [3, 0.18475832656376928, 4, 5, 0, 0]}, {"label": "Neut", "current": "(Bott, et al., 2012) describe a hybrid automatic text simplification system which combines a rule based core module with a statistical support module that controls the application of rules in the wrong contexts.", "context": ["Sentence simplification is expressed as the list of sub-sentences that are portions of the original sentence.", "(Bott, et al., 2012) describe a hybrid automatic text simplification system which combines a rule based core module with a statistical support module that controls the application of rules in the wrong contexts.", "The approaches to patent claim simplification can be roughly put into two groups."], "vector_1": {"origin": 1, "control": 1, "applic": 1, "text": 1, "modul": 2, "automat": 1, "two": 1, "group": 1, "describ": 1, "subsent": 1, "patent": 1, "support": 1, "hybrid": 1, "system": 1, "approach": 1, "core": 1, "sentenc": 2, "express": 1, "wrong": 1, "base": 1, "put": 1, "simplif": 3, "list": 1, "rule": 2, "claim": 1, "portion": 1, "combin": 1, "statist": 1, "context": 1, "roughli": 1}, "marker": "(Bott, et al., 2012)", "article": "W14-5605", "vector_2": [2, 0.2470806255077173, 1, 2, 0, 0]}, {"label": "Neut", "current": "Most recently (Shinmori et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while (Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and reformatting the original text so as to emphasis segments with the identified border marker.", "context": ["For example, in one of the earlier works a rule-based technique was developed for decomposing the complex sentence of a claim into a set of simple sentences while preserving the initial content (Sheremetyeva, 2003).", "Most recently (Shinmori et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while (Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and reformatting the original text so as to emphasis segments with the identified border marker.", "This approach does not involve any syntactic restructuring, just visualization of claim segments."], "vector_1": {"origin": 1, "claim": 4, "work": 1, "emphasi": 1, "text": 2, "decompos": 1, "one": 1, "set": 1, "marker": 1, "phrase": 1, "border": 2, "involv": 2, "develop": 1, "techniqu": 1, "suggest": 1, "section": 1, "identifi": 1, "content": 1, "complex": 1, "reformat": 1, "explanatori": 1, "restructur": 1, "syntact": 1, "preserv": 1, "sentenc": 2, "earlier": 1, "initi": 1, "visual": 1, "approach": 2, "segment": 3, "recent": 1, "simpl": 1, "align": 1, "rulebas": 1, "descript": 1, "exampl": 1, "highlight": 1, "propos": 1}, "marker": "(Shinmori et al., 2012)", "article": "W14-5605", "vector_2": [2, 0.27787367993501216, 3, 1, 1, 0]}, {"label": "Neut", "current": "The number of patents the authors use to evaluate their methodologies might seem quite limited, e.g., (Mille and Wanner, 2008) report evaluation results based on 30 patents; in (Bouayad-Agha et al.)", "context": ["Some of the researchers admit avoiding qualitative evaluation due to the lack of resources that would have made it possible (Mille and Wanner, 2008).", "The number of patents the authors use to evaluate their methodologies might seem quite limited, e.g., (Mille and Wanner, 2008) report evaluation results based on 30 patents; in (Bouayad-Agha et al.)", "the test corpus consisted of 29 patents; (Ferraro et al."], "vector_1": {"corpu": 1, "evalu": 3, "eg": 1, "lack": 1, "number": 1, "result": 1, "et": 2, "seem": 1, "quit": 1, "use": 1, "would": 1, "author": 1, "patent": 3, "avoid": 1, "due": 1, "research": 1, "test": 1, "might": 1, "ferraro": 1, "resourc": 1, "methodolog": 1, "al": 2, "base": 1, "report": 1, "made": 1, "consist": 1, "possibl": 1, "qualit": 1, "admit": 1, "limit": 1, "bouayadagha": 1}, "marker": "(Mille and Wanner, 2008)", "article": "W14-5605", "vector_2": [6, 0.8587276604386678, 2, 3, 0, 0]}, {"label": "Neut", "current": "Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005).", "context": ["The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages.", "Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005).", "2.1 Training, Development and Test Data"], "vector_1": {"bleu": 1, "al": 1, "rate": 1, "minimum": 1, "treetagg": 1, "et": 1, "languag": 1, "use": 2, "develop": 1, "perform": 1, "system": 1, "decod": 1, "test": 1, "venugop": 1, "reorder": 1, "optim": 1, "gener": 1, "train": 2, "translat": 1, "data": 1, "po": 1, "tag": 1, "sttk": 1, "error": 1, "model": 1, "toward": 1, "propos": 1}, "marker": "(Vogel, 2003)", "article": "W10-1719", "vector_2": [7, 0.14000385331706378, 3, 1, 5, 0]}, {"label": "Pos", "current": "Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005).", "context": ["The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages.", "Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005).", "2.1 Training, Development and Test Data"], "vector_1": {"bleu": 1, "al": 1, "rate": 1, "minimum": 1, "treetagg": 1, "et": 1, "languag": 1, "use": 2, "develop": 1, "perform": 1, "system": 1, "decod": 1, "test": 1, "venugop": 1, "reorder": 1, "optim": 1, "gener": 1, "train": 2, "translat": 1, "data": 1, "po": 1, "tag": 1, "sttk": 1, "error": 1, "model": 1, "toward": 1, "propos": 1}, "marker": "(2005)", "article": "W10-1719", "vector_2": [5, 0.14000385331706378, 3, 1, 2, 0]}, {"label": "Pos", "current": "When translating from German to English, we apply compound splitting as described in Koehn and Knight (2003) to the German corpus.", "context": ["If the new word is a correct word according to the hunspell lexicon using the new spelling rules, we map the words.", "When translating from German to English, we apply compound splitting as described in Koehn and Knight (2003) to the German corpus.", "As a last preprocessing step we remove sentences that are too long and empty lines to obtain the final training corpus."], "vector_1": {"corpu": 2, "lexicon": 1, "german": 2, "appli": 1, "obtain": 1, "empti": 1, "hunspel": 1, "use": 1, "describ": 1, "long": 1, "split": 1, "preprocess": 1, "new": 2, "correct": 1, "map": 1, "accord": 1, "koehn": 1, "sentenc": 1, "spell": 1, "step": 1, "train": 1, "translat": 1, "compound": 1, "last": 1, "line": 1, "word": 3, "knight": 1, "remov": 1, "rule": 1, "final": 1, "english": 1}, "marker": "(2003)", "article": "W10-1719", "vector_2": [7, 0.26645687495986126, 1, 1, 1, 0]}, {"label": "Pos", "current": "For short-range reorderings, continuous reordering rules were applied to the test sentences (Rottmann and Vogel, 2007).", "context": ["These possible reorderings were learned based on the POS of the source language words in the training corpus and the information about alignments between source and target language words in the corpus.", "For short-range reorderings, continuous reordering rules were applied to the test sentences (Rottmann and Vogel, 2007).", "To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction."], "vector_1": {"corpu": 2, "german": 1, "appli": 2, "longrang": 1, "continu": 1, "languag": 2, "differ": 1, "noncontinu": 1, "depend": 1, "shortrang": 1, "test": 1, "type": 1, "po": 1, "reorder": 5, "sourc": 2, "sentenc": 1, "english": 1, "direct": 1, "train": 1, "translat": 1, "word": 2, "target": 1, "possibl": 1, "align": 1, "rule": 2, "inform": 1, "base": 1, "learn": 1, "model": 1}, "marker": "(Rottmann and Vogel, 2007)", "article": "W10-1719", "vector_2": [3, 0.30287072121251046, 1, 1, 5, 0]}, {"label": "Neut", "current": "For more details see Niehues and Vogel (2008).", "context": ["The model was trained in three steps, first using maximum likelihood optimization and afterwards it was optimized towards the alignment error rate.", "For more details see Niehues and Vogel (2008).", "4.2 Lattice Phrase Extraction"], "vector_1": {"use": 1, "extract": 1, "see": 1, "vogel": 1, "rate": 1, "optim": 2, "detail": 1, "align": 1, "lattic": 1, "three": 1, "maximum": 1, "step": 1, "train": 1, "niehu": 1, "error": 1, "phrase": 1, "model": 1, "toward": 1, "likelihood": 1, "afterward": 1, "first": 1}, "marker": "(2008)", "article": "W10-1719", "vector_2": [2, 0.4204611136086314, 1, 3, 5, 0]}, {"label": "Neut", "current": "Motivated by the improvements in translation quality that could be achieved by using the n-gram based approach to statistical machine translation, for example by Allauzen et al (2009), we tried to integrate a bilingual language model into our phrase-based translation system.", "context": ["4.4 Bilingual Word language model", "Motivated by the improvements in translation quality that could be achieved by using the n-gram based approach to statistical machine translation, for example by Allauzen et al (2009), we tried to integrate a bilingual language model into our phrase-based translation system.", "To be able to integrate the approach easily into a standard phrase-based SMT system, a token in the bilingual language model is defined to consist of a target word and all source words it is aligned to."], "vector_1": {"qualiti": 1, "al": 1, "motiv": 1, "et": 1, "languag": 3, "use": 1, "defin": 1, "smt": 1, "abl": 1, "system": 2, "approach": 2, "statist": 1, "machin": 1, "sourc": 1, "standard": 1, "ngram": 1, "base": 1, "translat": 3, "tri": 1, "allauzen": 1, "word": 3, "target": 1, "consist": 1, "could": 1, "phrasebas": 2, "easili": 1, "achiev": 1, "exampl": 1, "integr": 2, "bilingu": 3, "improv": 1, "model": 3, "align": 1, "token": 1}, "marker": "(2009)", "article": "W10-1719", "vector_2": [1, 0.5896217327082397, 1, 1, 0, 0]}, {"label": "Neut", "current": "The implementation is based on the SmithWaterman algorithm (Smith and Waterman, 1981), initially proposed for determining similar regions between two protein or DNA sequences.", "context": ["), the values of which are optimised by employing the optimisation module.", "The implementation is based on the SmithWaterman algorithm (Smith and Waterman, 1981), initially proposed for determining similar regions between two protein or DNA sequences.", "The algorithm is guaranteed to find the optimal local alignment between the two input sequences at clause level."], "vector_1": {"modul": 1, "guarante": 1, "protein": 1, "find": 1, "optimis": 2, "dna": 1, "two": 2, "input": 1, "local": 1, "optim": 1, "sequenc": 2, "claus": 1, "initi": 1, "base": 1, "valu": 1, "algorithm": 2, "level": 1, "align": 1, "employ": 1, "determin": 1, "implement": 1, "smithwaterman": 1, "similar": 1, "region": 1, "propos": 1}, "marker": "(Smith and Waterman, 1981)", "article": "W12-0108", "vector_2": [31, 0.7575399637766753, 1, 1, 0, 0]}, {"label": "Neut", "current": "Philip Resnik (1999) showed that parallel corpora-until then a promising research avenue but largely constrained to the English-French Canadian Hansard-could be found on the Web: We can grow our own parallel corpus using the many Web pages that exist in parallel in local and in major languages.", "context": ["Rada Mihalcea and Dan Moldovan (1999) used hit counts for carefully constructed search engine queries to identify rank orders for word sense frequencies, as an input to a word sense disambiguation engine.", "Philip Resnik (1999) showed that parallel corpora-until then a promising research avenue but largely constrained to the English-French Canadian Hansard-could be found on the Web: We can grow our own parallel corpus using the many Web pages that exist in parallel in local and in major languages.", "We are glad to have the further development of this work (co-authored by Noah Smith) presented in this special issue."], "vector_1": {"corporauntil": 1, "corpu": 1, "major": 1, "identifi": 1, "noah": 1, "show": 1, "queri": 1, "special": 1, "rank": 1, "avenu": 1, "exist": 1, "mani": 1, "coauthor": 1, "languag": 1, "web": 2, "use": 2, "engin": 2, "rada": 1, "smith": 1, "construct": 1, "research": 1, "disambigu": 1, "moldovan": 1, "input": 1, "grow": 1, "local": 1, "glad": 1, "hit": 1, "dan": 1, "larg": 1, "promis": 1, "englishfrench": 1, "sens": 2, "develop": 1, "philip": 1, "parallel": 3, "mihalcea": 1, "care": 1, "count": 1, "search": 1, "word": 2, "resnik": 1, "canadian": 1, "frequenc": 1, "work": 1, "page": 1, "issu": 1, "hansardcould": 1, "present": 1, "found": 1, "constrain": 1, "order": 1}, "marker": "(1999)", "article": "J03-3001", "vector_2": [4, 0.18249246892641066, 2, 1, 0, 0]}, {"label": "Neut", "current": "In the student session of ACL 2000, Rosie Jones and Rayid Ghani (2001) showed how, using the Web, one can build a language-specific corpus from a single document in that language.", "context": ["We are glad to have the further development of this work (co-authored by Noah Smith) presented in this special issue.", "In the student session of ACL 2000, Rosie Jones and Rayid Ghani (2001) showed how, using the Web, one can build a language-specific corpus from a single document in that language.", "In the main session Atsushi Fujii and Tetsuya Ishikawa (2000) demonstrated that descriptive, definition-like collections can be acquired from the Web."], "vector_1": {"corpu": 1, "noah": 1, "show": 1, "definitionlik": 1, "one": 1, "fujii": 1, "session": 2, "glad": 1, "rayid": 1, "demonstr": 1, "special": 1, "web": 2, "use": 1, "develop": 1, "rosi": 1, "atsushi": 1, "smith": 1, "coauthor": 1, "build": 1, "jone": 1, "languag": 1, "document": 1, "singl": 1, "ghani": 1, "languagespecif": 1, "ishikawa": 1, "student": 1, "present": 1, "acquir": 1, "main": 1, "tetsuya": 1, "work": 1, "descript": 1, "acl": 1, "issu": 1, "collect": 1}, "marker": "(2001)", "article": "J03-3001", "vector_2": [2, 0.19317520188340126, 2, 1, 4, 1]}, {"label": "Neut", "current": "In the main session Atsushi Fujii and Tetsuya Ishikawa (2000) demonstrated that descriptive, definition-like collections can be acquired from the Web.", "context": ["In the student session of ACL 2000, Rosie Jones and Rayid Ghani (2001) showed how, using the Web, one can build a language-specific corpus from a single document in that language.", "In the main session Atsushi Fujii and Tetsuya Ishikawa (2000) demonstrated that descriptive, definition-like collections can be acquired from the Web.", "2.1 Some Current Themes"], "vector_1": {"corpu": 1, "show": 1, "one": 1, "fujii": 1, "session": 2, "rayid": 1, "languag": 1, "web": 2, "use": 1, "rosi": 1, "definitionlik": 1, "current": 1, "theme": 1, "build": 1, "jone": 1, "document": 1, "singl": 1, "ghani": 1, "languagespecif": 1, "ishikawa": 1, "student": 1, "demonstr": 1, "acquir": 1, "main": 1, "tetsuya": 1, "descript": 1, "acl": 1, "atsushi": 1, "collect": 1}, "marker": "(2000)", "article": "J03-3001", "vector_2": [3, 0.19568134065767157, 2, 1, 0, 0]}, {"label": "Neut", "current": "Lawrence and Giles (1999) compared the overlap between page lists returned by different Web browsers over the same set of queries and estimated that, in 1999, there were 800 million indexable Web pages available.", "context": ["Linguistic aspects take a little more work and can be estimated only by sampling and extrapolation.", "Lawrence and Giles (1999) compared the overlap between page lists returned by different Web browsers over the same set of queries and estimated that, in 1999, there were 800 million indexable Web pages available.", "By sampling pages, and estimating an average page length of seven to eight kilobytes of nonmarkup text, they concluded that there might be six terabytes of text available then."], "vector_1": {"seven": 1, "extrapol": 1, "queri": 1, "conclud": 1, "nonmarkup": 1, "set": 1, "sampl": 2, "overlap": 1, "aspect": 1, "web": 2, "differ": 1, "compar": 1, "six": 1, "littl": 1, "avail": 2, "estim": 3, "take": 1, "lawrenc": 1, "terabyt": 1, "might": 1, "kilobyt": 1, "return": 1, "index": 1, "gile": 1, "million": 1, "eight": 1, "averag": 1, "work": 1, "list": 1, "length": 1, "text": 2, "linguist": 1, "page": 4, "browser": 1}, "marker": "(1999)", "article": "J03-3001", "vector_2": [4, 0.364098929195251, 1, 1, 0, 0]}, {"label": "Neut", "current": "This hidden Web is vast (consider MedLine,8 just one such database, with more than five billion words; see also Ipeirotis, Gravano, and Sahami [2001]), and it is not considered at all in the AltaVista estimates.", "context": ["AltaVista indexes only pages that can be directly called by a URL and does not index text found in databases that are accessible through dialog windows on Web pages (the \"hidden Web\").", "This hidden Web is vast (consider MedLine,8 just one such database, with more than five billion words; see also Ipeirotis, Gravano, and Sahami [2001]), and it is not considered at all in the AltaVista estimates.", "Repeating the procedure after an interval, the second author and Nioche showed that the proportion of non-English text to English is growing."], "vector_1": {"directli": 1, "show": 1, "text": 2, "interv": 1, "procedur": 1, "one": 1, "nonenglish": 1, "see": 1, "proport": 1, "index": 2, "second": 1, "gravano": 1, "databas": 2, "access": 1, "also": 1, "window": 1, "estim": 1, "call": 1, "ipeiroti": 1, "hidden": 2, "nioch": 1, "repeat": 1, "sahami": 1, "web": 3, "five": 1, "altavista": 2, "consid": 2, "grow": 1, "vast": 1, "billion": 1, "word": 1, "url": 1, "author": 1, "medlin": 1, "dialog": 1, "english": 1, "found": 1, "page": 2}, "marker": "[2001]", "article": "J03-3001", "vector_2": [2, 0.5451484697364757, 1, 1, 4, 1]}, {"label": "Neut", "current": "Atkins, Clear, and Ostler (1992) describe the desiderata and criteria used for the BNC, and this stands as a good model for a general-purpose, general-language corpus.", "context": ["To date, corpus developers have been obliged to make pragmatic decisions about the sorts of text to go into a corpus.", "Atkins, Clear, and Ostler (1992) describe the desiderata and criteria used for the BNC, and this stands as a good model for a general-purpose, general-language corpus.", "The word representative has tended to fall out of discussions, to be replaced by the meeker balanced."], "vector_1": {"corpu": 3, "pragmat": 1, "text": 1, "repres": 1, "ostler": 1, "desiderata": 1, "go": 1, "use": 1, "develop": 1, "describ": 1, "make": 1, "tend": 1, "decis": 1, "criteria": 1, "bnc": 1, "balanc": 1, "sort": 1, "atkin": 1, "good": 1, "generalpurpos": 1, "fall": 1, "date": 1, "discuss": 1, "word": 1, "replac": 1, "clear": 1, "stand": 1, "model": 1, "oblig": 1, "meeker": 1, "generallanguag": 1}, "marker": "(1992)", "article": "J03-3001", "vector_2": [11, 0.7595119358023441, 1, 1, 0, 0]}, {"label": "Pos", "current": "The recent history of mathematically sophisticated modeling of language variation begins with Biber (1988), who identifies and quantifies the linguistic features associated with different spoken and written text types.", "context": ["Kilgarriff and Grefenstette Web as Corpus: Introduction", "The recent history of mathematically sophisticated modeling of language variation begins with Biber (1988), who identifies and quantifies the linguistic features associated with different spoken and written text types.", "Habert and colleagues (Folch et al."], "vector_1": {"corpu": 1, "featur": 1, "identifi": 1, "spoken": 1, "text": 1, "al": 1, "et": 1, "languag": 1, "web": 1, "differ": 1, "kilgarriff": 1, "biber": 1, "written": 1, "mathemat": 1, "grefenstett": 1, "quantifi": 1, "introduct": 1, "type": 1, "begin": 1, "variat": 1, "habert": 1, "sophist": 1, "folch": 1, "associ": 1, "recent": 1, "colleagu": 1, "histori": 1, "model": 1, "linguist": 1}, "marker": "(1988)", "article": "J03-3001", "vector_2": [15, 0.7696124344986457, 1, 1, 0, 0]}, {"label": "Pos", "current": "In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.", "context": ["2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others.", "In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.", "As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000)."], "vector_1": {"among": 1, "work": 1, "directli": 1, "nlp": 1, "text": 2, "continu": 1, "one": 1, "pass": 1, "et": 1, "cavaglia": 1, "use": 1, "develop": 1, "kilgarriff": 1, "system": 1, "subcorpora": 1, "other": 1, "roland": 1, "sekin": 1, "quantifi": 1, "analys": 1, "accord": 1, "relat": 1, "specifi": 1, "al": 1, "mention": 1, "address": 1, "gildea": 1, "line": 1, "workstat": 1, "present": 1, "biberstyl": 1, "type": 2, "corpora": 1, "item": 1, "similar": 1, "first": 1}, "marker": "(2001)", "article": "J03-3001", "vector_2": [2, 0.7751816317748019, 5, 1, 1, 0]}, {"label": "Neut", "current": "As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).", "context": ["In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.", "As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).", "Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation."], "vector_1": {"nlp": 1, "domain": 1, "corpora": 1, "sacaleanu": 1, "directli": 1, "text": 1, "continu": 1, "one": 1, "explor": 1, "pass": 1, "et": 1, "gildea": 1, "kilgarriff": 1, "system": 1, "cavaglia": 1, "roland": 1, "sekin": 1, "quantifi": 1, "type": 1, "relat": 2, "al": 1, "mention": 1, "address": 1, "line": 1, "buitelaar": 1, "present": 1, "work": 1, "item": 1, "disambigu": 1, "sens": 1, "similar": 1, "first": 1}, "marker": "(2000)", "article": "J03-3001", "vector_2": [3, 0.7800167075918285, 6, 1, 0, 0]}, {"label": "Neut", "current": "Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.", "context": ["As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).", "Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.", "A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain."], "vector_1": {"nlp": 1, "domain": 2, "sacaleanu": 1, "directli": 1, "text": 1, "address": 1, "al": 1, "one": 1, "explor": 1, "et": 1, "concern": 1, "gildea": 1, "system": 1, "technic": 1, "vossen": 1, "disambigu": 1, "sekin": 1, "roland": 1, "type": 1, "resourc": 1, "relat": 2, "mention": 1, "tailor": 1, "sens": 1, "buitelaar": 1, "discuss": 1, "central": 1, "practic": 1, "item": 1, "generallanguag": 1}, "marker": "(2001)", "article": "J03-3001", "vector_2": [2, 0.782421588233805, 5, 1, 0, 0]}, {"label": "Neut", "current": "Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures.", "context": ["A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain.", "Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures.", "His models have been developed with a specific, descriptive goal in mind and using a small number of short texts: It is unclear whether they can be usefully applied in NLP."], "vector_1": {"nlp": 1, "domain": 1, "unclear": 1, "text": 1, "mind": 1, "number": 1, "concern": 1, "use": 2, "develop": 1, "goal": 1, "technic": 1, "vossen": 1, "mathemat": 1, "baayen": 1, "appli": 1, "sublanguag": 1, "resourc": 1, "distribut": 1, "mixtur": 2, "sophist": 1, "tailor": 1, "word": 1, "discuss": 1, "present": 1, "practic": 1, "short": 1, "central": 1, "like": 1, "specif": 1, "whether": 1, "frequenc": 1, "descript": 1, "potenti": 1, "small": 1, "model": 4, "generallanguag": 1}, "marker": "(2001)", "article": "J03-3001", "vector_2": [2, 0.7892565121636331, 2, 1, 0, 0]}, {"label": "Neut", "current": "Exceptions focusing on genre include Kessler, Nunberg, and Schutze (1997) and Karlgren and Cutting (1994).", "context": ["Also, the focus is usually on content words and topics or domains, with other differences of genre or sublanguage remaining unexamined.", "Exceptions focusing on genre include Kessler, Nunberg, and Schutze (1997) and Karlgren and Cutting (1994).", "4.7 Representativeness: Conclusion"], "vector_1": {"domain": 1, "focus": 1, "repres": 1, "topic": 1, "karlgren": 1, "content": 1, "differ": 1, "cut": 1, "except": 1, "kessler": 1, "also": 1, "includ": 1, "unexamin": 1, "schutz": 1, "conclus": 1, "word": 1, "nunberg": 1, "genr": 2, "focu": 1, "sublanguag": 1, "remain": 1, "usual": 1}, "marker": "(1997)", "article": "J03-3001", "vector_2": [6, 0.8065463382527909, 2, 1, 0, 0]}, {"label": "Neut", "current": "Exceptions focusing on genre include Kessler, Nunberg, and Schutze (1997) and Karlgren and Cutting (1994).", "context": ["Also, the focus is usually on content words and topics or domains, with other differences of genre or sublanguage remaining unexamined.", "Exceptions focusing on genre include Kessler, Nunberg, and Schutze (1997) and Karlgren and Cutting (1994).", "4.7 Representativeness: Conclusion"], "vector_1": {"domain": 1, "focus": 1, "repres": 1, "topic": 1, "karlgren": 1, "content": 1, "differ": 1, "cut": 1, "except": 1, "kessler": 1, "also": 1, "includ": 1, "unexamin": 1, "schutz": 1, "conclus": 1, "word": 1, "nunberg": 1, "genr": 2, "focu": 1, "sublanguag": 1, "remain": 1, "usual": 1}, "marker": "(1994)", "article": "J03-3001", "vector_2": [9, 0.8065463382527909, 2, 1, 0, 0]}, {"label": "Neut", "current": "(Shaw et al., 2011) propose a set of algorithms to create a reverse dictionary in the context of single language by using converse mapping.", "context": ["They organize dictionaries in a graph topology and use random walks and probabilistic graph sampling.", "(Shaw et al., 2011) propose a set of algorithms to create a reverse dictionary in the context of single language by using converse mapping.", "In particular, given an English-English dictionary, they attempt to find the original words or terms given a synonymous word or phrase describing the meaning of a word."], "vector_1": {"origin": 1, "set": 1, "creat": 1, "random": 1, "walk": 1, "sampl": 1, "phrase": 1, "find": 1, "languag": 1, "use": 2, "synonym": 1, "describ": 1, "graph": 2, "englishenglish": 1, "topolog": 1, "singl": 1, "map": 1, "given": 2, "probabilist": 1, "dictionari": 3, "particular": 1, "word": 3, "convers": 1, "term": 1, "attempt": 1, "organ": 1, "algorithm": 1, "revers": 1, "context": 1, "mean": 1, "propos": 1}, "marker": "(Shaw et al., 2011)", "article": "N13-1057", "vector_2": [2, 0.13280545899317625, 1, 1, 0, 0]}, {"label": "Neut", "current": "The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.", "context": ["Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank.", "The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.", "While often ignored, the grammar constant |G |typically dominates the runtime in practice."], "vector_1": {"constitu": 1, "constant": 1, "often": 1, "weight": 1, "cfg": 1, "treebank": 1, "parser": 1, "find": 1, "dynam": 1, "use": 1, "tree": 1, "program": 1, "syntact": 1, "sentenc": 1, "time": 1, "given": 1, "g": 1, "ogn": 1, "pars": 1, "domin": 1, "practic": 1, "grammar": 2, "like": 1, "algorithm": 1, "contextfre": 1, "cki": 1, "n": 1, "employ": 1, "length": 1, "ignor": 1, "runtim": 1, "learn": 1, "typic": 1}, "marker": "(Cocke and Schwartz, 1970", "article": "W11-2921", "vector_2": [41, 0.04133616063247729, 3, 1, 0, 0]}, {"label": "Neut", "current": "The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.", "context": ["Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank.", "The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.", "While often ignored, the grammar constant |G |typically dominates the runtime in practice."], "vector_1": {"constitu": 1, "constant": 1, "often": 1, "weight": 1, "cfg": 1, "treebank": 1, "parser": 1, "find": 1, "dynam": 1, "use": 1, "tree": 1, "program": 1, "syntact": 1, "sentenc": 1, "time": 1, "given": 1, "g": 1, "ogn": 1, "pars": 1, "domin": 1, "practic": 1, "grammar": 2, "like": 1, "algorithm": 1, "contextfre": 1, "cki": 1, "n": 1, "employ": 1, "length": 1, "ignor": 1, "runtim": 1, "learn": 1, "typic": 1}, "marker": "Younger, 1967)", "article": "W11-2921", "vector_2": [44, 0.04133616063247729, 3, 1, 0, 0]}, {"label": "Neut", "current": "This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.", "context": ["While often ignored, the grammar constant |G |typically dominates the runtime in practice.", "This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.", "Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006)."], "vector_1": {"constant": 1, "often": 1, "process": 1, "converg": 1, "number": 1, "high": 1, "somewher": 1, "second": 1, "ghz": 1, "year": 1, "comput": 2, "nontermin": 1, "clock": 1, "doubl": 1, "system": 1, "accuraci": 1, "symbol": 1, "era": 1, "core": 1, "everi": 1, "around": 1, "sentenc": 1, "thousand": 1, "million": 1, "word": 1, "domin": 1, "averag": 1, "grammar": 2, "practic": 1, "g": 1, "contextfre": 1, "frequenc": 1, "rule": 1, "n": 1, "meanwhil": 1, "ignor": 1, "runtim": 1, "enter": 1, "typic": 1, "manycor": 1}, "marker": "(Collins, 1999", "article": "W11-2921", "vector_2": [12, 0.048299693709259406, 4, 1, 0, 0]}, {"label": "Neut", "current": "This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.", "context": ["While often ignored, the grammar constant |G |typically dominates the runtime in practice.", "This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.", "Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006)."], "vector_1": {"constant": 1, "often": 1, "process": 1, "converg": 1, "number": 1, "high": 1, "somewher": 1, "second": 1, "ghz": 1, "year": 1, "comput": 2, "nontermin": 1, "clock": 1, "doubl": 1, "system": 1, "accuraci": 1, "symbol": 1, "era": 1, "core": 1, "everi": 1, "around": 1, "sentenc": 1, "thousand": 1, "million": 1, "word": 1, "domin": 1, "averag": 1, "grammar": 2, "practic": 1, "g": 1, "contextfre": 1, "frequenc": 1, "rule": 1, "n": 1, "meanwhil": 1, "ignor": 1, "runtim": 1, "enter": 1, "typic": 1, "manycor": 1}, "marker": "Petrov et al., 2006)", "article": "W11-2921", "vector_2": [5, 0.048299693709259406, 4, 4, 4, 1]}, {"label": "Neut", "current": "We first present an overview of the general architecture of GPUs and the efficient synchronization provided by the Compute Unified Device Architecture (CUDA (Nickolls et al., 2008)) programming model (Section 3).", "context": ["Berkeley Parser) on a Graphics Processor Unit (GPU).", "We first present an overview of the general architecture of GPUs and the efficient synchronization provided by the Compute Unified Device Architecture (CUDA (Nickolls et al., 2008)) programming model (Section 3).", "We then discuss how the hundreds of cores available on a GPU can enable a fine-grained parallel execution of the CKY algorithm."], "vector_1": {"effici": 1, "execut": 1, "overview": 1, "parser": 1, "cki": 1, "finegrain": 1, "comput": 1, "unit": 1, "section": 1, "devic": 1, "parallel": 1, "avail": 1, "program": 1, "gpu": 3, "hundr": 1, "architectur": 2, "core": 1, "gener": 1, "berkeley": 1, "discuss": 1, "present": 1, "graphic": 1, "enabl": 1, "algorithm": 1, "provid": 1, "synchron": 1, "unifi": 1, "cuda": 1, "model": 1, "processor": 1, "first": 1}, "marker": "(Nickolls et al., 2008)", "article": "W11-2921", "vector_2": [3, 0.07047304903269719, 1, 3, 0, 0]}, {"label": "Neut", "current": "Using the lazy evaluation algorithm of Huang and Chiang (2005) the extrac", "context": ["We found this to be more efficient than keeping backpointers.2 One should also note that many real-world applications benefit from, or even expect n-best lists of possible parse trees.", "Using the lazy evaluation algorithm of Huang and Chiang (2005) the extrac", "1For feature-rich discriminative models a trivially parallelizable pass can be used to pre-compute the rule-potentials."], "vector_1": {"effici": 1, "evalu": 1, "list": 1, "one": 1, "expect": 1, "pass": 1, "trivial": 1, "even": 1, "use": 2, "for": 1, "backpoint": 1, "precomput": 1, "paralleliz": 1, "featurerich": 1, "note": 1, "also": 1, "applic": 1, "realworld": 1, "lazi": 1, "pars": 1, "huang": 1, "found": 1, "nbest": 1, "extrac": 1, "rulepotenti": 1, "algorithm": 1, "possibl": 1, "tree": 1, "keep": 1, "discrimin": 1, "benefit": 1, "chiang": 1, "mani": 1, "model": 1}, "marker": "(2005)", "article": "W11-2921", "vector_2": [6, 0.1804759286892327, 1, 1, 0, 0]}, {"label": "Neut", "current": "The GTX480 is the Fermi architecture (NVIDIA, 2009), with many features added to the GTX285.", "context": ["However, placing the rule information in texture memory improves the performance little as there are many more accesses to the scores array than to the rule information.", "The GTX480 is the Fermi architecture (NVIDIA, 2009), with many features added to the GTX285.", "The number of cores doubled from 240 to 480, but the number of SMs was halved from 30 to 15."], "vector_1": {"featur": 1, "ad": 1, "number": 2, "array": 1, "memori": 1, "perform": 1, "doubl": 1, "littl": 1, "access": 1, "score": 1, "gtx": 2, "fermi": 1, "architectur": 1, "core": 1, "textur": 1, "halv": 1, "improv": 1, "howev": 1, "rule": 2, "inform": 2, "place": 1, "sm": 1, "mani": 2}, "marker": "(NVIDIA, 2009)", "article": "W11-2921", "vector_2": [2, 0.8295766904892798, 1, 1, 0, 0]}, {"label": "Neut", "current": "A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "context": ["6 Related Work", "A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization."], "vector_1": {"finegrain": 1, "none": 1, "relat": 2, "last": 1, "compar": 1, "directli": 1, "possibl": 1, "provid": 1, "natur": 1, "howev": 1, "work": 3, "parser": 1, "two": 1, "bodi": 1, "accumul": 1, "much": 1, "substanti": 1, "gpu": 1, "decad": 1, "parallel": 2, "languag": 1}, "marker": "Manousopoulou et al., 1997)", "article": "W11-2921", "vector_2": [14, 0.8736092567868269, 4, 3, 0, 0]}, {"label": "Neut", "current": "Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "context": ["We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.", "Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).", "Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed."], "vector_1": {"often": 1, "process": 1, "natur": 1, "parser": 2, "focus": 1, "finegrain": 1, "exploit": 1, "result": 1, "massiv": 1, "languag": 1, "differ": 1, "anoth": 1, "priorit": 1, "intermedi": 1, "sentenc": 1, "previou": 1, "speedup": 1, "pars": 1, "inher": 1, "parallel": 2, "agendabas": 2, "refin": 1, "work": 1, "iter": 1, "order": 1, "queue": 1, "achiev": 1, "maintain": 1, "combin": 1, "whole": 1, "magnitud": 1}, "marker": "Pontelli et al., 1998", "article": "W11-2921", "vector_2": [13, 0.8912013403492238, 4, 2, 0, 0]}, {"label": "Neut", "current": "Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).", "context": ["It should be noted that there are a also number of orthogonal approaches for accelerating natural language parsers.", "Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).", "These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest."], "vector_1": {"acceler": 1, "often": 1, "orthogon": 1, "natur": 1, "approxim": 1, "parser": 1, "number": 1, "languag": 1, "use": 1, "space": 1, "note": 1, "also": 1, "reli": 1, "interest": 2, "appli": 1, "approach": 2, "final": 1, "pars": 1, "constrain": 1, "search": 1, "grammar": 1, "prune": 1, "possibl": 1, "tree": 1, "coars": 2, "model": 2}, "marker": "Charniak and Johnson, 2005", "article": "W11-2921", "vector_2": [6, 0.9485065054059006, 3, 1, 1, 0]}, {"label": "Neut", "current": "There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.", "context": ["Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements.", "There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.", "We suspect that most of the ideas therein are orthogonal to our approach, and therefore leave their integration into our GPU-based parser for future work."], "vector_1": {"therein": 1, "orthogon": 1, "parser": 1, "idea": 1, "speed": 2, "optim": 2, "techniqu": 1, "refactor": 1, "also": 1, "suspect": 1, "futur": 1, "leav": 1, "gpubas": 1, "therefor": 1, "approach": 4, "infer": 1, "asearch": 1, "integr": 1, "preserv": 2, "multipass": 1, "base": 1, "addit": 1, "grammar": 1, "could": 1, "work": 1, "cki": 1, "yield": 1, "aim": 1, "combin": 1, "contrast": 1, "improv": 1, "principl": 1}, "marker": "Pauls and Klein, 2009)", "article": "W11-2921", "vector_2": [2, 0.9600251315479463, 3, 1, 7, 0]}, {"label": "Neut", "current": "There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.", "context": ["Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements.", "There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.", "We suspect that most of the ideas therein are orthogonal to our approach, and therefore leave their integration into our GPU-based parser for future work."], "vector_1": {"therein": 1, "orthogon": 1, "parser": 1, "idea": 1, "speed": 2, "optim": 2, "techniqu": 1, "refactor": 1, "also": 1, "suspect": 1, "futur": 1, "leav": 1, "gpubas": 1, "therefor": 1, "approach": 4, "infer": 1, "asearch": 1, "integr": 1, "preserv": 2, "multipass": 1, "base": 1, "addit": 1, "grammar": 1, "could": 1, "work": 1, "cki": 1, "yield": 1, "aim": 1, "combin": 1, "contrast": 1, "improv": 1, "principl": 1}, "marker": "(Dunlop et al., 2011)", "article": "W11-2921", "vector_2": [0, 0.9600251315479463, 3, 2, 0, 0]}, {"label": "Neut", "current": "In recent years, many linguistic resources have been released as Linked Data (Chiarcos et al., 2011).", "context": ["1 Introduction", "In recent years, many linguistic resources have been released as Linked Data (Chiarcos et al., 2011).", "Most of the datasets that are part of the so called Linguistic Linked Open Data (LLOD) cloud consist of dictionaries, written corpora or lexica."], "vector_1": {"resourc": 1, "llod": 1, "consist": 1, "call": 1, "open": 1, "corpora": 1, "dataset": 1, "written": 1, "part": 1, "link": 2, "lexica": 1, "dictionari": 1, "year": 1, "releas": 1, "mani": 1, "introduct": 1, "linguist": 2, "data": 2, "cloud": 1, "recent": 1}, "marker": "(Chiarcos et al., 2011)", "article": "W13-5507", "vector_2": [2, 0.035559125260121935, 1, 1, 1, 0]}, {"label": "Neut", "current": "Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).", "context": ["2.", "Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).", "3."], "vector_1": {"negra": 1, "corpu": 1, "use": 1, "stanford": 1, "german": 1, "sentenc": 1, "train": 1, "parser": 1, "version": 1, "pars": 1, "utter": 1, "segment": 1}, "marker": "Klein and Manning, 2003)", "article": "W13-5507", "vector_2": [10, 0.3097002665108977, 3, 1, 2, 0]}, {"label": "Pos", "current": "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).", "context": ["3.1 Internal representation", "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).", "There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures."], "vector_1": {"represent": 1, "tei": 1, "among": 1, "intern": 1, "facil": 1, "shortcom": 1, "develop": 1, "spatiotempor": 1, "graph": 1, "xce": 1, "data": 1, "complex": 1, "speech": 1, "take": 1, "nite": 1, "approach": 3, "difficult": 1, "express": 1, "variou": 1, "format": 1, "acronym": 1, "object": 1, "fiesta": 1, "standard": 1, "extens": 1, "multimod": 1, "transcript": 1, "account": 1, "made": 1, "specif": 1, "annot": 2, "structur": 1, "p": 1, "model": 1}, "marker": "(Bird and Liberman, 2001)", "article": "W13-5507", "vector_2": [12, 0.3696104559891935, 4, 1, 0, 0]}, {"label": "Neut", "current": "Finding bilingual collocations [Smadja, 1992].", "context": ["Extracting word correspondences [Gale and Church, 1991a].", "Finding bilingual collocations [Smadja, 1992].", "Estimating parameters for statistically-based machine translation [Brown et al., 1992]."], "vector_1": {"machin": 1, "colloc": 1, "word": 1, "correspond": 1, "statisticallybas": 1, "estim": 1, "translat": 1, "bilingu": 1, "extract": 1, "find": 1, "paramet": 1}, "marker": "Smadja, 1992]", "article": "P93-1003", "vector_2": [1, 0.059260491158046456, 3, 10, 0, 0]}, {"label": "Neut", "current": "Estimating parameters for statistically-based machine translation [Brown et al., 1992].", "context": ["Finding bilingual collocations [Smadja, 1992].", "Estimating parameters for statistically-based machine translation [Brown et al., 1992].", "The work described here makes use of the aligned Canadian Hansards [Gale and Church, 1991b] to obtain noun phrase correspondences between the English and French text."], "vector_1": {"text": 1, "obtain": 1, "phrase": 1, "find": 1, "paramet": 1, "colloc": 1, "describ": 1, "make": 1, "estim": 1, "machin": 1, "bilingu": 1, "use": 1, "statisticallybas": 1, "french": 1, "noun": 1, "hansard": 1, "canadian": 1, "align": 1, "work": 1, "correspond": 1, "english": 1, "translat": 1}, "marker": "Brown et al., 1992]", "article": "P93-1003", "vector_2": [1, 0.06197682798381285, 3, 10, 0, 0]}, {"label": "Pos", "current": "The work described here makes use of the aligned Canadian Hansards [Gale and Church, 1991b] to obtain noun phrase correspondences between the English and French text.", "context": ["Estimating parameters for statistically-based machine translation [Brown et al., 1992].", "The work described here makes use of the aligned Canadian Hansards [Gale and Church, 1991b] to obtain noun phrase correspondences between the English and French text.", "The term \"correspondence\" is used here to signify a mapping between words in two aligned sentences."], "vector_1": {"text": 1, "obtain": 1, "signifi": 1, "phrase": 1, "paramet": 1, "use": 2, "describ": 1, "make": 1, "two": 1, "estim": 1, "sentenc": 1, "machin": 1, "map": 1, "word": 1, "statisticallybas": 1, "french": 1, "noun": 1, "term": 1, "hansard": 1, "canadian": 1, "align": 2, "work": 1, "correspond": 2, "english": 1, "translat": 1}, "marker": "Gale and Church, 1991b]", "article": "P93-1003", "vector_2": [2, 0.06580187371805532, 2, 2, 4, 0]}, {"label": "Neut", "current": "Single word correspondences have been investigated [Gale and Church, 1991a] using a statistic operating on contingency tables.", "context": ["A word sequence in Ei is defined here as the correspondence of another sequence in Fi if the words of one sequence are considered to represent the words in the other.", "Single word correspondences have been investigated [Gale and Church, 1991a] using a statistic operating on contingency tables.", "An algorithm for producing collocational correspondences has also been described [Smadja, 1992]."], "vector_1": {"oper": 1, "ei": 1, "one": 1, "tabl": 1, "conting": 1, "use": 1, "describ": 1, "anoth": 1, "also": 1, "singl": 1, "investig": 1, "sequenc": 3, "colloc": 1, "consid": 1, "fi": 1, "repres": 1, "word": 4, "algorithm": 1, "correspond": 3, "defin": 1, "statist": 1, "produc": 1}, "marker": "Gale and Church, 1991a]", "article": "P93-1003", "vector_2": [2, 0.0920228394035146, 2, 4, 2, 0]}, {"label": "Neut", "current": "In contrast to reservations that have been expressed [Gale and Church, 1991a] about using the EM algorithm to provide word correspondences, there have been no indications that prohibitive amounts of memory might be required, or that the approach lacks robustness.", "context": ["The algorithm described here is an instance of a general approach to statistical estimation, represented by the EM algorithm [Dempster et al., 1977].", "In contrast to reservations that have been expressed [Gale and Church, 1991a] about using the EM algorithm to provide word correspondences, there have been no indications that prohibitive amounts of memory might be required, or that the approach lacks robustness.", "Unlike the other methods that have been mentioned, the approach has the capability to accommodate more context to improve performance."], "vector_1": {"em": 2, "prohibit": 1, "lack": 1, "indic": 1, "repres": 1, "unlik": 1, "reserv": 1, "use": 1, "describ": 1, "memori": 1, "perform": 1, "estim": 1, "might": 1, "method": 1, "contrast": 1, "gener": 1, "express": 1, "mention": 1, "robust": 1, "requir": 1, "word": 1, "algorithm": 3, "capabl": 1, "provid": 1, "approach": 3, "correspond": 1, "accommod": 1, "amount": 1, "instanc": 1, "statist": 1, "context": 1, "improv": 1}, "marker": "Gale and Church, 1991a]", "article": "P93-1003", "vector_2": [2, 0.47508176728200013, 2, 4, 2, 0]}, {"label": "Pos", "current": "In this work we develop a segmentation model from the constraints suggested by Yang (2004) and evaluate it in idealized conditions and conditions that better approximate the environment of a child learner.", "context": ["Computational models present an opportunity to test the potentially innate constraints, structures, and algorithms that a child may be using to guide her acquisition.", "In this work we develop a segmentation model from the constraints suggested by Yang (2004) and evaluate it in idealized conditions and conditions that better approximate the environment of a child learner.", "We seek to determine how these limitations in the learner's input and memory affect the learner's performance and to demonstrate that the presented learner is robust even under non-ideal conditions."], "vector_1": {"comput": 1, "evalu": 1, "approxim": 1, "acquisit": 1, "robust": 1, "opportun": 1, "guid": 1, "seek": 1, "even": 1, "use": 1, "develop": 1, "memori": 1, "perform": 1, "suggest": 1, "learner": 4, "limit": 1, "better": 1, "ideal": 1, "demonstr": 1, "test": 1, "condit": 3, "determin": 1, "innat": 1, "may": 1, "child": 2, "affect": 1, "segment": 1, "present": 2, "nonid": 1, "input": 1, "algorithm": 1, "constraint": 2, "work": 1, "structur": 1, "yang": 1, "environ": 1, "potenti": 1, "model": 2}, "marker": "(2004)", "article": "W10-2912", "vector_2": [6, 0.04016957815929523, 1, 9, 0, 0]}, {"label": "Neut", "current": "Most recent work in word segmentation of childdirected speech has operated within statistical optimization frameworks, particularly Bayesian approaches (Goldwater et al., 2009; Johnson and Goldwater, 2009).", "context": ["2 Related Work", "Most recent work in word segmentation of childdirected speech has operated within statistical optimization frameworks, particularly Bayesian approaches (Goldwater et al., 2009; Johnson and Goldwater, 2009).", "These models have established the state-of-the-art for the task of selecting appropriate word boundaries from a stream of unstructured phonemes."], "vector_1": {"oper": 1, "stream": 1, "within": 1, "particularli": 1, "phonem": 1, "establish": 1, "select": 1, "boundari": 1, "speech": 1, "approach": 1, "statist": 1, "optim": 1, "relat": 1, "framework": 1, "task": 1, "bayesian": 1, "segment": 1, "recent": 1, "appropri": 1, "word": 2, "unstructur": 1, "work": 2, "childdirect": 1, "stateoftheart": 1, "model": 1}, "marker": "Johnson and Goldwater, 2009)", "article": "W10-2912", "vector_2": [1, 0.05914188640668691, 2, 2, 2, 0]}, {"label": "Neut", "current": "Children are incremental learners (Brown, 1973), and learners relying on statistical optimization are generally not.", "context": ["Trying to find cognitive insight from these types of models is difficult because of the inherent mismatch in the quality and types of hypotheses they maintain during learning.", "Children are incremental learners (Brown, 1973), and learners relying on statistical optimization are generally not.", "A child's competence grows gradually as she hears and produces more and more utterances, going through predictable changes to her working grammar (Marcus et al., 1992) that statistical optimization techniques typically do not go through and do not intend to replicate."], "vector_1": {"predict": 1, "gradual": 1, "insight": 1, "qualiti": 1, "increment": 1, "go": 2, "utter": 1, "cognit": 1, "children": 1, "techniqu": 1, "learner": 2, "find": 1, "reli": 1, "compet": 1, "type": 2, "inher": 1, "difficult": 1, "intend": 1, "optim": 2, "gener": 1, "mismatch": 1, "hear": 1, "child": 1, "grow": 1, "tri": 1, "replic": 1, "grammar": 1, "chang": 1, "hypothes": 1, "work": 1, "maintain": 1, "statist": 2, "learn": 1, "model": 1, "typic": 1, "produc": 1}, "marker": "(Brown, 1973)", "article": "W10-2912", "vector_2": [37, 0.07089810276917526, 2, 2, 0, 0]}, {"label": "Pos", "current": "of Brent & Cartwright (1996) produces a set of possible lexicons that describe the learning corpus, each of which is evaluated as the learner iterates until no further improvement is possible.", "context": ["Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 88-97, Uppsala, Sweden, 15-16 July 2010. c2010 Association for Computational Linguistics", "of Brent & Cartwright (1996) produces a set of possible lexicons that describe the learning corpus, each of which is evaluated as the learner iterates until no further improvement is possible.", "It is unlikely that an algorithm of this type is something a human learner is capable of using given the requirement to remember at the very least a long history of recent utterances encountered and constantly reanalyze them to find a optimal segmentation."], "vector_1": {"corpu": 1, "encount": 1, "lexicon": 1, "comput": 2, "human": 1, "evalu": 1, "natur": 1, "cartwright": 1, "unlik": 1, "set": 1, "long": 1, "fourteenth": 1, "utter": 1, "histori": 1, "find": 1, "brent": 1, "use": 1, "proceed": 1, "uppsala": 1, "segment": 1, "someth": 1, "learner": 2, "least": 1, "languag": 1, "rememb": 1, "recent": 1, "type": 1, "optim": 1, "juli": 1, "confer": 1, "capabl": 1, "given": 1, "associ": 1, "requir": 1, "describ": 1, "c": 1, "constantli": 1, "algorithm": 1, "possibl": 2, "iter": 1, "page": 1, "sweden": 1, "learn": 2, "improv": 1, "linguist": 1, "produc": 1, "reanalyz": 1}, "marker": "(1996)", "article": "W10-2912", "vector_2": [14, 0.11061667518566404, 1, 1, 1, 0]}, {"label": "Neut", "current": "Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.", "context": ["3 Constraining the Learning Space", "Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.", "If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand."], "vector_1": {"effici": 1, "modern": 1, "fail": 1, "equip": 1, "children": 2, "space": 2, "suggest": 1, "research": 1, "machin": 1, "knowledg": 1, "hand": 1, "succeed": 1, "model": 1, "like": 1, "task": 2, "essenti": 1, "algorithm": 1, "specif": 2, "constraint": 2, "realist": 1, "learn": 6, "domainneutr": 1, "constrain": 1}, "marker": "Valiant, 1984", "article": "W10-2912", "vector_2": [26, 0.2615833308277459, 3, 1, 0, 0]}, {"label": "Neut", "current": "This assumption is plausible given that 7.5-month-old infants are sensitive to strong/weak prosodic distinctions (Jusczyk, 1999).", "context": ["If the learner knows this, then it may limit the search for local minima to only the window between two syllables that both bear primary stress, e.g., between the two a's in the sequence languageacquisition.", "This assumption is plausible given that 7.5-month-old infants are sensitive to strong/weak prosodic distinctions (Jusczyk, 1999).", "Yang's stress-delimited algorithm achieves the precision of 73.5% and recall of 71.2%, a significant improvement over using TPs alone, but still below the baseline presented in our results."], "vector_1": {"minima": 1, "infant": 1, "eg": 1, "stressdelimit": 1, "signific": 1, "result": 1, "still": 1, "primari": 1, "baselin": 1, "precis": 1, "given": 1, "sensit": 1, "learner": 1, "two": 2, "tp": 1, "window": 1, "local": 1, "assumpt": 1, "recal": 1, "languageacquisit": 1, "may": 1, "sequenc": 1, "use": 1, "stress": 1, "bear": 1, "monthold": 1, "know": 1, "plausibl": 1, "present": 1, "search": 1, "algorithm": 1, "prosod": 1, "strongweak": 1, "alon": 1, "syllabl": 1, "distinct": 1, "achiev": 1, "yang": 1, "limit": 1, "improv": 1}, "marker": "(Jusczyk, 1999)", "article": "W10-2912", "vector_2": [11, 0.32727983402988664, 1, 2, 1, 0]}, {"label": "Neut", "current": "We later discuss a manipulation of the corpus used by Yang (2004) to address this concern.", "context": ["Stress is better organized into hierarchical patterns constructed on top of syllables that vary in relative prominence based on the domain of each level of the hierarchy, and generally languages avoid adjacent strong syllables (Liberman and Prince, 1977).", "We later discuss a manipulation of the corpus used by Yang (2004) to address this concern.", "Additionally, there are significant challenges in reconstructing stress from an acoustic signal (Van Kuijk and Boves, 1999)."], "vector_1": {"corpu": 1, "domain": 1, "challeng": 1, "signific": 1, "hierarchi": 1, "vari": 1, "languag": 1, "concern": 1, "hierarch": 1, "use": 1, "pattern": 1, "top": 1, "construct": 1, "better": 1, "adjac": 1, "rel": 1, "promin": 1, "gener": 1, "manipul": 1, "base": 1, "address": 1, "reconstruct": 1, "strong": 1, "discuss": 1, "addit": 1, "stress": 2, "acoust": 1, "organ": 1, "level": 1, "signal": 1, "later": 1, "syllabl": 2, "yang": 1, "avoid": 1}, "marker": "(2004)", "article": "W10-2912", "vector_2": [6, 0.35434017859827416, 3, 9, 0, 0]}, {"label": "Neut", "current": "While recent work from Bayesian approaches has used a Dirichlet Process to generate these distributions (Goldwater et al., 2006), in this learner the reuse of frequent items in learning is a result of the memory model rather than an explicit process of reusing old outcomes or generating new ones.", "context": ["Probabilistic word recall results in a \"rich get richer\" phenomenon as the learner segments; words that are used more often in segmentations are more likely to be reused in later segmentations.", "While recent work from Bayesian approaches has used a Dirichlet Process to generate these distributions (Goldwater et al., 2006), in this learner the reuse of frequent items in learning is a result of the memory model rather than an explicit process of reusing old outcomes or generating new ones.", "This growth is an inherent property of the cognitive model of memory used here rather than an externally imposed computational technique."], "vector_1": {"later": 1, "old": 1, "phenomenon": 1, "process": 2, "one": 1, "extern": 1, "growth": 1, "result": 2, "comput": 1, "cognit": 1, "often": 1, "use": 3, "techniqu": 1, "memori": 2, "rather": 2, "learner": 2, "rich": 1, "new": 1, "properti": 1, "approach": 1, "recal": 1, "distribut": 1, "get": 1, "gener": 2, "probabilist": 1, "inher": 1, "bayesian": 1, "segment": 3, "impos": 1, "recent": 1, "dirichlet": 1, "word": 2, "like": 1, "work": 1, "explicit": 1, "richer": 1, "item": 1, "reus": 3, "outcom": 1, "learn": 1, "model": 2, "frequent": 1}, "marker": "(Goldwater et al., 2006)", "article": "W10-2912", "vector_2": [4, 0.565922006073544, 1, 1, 3, 0]}, {"label": "Pos", "current": "The corpus we use to evaluate it is the same corpus used by Yang (2004).", "context": ["Our computational model is designed to process child-directed speech.", "The corpus we use to evaluate it is the same corpus used by Yang (2004).", "Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah."], "vector_1": {"corpu": 3, "comput": 1, "evalu": 1, "process": 1, "design": 1, "utter": 1, "extract": 1, "children": 1, "sarah": 1, "use": 2, "three": 1, "speech": 1, "brown": 1, "adult": 1, "child": 1, "data": 2, "consist": 1, "eve": 1, "childdirect": 1, "yang": 1, "adam": 1, "model": 1}, "marker": "(2004)", "article": "W10-2912", "vector_2": [6, 0.5811058660813614, 3, 9, 0, 0]}, {"label": "Neut", "current": "We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word.", "context": ["Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.", "We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word.", "In CMUdict, lexical stress information is preserved by numbers: 0 for unstressed, 1 for primary stress, 2 for secondary stress."], "vector_1": {"corpu": 1, "obtain": 1, "phonet": 1, "utter": 1, "extract": 1, "children": 1, "sarah": 1, "use": 1, "carnegi": 1, "pronounc": 1, "transcript": 1, "three": 1, "primari": 1, "version": 1, "cmudict": 2, "brown": 1, "mellon": 1, "preserv": 1, "lexic": 1, "number": 1, "adult": 1, "dictionari": 1, "child": 1, "pronunci": 1, "secondari": 1, "data": 2, "unstress": 1, "stress": 3, "word": 2, "consist": 1, "eve": 1, "inform": 1, "adam": 1, "first": 1}, "marker": "(Weide, 1998)", "article": "W10-2912", "vector_2": [12, 0.5864878679455185, 3, 1, 0, 0]}, {"label": "Neut", "current": "In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).", "context": ["Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.", "In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).", "However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation."], "vector_1": {"corpu": 1, "featur": 1, "process": 1, "decreas": 1, "loglinear": 1, "cw": 2, "sever": 1, "leverag": 1, "accur": 1, "develop": 1, "semisupervis": 1, "rather": 1, "data": 2, "smt": 1, "figur": 1, "translat": 1, "previous": 1, "method": 1, "unlabel": 1, "machin": 1, "perplex": 1, "cascad": 1, "standard": 1, "train": 1, "multilevel": 1, "vise": 1, "segment": 2, "word": 1, "howev": 1, "manual": 1, "focu": 1, "structur": 1, "caus": 1, "achiev": 1, "statist": 1, "bilingu": 2, "model": 2, "align": 1, "fact": 1, "propos": 1}, "marker": "(Chang et al., 2008)", "article": "D15-1142", "vector_2": [7, 0.1979655444052471, 5, 2, 0, 0]}, {"label": "Pos", "current": "Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et", "context": ["The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003).", "Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et", "1208"], "vector_1": {"featur": 1, "design": 1, "et": 1, "supervis": 1, "use": 1, "system": 1, "label": 1, "treat": 1, "approach": 1, "asahara": 1, "linear": 1, "sequenc": 1, "previou": 1, "address": 1, "segment": 1, "model": 1, "care": 1, "task": 1, "word": 1, "statist": 1, "popular": 1, "problem": 1, "first": 1, "propos": 1}, "marker": "(Peng et al., 2004)", "article": "D15-1142", "vector_2": [11, 0.2451964242901668, 2, 3, 2, 0]}, {"label": "Neut", "current": "al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)).", "context": ["1208", "al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)).", "However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce."], "vector_1": {"produc": 1, "amount": 1, "howev": 1, "al": 1, "timeconsum": 1, "label": 1, "shortcom": 1, "reli": 1, "larg": 1, "data": 1, "heavili": 1, "approach": 1, "primari": 1, "expens": 1}, "marker": "(Zhang and Clark, 2007)", "article": "D15-1142", "vector_2": [8, 0.2513614412439634, 2, 2, 0, 0]}, {"label": "Neut", "current": "al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)).", "context": ["1208", "al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)).", "However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce."], "vector_1": {"produc": 1, "amount": 1, "howev": 1, "al": 1, "timeconsum": 1, "label": 1, "shortcom": 1, "reli": 1, "larg": 1, "data": 1, "heavili": 1, "approach": 1, "primari": 1, "expens": 1}, "marker": "(Zhao et al., 2010)", "article": "D15-1142", "vector_2": [5, 0.2513614412439634, 2, 2, 0, 0]}, {"label": "Neut", "current": "The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.", "context": ["3.1 Character-level Feature", "The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.", "In this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003)."], "vector_1": {"featur": 2, "random": 1, "tag": 1, "manner": 1, "cw": 2, "use": 1, "detect": 1, "field": 1, "score": 1, "treat": 1, "condit": 1, "sequenc": 1, "effect": 1, "oov": 1, "problem": 1, "demonstr": 1, "task": 2, "word": 1, "defin": 1, "paper": 1, "crf": 1, "model": 2, "characterlevel": 2, "first": 1}, "marker": "(Lafferty et al., 2001)", "article": "D15-1142", "vector_2": [14, 0.3490769599616399, 3, 2, 3, 0]}, {"label": "Neut", "current": "The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.", "context": ["3.1 Character-level Feature", "The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.", "In this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003)."], "vector_1": {"featur": 2, "random": 1, "tag": 1, "manner": 1, "cw": 2, "use": 1, "detect": 1, "field": 1, "score": 1, "treat": 1, "condit": 1, "sequenc": 1, "effect": 1, "oov": 1, "problem": 1, "demonstr": 1, "task": 2, "word": 1, "defin": 1, "paper": 1, "crf": 1, "model": 2, "characterlevel": 2, "first": 1}, "marker": "(Xue et al., 2003)", "article": "D15-1142", "vector_2": [12, 0.3490769599616399, 3, 4, 0, 0]}, {"label": "Neut", "current": "We do not introduce the CRF-based CWS model in detail here, but more information can be obtained from (Lafferty et al., 2001) and (Xue et al., 2003).", "context": ["yj1 and yj represent the tags of the previous and current characters, respectively.", "We do not introduce the CRF-based CWS model in detail here, but more information can be obtained from (Lafferty et al., 2001) and (Xue et al., 2003).", "3.2 Phrase-level Features"], "vector_1": {"yj": 2, "crfbase": 1, "phraselevel": 1, "previou": 1, "obtain": 1, "detail": 1, "charact": 1, "current": 1, "inform": 1, "tag": 1, "respect": 1, "model": 1, "repres": 1, "cw": 1, "featur": 1, "introduc": 1}, "marker": "(Lafferty et al., 2001)", "article": "D15-1142", "vector_2": [14, 0.36729801006952767, 2, 2, 3, 0]}, {"label": "Neut", "current": "The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary.", "context": ["Specifically, we apply two standard log-linear phrase-based SMT models.", "The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary.", "The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables."], "vector_1": {"growdiagfinaland": 1, "appli": 1, "tabl": 1, "obtain": 1, "two": 1, "heurist": 1, "loglinear": 1, "phrase": 1, "extract": 1, "use": 1, "smt": 1, "ne": 1, "reorder": 1, "strategi": 1, "standard": 1, "bidirect": 1, "translat": 1, "dictionari": 1, "convert": 1, "word": 1, "specif": 1, "align": 3, "adopt": 1, "phrasebas": 1, "giza": 1, "combin": 1, "model": 1}, "marker": "(Och and Ney, 2000)", "article": "D15-1142", "vector_2": [15, 0.540637736753776, 2, 1, 6, 0]}, {"label": "Neut", "current": "A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language.", "context": ["The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables.", "A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language.", "Moses (Koehn et al., 2007) is used as a decoder."], "vector_1": {"heurist": 1, "srilm": 1, "tabl": 1, "phrase": 1, "extract": 1, "languag": 2, "use": 3, "decod": 1, "reorder": 1, "strategi": 1, "mose": 1, "train": 1, "bidirect": 1, "translat": 1, "target": 1, "align": 1, "growdiagfinaland": 1, "smooth": 1, "combin": 1, "gram": 1, "model": 1, "kneserney": 1}, "marker": "(Stolcke et al., 2002)", "article": "D15-1142", "vector_2": [13, 0.546049251635442, 3, 1, 0, 0]}, {"label": "Neut", "current": "Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset.", "context": ["Moses (Koehn et al., 2007) is used as a decoder.", "Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset.", "Given these two phrase-based translation models, we calculate each span < i, jm, jn > in AOne for the Chinese word wn using the following formula: Str(< i, jm, jn >) = Schen(< i, jm, jn >) +Sench(< i, jm, jn >) (6) where Schen(<i,jm,jn>) = DLev(Fletei, PTchen(Fpy(cjn jm))) means that the pronunciation conversion in the Chinese-English direction is performed as follows: First, the English word ei is split into its constituent letters; Second, the sequence of Chinese characters cjn jm is converted into its pronunciation; Third, this pronunciation is input into the Chinese-English phrase-based translation model, and the corresponding translation result is obtained; And finally, the Levenshtein distance between the English letters and the translation result is returned."], "vector_1": {"featur": 1, "ei": 1, "appli": 1, "mert": 1, "direct": 1, "levenshtein": 1, "dataset": 1, "rate": 1, "minimum": 1, "result": 2, "chineseenglish": 2, "follow": 2, "correspond": 1, "paramet": 1, "use": 2, "develop": 1, "perform": 1, "sench": 1, "cjn": 1, "two": 1, "decod": 1, "schenijmjn": 1, "wn": 1, "formula": 1, "final": 1, "split": 1, "mose": 1, "ptchenfpycjn": 1, "sequenc": 1, "return": 1, "aon": 1, "chines": 2, "english": 2, "given": 1, "schen": 1, "obtain": 1, "distanc": 1, "jm": 6, "jn": 4, "train": 1, "translat": 4, "letter": 2, "input": 1, "pronunci": 3, "span": 1, "tune": 1, "convert": 1, "word": 2, "convers": 1, "third": 1, "dlevfletei": 1, "second": 1, "phrasebas": 2, "charact": 1, "constitu": 1, "calcul": 1, "str": 1, "error": 1, "model": 2, "first": 1, "mean": 1}, "marker": "(Och et al., 2003)", "article": "D15-1142", "vector_2": [12, 0.5543377744288797, 2, 2, 2, 0]}, {"label": "Neut", "current": "Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.", "context": ["In the following, we refer to our methods as \"SLBD\" (segmenter leveraging bilingual data).", "Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.", "Moreover, we also evaluated the performance of our sub-models by"], "vector_1": {"evalu": 2, "zc": 1, "follow": 1, "cw": 1, "leverag": 1, "supervis": 1, "perform": 1, "ie": 1, "moreov": 1, "segment": 2, "peng": 1, "also": 1, "method": 2, "refer": 1, "asahara": 1, "whose": 1, "initi": 1, "train": 1, "slbd": 1, "data": 2, "manual": 1, "zhao": 1, "stateoftheart": 1, "bilingu": 1, "model": 1, "submodel": 1}, "marker": "(Asahara et al., 2005)", "article": "D15-1142", "vector_2": [10, 0.8553618522450936, 4, 1, 0, 0]}, {"label": "Neut", "current": "Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.", "context": ["In the following, we refer to our methods as \"SLBD\" (segmenter leveraging bilingual data).", "Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.", "Moreover, we also evaluated the performance of our sub-models by"], "vector_1": {"evalu": 2, "zc": 1, "follow": 1, "cw": 1, "leverag": 1, "supervis": 1, "perform": 1, "ie": 1, "moreov": 1, "segment": 2, "peng": 1, "also": 1, "method": 2, "refer": 1, "asahara": 1, "whose": 1, "initi": 1, "train": 1, "slbd": 1, "data": 2, "manual": 1, "zhao": 1, "stateoftheart": 1, "bilingu": 1, "model": 1, "submodel": 1}, "marker": "(Zhao et al., 2010)", "article": "D15-1142", "vector_2": [5, 0.8553618522450936, 4, 2, 0, 0]}, {"label": "CoCo", "current": "Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).", "context": ["Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model.", "Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).", "To ensure a fair comparison, we performed the evaluation in two steps."], "vector_1": {"featur": 1, "fair": 1, "zeng": 1, "ensur": 1, "monolingu": 1, "loglinear": 2, "sentencelevel": 1, "sever": 1, "use": 1, "outer": 1, "evalu": 1, "compar": 1, "semisupervis": 1, "perform": 1, "sun": 1, "moreov": 1, "two": 1, "next": 1, "inner": 1, "method": 2, "includ": 1, "rerank": 1, "effect": 1, "enhanc": 1, "step": 1, "candid": 1, "therebi": 1, "slbd": 1, "segment": 1, "demonstr": 1, "comparison": 1, "achiev": 1, "stateoftheart": 1, "x": 1, "model": 2, "produc": 1}, "marker": "(Sun and Xu, 2011)", "article": "D15-1142", "vector_2": [4, 0.9019077302462581, 3, 3, 4, 0]}, {"label": "Neut", "current": "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).", "context": ["The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.", "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).", "The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation."], "vector_1": {"xi": 1, "effici": 1, "evalu": 2, "obtain": 1, "indic": 2, "dataset": 1, "monolingu": 2, "decreas": 1, "significantli": 1, "tabl": 1, "leverag": 1, "use": 1, "accur": 1, "develop": 1, "primarili": 1, "semisupervis": 3, "perform": 1, "data": 1, "smt": 2, "highli": 1, "much": 3, "includ": 1, "xu": 1, "method": 5, "unlabel": 2, "rather": 1, "perplex": 1, "optim": 1, "step": 1, "zeng": 1, "slbd": 4, "preferenti": 1, "segment": 3, "demonstr": 2, "comparison": 1, "subsequ": 1, "provid": 1, "outperform": 1, "larger": 1, "stronger": 1, "focu": 1, "final": 1, "caus": 1, "inform": 1, "present": 1, "either": 1, "bilingu": 1, "produc": 1, "result": 2}, "marker": "(Xu et al., 2008)", "article": "D15-1142", "vector_2": [7, 0.9519128677603863, 4, 3, 5, 0]}, {"label": "CoCo", "current": "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).", "context": ["The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.", "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).", "The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation."], "vector_1": {"xi": 1, "effici": 1, "evalu": 2, "obtain": 1, "indic": 2, "dataset": 1, "monolingu": 2, "decreas": 1, "significantli": 1, "tabl": 1, "leverag": 1, "use": 1, "accur": 1, "develop": 1, "primarili": 1, "semisupervis": 3, "perform": 1, "data": 1, "smt": 2, "highli": 1, "much": 3, "includ": 1, "xu": 1, "method": 5, "unlabel": 2, "rather": 1, "perplex": 1, "optim": 1, "step": 1, "zeng": 1, "slbd": 4, "preferenti": 1, "segment": 3, "demonstr": 2, "comparison": 1, "subsequ": 1, "provid": 1, "outperform": 1, "larger": 1, "stronger": 1, "focu": 1, "final": 1, "caus": 1, "inform": 1, "present": 1, "either": 1, "bilingu": 1, "produc": 1, "result": 2}, "marker": "(Ma and Way, 2009)", "article": "D15-1142", "vector_2": [6, 0.9519128677603863, 4, 3, 0, 0]}, {"label": "Neut", "current": "Consequentlv, a significant amount of work has been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998).", "context": ["Machine transliteration is also useful for cross-language information retrieval (CLIR).", "Consequentlv, a significant amount of work has been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998).", "Machine transliteration can generallv be classified as rule-based or statistical depending on the approach."], "vector_1": {"machin": 2, "field": 1, "use": 1, "generallv": 1, "retriev": 1, "rulebas": 1, "inform": 1, "work": 1, "amount": 1, "classifi": 1, "also": 1, "signific": 1, "consequentlv": 1, "done": 1, "statist": 1, "transliter": 2, "clir": 1, "approach": 1, "crosslanguag": 1, "depend": 1}, "marker": "(Arbabi et al., 1994", "article": "W14-5502", "vector_2": [20, 0.18310956330477018, 2, 1, 0, 0]}, {"label": "Neut", "current": "Consequentlv, a significant amount of work has been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998).", "context": ["Machine transliteration is also useful for cross-language information retrieval (CLIR).", "Consequentlv, a significant amount of work has been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998).", "Machine transliteration can generallv be classified as rule-based or statistical depending on the approach."], "vector_1": {"machin": 2, "field": 1, "use": 1, "generallv": 1, "retriev": 1, "rulebas": 1, "inform": 1, "work": 1, "amount": 1, "classifi": 1, "also": 1, "signific": 1, "consequentlv": 1, "done": 1, "statist": 1, "transliter": 2, "clir": 1, "approach": 1, "crosslanguag": 1, "depend": 1}, "marker": "Knight and Graehl, 1998)", "article": "W14-5502", "vector_2": [16, 0.18310956330477018, 2, 1, 0, 0]}, {"label": "Neut", "current": "Similarlv, Kishorjit (2011) developed a rule-based transliteration svstem also based on direct graphemic correspondence between Meetei Mavek and Bengali script.", "context": ["It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script.", "Similarlv, Kishorjit (2011) developed a rule-based transliteration svstem also based on direct graphemic correspondence between Meetei Mavek and Bengali script.", "On the other hand, statistical machine transliteration svstems tvpicallv use procedures familiar from statistical machine translation, including character alignments and subsequent training on the aligned data."], "vector_1": {"tvpicallv": 1, "graphem": 2, "direct": 1, "procedur": 1, "meetei": 1, "use": 1, "develop": 1, "equival": 1, "script": 3, "similarlv": 1, "also": 1, "includ": 1, "translat": 1, "subsequ": 1, "machin": 2, "svstem": 2, "familiar": 1, "hand": 1, "base": 2, "mavek": 1, "transliter": 2, "devanagari": 1, "data": 1, "persoarab": 1, "kishorjit": 1, "align": 2, "rulebas": 1, "correspond": 1, "charact": 1, "train": 1, "statist": 2, "bengali": 1}, "marker": "(2011)", "article": "W14-5502", "vector_2": [3, 0.21197818815011685, 1, 1, 0, 0]}, {"label": "Neut", "current": "Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration.", "context": ["Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM).", "Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration.", "A hvbrid approach combining FSM based techniques with a statistical word language model with better performance was proposed bv Malik et al (2009)."], "vector_1": {"evalu": 1, "englishhindi": 1, "al": 2, "chinnakotla": 1, "englishkannada": 1, "meetei": 1, "et": 2, "languag": 2, "csm": 1, "use": 1, "techniqu": 1, "script": 1, "similarlv": 1, "three": 1, "better": 1, "hvbrid": 1, "singh": 1, "approach": 1, "method": 1, "englishtamil": 1, "sequenc": 1, "tool": 1, "focus": 1, "bv": 1, "base": 1, "bidirect": 1, "bengali": 1, "transliter": 1, "pair": 1, "word": 1, "finetun": 1, "rulebas": 1, "fsm": 1, "charact": 1, "perform": 1, "combin": 1, "statist": 2, "malik": 1, "model": 2, "propos": 1, "mavek": 1}, "marker": "(2012)", "article": "W14-5502", "vector_2": [2, 0.2423589790587912, 3, 1, 0, 0]}, {"label": "Pos", "current": "A hvbrid approach combining FSM based techniques with a statistical word language model with better performance was proposed bv Malik et al (2009).", "context": ["Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration.", "A hvbrid approach combining FSM based techniques with a statistical word language model with better performance was proposed bv Malik et al (2009).", "3 Initial Attempts"], "vector_1": {"evalu": 1, "al": 1, "et": 1, "meetei": 1, "languag": 1, "techniqu": 1, "script": 1, "perform": 1, "better": 1, "hvbrid": 1, "singh": 1, "approach": 1, "method": 1, "initi": 1, "bv": 1, "base": 1, "bidirect": 1, "mavek": 1, "transliter": 1, "attempt": 1, "word": 1, "rulebas": 1, "fsm": 1, "combin": 1, "statist": 2, "malik": 1, "model": 1, "propos": 1, "bengali": 1}, "marker": "(2009)", "article": "W14-5502", "vector_2": [5, 0.25083627365623423, 2, 1, 2, 0]}, {"label": "Neut", "current": "Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile those rules into finite-state transducers compatible with Openfst.", "context": ["The implementation of the framework has been done entirelv using OpenFst (Allauzen et al., 2007).", "Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile those rules into finite-state transducers compatible with Openfst.", "Thrax was also used to define finite state acceptors."], "vector_1": {"compil": 1, "compat": 1, "use": 3, "transduc": 1, "defin": 2, "also": 1, "finit": 1, "rule": 2, "finitest": 1, "framework": 1, "state": 1, "done": 1, "entirelv": 1, "openfst": 2, "rewrit": 1, "implement": 1, "thrax": 2, "contextdepend": 1, "acceptor": 1}, "marker": "(Tai et al., 2011)", "article": "W14-5502", "vector_2": [3, 0.35063923383586126, 2, 1, 3, 0]}, {"label": "Pos", "current": "Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).", "context": ["A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle.", "Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).", "Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years."], "vector_1": {"represent": 1, "rang": 1, "explor": 1, "approxim": 1, "monoton": 1, "semant": 2, "confound": 1, "particularli": 1, "confirm": 1, "impoverish": 1, "year": 1, "rabi": 1, "extract": 1, "deepbutbrittl": 1, "involv": 1, "use": 1, "acquir": 1, "p": 1, "broad": 1, "reli": 1, "overlap": 1, "patternbas": 1, "imprecis": 1, "approach": 2, "method": 1, "match": 1, "polar": 1, "infect": 1, "relat": 1, "effect": 1, "lexic": 1, "spectrum": 1, "fairli": 1, "broadli": 1, "robust": 1, "case": 1, "measur": 1, "shallowbutrobust": 1, "success": 1, "indigen": 1, "ubiquitu": 1, "past": 1, "predicateargu": 1, "structur": 1, "infer": 1, "easili": 1, "context": 1, "neg": 1}, "marker": "(Romano et al., 2006)", "article": "W07-1431", "vector_2": [1, 0.04954738718968591, 3, 1, 1, 0]}, {"label": "Neut", "current": "For this purpose, we have chosen to use the Stanford RTE system described in (de Marneffe et al., 2006).", "context": ["If we succeed, then we may be able to hybridize with a broad-coverage RTE system to obtain better results than either system individually-the same strategy that was adopted by (Bos and Markert, 2006) for their FOL-based system.", "For this purpose, we have chosen to use the Stanford RTE system described in (de Marneffe et al., 2006).", "In applying NatLog to RTE problems, we use alignments from the Stanford system as input to our entailment model."], "vector_1": {"rte": 3, "entail": 1, "hybrid": 1, "obtain": 1, "result": 1, "individuallyth": 1, "use": 2, "describ": 1, "chosen": 1, "abl": 1, "system": 5, "better": 1, "appli": 1, "input": 1, "folbas": 1, "strategi": 1, "natlog": 1, "may": 1, "succeed": 1, "model": 1, "stanford": 2, "align": 1, "adopt": 1, "broadcoverag": 1, "either": 1, "problem": 1, "purpos": 1}, "marker": "(de Marneffe et al., 2006)", "article": "W07-1431", "vector_2": [1, 0.7885406665752297, 2, 2, 1, 0]}, {"label": "Neut", "current": "A small current of theoretical work has continued up to the present, for example (Zamansky et al., 2006).", "context": ["tonicity (Sanchez Valencia, 1991).", "A small current of theoretical work has continued up to the present, for example (Zamansky et al., 2006).", "There has been surprisingly little work on building computational models of natural logic."], "vector_1": {"tonic": 1, "comput": 1, "natur": 1, "work": 2, "continu": 1, "theoret": 1, "current": 1, "exampl": 1, "littl": 1, "logic": 1, "small": 1, "model": 1, "surprisingli": 1, "present": 1, "build": 1}, "marker": "(Zamansky et al., 2006)", "article": "W07-1431", "vector_2": [1, 0.9488753257440681, 2, 1, 1, 0]}, {"label": "Neut", "current": "(Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell.", "context": ["There has been surprisingly little work on building computational models of natural logic.", "(Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell.", "Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985)."], "vector_1": {"represent": 1, "comput": 1, "natur": 2, "parser": 1, "haskel": 1, "close": 1, "surprisingli": 1, "languag": 1, "describ": 2, "littl": 1, "categori": 1, "also": 1, "draft": 1, "build": 1, "advoc": 1, "infer": 1, "preliminari": 1, "fragment": 1, "logic": 1, "unpublish": 1, "base": 1, "model": 1, "grammar": 1, "work": 1, "jerri": 1, "hobb": 1, "english": 1, "small": 1, "implement": 2, "prolog": 1}, "marker": "(van Eijck, 2005)", "article": "W07-1431", "vector_2": [2, 0.9536071869428062, 3, 1, 0, 0]}, {"label": "Neut", "current": "(Sukkarieh, 2003) describes applying a deductive system to some FraCaS inferences, but does not perform a complete evaluation or report quantitative results.", "context": ["To our knowledge, the FraCaS results reported here represent the first such evaluation.", "(Sukkarieh, 2003) describes applying a deductive system to some FraCaS inferences, but does not perform a complete evaluation or report quantitative results.", "7 Conclusion"], "vector_1": {"deduct": 1, "fraca": 2, "describ": 1, "evalu": 2, "perform": 1, "appli": 1, "knowledg": 1, "system": 1, "quantit": 1, "repres": 1, "result": 2, "report": 2, "complet": 1, "infer": 1, "conclus": 1, "first": 1}, "marker": "(Sukkarieh, 2003)", "article": "W07-1431", "vector_2": [4, 0.96581401728158, 1, 1, 0, 0]}, {"label": "Pos", "current": "Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003).", "context": ["These statistics emphasize the key role of the lexicalized head word feature in capturing the collocation between verbs and their arguments.", "Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003).", "For example, \"7 26 /July 26\" may not be seen in the training, but its POS, NT(temporal noun) , is a good indicator that it is a temporal."], "vector_1": {"featur": 2, "key": 1, "captur": 1, "train": 1, "al": 1, "indic": 1, "seen": 1, "follow": 1, "et": 1, "use": 1, "nttempor": 1, "due": 1, "also": 1, "role": 1, "po": 1, "head": 3, "good": 1, "may": 1, "emphas": 1, "juli": 1, "colloc": 1, "lexic": 1, "argument": 1, "verb": 1, "partofspeech": 1, "noun": 1, "surdeanu": 1, "sparsiti": 1, "word": 3, "tempor": 1, "exampl": 1, "statist": 1}, "marker": "(2003)", "article": "N04-1032", "vector_2": [1, 0.4150986994124249, 1, 1, 0, 0]}, {"label": "Neut", "current": "In this section we describe experiments on semantic parsing when given automatic parses produced by an automatic parser, the Collins (1999) parser, ported to Chinese.", "context": ["In practical use, of course, automatic parses will not be as accurate.", "In this section we describe experiments on semantic parsing when given automatic parses produced by an automatic parser, the Collins (1999) parser, ported to Chinese.", "We first describe how we ported the Collins parser to Chinese and then present the results of the semantic parser with features drawn from the automatic parses."], "vector_1": {"chines": 2, "use": 1, "accur": 1, "practic": 1, "given": 1, "section": 1, "parser": 4, "semant": 2, "automat": 4, "port": 2, "first": 1, "cours": 1, "pars": 4, "result": 1, "collin": 2, "drawn": 1, "experi": 1, "featur": 1, "produc": 1, "present": 1, "describ": 2}, "marker": "(1999)", "article": "N04-1032", "vector_2": [5, 0.5778371954842543, 1, 4, 0, 0]}, {"label": "Pos", "current": "The Collins parser is a state-of-the-art statistical parser that has high performance on English (Collins, 1999) and Czech(Collins et al.", "context": ["4.1 The Collins parser for Chinese", "The Collins parser is a state-of-the-art statistical parser that has high performance on English (Collins, 1999) and Czech(Collins et al.", "1999)."], "vector_1": {"czechcollin": 1, "perform": 1, "collin": 2, "chines": 1, "parser": 3, "al": 1, "high": 1, "stateoftheart": 1, "english": 1, "et": 1, "statist": 1}, "marker": "(Collins, 1999)", "article": "N04-1032", "vector_2": [5, 0.5898197662903545, 1, 4, 0, 0]}, {"label": "Neut", "current": "Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.", "context": ["Such a large variety of vocabulary naturally leads to long names with productive use of general words, making the task difficult to be solved by systems with naive Markov assumption of label sequences, because such systems must perform their prediction without seeing the entire string of the entities.", "Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.", "After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000)."], "vector_1": {"entropi": 1, "predict": 1, "natur": 1, "naiv": 1, "global": 1, "share": 2, "assumpt": 1, "indic": 1, "see": 1, "attain": 1, "seem": 1, "research": 1, "improv": 1, "best": 1, "entiti": 2, "use": 3, "lead": 1, "perform": 3, "make": 2, "enabl": 1, "particip": 1, "system": 4, "long": 2, "label": 2, "field": 1, "score": 1, "larg": 1, "treatment": 1, "postprocess": 1, "varieti": 1, "import": 1, "tackl": 1, "might": 1, "difficult": 1, "product": 1, "random": 1, "string": 1, "optim": 1, "sequenc": 1, "gener": 1, "jnlpba": 1, "like": 1, "memm": 1, "extens": 1, "entir": 1, "promis": 1, "vocabulari": 1, "condit": 1, "solv": 1, "implicitli": 1, "model": 2, "must": 1, "comparison": 1, "task": 4, "word": 1, "name": 2, "local": 1, "maximum": 1, "inform": 1, "without": 1, "crf": 1, "mani": 1, "markov": 2}, "marker": "(Zhou and Su, 2004)", "article": "W07-1033", "vector_2": [3, 0.0606204495093384, 4, 5, 0, 0]}, {"label": "Pos", "current": "After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).", "context": ["Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.", "After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).", "However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al."], "vector_1": {"entropi": 1, "tackl": 1, "global": 1, "share": 3, "al": 1, "indic": 1, "attain": 1, "fail": 1, "et": 1, "seem": 1, "research": 1, "improv": 1, "best": 2, "entiti": 1, "use": 2, "develop": 1, "perform": 3, "make": 1, "particip": 1, "system": 3, "long": 1, "label": 1, "field": 1, "score": 1, "treatment": 1, "postprocess": 1, "import": 1, "might": 1, "random": 1, "zhou": 1, "optim": 1, "reach": 1, "jnlpba": 1, "like": 1, "memm": 1, "extens": 1, "promis": 1, "condit": 1, "implicitli": 1, "markov": 1, "enabl": 1, "comparison": 1, "task": 4, "name": 1, "local": 1, "howev": 1, "maximum": 1, "inform": 1, "achiev": 1, "crf": 2, "mani": 2, "model": 2}, "marker": "(McCallum et al., 2000)", "article": "W07-1033", "vector_2": [7, 0.07043368154479265, 4, 2, 4, 0]}, {"label": "Neut", "current": "The latter, pipelined systems include a recent study by Krishnan et al (2006), as well as our reranking system.", "context": ["A major drawback of this kind of systems may be heavy computational cost of inference both for training and running the systems, because non-local dependency forces such models to use expensive approximate inference instead of dynamic-programming-based exact inference.", "The latter, pipelined systems include a recent study by Krishnan et al (2006), as well as our reranking system.", "Their method is a two stage model of CRFs, where the second CRF uses the global information of the output of the first CRF."], "vector_1": {"major": 1, "comput": 1, "approxim": 1, "global": 1, "al": 1, "two": 1, "heavi": 1, "second": 1, "cost": 1, "et": 1, "use": 2, "depend": 1, "latter": 1, "system": 4, "forc": 1, "drawback": 1, "includ": 1, "instead": 1, "stage": 1, "infer": 3, "pipelin": 1, "nonloc": 1, "run": 1, "dynamicprogrammingbas": 1, "rerank": 1, "may": 1, "train": 1, "krishnan": 1, "exact": 1, "studi": 1, "recent": 1, "kind": 1, "well": 1, "method": 1, "inform": 1, "crf": 3, "expens": 1, "output": 1, "model": 2, "first": 1}, "marker": "(2006)", "article": "W07-1033", "vector_2": [1, 0.15744697689142134, 1, 1, 0, 0]}, {"label": "Neut", "current": "The training data provided by the shared task consisted of 2000 abstracts of biomedical articles taken from the GENIA corpus version 3 (Ohta et al., 2002), which consists of the MEDLINE abstracts with publication years from 1990 to 1999.", "context": ["This section overviews the task of biomedical named entity recognition as presented in JNLPBA shared task held at COLING 2004, and the systems that were successfully applied to the task.", "The training data provided by the shared task consisted of 2000 abstracts of biomedical articles taken from the GENIA corpus version 3 (Ohta et al., 2002), which consists of the MEDLINE abstracts with publication years from 1990 to 1999.", "The articles are annotated with named-entity BIO tags as an example shown in Table 1."], "vector_1": {"corpu": 1, "namedent": 1, "overview": 1, "abstract": 2, "tabl": 1, "share": 2, "biomed": 2, "annot": 1, "held": 1, "tag": 1, "year": 1, "cole": 1, "entiti": 1, "shown": 1, "section": 1, "system": 1, "articl": 2, "recognit": 1, "version": 1, "consist": 2, "appli": 1, "taken": 1, "public": 1, "bio": 1, "jnlpba": 1, "train": 1, "data": 1, "present": 1, "task": 4, "name": 1, "success": 1, "provid": 1, "medlin": 1, "exampl": 1, "genia": 1}, "marker": "(Ohta et al., 2002)", "article": "W07-1033", "vector_2": [5, 0.21070750237416905, 1, 1, 5, 0]}, {"label": "Neut", "current": "Though it is impossible to observe clear correlation between the performance and classification models or resources used, an important characteristic of the best system by Zhou et al (2004) seems to be extensive use of rule-based post processing they apply to the output of their classifier.", "context": ["The systems use various classification models including CRFs, hidden Markov models (HMMs), support vector machines (SVMs), and MEMMs, with various features and external resources.", "Though it is impossible to observe clear correlation between the performance and classification models or resources used, an important characteristic of the best system by Zhou et al (2004) seems to be extensive use of rule-based post processing they apply to the output of their classifier.", "After the shared task, several researchers tackled the problem using the CRFs and their extensions."], "vector_1": {"classif": 2, "featur": 1, "process": 1, "appli": 1, "share": 1, "al": 1, "correl": 1, "et": 1, "seem": 1, "best": 1, "use": 4, "zhou": 1, "perform": 1, "support": 1, "tackl": 1, "system": 2, "classifi": 1, "research": 1, "includ": 1, "sever": 1, "import": 1, "hidden": 1, "machin": 1, "resourc": 2, "variou": 2, "though": 1, "memm": 1, "problem": 1, "hmm": 1, "task": 1, "extern": 1, "post": 1, "characterist": 1, "model": 3, "svm": 1, "extens": 2, "clear": 1, "rulebas": 1, "observ": 1, "vector": 1, "crf": 2, "output": 1, "markov": 1, "imposs": 1}, "marker": "(2004)", "article": "W07-1033", "vector_2": [3, 0.257201646090535, 1, 5, 0, 0]}, {"label": "Pos", "current": "of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.", "context": ["Table 4: Performance of the reranker.", "of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.", "The information of the previous labels was also quite effective, which indicates that label unigram models (i.e."], "vector_1": {"also": 1, "featur": 1, "unigram": 1, "contribut": 1, "rerank": 1, "perform": 2, "shallow": 1, "previou": 2, "parser": 1, "quit": 1, "indic": 1, "label": 2, "inform": 1, "effect": 1, "larg": 1, "tabl": 1, "studi": 1, "ie": 1, "model": 1}, "marker": "Tzong-Han Tsai et al., 2006)", "article": "W07-1033", "vector_2": [1, 0.7420465337132004, 3, 6, 0, 0]}, {"label": "Neut", "current": "F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF", "context": ["Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.", "F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF", "Table 7: Performance comparison on the test set."], "vector_1": {"gazett": 1, "set": 1, "evalu": 1, "al": 4, "paper": 1, "tagger": 1, "tabl": 2, "et": 4, "best": 1, "25": 1, "zhou": 1, "perform": 1, "friedrich": 1, "postprocess": 2, "test": 1, "okanohara": 1, "method": 1, "fscore": 2, "variou": 1, "rerank": 2, "memm": 1, "hmm": 1, "train": 1, "semicrf": 1, "11": 1, "comparison": 2, "svm": 1, "tsai": 1, "crf": 1, "crfgazett": 1}, "marker": "(2006)", "article": "W07-1033", "vector_2": [1, 0.9417141500474834, 4, 6, 0, 0]}, {"label": "Neut", "current": "We have drawn from the Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) a collection of British newspaper reports on court stories to evaluate our approach.", "context": ["multiword expressions.", "We have drawn from the Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) a collection of British newspaper reports on court stories to evaluate our approach.", "Our experiment shows that it is efficient in identifying MWEs, in particular MWEs of low frequencies."], "vector_1": {"corpu": 1, "newspap": 1, "court": 1, "mwe": 2, "evalu": 1, "show": 1, "frequenc": 1, "express": 1, "meter": 1, "collect": 1, "stori": 1, "effici": 1, "british": 1, "multiword": 1, "particular": 1, "report": 1, "drawn": 1, "identifi": 1, "experi": 1, "approach": 1, "low": 1}, "marker": "(Gaizauskas et al., 2001", "article": "W03-1807", "vector_2": [2, 0.09597078614803874, 2, 2, 3, 0]}, {"label": "Neut", "current": "Such approaches include Dagan and Church's (1994) Termight Tool.", "context": ["In practice, most statistical approaches use linguistic filters to collect candidate MWEs.", "Such approaches include Dagan and Church's (1994) Termight Tool.", "In this tool, they first collect candidate nominal terms with a POS syntactic pattern filter, then use concordances to identify frequently co-occurring multiword units."], "vector_1": {"identifi": 1, "concord": 1, "multiword": 1, "church": 1, "unit": 1, "use": 2, "nomin": 1, "pattern": 1, "includ": 1, "mwe": 1, "approach": 2, "po": 1, "syntact": 1, "dagan": 1, "tool": 2, "termight": 1, "cooccur": 1, "candid": 2, "term": 1, "practic": 1, "filter": 2, "collect": 2, "statist": 1, "linguist": 1, "frequent": 1, "first": 1}, "marker": "(1994)", "article": "W03-1807", "vector_2": [9, 0.12941079927785984, 1, 1, 0, 0]}, {"label": "Neut", "current": "Similarly, Merkel and Andersson (2000) compared frequency-based and entropy based algorithms, each of which was combined with a language filter.", "context": ["In his Xtract system, Smadja (1993) first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance, strength and spread, and then examined concordances of the bi-grams to find longer frequent multiword units.", "Similarly, Merkel and Andersson (2000) compared frequency-based and entropy based algorithms, each of which was combined with a language filter.", "They reported that the entropy-based algorithm produced better results."], "vector_1": {"distanc": 1, "bigram": 1, "concord": 1, "within": 1, "frequencybas": 1, "report": 1, "signific": 1, "result": 1, "entropi": 1, "extract": 1, "find": 1, "smadja": 1, "use": 1, "strength": 1, "score": 1, "compar": 1, "entropybas": 1, "system": 1, "better": 1, "spread": 1, "call": 1, "unit": 1, "multiword": 1, "similarli": 1, "algorithm": 2, "languag": 1, "xtract": 1, "singl": 1, "syntact": 1, "merkel": 1, "cooccur": 1, "base": 1, "examin": 1, "pair": 1, "word": 1, "longer": 1, "consist": 1, "andersson": 1, "structur": 1, "filter": 1, "combin": 1, "statist": 1, "produc": 1, "frequent": 1, "first": 1}, "marker": "(2000)", "article": "W03-1807", "vector_2": [3, 0.14364844903988183, 2, 1, 0, 0]}, {"label": "Neut", "current": "For example, Smadja (1993) suggests a basic characteristic of collocations and multiword units is recurrent, domain-dependent and cohesive lexical clusters.", "context": ["We noticed various possible definitions have been suggested for MWE/MWU.", "For example, Smadja (1993) suggests a basic characteristic of collocations and multiword units is recurrent, domain-dependent and cohesive lexical clusters.", "Sag et el."], "vector_1": {"el": 1, "colloc": 1, "variou": 1, "possibl": 1, "lexic": 1, "suggest": 2, "sag": 1, "cohes": 1, "definit": 1, "notic": 1, "mwemwu": 1, "cluster": 1, "domaindepend": 1, "exampl": 1, "unit": 1, "multiword": 1, "basic": 1, "et": 1, "characterist": 1, "recurr": 1, "smadja": 1}, "marker": "(1993)", "article": "W03-1807", "vector_2": [10, 0.5930986377810602, 1, 2, 0, 0]}, {"label": "Neut", "current": "Biber et al (2003) describe MWEs as lexical bundles, which they go on to define as combinations of words that can be repeated frequently and tend to be used frequently by many different speakers/writers within a register.", "context": ["(2001b) suggest that MWEs can roughly be defined as \"idiosyncratic interpretations that cross word boundaries (or spaces)\".", "Biber et al (2003) describe MWEs as lexical bundles, which they go on to define as combinations of words that can be repeated frequently and tend to be used frequently by many different speakers/writers within a register.", "Although it is not difficult to interpret these deifications in theory, things became much more complicated when we undertook our practical checking of the MWE candidates."], "vector_1": {"within": 1, "al": 1, "speakerswrit": 1, "go": 1, "et": 1, "biber": 1, "check": 1, "use": 1, "describ": 1, "space": 1, "suggest": 1, "boundari": 1, "regist": 1, "cross": 1, "tend": 1, "much": 1, "becam": 1, "mwe": 3, "idiosyncrat": 1, "complic": 1, "undertook": 1, "difficult": 1, "repeat": 1, "bundl": 1, "differ": 1, "lexic": 1, "candid": 1, "deific": 1, "although": 1, "thing": 1, "interpret": 2, "word": 2, "practic": 1, "defin": 2, "combin": 1, "mani": 1, "roughli": 1, "frequent": 2, "theori": 1}, "marker": "(2003)", "article": "W03-1807", "vector_2": [0, 0.6036845560479238, 2, 1, 0, 0]}, {"label": "Neut", "current": "This is consistent with other accounts of text structure for text generation in technical domains, e.g., (McKeown, 1985; Paris, 1993; Kittredge et al., 1991).", "context": ["It will also allow us to determine the text structures appropriate in each genre, a study we are currently undertaking.", "This is consistent with other accounts of text structure for text generation in technical domains, e.g., (McKeown, 1985; Paris, 1993; Kittredge et al., 1991).", "For those cases where the realisation remains under-determined by the task element type, we conducted a finer-grained analysis, by overlaying a genre partition on the undifferentiated data."], "vector_1": {"domain": 1, "text": 3, "eg": 1, "us": 1, "overlay": 1, "current": 1, "also": 1, "conduct": 1, "determin": 1, "type": 1, "partit": 1, "analysi": 1, "undertak": 1, "gener": 1, "finergrain": 1, "task": 1, "realis": 1, "data": 1, "technic": 1, "case": 1, "account": 1, "appropri": 1, "consist": 1, "undifferenti": 1, "genr": 2, "underdetermin": 1, "structur": 2, "element": 1, "remain": 1, "allow": 1, "studi": 1}, "marker": "(McKeown, 1985", "article": "P96-1026", "vector_2": [11, 0.6680476437079234, 3, 1, 0, 0]}, {"label": "Pos", "current": "For the CRF results, we used MALLET's SimpleTagger (McCallum, 2002), with each token encoded with a set of binary features (one for each observed literal, as well as the eight token generalizations).", "context": ["The results for ELIE were generated by the current implementation [http://smi.ucd.ie/aidan/Software.html].", "For the CRF results, we used MALLET's SimpleTagger (McCallum, 2002), with each token encoded with a set of binary features (one for each observed literal, as well as the eight token generalizations).", "Our results in Fig."], "vector_1": {"httpsmiucdieaidansoftwarehtml": 1, "use": 1, "eli": 1, "mallet": 1, "well": 1, "gener": 2, "encod": 1, "liter": 1, "simpletagg": 1, "current": 1, "binari": 1, "token": 2, "set": 1, "crf": 1, "result": 3, "eight": 1, "implement": 1, "one": 1, "featur": 1, "observ": 1, "fig": 1}, "marker": "(McCallum, 2002)", "article": "W06-2204", "vector_2": [4, 0.8167161000836566, 1, 1, 2, 0]}, {"label": "Neut", "current": "The first version annotated with the UD representation was released in 2015 (Nivre et al., 2015)1.", "context": ["The result is the first human-checked largescale gold standard for syntactic dependency annotation of English text.", "The first version annotated with the UD representation was released in 2015 (Nivre et al., 2015)1.", "1http://universaldependencies.github.io/docs/"], "vector_1": {"represent": 1, "depend": 1, "syntact": 1, "gold": 1, "httpuniversaldependenciesgithubiodoc": 1, "text": 1, "annot": 2, "standard": 1, "ud": 1, "humancheck": 1, "result": 1, "version": 1, "english": 1, "releas": 1, "largescal": 1, "first": 2}, "marker": "(Nivre et al., 2015)", "article": "W15-2134", "vector_2": [0, 0.31366449769877797, 1, 2, 11, 0]}, {"label": "Pos", "current": "Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons.", "context": ["However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning.", "Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons.", "In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015)."], "vector_1": {"classif": 1, "featur": 2, "special": 2, "paper": 1, "still": 1, "best": 1, "use": 1, "depend": 1, "perform": 1, "make": 1, "system": 3, "messag": 1, "larg": 1, "includ": 1, "multiword": 1, "semev": 2, "hundr": 1, "treatment": 1, "emoticon": 1, "polar": 1, "resourc": 2, "string": 1, "thousand": 1, "express": 1, "extens": 2, "engin": 1, "inescid": 1, "tune": 1, "like": 1, "task": 1, "look": 1, "howev": 1, "negat": 1, "inde": 1, "present": 1, "stateoftheart": 1, "linguist": 1}, "marker": "(Zhu et al., 2014)", "article": "S15-2109", "vector_2": [1, 0.11707249965607373, 3, 1, 1, 0]}, {"label": "Pos", "current": "We used the corpus of 52 million tweets used in (Owoputi et al., 2013) with the tokenizer described in the same work.", "context": ["The embedding matrix E was trained in unsupervised fashion using the structured skip-gram model, described in Section 2.", "We used the corpus of 52 million tweets used in (Owoputi et al., 2013) with the tokenizer described in the same work.", "The words that occurred less than 40 times in the data were discarded from the vocabulary."], "vector_1": {"skipgram": 1, "corpu": 1, "fashion": 1, "less": 1, "occur": 1, "use": 3, "matrix": 1, "section": 1, "5": 1, "million": 1, "train": 1, "vocabulari": 1, "word": 1, "data": 1, "model": 1, "describ": 2, "e": 1, "unsupervis": 1, "work": 1, "structur": 1, "token": 1, "time": 1, "discard": 1, "embed": 1, "tweet": 1}, "marker": "(Owoputi et al., 2013)", "article": "S15-2109", "vector_2": [2, 0.5770394827349017, 1, 1, 2, 0]}, {"label": "Neut", "current": "In Cross-Language Information Retrieval (CLIR) systems, they play an even more important role as the accuracy of their transliterations is shown to correlate highly with the performance of the CLIR systems (Mandl and Womser-Hacker, 2005, Xu and Weischedel, 2005).", "context": ["Named Entities (NEs) play a critical role in many Natural Language Processing and Information Retrieval (IR) tasks.", "In Cross-Language Information Retrieval (CLIR) systems, they play an even more important role as the accuracy of their transliterations is shown to correlate highly with the performance of the CLIR systems (Mandl and Womser-Hacker, 2005, Xu and Weischedel, 2005).", "Traditional methods for transliterations have not proven to be very effective in CLIR."], "vector_1": {"clir": 3, "process": 1, "natur": 1, "ir": 1, "proven": 1, "system": 2, "critic": 1, "correl": 1, "crosslanguag": 1, "languag": 1, "entiti": 1, "even": 1, "shown": 1, "perform": 1, "ne": 1, "accuraci": 1, "role": 2, "import": 1, "highli": 1, "method": 1, "play": 2, "effect": 1, "transliter": 2, "tradit": 1, "task": 1, "retriev": 2, "name": 1, "inform": 2, "mani": 1}, "marker": "(Mandl and Womser-Hacker, 2005, ", "article": "E09-1091", "vector_2": [4, 0.03704612365063788, 2, 2, 0, 0]}, {"label": "CoCo", "current": "We show that MINT's performance is significantly better than a state of the art method (Klementiev and Roth, 2006).", "context": ["We demonstrate MINT's effectiveness on 4 language pairs involving 5 languages (English, Hindi, Kannada, Russian, and Tamil) from 3 different language families, and its scalability on corpora of vastly different sizes (2,000 to 200,000 articles).", "We show that MINT's performance is significantly better than a state of the art method (Klementiev and Roth, 2006).", "Proceedings of the 12th Conference of the European Chapter of the ACL, pages 799-807, Athens, Greece, 30 March - 3 April 2009. c2009 Association for Computational Linguistics"], "vector_1": {"art": 1, "show": 1, "kannada": 1, "pair": 1, "significantli": 1, "famili": 1, "comput": 1, "mint": 2, "languag": 3, "size": 1, "involv": 1, "differ": 2, "proceed": 1, "perform": 1, "associ": 1, "articl": 1, "better": 1, "0": 1, "state": 1, "hindi": 1, "th": 1, "method": 1, "european": 1, "confer": 1, "march": 1, "vastli": 1, "athen": 1, "00": 1, "effect": 1, "russian": 1, "tamil": 1, "demonstr": 1, "chapter": 1, "c": 1, "scalabl": 1, "corpora": 1, "acl": 1, "april": 1, "english": 1, "greec": 1, "linguist": 1, "page": 1}, "marker": "(Klementiev and Roth, 2006)", "article": "E09-1091", "vector_2": [3, 0.11769241553343614, 1, 5, 0, 0]}, {"label": "Neut", "current": "News stories are typically rich in NEs and therefore, comparable news corpora can be expected to contain NETEs (Klementiev and Roth, 2006; Tao et al., 2006).", "context": ["The ubiquitous availability of comparable news corpora in multiple languages suggests a promising alternative to Machine Transliteration, namely, the mining of Named Entity Transliteration Equivalents (NETEs) from such corpora.", "News stories are typically rich in NEs and therefore, comparable news corpora can be expected to contain NETEs (Klementiev and Roth, 2006; Tao et al., 2006).", "The large quantity and the perpetual availability of news corpora in many of the world's languages, make mining of NETEs a viable alternative to traditional approaches."], "vector_1": {"machin": 1, "nete": 3, "mine": 2, "expect": 1, "equival": 1, "languag": 2, "entiti": 1, "multipl": 1, "compar": 2, "suggest": 1, "make": 1, "ubiquit": 1, "ne": 1, "perpetu": 1, "avail": 2, "stori": 1, "larg": 1, "rich": 1, "therefor": 1, "approach": 1, "altern": 2, "viabl": 1, "promis": 1, "transliter": 2, "news": 4, "tradit": 1, "name": 2, "world": 1, "corpora": 4, "quantiti": 1, "contain": 1, "mani": 1, "typic": 1}, "marker": "(Klementiev and Roth, 2006", "article": "E09-1091", "vector_2": [3, 0.06946586289078929, 2, 5, 0, 0]}, {"label": "CoCo", "current": "In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996).", "context": ["The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman & Oard, 2002; Mandl & Womser-hacker, 2005; Xu &Weischedel, 2005).", "In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996).", "While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs."], "vector_1": {"crosslingu": 1, "clir": 1, "andor": 1, "appli": 1, "focus": 1, "ne": 1, "lexicon": 1, "work": 1, "find": 2, "differ": 1, "compar": 1, "transform": 1, "coverag": 1, "taken": 1, "approach": 1, "recogn": 1, "spell": 1, "variant": 1, "translat": 2, "dictionari": 2, "address": 1, "class": 1, "kind": 1, "word": 1, "equival": 2, "specif": 1, "corpora": 1, "focu": 1, "rule": 2, "transliter": 1, "mt": 1, "monolingu": 1, "limit": 1, "learn": 2, "problem": 2, "order": 1}, "marker": "(Pirkola et al., 2003)", "article": "E09-1091", "vector_2": [6, 0.9095051170615449, 8, 1, 0, 0]}, {"label": "Pos", "current": "We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding (Lafferty et al., 2001).", "context": ["This paper presents an alternative discriminative method for word alignment.", "We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding (Lafferty et al., 2001).", "The inference algo"], "vector_1": {"altern": 1, "use": 1, "global": 1, "word": 1, "decod": 1, "field": 1, "optim": 1, "sequenc": 1, "align": 1, "random": 1, "infer": 1, "discrimin": 1, "paper": 1, "crf": 1, "algo": 1, "allow": 1, "train": 1, "condit": 1, "model": 1, "method": 1, "present": 1}, "marker": "(Lafferty et al., 2001)", "article": "P06-1009", "vector_2": [5, 0.09912516594614834, 1, 1, 1, 0]}, {"label": "CoCo", "current": "We use a similar graphical structure to the directed hidden Markov model (HMM) from GIZA++ (Och and Ney, 2003).", "context": ["Furthermore, the model allows regularisation using a prior over the parameters, a very effective and simple method for limiting over-fitting.", "We use a similar graphical structure to the directed hidden Markov model (HMM) from GIZA++ (Och and Ney, 2003).", "This models one-to-many alignments, where each target word is aligned with zero or more source words."], "vector_1": {"regularis": 1, "direct": 1, "zero": 1, "overfit": 1, "onetomani": 1, "paramet": 1, "use": 2, "hidden": 1, "simpl": 1, "method": 1, "sourc": 1, "effect": 1, "hmm": 1, "model": 3, "graphic": 1, "word": 2, "target": 1, "furthermor": 1, "align": 2, "structur": 1, "giza": 1, "prior": 1, "limit": 1, "allow": 1, "markov": 1, "similar": 1}, "marker": "(Och and Ney, 2003)", "article": "P06-1009", "vector_2": [3, 0.12298737107260782, 1, 6, 2, 0]}, {"label": "Neut", "current": "After reviewing those that were employed by the Automatic Language Processing Advisory Committee (1966) and those that were designed in the early 1990s on behalf of ARPA, a major U.S. funding agency, the author concludes that translation quality cannot be defined in the abstract, mainly because there is no such thing as \"the correct translation,\" and pleads for an evaluation that explicitly takes into account the specific purpose for which a translation has been made.", "context": ["251-263) is actually an exercise in metaevaluation, for the object of her evaluation is not any (group of) MT systems, but rather the criteria in terms of which MT systems are commonly evaluated.", "After reviewing those that were employed by the Automatic Language Processing Advisory Committee (1966) and those that were designed in the early 1990s on behalf of ARPA, a major U.S. funding agency, the author concludes that translation quality cannot be defined in the abstract, mainly because there is no such thing as \"the correct translation,\" and pleads for an evaluation that explicitly takes into account the specific purpose for which a translation has been made.", "Now that I have gone through the individual papers, the obvious question is how much they contribute to the overall goal of the book, i.e., to help build a bridge between MT and translation theory."], "vector_1": {"gone": 1, "major": 1, "help": 1, "evalu": 3, "process": 1, "question": 1, "abstract": 1, "qualiti": 1, "automat": 1, "advisori": 1, "paper": 1, "design": 1, "plead": 1, "agenc": 1, "thing": 1, "mainli": 1, "languag": 1, "explicitli": 1, "group": 1, "obviou": 1, "author": 1, "rather": 1, "much": 1, "review": 1, "arpa": 1, "system": 2, "defin": 1, "metaevalu": 1, "take": 1, "exercis": 1, "criteria": 1, "individu": 1, "correct": 1, "build": 1, "earli": 1, "overal": 1, "contribut": 1, "conclud": 1, "object": 1, "account": 1, "fund": 1, "cannot": 1, "translat": 4, "behalf": 1, "goal": 1, "term": 1, "made": 1, "actual": 1, "specif": 1, "us": 1, "employ": 1, "mt": 3, "s": 1, "bridg": 1, "committe": 1, "ie": 1, "purpos": 1, "book": 1, "commonli": 1, "theori": 1}, "marker": "(1966)", "article": "J98-3009", "vector_2": [32, 0.8352554744525548, 1, 2, 0, 1]}, {"label": "Pos", "current": "We used Moses Experiment Management System (Koehn, 2010) with all default options to build the SMT system.7 Because the common crawl corpus contained English sentences in the Spanish side, we applied an LM-based filter to select only sentence pairs in which the Spanish side was better scored by the Spanish LM than with the English LM, and conversely for the English side.", "context": ["The LM was an interpolation of LMs trained with the target part of the parallel corpora and with the rest of the Booking and Trip Advisor data (last 2 rows of Table 2).", "We used Moses Experiment Management System (Koehn, 2010) with all default options to build the SMT system.7 Because the common crawl corpus contained English sentences in the Spanish side, we applied an LM-based filter to select only sentence pairs in which the Spanish side was better scored by the Spanish LM than with the English LM, and conversely for the English side.", "We conducted supervised sentiment classification experiments for settings a and b of use case I (see Section 2)."], "vector_1": {"corpu": 1, "classif": 1, "smt": 1, "lmbase": 1, "appli": 1, "spanish": 3, "rest": 1, "see": 1, "set": 1, "tabl": 1, "trip": 1, "select": 1, "row": 1, "supervis": 1, "use": 2, "score": 1, "sentiment": 1, "crawl": 1, "section": 1, "lm": 4, "system": 2, "better": 1, "book": 1, "build": 1, "conduct": 1, "experi": 2, "mose": 1, "option": 1, "sentenc": 2, "contain": 1, "train": 1, "advisor": 1, "pair": 1, "data": 1, "parallel": 1, "convers": 1, "case": 1, "b": 1, "last": 1, "target": 1, "english": 3, "default": 1, "corpora": 1, "side": 3, "filter": 1, "part": 1, "common": 1, "interpol": 1, "manag": 1}, "marker": "(Koehn, 2010)", "article": "P15-2128", "vector_2": [5, 0.7084801762114538, 1, 1, 1, 0]}, {"label": "Neut", "current": "Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation (Moens and Steedman, 1988; Passonneau, 1988; Dorr, 1992; Klavans, 1994).", "context": ["Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause.", "Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation (Moens and Steedman, 1988; Passonneau, 1988; Dorr, 1992; Klavans, 1994).", "Researchers have used empirical analysis of corpora to develop linguistically-based numerical indicators that aid in aspectual classification (Klavans and Chodorow, 1992; Siegel and McKeown, 1996)."], "vector_1": {"stativ": 1, "classif": 2, "indic": 1, "three": 1, "empir": 1, "select": 1, "use": 1, "develop": 1, "distinct": 1, "linguisticallybas": 1, "system": 1, "research": 1, "compon": 1, "compos": 1, "tens": 1, "analyz": 1, "machin": 1, "aspectu": 3, "numer": 1, "claus": 1, "lexic": 1, "analysi": 1, "translat": 1, "class": 1, "choic": 1, "furthermor": 1, "constraint": 1, "corpora": 1, "tempor": 2, "perform": 1, "necessari": 1, "fundament": 1, "aid": 1, "first": 1}, "marker": "(Moens and Steedman, 1988", "article": "W97-0318", "vector_2": [9, 0.03975170599358711, 6, 3, 0, 0]}, {"label": "Neut", "current": "and for the automatic identification of semantically related groups of words (Pereira, Tishby, and Lee, 1993; Hatzivassiloglou and McKeown, 1993).", "context": ["Table 4: Breakdown of verb occurrences.", "and for the automatic identification of semantically related groups of words (Pereira, Tishby, and Lee, 1993; Hatzivassiloglou and McKeown, 1993).", "For more detail on the machine learning experiments described here, see Siegel (1997)."], "vector_1": {"breakdown": 1, "machin": 1, "semant": 1, "learn": 1, "group": 1, "describ": 1, "relat": 1, "detail": 1, "automat": 1, "siegel": 1, "identif": 1, "see": 1, "verb": 1, "tabl": 1, "word": 1, "experi": 1, "occurr": 1}, "marker": "(Pereira, Tishby, and Lee, 1993", "article": "W97-0318", "vector_2": [4, 0.5266381649264161, 3, 1, 0, 0]}, {"label": "Neut", "current": "A well-written document is coherent (Halliday and Hasan, 1976)- it structures information so that each new piece of information is interpretable given the preceding context.", "context": ["1 Introduction", "A well-written document is coherent (Halliday and Hasan, 1976)- it structures information so that each new piece of information is interpretable given the preceding context.", "Models that distinguish coherent from incoherent documents are widely used in generation, summarization and text evaluation."], "vector_1": {"wide": 1, "given": 1, "evalu": 1, "context": 1, "wellwritten": 1, "text": 1, "gener": 1, "use": 1, "incoher": 1, "piec": 1, "structur": 1, "inform": 2, "preced": 1, "coher": 2, "distinguish": 1, "new": 1, "model": 1, "document": 2, "summar": 1, "interpret": 1, "introduct": 1}, "marker": "(Halliday and Hasan, 1976)", "article": "P11-2022", "vector_2": [35, 0.04230590423059042, 1, 1, 0, 0]}, {"label": "CoCo", "current": "In document discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering4.", "context": ["We test our model on two experimental tasks, both testing its ability to distinguish between correct and incorrect orderings for wSi articles.", "In document discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering4.", "We also evaluate on the more difficult task of sentence insertion (Chen et al., 2007; Elsner and Charniak, 2008)."], "vector_1": {"origin": 1, "evalu": 1, "random": 1, "prefer": 1, "system": 1, "permut": 1, "abil": 1, "incorrect": 1, "compar": 1, "two": 1, "articl": 1, "also": 1, "score": 1, "test": 2, "document": 2, "correct": 2, "difficult": 1, "experiment": 1, "sentenc": 2, "distinguish": 1, "insert": 1, "task": 2, "wsi": 1, "discrimin": 1, "model": 1, "order": 2}, "marker": "(Barzilay and Lapata, 2005)", "article": "P11-2022", "vector_2": [6, 0.386996081556751, 3, 5, 8, 0]}, {"label": "Pos", "current": "The best WSJ results in previous work are those of Elsner and Charniak (2008), who combine the entity grid with models based on pronoun coreference and discourse-new NP detection.", "context": ["This model is significantly better than the standard grid on discrimination (84% versus 80%) and has a higher mean score on insertion (24% versus 21%)8.", "The best WSJ results in previous work are those of Elsner and Charniak (2008), who combine the entity grid with models based on pronoun coreference and discourse-new NP detection.", "We report their scores in the table."], "vector_1": {"work": 1, "pronoun": 1, "tabl": 1, "discoursenew": 1, "result": 1, "charniak": 1, "best": 1, "entiti": 1, "versu": 2, "detect": 1, "corefer": 1, "better": 1, "score": 2, "np": 1, "higher": 1, "previou": 1, "standard": 1, "base": 1, "grid": 2, "report": 1, "elsner": 1, "insert": 1, "wsj": 1, "discrimin": 1, "combin": 1, "model": 2, "mean": 1, "significantli": 1}, "marker": "(2008)", "article": "P11-2022", "vector_2": [3, 0.8178255960682739, 1, 4, 4, 0]}, {"label": "Pos", "current": "Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995).", "context": ["However, 'explicit' feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier.", "Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995).", "The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008)."], "vector_1": {"function": 1, "kernel": 1, "classif": 1, "featur": 4, "effici": 1, "weight": 1, "nlp": 1, "known": 1, "significantli": 1, "use": 1, "slow": 2, "space": 1, "support": 1, "classifi": 2, "also": 1, "spaceeffici": 1, "test": 1, "method": 1, "machin": 1, "conjunct": 1, "task": 1, "polynomi": 1, "train": 1, "consid": 1, "increas": 1, "svm": 1, "howev": 2, "explicit": 2, "vector": 1, "combin": 2, "kernelbas": 2, "sum": 1}, "marker": "(Cortes and Vapnik, 1995)", "article": "D09-1160", "vector_2": [14, 0.056238908267547884, 4, 2, 0, 0]}, {"label": "CoCo", "current": "This result conforms to the results reported in (Kudo and Matsumoto, 2003).", "context": ["Without fstries, little speed-up of SVM-HKE classifiers versus the SVM-KE classifiers (in Table 3) was obtained due to the mild reduction in the average number of active features |xd |in the classification.", "This result conforms to the results reported in (Kudo and Matsumoto, 2003).", "The parsing speed reached 14,937 sentences per second with accuracy of 90.91% (SVM-HKE, d = 3, Q = 0.002)."], "vector_1": {"conform": 1, "classif": 1, "featur": 1, "number": 1, "xd": 1, "mild": 1, "classifi": 2, "tabl": 1, "speed": 1, "reduct": 1, "per": 1, "versu": 1, "fstri": 1, "activ": 1, "due": 1, "littl": 1, "accuraci": 1, "svmhke": 2, "svmke": 1, "sentenc": 1, "reach": 1, "speedup": 1, "pars": 1, "1497": 1, "report": 1, "averag": 1, "second": 1, "q": 1, "without": 1, "obtain": 1, "result": 2}, "marker": "(Kudo and Matsumoto, 2003)", "article": "D09-1160", "vector_2": [6, 0.905850315158191, 1, 7, 4, 0]}, {"label": "Pos", "current": "We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1).", "context": ["Due to space limitations, we omit the details of the parsing algorithm.", "We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1).", "Note that features listed in the 'Between bunsetsus' row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector."], "vector_1": {"featur": 3, "independ": 1, "repres": 1, "prefix": 1, "set": 1, "tabl": 1, "find": 1, "row": 1, "use": 1, "appear": 1, "space": 1, "detail": 1, "note": 1, "obstacl": 1, "bunsetsu": 2, "standard": 1, "tailor": 1, "pars": 1, "longest": 1, "pair": 1, "becom": 1, "task": 1, "target": 1, "algorithm": 1, "omit": 1, "list": 1, "vector": 1, "limit": 1, "due": 1, "context": 1}, "marker": "(Kudo and Matsumoto, 2002", "article": "D09-1160", "vector_2": [7, 0.5945474573159537, 3, 1, 10, 0]}, {"label": "Pos", "current": "We used Amazon Mechanical Turk to manually label each user with their gender, using a language-agnostic labeling strategy (Liu and Ruths, 2013).", "context": ["Each dataset consisted of approximately 1000 users who tweeted primarily in a given language.", "We used Amazon Mechanical Turk to manually label each user with their gender, using a language-agnostic labeling strategy (Liu and Ruths, 2013).", "For classification, we employed a performant support vector machine-based (SVM) technique that has been used in a range of studies, e.g."], "vector_1": {"classif": 1, "primarili": 1, "approxim": 1, "rang": 1, "dataset": 1, "languag": 1, "given": 1, "techniqu": 1, "perform": 1, "support": 1, "label": 2, "eg": 1, "strategi": 1, "use": 3, "amazon": 1, "mechan": 1, "user": 2, "svm": 1, "machinebas": 1, "consist": 1, "gender": 1, "manual": 1, "employ": 1, "vector": 1, "turk": 1, "studi": 1, "languageagnost": 1, "tweet": 1}, "marker": "(Liu and Ruths, 2013)", "article": "D13-1114", "vector_2": [0, 0.08015132134011921, 1, 4, 10, 1]}, {"label": "Neut", "current": "Research by Kennedy and Inkpen (2006) dealt with negation and intensity by creating a discrete modifier scale, namely, the occurrence of good might be either good, not good, intensified good, or diminished good.", "context": ["Taboada et al (2011) presented a polarity lexicon with negation words and intensifiers, which they refer to as contextual valence shifters (Polanyi and Zaenen, 2006).", "Research by Kennedy and Inkpen (2006) dealt with negation and intensity by creating a discrete modifier scale, namely, the occurrence of good might be either good, not good, intensified good, or diminished good.", "A similar approach was taken by Steinberger et al (2012)."], "vector_1": {"lexicon": 1, "creat": 1, "al": 2, "et": 2, "scale": 1, "inkpen": 1, "research": 1, "discret": 1, "taken": 1, "occurr": 1, "refer": 1, "polar": 1, "good": 5, "dealt": 1, "intensifi": 2, "taboada": 1, "shifter": 1, "diminish": 1, "modifi": 1, "present": 1, "either": 1, "word": 1, "name": 1, "might": 1, "negat": 2, "contextu": 1, "intens": 1, "steinberg": 1, "valenc": 1, "approach": 1, "kennedi": 1, "similar": 1}, "marker": "(2006)", "article": "W15-2911", "vector_2": [9, 0.2139785454373612, 4, 1, 0, 0]}, {"label": "Weak", "current": "Nevertheless, search performance remains unsatisfactory at most e-commerce sites (Hagen et al., 2000).", "context": ["Keyword-based search engines have been one of the most highly utilized internet tools in recent years.", "Nevertheless, search performance remains unsatisfactory at most e-commerce sites (Hagen et al., 2000).", "Librarians and search professionals have traditionally favored Boolean keyword search systems, which, when successful, return a small set of relevant hits."], "vector_1": {"set": 1, "profession": 1, "site": 1, "one": 1, "unsatisfactori": 1, "boolean": 1, "year": 1, "hit": 1, "keywordbas": 1, "engin": 1, "perform": 1, "system": 1, "favor": 1, "ecommerc": 1, "internet": 1, "highli": 1, "return": 1, "tool": 1, "util": 1, "librarian": 1, "relev": 1, "recent": 1, "tradit": 1, "search": 4, "success": 1, "keyword": 1, "nevertheless": 1, "remain": 1, "small": 1}, "marker": "(Hagen et al., 2000)", "article": "W02-1024", "vector_2": [2, 0.05330955933190204, 1, 1, 0, 0]}, {"label": "CoCo", "current": "Work by (Somasundaran and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics.", "context": ["We predict that this difference makes it more difficult to achieve accuracies as high for 4FORUMS discussions as can be achieved for the congressional debates corpus.", "Work by (Somasundaran and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics.", "Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker's stance in a corpus of congressional floor debates."], "vector_1": {"corpu": 2, "identifi": 1, "show": 1, "predict": 1, "approxim": 1, "argument": 1, "high": 1, "congression": 2, "debat": 3, "research": 1, "best": 1, "differ": 1, "floor": 1, "perform": 2, "make": 1, "stanc": 1, "classifi": 1, "accuraci": 2, "speaker": 1, "6": 1, "difficult": 1, "discuss": 1, "topic": 1, "forum": 1, "work": 1, "structur": 1, "achiev": 2, "improv": 1, "similar": 1, "idealog": 1}, "marker": "(Somasundaran and Wiebe, 2009)", "article": "W14-2715", "vector_2": [5, 0.9255819286779039, 5, 2, 1, 0]}, {"label": "Neut", "current": "Reversing the order the words of a given sentence is a simple way to yield heterogeneous parsing models, thus improving parsing accuracy of the model ensemble (Sagae, 2007).", "context": ["3.3 Sentence Reversal", "Reversing the order the words of a given sentence is a simple way to yield heterogeneous parsing models, thus improving parsing accuracy of the model ensemble (Sagae, 2007).", "In our experiments, one transition system produces two models, one trained on the normal corpus, and the other on the corpus of reversed sentences."], "vector_1": {"corpu": 2, "transit": 1, "one": 2, "given": 1, "heterogen": 1, "system": 1, "accuraci": 1, "way": 1, "experi": 1, "simpl": 1, "normal": 1, "sentenc": 3, "train": 1, "pars": 2, "two": 1, "word": 1, "thu": 1, "revers": 3, "yield": 1, "order": 1, "ensembl": 1, "improv": 1, "model": 3, "produc": 1}, "marker": "(Sagae, 2007)", "article": "S14-2080", "vector_2": [7, 0.4047790652547541, 1, 1, 1, 0]}, {"label": "CoCo", "current": "Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and \"algebraic\" (as opposed to \"statistical\") approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012).", "context": ["tion of its effectiveness in adult speech processing (Cutler et al., 1986).", "Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and \"algebraic\" (as opposed to \"statistical\") approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012).", "Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007), however, have until recently completely ignored stress."], "vector_1": {"comput": 1, "process": 1, "oppos": 1, "sever": 1, "use": 2, "network": 1, "speech": 1, "role": 1, "approach": 1, "complet": 1, "investig": 1, "algebra": 1, "effect": 1, "adult": 1, "bayesian": 1, "studi": 1, "segment": 2, "tion": 1, "recent": 1, "stress": 2, "word": 2, "neural": 1, "howev": 1, "ignor": 1, "statist": 1, "model": 2}, "marker": "(Christiansen et al., 1998", "article": "Q14-1008", "vector_2": [16, 0.04700021088148461, 8, 4, 1, 0]}, {"label": "Pos", "current": "We also added stress information to the BrentBernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent, 1999), following the procedure just outlined.", "context": ["As function words account for roughly 50% of the tokens but only roughly 5% of the types in our corpora, this means that the type and token distribution of stress patterns differs dramatically in all our corpora, as can be seen from Table 2.", "We also added stress information to the BrentBernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent, 1999), following the procedure just outlined.", "This corpus is a de-facto standard for evaluat"], "vector_1": {"corpu": 2, "outlin": 1, "ad": 1, "evaluat": 1, "procedur": 1, "defacto": 1, "seen": 1, "tabl": 1, "follow": 1, "differ": 1, "pattern": 1, "also": 1, "type": 2, "function": 1, "distribut": 1, "stress": 2, "standard": 1, "account": 1, "word": 1, "corpora": 2, "brentbernsteinratn": 1, "dramat": 1, "inform": 1, "token": 2, "roughli": 2, "mean": 1}, "marker": "(Bernstein-Ratner, 1987", "article": "Q14-1008", "vector_2": [27, 0.4849219738506959, 2, 1, 0, 0]}, {"label": "CoCo", "current": "This results 2We follow Johnson and Goldwater (2009) in limiting the length of possible words to four syllables to speed up runtime.", "context": ["Alternatively, one could limit the models ability to capture word-to-word dependencies by removing rules (1) to (3).", "This results 2We follow Johnson and Goldwater (2009) in limiting the length of possible words to four syllables to speed up runtime.", "In pilot experiments, this choice did not have a noticeable effect on segmentation performance."], "vector_1": {"captur": 1, "one": 1, "four": 1, "result": 1, "abil": 1, "follow": 1, "speed": 1, "depend": 1, "goldwat": 1, "perform": 1, "experi": 1, "altern": 1, "we": 1, "effect": 1, "notic": 1, "wordtoword": 1, "segment": 1, "pilot": 1, "choic": 1, "word": 1, "johnson": 1, "possibl": 1, "could": 1, "rule": 1, "syllabl": 1, "length": 1, "limit": 2, "runtim": 1, "model": 1, "remov": 1}, "marker": "(2009)", "article": "Q14-1008", "vector_2": [5, 0.29644664698439477, 1, 11, 20, 0]}, {"label": "Neut", "current": "In Andreev (1965b) the method is tested extensively on Russian, which is the subject of several papers in the volume, and a number of other languages: Albanian (Persikov 1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (Ozigova 1965), English (Malahovskij 1965), Estonian (Hol'm 1965), French (Kordi 1965), German (Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis 1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965), and Vietnamese (Jakuseva 1965).", "context": ["The initial suffix candidate, -oj, has a high functional load and consequently a high text frequency: It is the most ambiguous of the Russian adjective suffixes, appearing in four different slots in the adjective paradigm, and is also homonymous with a noun suffix.", "In Andreev (1965b) the method is tested extensively on Russian, which is the subject of several papers in the volume, and a number of other languages: Albanian (Persikov 1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (Ozigova 1965), English (Malahovskij 1965), Estonian (Hol'm 1965), French (Kordi 1965), German (Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis 1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965), and Vietnamese (Jakuseva 1965).", "As an aside, we may note that only after the turn of the millennium are we again seeing this variety of languages in ULM work."], "vector_1": {"load": 1, "jakuseva": 1, "homonym": 1, "suffix": 3, "kordi": 1, "czech": 1, "text": 1, "number": 1, "four": 1, "consequ": 1, "see": 1, "paper": 1, "malahovskij": 1, "eliseeva": 1, "turn": 1, "sever": 1, "subject": 1, "slot": 1, "differ": 1, "appear": 1, "persikov": 1, "asid": 1, "melkumjan": 1, "note": 1, "also": 1, "adject": 2, "millennium": 1, "languag": 2, "varieti": 1, "test": 1, "andreev": 2, "b": 1, "method": 1, "serbocroatian": 1, "function": 1, "swahili": 1, "bulgarian": 1, "hungarian": 1, "ukrainian": 1, "may": 1, "ulm": 1, "fedulova": 1, "initi": 1, "extens": 1, "holm": 1, "candid": 1, "volum": 1, "armenian": 1, "german": 1, "russian": 2, "fitialova": 1, "estonian": 1, "a": 2, "high": 2, "panina": 1, "noun": 1, "oj": 1, "vietnames": 1, "frequenc": 1, "albanian": 1, "work": 1, "ambigu": 1, "french": 1, "fihman": 2, "latvian": 1, "english": 1, "jakubajti": 1, "paradigm": 1, "ozigova": 1, "hausa": 1}, "marker": "(1965b)", "article": "J11-2002", "vector_2": [46, 0.21299466052880628, 1, 0, 45, 1]}, {"label": "CoCo", "current": "In the group-and-abstract paradigm, working with feature sets of a word, as in De Pauw and Wagacha (2007), is an ingenious generalization that holds numerous advantages over string edit distances.", "context": ["The recent increased interest in Bayesian generative models in general in NLP may possibly serve as a catalyst.", "In the group-and-abstract paradigm, working with feature sets of a word, as in De Pauw and Wagacha (2007), is an ingenious generalization that holds numerous advantages over string edit distances.", "Feature set comparisons are naturally defined over arbitrary collections, whereas string edit distances work on pairs of strings."], "vector_1": {"nlp": 1, "featur": 2, "advantag": 1, "pauw": 1, "natur": 1, "increas": 1, "set": 2, "string": 3, "collect": 1, "numer": 1, "pair": 1, "interest": 1, "wagacha": 1, "may": 1, "gener": 3, "de": 1, "wherea": 1, "distanc": 2, "arbitrari": 1, "ingeni": 1, "catalyst": 1, "bayesian": 1, "hold": 1, "recent": 1, "serv": 1, "comparison": 1, "word": 1, "possibl": 1, "edit": 2, "work": 2, "groupandabstract": 1, "defin": 1, "model": 1, "paradigm": 1}, "marker": "(2007)", "article": "J11-2002", "vector_2": [4, 0.9614553272032621, 1, 39, 6, 1]}, {"label": "Neut", "current": "In contrast, other approaches have used a pipelined approach, with separate models to first do tokenization and then part-of-speech tagging (Diab et al., 2007; Diab, 2009).", "context": ["1SAMA-v3.1 is an updated version of BAMA, with many significant differences in analysis.", "In contrast, other approaches have used a pipelined approach, with separate models to first do tokenization and then part-of-speech tagging (Diab et al., 2007; Diab, 2009).", "While these approaches have somewhat lower performance than the joint approach, they have the advantage that they do not rely on the presence of a full-blown morphological analyzer, which may not always be available or appropriate as the data shifts to different genres or Arabic dialects."], "vector_1": {"partofspeech": 1, "advantag": 1, "dialect": 1, "bama": 1, "signific": 1, "tag": 1, "avail": 1, "differ": 2, "fullblown": 1, "perform": 1, "morpholog": 1, "alway": 1, "reli": 1, "version": 1, "analyz": 1, "approach": 4, "contrast": 1, "updat": 1, "analysi": 1, "samav3": 1, "pipelin": 1, "may": 1, "presenc": 1, "use": 1, "arab": 1, "joint": 1, "somewhat": 1, "data": 1, "lower": 1, "appropri": 1, "shift": 1, "genr": 1, "separ": 1, "token": 1, "mani": 1, "model": 1, "first": 1}, "marker": "(Diab et al., 2007", "article": "P10-2063", "vector_2": [3, 0.0850498338870432, 2, 4, 4, 0]}, {"label": "CoCo", "current": "The \"MorphPOS\" task in (Roth et al., 2008), 96.4%, is somewhat similar to ours in that it scores on a \"core tag\", but unlike for us there is only one such tag for a source token (easier) but it distinguishes between NOUN and ADJ (harder).", "context": ["However, both (Habash and Rambow, 2005; Diab et al., 2007) assume gold tokenization for evaluation of POS results, which we do not.", "The \"MorphPOS\" task in (Roth et al., 2008), 96.4%, is somewhat similar to ours in that it scores on a \"core tag\", but unlike for us there is only one such tag for a source token (easier) but it distinguishes between NOUN and ADJ (harder).", "We would like to do a direct comparison by simply runing the above systems on the exact same data and evaluating them the same way."], "vector_1": {"simpli": 1, "gold": 1, "direct": 1, "one": 1, "unlik": 1, "tag": 2, "result": 1, "assum": 1, "evalu": 2, "would": 1, "data": 1, "system": 1, "score": 1, "way": 1, "adj": 1, "easier": 1, "core": 1, "sourc": 1, "somewhat": 1, "distinguish": 1, "exact": 1, "comparison": 1, "task": 1, "noun": 1, "like": 1, "morphpo": 1, "harder": 1, "us": 1, "po": 1, "token": 2, "rune": 1, "similar": 1, "howev": 1}, "marker": "(Roth et al., 2008)", "article": "P10-2063", "vector_2": [2, 0.9260925121390238, 3, 3, 9, 0]}, {"label": "Neut", "current": "Random restarts and random walks (Moore and Quirk, 2008) are commonly used to combat the fact the search space is highly non-convex, often with multiple minima.", "context": ["Recent efforts extended MERT to work on lattices (Macherey et al., 2008) and hypergraphs (Kumar et al., 2009).", "Random restarts and random walks (Moore and Quirk, 2008) are commonly used to combat the fact the search space is highly non-convex, often with multiple minima.", "Several problems still remain with MERT, three of which are addressed by this work."], "vector_1": {"minima": 1, "often": 1, "random": 2, "mert": 2, "walk": 1, "still": 1, "sever": 1, "highli": 1, "use": 1, "hypergraph": 1, "extend": 1, "space": 1, "three": 1, "lattic": 1, "combat": 1, "address": 1, "multipl": 1, "effort": 1, "restart": 1, "recent": 1, "search": 1, "work": 2, "remain": 1, "commonli": 1, "nonconvex": 1, "problem": 1, "fact": 1}, "marker": "(Moore and Quirk, 2008)", "article": "W12-3159", "vector_2": [4, 0.045673197374354865, 3, 1, 1, 0]}, {"label": "Pos", "current": "Since it is impractical to calculate derivatives of this function, we use derivative-free optimization methods such as the downhill simplex method (Helder and Mead, 1965) and Powell's method (Powell, 1964), which generally handle such difficult search conditions relatively well.", "context": ["which takes source sentences and a set of weights as inputs, and outputs the evaluation score (e.g., BLEU score) computed on the decoded sentences.", "Since it is impractical to calculate derivatives of this function, we use derivative-free optimization methods such as the downhill simplex method (Helder and Mead, 1965) and Powell's method (Powell, 1964), which generally handle such difficult search conditions relatively well.", "This approach confers several benefits over MERT."], "vector_1": {"bleu": 1, "set": 1, "comput": 1, "derivativefre": 1, "evalu": 1, "eg": 1, "mert": 1, "downhil": 1, "sever": 1, "use": 1, "weight": 1, "benefit": 1, "handl": 1, "score": 2, "decod": 1, "take": 1, "rel": 1, "input": 1, "approach": 1, "method": 3, "impract": 1, "function": 1, "sourc": 1, "deriv": 1, "optim": 1, "sentenc": 2, "gener": 1, "confer": 1, "simplex": 1, "condit": 1, "powel": 1, "sinc": 1, "search": 1, "well": 1, "calcul": 1, "output": 1, "difficult": 1}, "marker": "(Helder and Mead, 1965)", "article": "W12-3159", "vector_2": [47, 0.08803928446159243, 2, 2, 0, 0]}, {"label": "Pos", "current": "First, we use a model selection acceleration technique called racing (Moore and Lee, 1994) in conjunction with randomization tests (Riezler and Maxwell, 2005) to avoid decoding the entire development set at each function evaluation.", "context": ["In this paper, we make direct search reasonably fast thanks to two speedup techniques.", "First, we use a model selection acceleration technique called racing (Moore and Lee, 1994) in conjunction with randomization tests (Riezler and Maxwell, 2005) to avoid decoding the entire development set at each function evaluation.", "This approach discards the current model whenever performance on the translated subset of the development data is deemed significantly worse in comparison to the current best model."], "vector_1": {"subset": 1, "acceler": 1, "set": 1, "evalu": 1, "random": 1, "speedup": 1, "paper": 1, "significantli": 1, "select": 1, "use": 1, "develop": 2, "thank": 1, "perform": 1, "make": 1, "two": 1, "fast": 1, "current": 2, "wors": 1, "call": 1, "best": 1, "test": 1, "decod": 1, "approach": 1, "function": 1, "conjunct": 1, "direct": 1, "reason": 1, "entir": 1, "translat": 1, "data": 1, "techniqu": 2, "comparison": 1, "search": 1, "whenev": 1, "race": 1, "deem": 1, "discard": 1, "model": 3, "avoid": 1, "first": 1}, "marker": "(Moore and Lee, 1994)", "article": "W12-3159", "vector_2": [18, 0.1094352858646089, 2, 3, 0, 0]}, {"label": "Pos", "current": "We use another technique to speed up direct search by storing and re-using search graphs, which consist of lattices in the case of phrase-based decoding (Och et al., 1999) and hypergraphs in the case of hierarchical decoding (Chiang, 2005).", "context": ["5.2 Lattice-based decoding", "We use another technique to speed up direct search by storing and re-using search graphs, which consist of lattices in the case of phrase-based decoding (Och et al., 1999) and hypergraphs in the case of hierarchical decoding (Chiang, 2005).", "The successive expansion of translation options in order to construct the search graph is generally done from scratch, but this can be wasteful when the same sentences are translated multiple times, as it is the case with direct search."], "vector_1": {"scratch": 1, "direct": 2, "done": 1, "speed": 1, "hierarch": 1, "use": 1, "hypergraph": 1, "techniqu": 1, "sentenc": 1, "anoth": 1, "construct": 1, "decod": 3, "lattic": 1, "store": 1, "option": 1, "expans": 1, "gener": 1, "time": 1, "wast": 1, "translat": 2, "multipl": 1, "latticebas": 1, "case": 3, "search": 4, "success": 1, "consist": 1, "phrasebas": 1, "reus": 1, "graph": 2, "order": 1}, "marker": "(Och et al., 1999)", "article": "W12-3159", "vector_2": [13, 0.6475422157638924, 2, 1, 10, 0]}, {"label": "Pos", "current": "For our experiments, we use a phrase-based translation system similar to Moses (Koehn et al., 2007).", "context": ["6.1 Setup", "For our experiments, we use a phrase-based translation system similar to Moses (Koehn et al., 2007).", "Our decoder uses many of the same features as Moses, including four phrasal and lexicalized translation scores, phrase penalty, word penalty, a language model score, linear distortion, and six lexicalized reordering scores."], "vector_1": {"featur": 1, "four": 1, "phrase": 1, "languag": 1, "penalti": 2, "use": 2, "six": 1, "distort": 1, "system": 1, "score": 3, "decod": 1, "includ": 1, "experi": 1, "reorder": 1, "mose": 2, "linear": 1, "lexic": 2, "translat": 2, "model": 1, "word": 1, "setup": 1, "phrasebas": 1, "mani": 1, "phrasal": 1, "similar": 1}, "marker": "(Koehn et al., 2007)", "article": "W12-3159", "vector_2": [5, 0.7485844565816505, 1, 1, 0, 0]}, {"label": "Neut", "current": "There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013).", "context": ["Quite an amount of work leverage query logs (Jiang et al., 2013), including query reformulations (Radlinski et al., 2010), click-through data (Li et al., 2008).", "There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013).", "The new trend of integrating knowledge graph will be discussed next."], "vector_1": {"quit": 1, "use": 1, "sponser": 1, "log": 1, "reformul": 1, "clickthrough": 1, "trend": 1, "queri": 2, "interact": 1, "work": 2, "next": 1, "also": 1, "amount": 1, "integr": 1, "knowledg": 1, "includ": 1, "graph": 1, "new": 1, "data": 3, "discuss": 1, "leverag": 1}, "marker": "(Yamamoto et al., 2012)", "article": "D14-1114", "vector_2": [2, 0.9402451481103167, 5, 2, 1, 0]}, {"label": "Neut", "current": "We use random hyperplane based hash family proposed in (Charikar, 2002) and set the hash code dimension and hash table numbers empirically to ensure the number of nodes falling into each bucket is relatively stable.", "context": ["As query log induced intent topic graph is of considerable large size, the pair-wise similarity is computationally prohibitive, hence we use Local Sensitive Hash (Indyk and Motwani, 1998) for each similarity metric so as to compute ISim just in candidate set.", "We use random hyperplane based hash family proposed in (Charikar, 2002) and set the hash code dimension and hash table numbers empirically to ensure the number of nodes falling into each bucket is relatively stable.", "3.2 Merging nodes"], "vector_1": {"dimens": 1, "prohibit": 1, "set": 2, "comput": 2, "consider": 1, "queri": 1, "metric": 1, "random": 1, "henc": 1, "number": 2, "topic": 1, "code": 1, "famili": 1, "tabl": 1, "ensur": 1, "stabl": 1, "hyperplan": 1, "size": 1, "use": 2, "log": 1, "sensit": 1, "graph": 1, "larg": 1, "rel": 1, "merg": 1, "local": 1, "node": 2, "hash": 4, "induc": 1, "candid": 1, "intent": 1, "fall": 1, "empir": 1, "bucket": 1, "base": 1, "pairwis": 1, "isim": 1, "similar": 2, "propos": 1}, "marker": "(Charikar, 2002)", "article": "D14-1114", "vector_2": [12, 0.393226506639428, 2, 1, 0, 0]}, {"label": "Neut", "current": "Pasula et al (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic model of identity uncertainty.", "context": ["There has also been some previous work on unsupervised learning of field segmentation models in particular domains.", "Pasula et al (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic model of identity uncertainty.", "However, their system does not explicitly learn a field segmentation model for the citations, and encodes a large amount of hand-supplied information about name forms, abbreviation schemes, and so on."], "vector_1": {"domain": 1, "larger": 1, "citat": 2, "encod": 1, "al": 1, "et": 1, "pasula": 1, "explicitli": 1, "also": 1, "perform": 1, "system": 1, "field": 2, "abbrevi": 1, "larg": 1, "scheme": 1, "form": 1, "previou": 1, "probabilist": 1, "bibliograph": 1, "part": 1, "handsuppli": 1, "particular": 1, "segment": 3, "ident": 1, "name": 1, "unsupervis": 2, "work": 1, "inform": 1, "amount": 1, "limit": 1, "learn": 2, "small": 1, "model": 3, "uncertainti": 1, "howev": 1}, "marker": "(2002)", "article": "P05-1046", "vector_2": [3, 0.9006099251455503, 1, 1, 0, 0]}, {"label": "Neut", "current": "The use of supervised machine learning techniques for metaphor identification has increased manyfold in the recent years (see section 10, Related Work, for a review and references), partially due to the availability of largescale annotated resources for training and evaluating the algorithms, such as the VU Amsterdam corpus (Steen et al., 2010), datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (Mohler et al., 2013; Strzalkowski et al., 2013), and recent annotation efforts with other kinds of data (Beigman Klebanov and Flor, 2013; Jang et al., 2014).", "context": ["In this paper, we present a set of experiments aimed at improving on previous work on the task of supervised word-level detection of linguistic metaphor in running text.", "The use of supervised machine learning techniques for metaphor identification has increased manyfold in the recent years (see section 10, Related Work, for a review and references), partially due to the availability of largescale annotated resources for training and evaluating the algorithms, such as the VU Amsterdam corpus (Steen et al., 2010), datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (Mohler et al., 2013; Strzalkowski et al., 2013), and recent annotation efforts with other kinds of data (Beigman Klebanov and Flor, 2013; Jang et al., 2014).", "Some of these data are publicly available (Steen et al., 2010), allowing for benchmarking and for measuring incremental improvements, which is the approach taken in this paper."], "vector_1": {"corpu": 1, "set": 1, "partial": 1, "evalu": 1, "text": 1, "resourc": 1, "dataset": 1, "see": 1, "paper": 2, "vu": 1, "increment": 1, "year": 1, "largescal": 1, "supervis": 2, "use": 1, "detect": 1, "metaphor": 3, "techniqu": 1, "data": 2, "section": 1, "amsterdam": 1, "due": 1, "identif": 2, "manyfold": 1, "interpret": 1, "avail": 2, "taken": 1, "experi": 1, "approach": 1, "refer": 1, "machin": 1, "previou": 1, "run": 1, "stateofart": 1, "advanc": 1, "wordlevel": 1, "benchmark": 1, "governmentfund": 1, "built": 1, "train": 1, "initi": 1, "increas": 1, "relat": 1, "effort": 1, "present": 1, "recent": 2, "measur": 1, "kind": 1, "task": 1, "algorithm": 1, "work": 2, "annot": 2, "us": 1, "aim": 1, "part": 1, "allow": 1, "publicli": 1, "learn": 1, "improv": 2, "linguist": 1, "review": 1}, "marker": "(Steen et al., 2010)", "article": "W15-1402", "vector_2": [5, 0.03344603553434102, 6, 3, 0, 0]}, {"label": "Pos", "current": "We start with a baseline set of features and training regime from Beigman Klebanov et al (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations.", "context": ["Content tokens are nouns, adjectives, adverbs, and verbs.", "We start with a baseline set of features and training regime from Beigman Klebanov et al (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations.", "The usage of concreteness features was previously discussed in the literature; to our knowledge, these features have not yet been evaluated for their impact in a comprehensive system for word-level metaphor detection, apart from the concreteness features as used in Beigman Klebanov et al (2014), which we use as a baseline."], "vector_1": {"concret": 4, "klebanov": 2, "set": 1, "evalu": 1, "certain": 1, "within": 1, "al": 2, "featur": 5, "et": 2, "yet": 1, "baselin": 2, "impact": 2, "differ": 1, "detect": 1, "depend": 1, "adverb": 1, "concept": 1, "system": 1, "content": 1, "start": 1, "beigman": 2, "adject": 1, "knowledg": 1, "suit": 1, "comprehens": 1, "previous": 1, "type": 1, "wordlevel": 1, "investig": 1, "relat": 2, "usag": 1, "use": 2, "regim": 1, "verb": 1, "metaphor": 1, "discuss": 1, "apart": 1, "noun": 1, "target": 1, "reweight": 1, "well": 1, "literatur": 1, "exampl": 1, "token": 1, "train": 2}, "marker": "(2014)", "article": "W15-1402", "vector_2": [1, 0.06606337841421374, 2, 8, 5, 0]}, {"label": "Neut", "current": "Shutova and Sun (2013) and Shutova et al (2013) explored unsupervised clustering-based approaches.", "context": ["The field of automated identification of metaphor has grown dramatically over the last few years, and there exists a plurality of approaches to the task.", "Shutova and Sun (2013) and Shutova et al (2013) explored unsupervised clustering-based approaches.", "Features used in supervised learning approaches include selectional preferences violation, outlier detection, semantic analysis using topical signatures and ontologies, as well as n-gram features, among others (Tsvetkov et al., 2014; Schulder and Hovy, 2014; Beigman Klebanov et al., 2014; Mohler et al., 2013; Dunn, 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Strzalkowski et al., 2013; Bethard et al., 2009; Pasanek and Sculley, 2008)."], "vector_1": {"among": 1, "semant": 1, "featur": 2, "ontolog": 1, "prefer": 1, "al": 1, "topic": 1, "exist": 1, "explor": 1, "year": 1, "et": 1, "select": 1, "grown": 1, "clusteringbas": 1, "use": 2, "detect": 1, "metaphor": 1, "shutova": 2, "outlier": 1, "sun": 1, "violat": 1, "identif": 1, "field": 1, "other": 1, "includ": 1, "approach": 3, "analysi": 1, "signatur": 1, "supervis": 1, "autom": 1, "ngram": 1, "task": 1, "last": 1, "plural": 1, "unsupervis": 1, "well": 1, "dramat": 1, "learn": 1}, "marker": "(2013)", "article": "W15-1402", "vector_2": [2, 0.8701272871917264, 12, 3, 2, 0]}, {"label": "Neut", "current": "For all our classification experiments, we used the WEKA (Hall et al., 2009) toolkit.", "context": ["Hence, we further investigated the problem as a collection of multi-stage twoclass cascades instead of a single stage three class classification.", "For all our classification experiments, we used the WEKA (Hall et al., 2009) toolkit.", "We report the overall classification accuracy as our evaluation metric."], "vector_1": {"classif": 3, "evalu": 1, "metric": 1, "henc": 1, "toolkit": 1, "use": 1, "weka": 1, "twoclass": 1, "three": 1, "accuraci": 1, "instead": 1, "experi": 1, "singl": 1, "investig": 1, "overal": 1, "multistag": 1, "report": 1, "class": 1, "stage": 1, "collect": 1, "cascad": 1, "problem": 1}, "marker": "(Hall et al., 2009)", "article": "W13-1708", "vector_2": [4, 0.5444903828869315, 1, 1, 0, 0]}, {"label": "Neut", "current": "Cube pruning (Chiang, 2007) is a widely used search strategy in state-of-the-art hierarchical decoders.", "context": ["1 Introduction", "Cube pruning (Chiang, 2007) is a widely used search strategy in state-of-the-art hierarchical decoders.", "Some alternatives and extensions to the classical algorithm as proposed by David Chiang have been presented in the literature since, e.g."], "vector_1": {"hierarch": 1, "wide": 1, "use": 1, "cube": 1, "prune": 1, "algorithm": 1, "classic": 1, "eg": 1, "altern": 1, "search": 1, "david": 1, "extens": 1, "decod": 1, "stateoftheart": 1, "propos": 1, "chiang": 1, "literatur": 1, "strategi": 1, "sinc": 1, "present": 1, "introduct": 1}, "marker": "(Chiang, 2007)", "article": "W13-0804", "vector_2": [6, 0.044932834551946944, 1, 3, 2, 0]}, {"label": "Neut", "current": "Good descriptions of the cube pruning implementation in the Joshua decoder have been provided by Li and Khudanpur (2008) and Li et al (2009b).", "context": ["It is basically an adaptation of one of the k-best parsing algorithms by Huang and Chiang (2005).", "Good descriptions of the cube pruning implementation in the Joshua decoder have been provided by Li and Khudanpur (2008) and Li et al (2009b).", "Xu and Koehn (2012) implemented hierarchical search with the cube growing algorithm in Moses and compared its performance to Moses' cube pruning implementation."], "vector_1": {"joshua": 1, "al": 1, "one": 1, "cube": 3, "et": 1, "xu": 1, "hierarch": 1, "compar": 1, "perform": 1, "li": 2, "decod": 1, "adapt": 1, "basic": 1, "kbest": 1, "grow": 1, "mose": 2, "good": 1, "koehn": 1, "pars": 1, "huang": 1, "khudanpur": 1, "search": 1, "prune": 2, "algorithm": 2, "provid": 1, "descript": 1, "chiang": 1, "implement": 3}, "marker": "(2008)", "article": "W13-0804", "vector_2": [5, 0.16128209488182282, 4, 1, 1, 0]}, {"label": "Pos", "current": "During decoding, a maximum length constraint of ten is applied to all non-terminals except the initial symbol S. Model weights are optimized with MERT (Och, 2003) on 100-best lists.", "context": ["The language models are 4-grams with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) which have been trained with the SRILM toolkit (Stolcke, 2002).", "During decoding, a maximum length constraint of ten is applied to all non-terminals except the initial symbol S. Model weights are optimized with MERT (Och, 2003) on 100-best lists.", "The optimized weights are obtained (separately for deep and for shallow-1 grammars) with a k-best generation size of 1000 for Chinese-*English and of 500 for Arabic-*English and kept for all setups."], "vector_1": {"weight": 2, "appli": 1, "mert": 1, "toolkit": 1, "deep": 1, "srilm": 1, "languag": 1, "size": 1, "ten": 1, "0": 1, "arabicenglish": 1, "except": 1, "list": 1, "decod": 1, "best": 1, "kbest": 1, "gener": 1, "optim": 2, "symbol": 1, "shallow": 1, "initi": 1, "obtain": 1, "train": 1, "chineseenglish": 1, "kept": 1, "modifi": 1, "grammar": 1, "constraint": 1, "setup": 1, "smooth": 1, "maximum": 1, "nontermin": 1, "separ": 1, "length": 1, "gram": 1, "model": 2, "kneserney": 1}, "marker": "(Och, 2003)", "article": "W13-0804", "vector_2": [10, 0.4882673014793402, 4, 2, 6, 0]}, {"label": "Pos", "current": "The most important are Vendler(1967), Bache(1982), and Smith (1985) .", "context": ["In resent years, Western researchers have published a large volume of papers, which present many points of view.", "The most important are Vendler(1967), Bache(1982), and Smith (1985) .", "They approximately classify the situation as four types: state, activity, accomplishment, and achievement."], "vector_1": {"point": 1, "activ": 1, "approxim": 1, "bach": 1, "four": 1, "paper": 1, "classifi": 1, "year": 1, "vendler": 1, "accomplish": 1, "smith": 1, "publish": 1, "research": 1, "state": 1, "larg": 1, "import": 1, "type": 1, "situat": 1, "resent": 1, "volum": 1, "western": 1, "present": 1, "achiev": 1, "mani": 1, "view": 1}, "marker": "(1967)", "article": "W00-1220", "vector_2": [33, 0.3175097276264591, 3, 3, 1, 0]}, {"label": "Pos", "current": "According to table 2.1, a classification algorithm was designed, and we use two resources to implement our algorithm: The Contemporary Chinese Cihai [11] (which we will refer to as the Cihai below) dictionary and the Machine Tractable Dictionary of Contemporary Chinese Predicate Verbs [12}(which we will refer to as the predicate dictionary below).", "context": ["2.3 Implementation of the algorithm", "According to table 2.1, a classification algorithm was designed, and we use two resources to implement our algorithm: The Contemporary Chinese Cihai [11] (which we will refer to as the Cihai below) dictionary and the Machine Tractable Dictionary of Contemporary Chinese Predicate Verbs [12}(which we will refer to as the predicate dictionary below).", "The Cihai dictionary includes 12,000 entries and 700,000 collocation instances, predicate dictionary includes about 3000 verbs with their semantic information, case relations and detailed collocation information."], "vector_1": {"classif": 1, "semant": 1, "detail": 1, "tractabl": 1, "design": 1, "tabl": 1, "contemporari": 2, "use": 1, "cihai": 3, "two": 1, "instanc": 1, "includ": 2, "which": 1, "refer": 2, "machin": 1, "accord": 1, "resourc": 1, "chines": 2, "relat": 1, "colloc": 2, "000": 1, "dictionari": 5, "case": 1, "algorithm": 3, "entri": 1, "predic": 3, "inform": 2, "verb": 2, "implement": 2}, "marker": "[11]", "article": "W00-1220", "vector_2": [6, 0.6731517509727627, 1, 1, 0, 0]}, {"label": "Neut", "current": "Davidov et al (2010) examined hashtags that indicated sarcasm to identify if such labelled tweets can be a reliable source of sarcasm.", "context": ["2010).", "Davidov et al (2010) examined hashtags that indicated sarcasm to identify if such labelled tweets can be a reliable source of sarcasm.", "They concluded that user-labelled sarcastic tweets can be noisy and constitute the hardest form of sarcasm."], "vector_1": {"sarcast": 1, "davidov": 1, "sourc": 1, "identifi": 1, "form": 1, "sarcasm": 3, "tweet": 2, "al": 1, "indic": 1, "label": 1, "userlabel": 1, "hardest": 1, "examin": 1, "hashtag": 1, "et": 1, "noisi": 1, "constitut": 1, "reliabl": 1, "conclud": 1}, "marker": "(2010)", "article": "S15-2120", "vector_2": [5, 0.24314635175031632, 1, 1, 0, 0]}, {"label": "Neut", "current": "those MT outputs that are guaranteed to require less post-editing effort than the best corresponding TM match are presented to the post-editor (He et al., 2010a).", "context": ["The confidence metric ensures that only Author did this work during his post doctoral research at CNGL.", "those MT outputs that are guaranteed to require less post-editing effort than the best corresponding TM match are presented to the post-editor (He et al., 2010a).", "The MT is integrated seamlessly, and established localisation cost estimation models based on TM technologies still apply as upper bounds."], "vector_1": {"tm": 2, "work": 1, "less": 1, "appli": 1, "metric": 1, "bound": 1, "cost": 1, "ensur": 1, "guarante": 1, "postedit": 1, "still": 1, "establish": 1, "best": 1, "doctor": 1, "research": 1, "posteditor": 1, "requir": 1, "estim": 1, "match": 1, "confid": 1, "base": 1, "post": 1, "effort": 1, "present": 1, "upper": 1, "seamlessli": 1, "author": 1, "technolog": 1, "correspond": 1, "mt": 2, "localis": 1, "integr": 1, "output": 1, "model": 1, "cngl": 1}, "marker": "(He et al., 2010a)", "article": "N13-3003", "vector_2": [3, 0.16175975773889636, 1, 3, 3, 0]}, {"label": "Neut", "current": "Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004).", "context": ["Translating a sentence amounts to finding the best scoring translation hypothesis in the search space.", "Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004).", "Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics."], "vector_1": {"hillclimb": 1, "natur": 1, "combinatori": 1, "heurist": 2, "find": 1, "best": 1, "use": 1, "techniqu": 1, "space": 2, "moreov": 1, "complex": 1, "reli": 1, "score": 1, "decod": 2, "simpl": 1, "hypothesi": 1, "sentenc": 1, "overal": 1, "variant": 1, "translat": 3, "typic": 1, "greedi": 1, "prune": 1, "reduc": 1, "search": 4, "like": 1, "bestfirst": 1, "multistack": 1, "amount": 1, "problem": 1}, "marker": "(Germann, 2003)", "article": "D10-1091", "vector_2": [7, 0.06347407197506376, 2, 1, 2, 0]}, {"label": "Pos", "current": "To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS.", "context": ["1.3 Contribution and organization", "To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS.", "We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure."], "vector_1": {"identifi": 1, "evalu": 1, "assess": 1, "heurist": 1, "follow": 1, "systemat": 1, "best": 1, "impact": 1, "caus": 1, "multipl": 1, "perform": 1, "score": 1, "decod": 1, "decis": 1, "quantifi": 1, "contribut": 1, "power": 1, "tool": 1, "express": 1, "oracl": 1, "train": 1, "failur": 1, "pbt": 2, "made": 1, "organ": 1, "provid": 1, "work": 1, "aim": 1, "achiev": 1, "studi": 1, "propos": 1}, "marker": "(Dreyer et al., 2007", "article": "D10-1091", "vector_2": [3, 0.13729101728534995, 2, 2, 0, 0]}, {"label": "Pos", "current": "Implementing the \"local\" or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints:", "context": ["l < m  1, ai,j,k,lai0,j0,l+1,l0  |i0  j  1 |< d", "Implementing the \"local\" or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints:", "Xai0,j0,k0,l0  k0k"], "vector_1": {"mjd": 1, "impli": 1, "straightforward": 1, "constraint": 1, "i": 1, "j": 1, "l": 1, "also": 1, "kk": 1, "use": 1, "xaijkl": 1, "aijklaijl": 1, "follow": 1, "implement": 1, "local": 1, "reorder": 1, "strategi": 1}, "marker": "(Kumar and Byrne, 2005)", "article": "D10-1091", "vector_2": [5, 0.4735996977425144, 1, 1, 0, 0]}, {"label": "Neut", "current": "Oracle decoding has also been used to assess the limitations induced by various reordering constraints in (Dreyer et al., 2007).", "context": ["This is the main motivation of (Tillmann and Zhang, 2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering model.", "Oracle decoding has also been used to assess the limitations induced by various reordering constraints in (Dreyer et al., 2007).", "To this end, the authors propose to use a beam-search based oracle decoder, which computes lower bounds of the best achievable BLEU-4 using dynamic programming techniques over finite-state (for so-called local and IBM constraints) or hierarchically structured (for ITG constraints) sets of hypotheses."], "vector_1": {"bleu": 3, "set": 1, "comput": 2, "itg": 1, "approxim": 1, "maxim": 1, "socal": 1, "assess": 1, "motiv": 1, "best": 1, "hierarch": 1, "use": 3, "end": 1, "techniqu": 1, "convent": 1, "author": 2, "also": 1, "program": 1, "decod": 3, "dynam": 1, "main": 1, "simpl": 1, "reorder": 2, "beamsearch": 1, "run": 1, "variou": 1, "induc": 1, "oracl": 2, "bound": 1, "base": 1, "high": 1, "lower": 1, "ibm": 1, "constraint": 3, "local": 2, "hypothes": 2, "structur": 1, "finitest": 1, "achiev": 1, "limit": 1, "model": 1, "persent": 1, "propos": 1}, "marker": "(Dreyer et al., 2007)", "article": "D10-1091", "vector_2": [3, 0.9085198828752243, 2, 2, 0, 0]}, {"label": "Pos", "current": "In his comprehensive review of outlier detection methods in textual data, Guthrie (2008) compares a variety of vectorization methods along with a variety of generic outlier methods.", "context": ["Subsequent work, such as the StahelDonoho Estimator (Stahel, 1981; Donoho, 1982), PCout (Filzmoser et al., 2008), LOF (Breunig and Kriegel, 2000) and ABOD (Kriegel et al., 2008) have generalized univariate methods to highdimensional data points.", "In his comprehensive review of outlier detection methods in textual data, Guthrie (2008) compares a variety of vectorization methods along with a variety of generic outlier methods.", "The vectorization methods employ a variety of lexical and syntactic stylistic features, while the outlier detection methods use a variety of similarity/distance measures such as cosine and Euclidean distance."], "vector_1": {"featur": 1, "point": 1, "similaritydist": 1, "use": 1, "detect": 2, "lof": 1, "guthri": 1, "compar": 1, "review": 1, "textual": 1, "estim": 1, "pcout": 1, "varieti": 4, "comprehens": 1, "subsequ": 1, "method": 6, "cosin": 1, "syntact": 1, "outlier": 3, "staheldonoho": 1, "gener": 2, "highdimension": 1, "lexic": 1, "distanc": 1, "along": 1, "data": 2, "measur": 1, "abod": 1, "stylist": 1, "work": 1, "employ": 1, "euclidean": 1, "vector": 2, "univari": 1}, "marker": "(2008)", "article": "D13-1151", "vector_2": [5, 0.26240489042075826, 6, 2, 0, 0]}, {"label": "CoCo", "current": "The idea is similar to Agrawal and Srikant's (1995) notion of generalized association rules.", "context": ["We generally look for rules that contain attitude type, orientation, thing type, and a product name, when these rules occur more frequently than expected.", "The idea is similar to Agrawal and Srikant's (1995) notion of generalized association rules.", "We treat each appraisal expression as a transaction, with the attributes of attitude type, orientation, polarity, force, and thing type, as well as the document attributes product name, product type, and document classification (based on the number of stars the reviewer gave the product)."], "vector_1": {"apprais": 1, "classif": 1, "polar": 1, "idea": 1, "attitud": 2, "expect": 1, "occur": 1, "transact": 1, "orient": 2, "treat": 1, "document": 2, "type": 5, "attribut": 2, "srikant": 1, "product": 4, "star": 1, "forc": 1, "gener": 2, "express": 1, "number": 1, "base": 1, "review": 1, "associ": 1, "name": 2, "agraw": 1, "look": 1, "well": 1, "rule": 3, "thing": 2, "contain": 1, "gave": 1, "similar": 1, "notion": 1, "frequent": 1}, "marker": "(1995)", "article": "N07-1039", "vector_2": [12, 0.809819046932449, 1, 1, 0, 0]}, {"label": "Pos", "current": "For adjectival attitudes, we used the lexicon developed we developed in our previous work (Whitelaw et al., 2005) on appraisal.", "context": ["311", "For adjectival attitudes, we used the lexicon developed we developed in our previous work (Whitelaw et al., 2005) on appraisal.", "We reviewed the entire lexicon to determine its accuracy and made numerous improvements."], "vector_1": {"apprais": 1, "use": 1, "lexicon": 2, "develop": 2, "made": 1, "numer": 1, "review": 1, "previou": 1, "accuraci": 1, "attitud": 1, "entir": 1, "determin": 1, "improv": 1, "work": 1, "adjectiv": 1}, "marker": "(Whitelaw et al., 2005)", "article": "N07-1039", "vector_2": [2, 0.5437243169201932, 1, 4, 0, 0]}, {"label": "Pos", "current": "We use CLOSET+ (Wang et al., 2003) to find all of the frequent closed itemsets in the data, with a support greater than or equal to 20 occurrences.", "context": ["We treat each appraisal expression as a transaction, with the attributes of attitude type, orientation, polarity, force, and thing type, as well as the document attributes product name, product type, and document classification (based on the number of stars the reviewer gave the product).", "We use CLOSET+ (Wang et al., 2003) to find all of the frequent closed itemsets in the data, with a support greater than or equal to 20 occurrences.", "Let (b, a1, a2,... an) or (b, A) denote the contents of an itemset, and c ((b, A)) denote the support for this itemset."], "vector_1": {"apprais": 1, "classif": 1, "number": 1, "attitud": 1, "close": 1, "orient": 1, "denot": 2, "use": 1, "closet": 1, "transact": 1, "support": 2, "find": 1, "content": 1, "treat": 1, "document": 2, "type": 3, "gave": 1, "polar": 1, "product": 3, "star": 1, "greater": 1, "forc": 1, "express": 1, "itemset": 3, "base": 1, "let": 1, "data": 1, "a": 2, "c": 1, "b": 3, "name": 1, "occurr": 1, "well": 1, "equal": 1, "thing": 1, "attribut": 2, "review": 1, "frequent": 1}, "marker": "(Wang et al., 2003)", "article": "N07-1039", "vector_2": [4, 0.8189387931655973, 1, 1, 0, 0]}, {"label": "Neut", "current": "Earlier work already establishes the importance of lexical changes for text simplification (Carroll et al., 1998; Caseli et al., 2009; De Belder et al., 2010).", "context": ["The present work, however, deals with lexical simplification and is centred around a corpus analysis, a preparatory stage for the development of a separate lexical module in the future.", "Earlier work already establishes the importance of lexical changes for text simplification (Carroll et al., 1998; Caseli et al., 2009; De Belder et al., 2010).", "Upon examining a parallel corpus consisting of original and manually simplified newspaper articles in Spanish, we have found that by far the most common type of changes applied by human editors are precisely lexical changes, accounting for 17.48% of all annotated operations (Bott and Saggion, 2012a)."], "vector_1": {"corpu": 2, "origin": 1, "deal": 1, "centr": 1, "modul": 1, "newspap": 1, "alreadi": 1, "human": 1, "spanish": 1, "oper": 1, "establish": 1, "precis": 1, "develop": 1, "articl": 1, "futur": 1, "text": 1, "import": 1, "preparatori": 1, "type": 1, "editor": 1, "analysi": 1, "simplifi": 1, "around": 1, "far": 1, "earlier": 1, "upon": 1, "lexic": 4, "examin": 1, "appli": 1, "parallel": 1, "present": 1, "stage": 1, "account": 1, "consist": 1, "simplif": 2, "howev": 1, "work": 2, "manual": 1, "annot": 1, "separ": 1, "common": 1, "found": 1, "chang": 3}, "marker": "(Carroll et al., 1998", "article": "W12-2202", "vector_2": [14, 0.09066292247517801, 4, 2, 1, 0]}, {"label": "Neut", "current": "Upon examining a parallel corpus consisting of original and manually simplified newspaper articles in Spanish, we have found that by far the most common type of changes applied by human editors are precisely lexical changes, accounting for 17.48% of all annotated operations (Bott and Saggion, 2012a).", "context": ["Earlier work already establishes the importance of lexical changes for text simplification (Carroll et al., 1998; Caseli et al., 2009; De Belder et al., 2010).", "Upon examining a parallel corpus consisting of original and manually simplified newspaper articles in Spanish, we have found that by far the most common type of changes applied by human editors are precisely lexical changes, accounting for 17.48% of all annotated operations (Bott and Saggion, 2012a).", "Words perceived as more complicated are replaced"], "vector_1": {"corpu": 1, "origin": 1, "text": 1, "newspap": 1, "alreadi": 1, "human": 1, "spanish": 1, "establish": 1, "articl": 1, "editor": 1, "appli": 1, "import": 1, "type": 1, "simplifi": 1, "far": 1, "earlier": 1, "upon": 1, "lexic": 2, "examin": 1, "parallel": 1, "oper": 1, "account": 1, "perceiv": 1, "word": 1, "consist": 1, "simplif": 1, "replac": 1, "complic": 1, "work": 1, "manual": 1, "annot": 1, "precis": 1, "common": 1, "found": 1, "chang": 3}, "marker": "(Bott and Saggion, 2012a)", "article": "W12-2202", "vector_2": [0, 0.09968909838531742, 4, 1, 1, 0]}, {"label": "Neut", "current": "The automatic alignments have then been manually corrected through a graphical editing tool within the GATE framework (Cunningham et al., 2002).", "context": ["aligning algorithm based on Hidden Markov Models (Bott and Saggion, 2011) has been applied to obtain sentence-level alignments.", "The automatic alignments have then been manually corrected through a graphical editing tool within the GATE framework (Cunningham et al., 2002).", "A total of 570 sentences have been aligned (246 in original and 324 in simple texts), with the following correlations between them: one to one, one to many or many to one, as well as cases where there is no correlation (cases of content reduction through summarisation or information expansion through the introduction of definitions)."], "vector_1": {"origin": 1, "expans": 1, "appli": 1, "within": 1, "obtain": 1, "automat": 1, "one": 4, "correl": 2, "follow": 1, "sentencelevel": 1, "total": 1, "well": 1, "definit": 1, "reduct": 1, "content": 1, "text": 1, "gate": 1, "hidden": 1, "simpl": 1, "correct": 1, "sentenc": 1, "tool": 1, "framework": 1, "base": 1, "model": 1, "introduct": 1, "case": 2, "graphic": 1, "algorithm": 1, "edit": 1, "align": 4, "manual": 1, "inform": 1, "summaris": 1, "mani": 2, "markov": 1}, "marker": "(Cunningham et al., 2002)", "article": "W12-2202", "vector_2": [10, 0.2495570487747802, 2, 1, 0, 0]}, {"label": "Neut", "current": "The treebanks (Hajic et al., 2004; Chen et al., 2003; Bohmova et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair.", "context": ["This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques.", "The treebanks (Hajic et al., 2004; Chen et al., 2003; Bohmova et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair.", "An initial model is built to identify dependency relations between adjacent word pairs using a decision list learning algorithm."], "vector_1": {"classif": 1, "set": 1, "identifi": 1, "treebank": 1, "share": 1, "direct": 1, "indic": 1, "paper": 1, "exist": 1, "languag": 1, "supervis": 1, "use": 2, "depend": 3, "techniqu": 1, "adjac": 1, "decis": 1, "approach": 1, "attribut": 1, "machin": 1, "conll": 1, "relat": 2, "initi": 1, "standard": 1, "candid": 1, "link": 1, "pair": 3, "present": 1, "built": 1, "convert": 1, "task": 1, "word": 2, "algorithm": 1, "consist": 1, "provid": 1, "type": 1, "list": 1, "instanc": 1, "learn": 3, "model": 1}, "marker": "(Hajic et al., 2004", "article": "W06-2938", "vector_2": [2, 0.06630021715526602, 14, 1, 1, 0]}, {"label": "Neut", "current": "21: yes/no sounds: Use \"uh-huh\" or \"um-hum\" (yes) and \"huh-uh\" or \"hum-um\" (no) for anything remotely resembling these sounds of assent or denial\" Another scheme (Lander, 1996) lists several \"miscellaneous words\", including:", "context": ["<other speaker responds> urn ok, I see your point.\"", "21: yes/no sounds: Use \"uh-huh\" or \"um-hum\" (yes) and \"huh-uh\" or \"hum-um\" (no) for anything remotely resembling these sounds of assent or denial\" Another scheme (Lander, 1996) lists several \"miscellaneous words\", including:", "30"], "vector_1": {"respond": 1, "yesno": 1, "point": 1, "assent": 1, "see": 1, "resembl": 1, "uhhuh": 1, "sever": 1, "use": 1, "denial": 1, "miscellan": 1, "humum": 1, "speaker": 1, "includ": 1, "scheme": 1, "umhum": 1, "ye": 1, "word": 1, "sound": 2, "ok": 1, "huhuh": 1, "anyth": 1, "remot": 1, "urn": 1, "list": 1, "anoth": 1}, "marker": "(Lander, 1996)", "article": "W00-1004", "vector_2": [4, 0.3496968407616211, 1, 1, 0, 0]}, {"label": "Neut", "current": "phrases in statistical machine transliteration and improved transliteration performance (Finch and Sumita, 2010).", "context": ["BASELINE 104,563 899,080 1,372,993 PROPOSED 104,561 893,366 1,317,256", "phrases in statistical machine transliteration and improved transliteration performance (Finch and Sumita, 2010).", "We extracted them by: 1) generate many-to-many word alignment, in which all possible word alignment links in many-to-many correspondences (e.g., 0-0 0-1 0-2 1-0 1-1 1-2 for  , c o m), 2) run phrase extraction and scoring same as a standard Moses training."], "vector_1": {"mose": 1, "eg": 1, "phrase": 2, "extract": 2, "baselin": 1, "perform": 1, "0": 2, "score": 1, "manytomani": 2, "machin": 1, "run": 1, "gener": 1, "standard": 1, "train": 1, "link": 1, "transliter": 2, "c": 1, "word": 2, "possibl": 1, "align": 2, "correspond": 1, "statist": 1, "improv": 1, "propos": 1}, "marker": "(Finch and Sumita, 2010)", "article": "D13-1021", "vector_2": [3, 0.7428835489833642, 1, 9, 2, 0]}, {"label": "Pos", "current": "We rescore the ASR N-best lists with the standard HMM (Vogel et al., 1996) and IBM (Brown et al., 1993) MT models.", "context": ["better MT system, and generating a larger N-best list from the ASR word graphs.", "We rescore the ASR N-best lists with the standard HMM (Vogel et al., 1996) and IBM (Brown et al., 1993) MT models.", "The development and evaluation sets Nbest lists sizes are sufficiently large to achieve almost the best possible results, on average 1738 hypotheses per each source sentence are extracted from the ASR word graphs."], "vector_1": {"set": 1, "evalu": 1, "almost": 1, "result": 1, "rescor": 1, "best": 1, "size": 1, "develop": 1, "suffici": 1, "graph": 2, "system": 1, "per": 1, "better": 1, "larg": 1, "asr": 3, "sourc": 1, "sentenc": 1, "gener": 1, "standard": 1, "hmm": 1, "nbest": 3, "averag": 1, "extract": 1, "word": 2, "ibm": 1, "possibl": 1, "hypothes": 1, "larger": 1, "list": 3, "mt": 2, "achiev": 1, "model": 1}, "marker": "(Vogel et al., 1996)", "article": "P06-2061", "vector_2": [10, 0.425, 2, 1, 17, 1]}, {"label": "Pos", "current": "In this work, we used the confidence method proposed by Vlachos (2008).", "context": ["In a real annotation setting, it is important to decide when to stop adding new instances to the training set.", "In this work, we used the confidence method proposed by Vlachos (2008).", "This is an method that measures the model's confidence on a held-out non-annotated dataset every time a new instance is added to the training set and stops the AL procedure when this confidence starts to drop."], "vector_1": {"set": 3, "ad": 2, "al": 1, "procedur": 1, "dataset": 1, "drop": 1, "decid": 1, "start": 1, "new": 2, "method": 2, "confid": 3, "real": 1, "everi": 1, "stop": 2, "use": 1, "train": 2, "import": 1, "nonannot": 1, "measur": 1, "vlacho": 1, "work": 1, "annot": 1, "instanc": 2, "time": 1, "model": 1, "heldout": 1, "propos": 1}, "marker": "(2008)", "article": "W13-2241", "vector_2": [5, 0.6293741307371349, 1, 2, 0, 0]}, {"label": "Neut", "current": "1 shows a sample Speech Graffiti dialog User interactions with Speech Graffiti (independent of other speech interfaces) have previously been assessed in Rosenfeld et al (2000).", "context": ["Fig.", "1 shows a sample Speech Graffiti dialog User interactions with Speech Graffiti (independent of other speech interfaces) have previously been assessed in Rosenfeld et al (2000).", "Here we consider a head-to-head comparison: given the chance to interact with both types of interfaces, which would people choose?"], "vector_1": {"rosenfeld": 1, "show": 1, "independ": 1, "al": 1, "assess": 1, "sampl": 1, "headtohead": 1, "et": 1, "previous": 1, "given": 1, "would": 1, "interact": 2, "speech": 3, "fig": 1, "interfac": 2, "type": 1, "peopl": 1, "user": 1, "consid": 1, "comparison": 1, "dialog": 1, "choos": 1, "graffiti": 2, "chanc": 1}, "marker": "(2000)", "article": "N04-4019", "vector_2": [4, 0.1427758816837315, 1, 1, 2, 1]}, {"label": "Neut", "current": "General information about the Speech Graffiti project and its motivation can be found in Rosenfeld et al (2001).", "context": ["As far as we know, no studies have been done comparing constrained, \"universal\" languages and natural language interfaces directly as we have done in this study.", "General information about the Speech Graffiti project and its motivation can be found in Rosenfeld et al (2001).", "2 Method"], "vector_1": {"rosenfeld": 1, "directli": 1, "natur": 1, "al": 1, "graffiti": 1, "et": 1, "languag": 2, "compar": 1, "univers": 1, "speech": 1, "interfac": 1, "method": 1, "far": 1, "gener": 1, "know": 1, "found": 1, "studi": 2, "project": 1, "inform": 1, "done": 2, "motiv": 1, "constrain": 1}, "marker": "(2001)", "article": "N04-4019", "vector_2": [3, 0.2260144103147516, 1, 1, 2, 1]}, {"label": "Neut", "current": "MEANT, Lo and Wu (2011)), as we go beyond a \"flat\" n-gram matching but evaluate a meaningful entity, in our case a preposition-noun-verb triple.", "context": ["Conceptually, this is loosely related to semantically focused metrics (e.g.", "MEANT, Lo and Wu (2011)), as we go beyond a \"flat\" n-gram matching but evaluate a meaningful entity, in our case a preposition-noun-verb triple.", "3 Methodology"], "vector_1": {"semant": 1, "evalu": 1, "eg": 1, "focus": 1, "go": 1, "beyond": 1, "entiti": 1, "lo": 1, "prepositionnounverb": 1, "metric": 1, "match": 1, "meant": 1, "flat": 1, "methodolog": 1, "relat": 1, "ngram": 1, "case": 1, "tripl": 1, "wu": 1, "conceptu": 1, "loos": 1, "meaning": 1}, "marker": "(2011)", "article": "W15-4923", "vector_2": [4, 0.18090272119928305, 1, 1, 0, 0]}, {"label": "CoCo", "current": "These features are similar to the position features proposed by Collobert et al (2011) for the Semantic Role Labeling task.", "context": ["Zeng et al (2014) propose the use of word position embeddings (position features) which help the CNN by keeping track of how close words are to the target nouns.", "These features are similar to the position features proposed by Collobert et al (2011) for the Semantic Role Labeling task.", "In this work we also experiment with the word position embeddings (WPE) proposed by Zeng et al (2014)."], "vector_1": {"semant": 1, "featur": 3, "help": 1, "al": 3, "zeng": 2, "et": 3, "close": 1, "use": 1, "label": 1, "also": 1, "role": 1, "cnn": 1, "experi": 1, "track": 1, "noun": 1, "wpe": 1, "task": 1, "word": 3, "target": 1, "work": 1, "keep": 1, "posit": 4, "collobert": 1, "embed": 2, "similar": 1, "propos": 3}, "marker": "(2011)", "article": "P15-1061", "vector_2": [4, 0.20082843399251737, 3, 3, 0, 0]}, {"label": "CoCo", "current": "This approach is similar to the one used by Weston et al (2014) to select negative examples.", "context": ["For tasks where the number of classes is large, we can fix a number of negative classes to be considered at each example and select the one with the largest score to perform a gradient step.", "This approach is similar to the one used by Weston et al (2014) to select negative examples.", "We use the backpropagation algorithm to compute gradients of the network."], "vector_1": {"comput": 1, "al": 1, "one": 2, "weston": 1, "et": 1, "select": 2, "use": 2, "network": 1, "perform": 1, "neg": 2, "fix": 1, "score": 1, "larg": 1, "largest": 1, "approach": 1, "number": 2, "step": 1, "consid": 1, "class": 2, "task": 1, "algorithm": 1, "backpropag": 1, "gradient": 2, "exampl": 2, "similar": 1}, "marker": "(2014)", "article": "P15-1061", "vector_2": [1, 0.3973476750400855, 1, 2, 1, 0]}, {"label": "Pos", "current": "We used unsupervised maximum likelihood linear regression (MLLR) AM adaptation on top of the previous adaptation and optimization steps (Tomokiyo and Waibel, 2001; Wang et al., 2003).", "context": ["4.4 Unsupervised speaker adaptation", "We used unsupervised maximum likelihood linear regression (MLLR) AM adaptation on top of the previous adaptation and optimization steps (Tomokiyo and Waibel, 2001; Wang et al., 2003).", "In this step, all words whose confidence score was higher than a pre-set threshold were collected and their acoustic information was used to adapt the acoustic model."], "vector_1": {"inform": 1, "threshold": 1, "whose": 1, "use": 2, "mllr": 1, "top": 1, "score": 1, "speaker": 1, "adapt": 4, "higher": 1, "linear": 1, "optim": 1, "previou": 1, "step": 2, "preset": 1, "likelihood": 1, "acoust": 2, "word": 1, "unsupervis": 2, "maximum": 1, "collect": 1, "regress": 1, "model": 1, "confid": 1}, "marker": "(Tomokiyo and Waibel, 2001", "article": "W08-0912", "vector_2": [7, 0.605255677331733, 2, 2, 2, 0]}, {"label": "Neut", "current": "Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al (2012) showed that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure.", "context": ["The ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave tokens unaligned instead of aligning them anyway as the other strategies tend to do.", "Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al (2012) showed that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure.", "5 Conclusion"], "vector_1": {"semant": 1, "simpli": 1, "show": 1, "itg": 2, "prefer": 1, "anyway": 1, "accept": 2, "leav": 1, "manner": 1, "threshold": 1, "et": 1, "select": 1, "better": 1, "appear": 1, "fit": 1, "filler": 1, "tend": 2, "note": 1, "role": 1, "instead": 1, "approach": 1, "strategi": 2, "competit": 1, "spars": 1, "wherea": 1, "al": 1, "link": 1, "tumuluru": 1, "consid": 1, "conclus": 1, "lower": 1, "appropri": 1, "whenev": 1, "align": 4, "work": 1, "well": 1, "structur": 1, "matter": 1, "token": 3, "paper": 1, "clean": 1, "unalign": 1}, "marker": "(2012)", "article": "W14-4719", "vector_2": [2, 0.864093204660233, 2, 3, 23, 0]}, {"label": "Pos", "current": "To calculate the ( ) inside probability (or more accurately, inside score) of a pair of segments, P A   e/f|G , we use the algorithm described in Saers et al (2009).", "context": ["The rule probability (or more accurately, rule weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is defined by MEANT's lexical similarity measure on English Gigaword context vectors.", "To calculate the ( ) inside probability (or more accurately, inside score) of a pair of segments, P A   e/f|G , we use the algorithm described in Saers et al (2009).", "Given this, si,pred and si, now represent the length normalized BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and machine translations."], "vector_1": {"gigaword": 1, "meant": 1, "set": 1, "weight": 1, "al": 1, "repres": 1, "saer": 1, "structur": 1, "sipr": 1, "et": 1, "probabl": 2, "use": 1, "accur": 2, "describ": 1, "p": 2, "bitg": 1, "si": 1, "score": 2, "role": 1, "length": 1, "type": 1, "efg": 1, "function": 1, "insid": 2, "normal": 1, "transduct": 2, "given": 1, "lexic": 2, "argument": 1, "machin": 1, "pars": 1, "translat": 1, "pair": 1, "segment": 1, "refer": 1, "measur": 1, "algorithm": 1, "j": 1, "rule": 4, "predic": 1, "defin": 1, "calcul": 1, "vector": 1, "context": 1, "english": 1, "filler": 1, "similar": 1}, "marker": "(2009)", "article": "W14-4719", "vector_2": [5, 0.6744337216860843, 1, 4, 22, 0]}, {"label": "Neut", "current": "The objective of the Recognizing Textual Entailment Challenge is determining whether the meaning of the Hypothesis (H) can be inferred from a text (T) (Ido Dagan et al., 2006).", "context": ["1 Introduction", "The objective of the Recognizing Textual Entailment Challenge is determining whether the meaning of the Hypothesis (H) can be inferred from a text (T) (Ido Dagan et al., 2006).", "This challenge has been organized by NIST in recent years."], "vector_1": {"recogn": 1, "entail": 1, "hypothesi": 1, "whether": 1, "h": 1, "challeng": 2, "object": 1, "year": 1, "textual": 1, "recent": 1, "determin": 1, "text": 1, "organ": 1, "mean": 1, "infer": 1, "nist": 1, "introduct": 1}, "marker": "(Ido Dagan et al., 2006)", "article": "W10-1609", "vector_2": [4, 0.05126655310731359, 1, 1, 0, 0]}, {"label": "Pos", "current": "We used four classifiers to learn every development set: (1) Support Vector Machine, (2) Ada Boost, (3) Multilayer Perceptron (MLP) and (4) Decision Tree using the open source WEKA Data Mining Software (Witten & Frank, 2005).", "context": ["The training set SPARTE-Balanced was created by taking all true cases and randomly taking false cases, and then we build a balanced training set containing 1352 pairs, with 676 true and 676 false pairs.", "We used four classifiers to learn every development set: (1) Support Vector Machine, (2) Ada Boost, (3) Multilayer Perceptron (MLP) and (4) Decision Tree using the open source WEKA Data Mining Software (Witten & Frank, 2005).", "In all the tables results we show only the accuracy of the best classifier."], "vector_1": {"randomli": 1, "softwar": 1, "set": 3, "creat": 1, "show": 1, "mine": 1, "four": 1, "spartebalanc": 1, "result": 1, "tabl": 1, "accuraci": 1, "perceptron": 1, "best": 1, "use": 2, "develop": 1, "open": 1, "support": 1, "classifi": 2, "data": 1, "take": 2, "ada": 1, "boost": 1, "build": 1, "machin": 1, "sourc": 1, "everi": 1, "fals": 2, "learn": 1, "multilay": 1, "train": 2, "pair": 2, "weka": 1, "true": 2, "case": 2, "tree": 1, "mlp": 1, "vector": 1, "balanc": 1, "contain": 1, "decis": 1}, "marker": "(Witten & Frank, 2005)", "article": "W10-1609", "vector_2": [5, 0.5934941480301116, 1, 1, 0, 0]}, {"label": "Neut", "current": "Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003).", "context": ["Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999).", "Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003).", "In lexical semantics, smoothing is often achieved by backing"], "vector_1": {"semant": 1, "often": 1, "deal": 1, "appli": 1, "back": 1, "languag": 1, "also": 1, "promin": 1, "document": 1, "method": 2, "expans": 1, "mismatch": 1, "lexic": 2, "ngram": 1, "queri": 2, "unseen": 1, "address": 1, "case": 1, "retriev": 1, "smooth": 2, "inform": 1, "achiev": 1, "mani": 1, "model": 1, "first": 1}, "marker": "(Voorhees, 1994", "article": "P13-2128", "vector_2": [19, 0.21322655321278858, 5, 1, 0, 0]}, {"label": "Neut", "current": "Klein and Manning (2003b), for example, show that the performance of an unlexicalised model can be substantially improved by splitting the existing symbols down into finer categories.", "context": ["While an unlexicalized parser that uses syntactic categories based solely on the symbols found in the Penn Treebank will generally perform poorly, a number of results show that refining these categories can substantially improve performance.", "Klein and Manning (2003b), for example, show that the performance of an unlexicalised model can be substantially improved by splitting the existing symbols down into finer categories.", "Their subcategorizations were developed by hand based on linguistic intuitions and a careful error analysis."], "vector_1": {"show": 2, "treebank": 1, "parser": 1, "unlexicalis": 1, "number": 1, "unlexic": 1, "exist": 1, "result": 1, "substanti": 2, "improv": 2, "use": 1, "develop": 1, "perform": 3, "categori": 3, "split": 1, "subcategor": 1, "penn": 1, "care": 1, "gener": 1, "finer": 1, "analysi": 1, "syntact": 1, "klein": 1, "symbol": 2, "sole": 1, "intuit": 1, "hand": 1, "base": 2, "refin": 1, "poorli": 1, "man": 1, "exampl": 1, "error": 1, "found": 1, "model": 1, "linguist": 1}, "marker": "(2003b)", "article": "W15-2610", "vector_2": [12, 0.4708396101700126, 1, 2, 8, 0]}, {"label": "CoCo", "current": "Although similarity in these cases is commonly being assessed between token sequences, as opposed to word types, the features used are similar to the ngram templates used here and the bigram distributions used by Koo et al (2008).", "context": ["POS tags are then propagated through the graph from labelled to unlabelled data.", "Although similarity in these cases is commonly being assessed between token sequences, as opposed to word types, the features used are similar to the ngram templates used here and the bigram distributions used by Koo et al (2008).", "A major difference in our approach is that it does not require retraining the parser or constructing a full model on the unlabelled data."], "vector_1": {"propag": 1, "major": 1, "featur": 1, "bigram": 1, "parser": 1, "al": 1, "templat": 1, "assess": 1, "tag": 1, "oppos": 1, "et": 1, "use": 3, "koo": 1, "graph": 1, "construct": 1, "label": 1, "type": 1, "po": 1, "unlabel": 2, "full": 1, "distribut": 1, "sequenc": 1, "differ": 1, "retrain": 1, "ngram": 1, "although": 1, "data": 2, "requir": 1, "case": 1, "word": 1, "approach": 1, "token": 1, "model": 1, "similar": 2, "commonli": 1}, "marker": "(2008)", "article": "W15-2610", "vector_2": [7, 0.9600946905647616, 1, 2, 1, 0]}, {"label": "Pos", "current": "A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high.", "context": ["However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a).", "A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high.", "On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004)."], "vector_1": {"often": 1, "proven": 1, "high": 1, "particularli": 1, "higherord": 1, "use": 2, "accur": 1, "depend": 2, "label": 1, "complex": 1, "factor": 1, "import": 1, "method": 1, "unlabel": 1, "graphbas": 1, "hand": 1, "joint": 2, "twostag": 1, "edgelabel": 1, "algorithm": 1, "provid": 1, "howev": 1, "tree": 2, "structur": 1, "inform": 1, "benefit": 1, "unaccept": 1, "learn": 2, "pars": 1, "model": 2, "produc": 1}, "marker": "(McDonald, 2006)", "article": "D15-1154", "vector_2": [9, 0.07520598375504003, 6, 8, 2, 0]}, {"label": "Neut", "current": "In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003).", "context": ["In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations.", "In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003).", "Surprisingly, genre (or style) adaptation has only been addressed to a limited extent (Bisazza and Federico, 2012; Wang et al., 2012), with methods requiring the availability of clearly separable in-domain and out-of-domain training corpora."], "vector_1": {"domain": 1, "concept": 1, "address": 2, "clearli": 1, "proven": 1, "topic": 1, "year": 1, "surprisingli": 1, "hardlabel": 1, "outofdomain": 1, "use": 1, "explicitli": 1, "latent": 1, "smt": 1, "avail": 1, "adapt": 2, "limit": 1, "corpora": 1, "method": 1, "topicgenr": 1, "previou": 1, "style": 1, "train": 1, "extent": 1, "particular": 1, "requir": 1, "recent": 1, "alloc": 1, "dirichlet": 1, "indomain": 1, "work": 2, "genr": 1, "correspond": 1, "separ": 1, "combin": 1, "typic": 1}, "marker": "(Eidelman et al., 2012", "article": "W15-2518", "vector_2": [3, 0.20976432089379565, 7, 2, 0, 0]}, {"label": "Neut", "current": "Kessler et al (1997) categorize four types of genre-revealing cues: structural cues (e.g., part-of-speech (POS) tag counts), lexical cues (specific words), character-level cues (e.g., punctuation marks), and derivative cues (ratios and variation measures based on other types of cues).", "context": ["Karlgren and Cutting (1994) were among the first to use simple document statistics, such as common word frequencies, first-person pronoun count, and average sentence length.", "Kessler et al (1997) categorize four types of genre-revealing cues: structural cues (e.g., part-of-speech (POS) tag counts), lexical cues (specific words), character-level cues (e.g., punctuation marks), and derivative cues (ratios and variation measures based on other types of cues).", "Dewdney et al (2001) compare a large number of document features and show that these outperform bag-of-words approaches, which are traditionally used in topic-based text classifica"], "vector_1": {"among": 1, "featur": 1, "pronoun": 1, "show": 1, "text": 1, "eg": 2, "al": 2, "four": 1, "tag": 1, "type": 2, "classifica": 1, "et": 2, "karlgren": 1, "genrerev": 1, "use": 2, "cut": 1, "ratio": 1, "compar": 1, "topicbas": 1, "mark": 1, "kessler": 1, "cue": 6, "approach": 1, "larg": 1, "variat": 1, "firstperson": 1, "document": 2, "simpl": 1, "po": 1, "statist": 1, "bagofword": 1, "deriv": 1, "sentenc": 1, "lexic": 1, "number": 1, "punctuat": 1, "base": 1, "partofspeech": 1, "categor": 1, "averag": 1, "count": 2, "measur": 1, "word": 2, "tradit": 1, "specif": 1, "frequenc": 1, "structur": 1, "length": 1, "dewdney": 1, "common": 1, "outperform": 1, "characterlevel": 1, "first": 1}, "marker": "(1997)", "article": "W15-2518", "vector_2": [18, 0.2312828479820183, 3, 1, 0, 0]}, {"label": "Pos", "current": "For the task of genre adaptation to the genres newswire (NW) and UG comments or weblogs, we use a flexible translation model adaptation approach based on phrase pair weighting using a vector space model (VSM) inspired by Chen et al (2013).", "context": ["3 Translation model genre adaptation", "For the task of genre adaptation to the genres newswire (NW) and UG comments or weblogs, we use a flexible translation model adaptation approach based on phrase pair weighting using a vector space model (VSM) inspired by Chen et al (2013).", "The reason we choose an instanceweighting method rather than a mixture modeling approach is twofold: First, mixture modeling approaches intrinsically depend on subcorpus boundaries, which resemble provenance or require manual labeling."], "vector_1": {"comment": 1, "weight": 1, "manual": 1, "inspir": 1, "al": 1, "proven": 1, "instanceweight": 1, "et": 1, "chen": 1, "phrase": 1, "flexibl": 1, "vsm": 1, "use": 2, "twofold": 1, "space": 1, "rather": 1, "resembl": 1, "boundari": 1, "newswir": 1, "label": 1, "adapt": 3, "approach": 3, "method": 1, "nw": 1, "mixtur": 2, "reason": 1, "base": 1, "translat": 2, "pair": 1, "depend": 1, "ug": 1, "requir": 1, "task": 1, "genr": 3, "intrins": 1, "weblog": 1, "vector": 1, "choos": 1, "model": 5, "subcorpu": 1, "first": 1}, "marker": "(2013)", "article": "W15-2518", "vector_2": [2, 0.27098139027534457, 1, 6, 2, 0]}, {"label": "Neut", "current": "Second, Irvine et al (2013) have shown that including relevant training data in a mixture modeling approach solves many coverage errors, but also introduces substantial amounts of new scoring errors.", "context": ["The reason we choose an instanceweighting method rather than a mixture modeling approach is twofold: First, mixture modeling approaches intrinsically depend on subcorpus boundaries, which resemble provenance or require manual labeling.", "Second, Irvine et al (2013) have shown that including relevant training data in a mixture modeling approach solves many coverage errors, but also introduces substantial amounts of new scoring errors.", "With phrase-pair weighting we aim to optimize phrase translation selection while keeping our training data fixed, and we can thus compare the impact of several methodological variants on genre adaptation for SMT."], "vector_1": {"phrase": 1, "weight": 1, "genr": 1, "al": 1, "proven": 1, "second": 1, "substanti": 1, "phrasepair": 1, "et": 1, "select": 1, "impact": 1, "shown": 1, "twofold": 1, "compar": 1, "rather": 1, "resembl": 1, "boundari": 1, "smt": 1, "coverag": 1, "label": 1, "also": 1, "score": 1, "includ": 1, "sever": 1, "new": 1, "approach": 3, "method": 1, "introduc": 1, "adapt": 1, "optim": 1, "methodolog": 1, "error": 2, "variant": 1, "mixtur": 3, "reason": 1, "train": 2, "translat": 1, "solv": 1, "depend": 1, "data": 2, "relev": 1, "requir": 1, "irvin": 1, "instanceweight": 1, "thu": 1, "manual": 1, "intrins": 1, "keep": 1, "aim": 1, "amount": 1, "choos": 1, "mani": 1, "model": 3, "subcorpu": 1, "fix": 1, "first": 1}, "marker": "(2013)", "article": "W15-2518", "vector_2": [2, 0.27907976068489077, 1, 1, 1, 0]}, {"label": "Pos", "current": "Since our aim is to adapt to multiple genres in a test corpus, we follow Chen et al (2013) and manually group our training data into subcorpora that reflect various genres (see Table 3).", "context": ["different subcorpora.", "Since our aim is to adapt to multiple genres in a test corpus, we follow Chen et al (2013) and manually group our training data into subcorpora that reflect various genres (see Table 3).", "While this definition of the vector space can approximate genres at different levels of granularity, manual subcorpus labels are labor intensive to generate, particularly in the scenario where provenance information is not available, and may not generalize well to new translation tasks."], "vector_1": {"corpu": 1, "genr": 3, "approxim": 1, "al": 1, "proven": 1, "see": 1, "particularli": 1, "tabl": 1, "chen": 1, "et": 1, "labor": 1, "well": 1, "granular": 1, "differ": 2, "multipl": 1, "space": 1, "definit": 1, "label": 1, "subcorpora": 2, "adapt": 1, "test": 1, "new": 1, "variou": 1, "may": 1, "gener": 2, "reflect": 1, "train": 1, "translat": 1, "group": 1, "data": 1, "sinc": 1, "avail": 1, "task": 1, "scenario": 1, "level": 1, "manual": 2, "aim": 1, "inform": 1, "vector": 1, "follow": 1, "subcorpu": 1, "intens": 1}, "marker": "(2013)", "article": "W15-2518", "vector_2": [2, 0.37579744157604206, 1, 6, 2, 0]}, {"label": "Pos", "current": "We use approximate randomization (Noreen, 1989) for significance testing (Riezler and Maxwell, 2005).", "context": ["Translation quality of all experiments is measured with case-insensitive BLEU (Papineni et al., 2002) using the closest-reference brevity penalty.", "We use approximate randomization (Noreen, 1989) for significance testing (Riezler and Maxwell, 2005).", "Statistically significant differences are marked by A and  for the p < 0.05 and the p < 0.01 level, respectively."], "vector_1": {"bleu": 1, "measur": 1, "use": 2, "caseinsensit": 1, "breviti": 1, "closestrefer": 1, "penalti": 1, "approxim": 1, "random": 1, "qualiti": 1, "differ": 1, "mark": 1, "p": 2, "signific": 2, "translat": 1, "test": 1, "respect": 1, "experi": 1, "level": 1, "statist": 1}, "marker": "(Noreen, 1989)", "article": "W15-2518", "vector_2": [26, 0.6692559415595147, 3, 1, 0, 0]}, {"label": "Pos", "current": "We perform our experiments using an inhouse phrase-based SMT system similar to Moses (Koehn et al., 2007).", "context": ["Note that Gen&Topic contains one reference translation per sentence, while NIST has four sets of reference translations.", "We perform our experiments using an inhouse phrase-based SMT system similar to Moses (Koehn et al., 2007).", "All runs use lexicalized reordering, distinguishing between monotone, swap, and discontinuous reordering, with respect to the previous and next phrase (Koehn et al., 2005)."], "vector_1": {"set": 1, "monoton": 1, "one": 1, "four": 1, "phrase": 1, "respect": 1, "use": 2, "perform": 1, "smt": 1, "system": 1, "per": 1, "next": 1, "note": 1, "swap": 1, "experi": 1, "discontinu": 1, "reorder": 2, "refer": 2, "mose": 1, "run": 1, "sentenc": 1, "previou": 1, "lexic": 1, "inhous": 1, "translat": 2, "distinguish": 1, "gentop": 1, "phrasebas": 1, "contain": 1, "similar": 1, "nist": 1}, "marker": "(Koehn et al., 2007)", "article": "W15-2518", "vector_2": [8, 0.5575975936270783, 2, 2, 5, 0]}, {"label": "Pos", "current": "Our corpus study adopts similar features of annotation used in Botley and McEnery (2001) and provides some linguistic hypotheses on grammatical functions of Korean demonstratives to be further explored.", "context": ["Through the development of an annotation scheme and the use of spoken and written corpora, we aim to determine different functions of demonstratives and to examine their distributional properties.", "Our corpus study adopts similar features of annotation used in Botley and McEnery (2001) and provides some linguistic hypotheses on grammatical functions of Korean demonstratives to be further explored.", "1 Introduction"], "vector_1": {"corpu": 1, "corpora": 1, "spoken": 1, "annot": 2, "featur": 1, "explor": 1, "use": 2, "develop": 1, "written": 1, "mceneri": 1, "scheme": 1, "function": 2, "distribut": 1, "differ": 1, "examin": 1, "korean": 1, "demonstr": 2, "introduct": 1, "grammat": 1, "provid": 1, "hypothes": 1, "properti": 1, "botley": 1, "adopt": 1, "aim": 1, "determin": 1, "studi": 1, "linguist": 1, "similar": 1}, "marker": "(2001)", "article": "W10-1824", "vector_2": [9, 0.021137705937208578, 1, 5, 0, 0]}, {"label": "Neut", "current": "We use WordNet version 3.0 accessed using NLTK version 2.0 (Bird et al., 2009).", "context": ["A synset feature is on for a word if the synset occurs on the path from all senses of the word to the root, and off otherwise.", "We use WordNet version 3.0 accessed using NLTK version 2.0 (Bird et al., 2009).", "Polarity Because of the observation that the verb in the target construction, in particular, has some property of negativity in the \"no\" interpretation, we also use features representing the semantic polarity of the noun, adjective, and verb in each instance."], "vector_1": {"semant": 1, "featur": 2, "noun": 1, "repres": 1, "occur": 1, "wordnet": 1, "use": 3, "neg": 1, "construct": 1, "access": 1, "also": 1, "version": 2, "synset": 2, "sens": 1, "polar": 2, "verb": 2, "particular": 1, "adject": 1, "path": 1, "interpret": 1, "word": 2, "target": 1, "properti": 1, "nltk": 1, "instanc": 1, "otherwis": 1, "root": 1, "observ": 1}, "marker": "(Bird et al., 2009)", "article": "W10-2109", "vector_2": [1, 0.580249063833493, 1, 1, 0, 0]}, {"label": "Neut", "current": "Generally speaking, there are two unsupervised scenarios for \"borrowing\" English resources for sentiment analysis in other languages: one is to generate resources in a new language by leveraging on the resources available in English via cross-lingual projections, and then perform sentiment analysis in the English language based on the generated resources, which has been investigated by Mihalcea et al (2007); the other is to translate the texts in a new language into English texts, and then perform sentiment analysis in the English language, which has not yet been investigated.", "context": ["Instead of using only the limited Chinese knowledge, this study aims to improve Chinese sentiment analysis by making full use of bilingual knowledge in an unsupervised way, including both Chinese resources and English resources.", "Generally speaking, there are two unsupervised scenarios for \"borrowing\" English resources for sentiment analysis in other languages: one is to generate resources in a new language by leveraging on the resources available in English via cross-lingual projections, and then perform sentiment analysis in the English language based on the generated resources, which has been investigated by Mihalcea et al (2007); the other is to translate the texts in a new language into English texts, and then perform sentiment analysis in the English language, which has not yet been investigated.", "In this study, we first translate Chinese reviews into English reviews by using machine translation services, and then identify the sentiment polarity of English reviews by directly leveraging English resources."], "vector_1": {"machin": 1, "via": 1, "identifi": 1, "directli": 1, "text": 2, "knowledg": 2, "al": 1, "one": 1, "crosslingu": 1, "et": 1, "yet": 1, "languag": 5, "speak": 1, "project": 1, "use": 3, "servic": 1, "sentiment": 5, "perform": 2, "make": 1, "two": 1, "avail": 1, "includ": 1, "way": 1, "instead": 1, "leverag": 2, "gener": 3, "analysi": 4, "polar": 1, "full": 1, "resourc": 7, "chines": 4, "bilingu": 1, "base": 1, "translat": 3, "investig": 2, "new": 2, "mihalcea": 1, "scenario": 1, "unsupervis": 2, "borrow": 1, "aim": 1, "limit": 1, "english": 9, "improv": 1, "studi": 2, "review": 3, "first": 1}, "marker": "(2007)", "article": "D08-1058", "vector_2": [1, 0.08537754268877135, 1, 3, 9, 0]}, {"label": "Pos", "current": "The method uses improved Kneser-Ney smoothing algorithm (Chen and Goodman, 1999) to compute sequence probabilities.", "context": ["We trained a language model of order 5 built on the entire EUROPARL corpus using the SRILM package.", "The method uses improved Kneser-Ney smoothing algorithm (Chen and Goodman, 1999) to compute sequence probabilities.", "5 Dataset"], "vector_1": {"corpu": 1, "use": 2, "comput": 1, "built": 1, "algorithm": 1, "dataset": 1, "sequenc": 1, "train": 1, "smooth": 1, "method": 1, "packag": 1, "entir": 1, "srilm": 1, "europarl": 1, "improv": 1, "model": 1, "kneserney": 1, "order": 1, "languag": 1, "probabl": 1}, "marker": "(Chen and Goodman, 1999)", "article": "W10-3805", "vector_2": [11, 0.7296517373601613, 1, 1, 0, 0]}, {"label": "CoCo", "current": "This model is similar to the global lexical selection (GLS) model described in (Bangalore et al., 2007; Venkatapathy and Bangalore, 2009) except that in GLS, the predicted target blocks are not associated with any particular source word unlike the case here.", "context": ["These classifiers predict if a particular target block should be present given the source word and its context.", "This model is similar to the global lexical selection (GLS) model described in (Bangalore et al., 2007; Venkatapathy and Bangalore, 2009) except that in GLS, the predicted target blocks are not associated with any particular source word unlike the case here.", "For the set of experiments in this paper, we used a context of size 6, containing three words to the left and three words to the right."], "vector_1": {"set": 1, "predict": 2, "global": 1, "three": 2, "unlik": 1, "paper": 1, "right": 1, "gl": 2, "select": 1, "size": 1, "given": 1, "describ": 1, "except": 1, "classifi": 1, "experi": 1, "sourc": 2, "use": 1, "lexic": 1, "particular": 2, "associ": 1, "present": 1, "case": 1, "word": 4, "target": 2, "context": 2, "contain": 1, "model": 2, "similar": 1, "block": 2, "left": 1}, "marker": "(Bangalore et al., 2007", "article": "W10-3805", "vector_2": [3, 0.4601336126813456, 2, 4, 3, 1]}, {"label": "Neut", "current": "tween an initiating speech act and a responding one-the analog of adjacency pairs (Sacks et al., 1974).", "context": ["357", "tween an initiating speech act and a responding one-the analog of adjacency pairs (Sacks et al., 1974).", "The most closely related effort is (Galley et al., 2004), which aims to automatically identify adjacency pairs in the ICSI Meeting corpus, a large corpus of 75 meetings, using a small tagset."], "vector_1": {"respond": 1, "corpu": 2, "identifi": 1, "automat": 1, "close": 1, "icsi": 1, "oneth": 1, "analog": 1, "use": 1, "tween": 1, "adjac": 2, "larg": 1, "tagset": 1, "speech": 1, "relat": 1, "initi": 1, "pair": 2, "effort": 1, "aim": 1, "act": 1, "small": 1, "meet": 2}, "marker": "(Sacks et al., 1974)", "article": "W09-3953", "vector_2": [35, 0.1444322358201038, 2, 2, 0, 0]}, {"label": "Neut", "current": "For twenty narratives each segmented by the same seven annotators, using Cochran's Q (Cochran, 1950), we found the probabilities associated with the null hypothesis that the observed distributions could have arisen by chance to be at or below p=0.1 x106.", "context": ["We have previously shown (Passonneau and Litman, 1997) that intention-based segmentation can be done reliably by multiple annotators.", "For twenty narratives each segmented by the same seven annotators, using Cochran's Q (Cochran, 1950), we found the probabilities associated with the null hypothesis that the observed distributions could have arisen by chance to be at or below p=0.1 x106.", "Partitioning Q by number of annotators gave significant results for all values of A ranging over the number of annotators apart from A = 2."], "vector_1": {"intentionbas": 1, "seven": 1, "signific": 1, "done": 1, "result": 1, "null": 1, "probabl": 1, "shown": 1, "multipl": 1, "segment": 2, "narr": 1, "valu": 1, "twenti": 1, "previous": 1, "gave": 1, "distribut": 1, "hypothesi": 1, "use": 1, "number": 2, "cochran": 1, "x": 1, "rang": 1, "associ": 1, "arisen": 1, "apart": 1, "reliabl": 1, "annot": 4, "observ": 1, "q": 2, "p": 1, "partit": 1, "found": 1, "could": 1, "chanc": 1}, "marker": "(Cochran, 1950)", "article": "W09-3953", "vector_2": [59, 0.3485772742772705, 2, 4, 0, 0]}, {"label": "CoCo", "current": "A forward link (Flink) is the analog of a \"first pair-part\" of an adjacency pair (Sacks et al., 1974), and is similarly restricted to specific speech act types.", "context": ["DFU Links, or simply Links, correspond to adjacency pairs, but need not be adjacent.", "A forward link (Flink) is the analog of a \"first pair-part\" of an adjacency pair (Sacks et al., 1974), and is similarly restricted to specific speech act types.", "All Request-Information and Request-Action DFUs are assigned Flinks."], "vector_1": {"pairpart": 1, "simpli": 1, "specif": 1, "requestinform": 1, "dfu": 2, "flink": 2, "correspond": 1, "forward": 1, "restrict": 1, "need": 1, "adjac": 3, "requestact": 1, "link": 3, "act": 1, "similarli": 1, "pair": 2, "speech": 1, "type": 1, "assign": 1, "analog": 1, "first": 1}, "marker": "(Sacks et al., 1974)", "article": "W09-3953", "vector_2": [35, 0.3708180199295268, 1, 2, 0, 0]}, {"label": "Pos", "current": "However, Structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2005) is also a compelling method which has the potential to handle the interdependence between labeling and sequencing, due to its ability to handle dependencies among features and prediction results within the structure.", "context": ["Taskar, 2007) such as link prediction.", "However, Structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2005) is also a compelling method which has the potential to handle the interdependence between labeling and sequencing, due to its ability to handle dependencies among features and prediction results within the structure.", "sequence labeling (Tsochantaridis et al., 2005)."], "vector_1": {"among": 1, "svm": 1, "depend": 1, "abil": 1, "predict": 2, "sequenc": 2, "howev": 1, "interdepend": 1, "compel": 1, "due": 1, "structur": 2, "label": 2, "within": 1, "also": 1, "featur": 1, "link": 1, "result": 1, "potenti": 1, "handl": 2, "method": 1}, "marker": "(Taskar et al., 2003", "article": "W09-3953", "vector_2": [6, 0.5928086992763233, 4, 1, 0, 0]}, {"label": "Neut", "current": "As shown by Tversky (1977) and others, human judgments of object similarity have been found to shift both when the set of objects under discussion are altered (e.g., a violin and an electric guitar may be judged quite similar when in a group with a clarinet and an oboe, and may be judged quite different when the other members of the group are Computational Linguistics, Volume 14, Number 3, September 1988 57 Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions a cello and an electric bass), and when the salience of attributes are altered (e.g., in a group containing a red triangle, a blue triangle, and a red square, the red triangle might be judged similar to the blue triangle when attribute shape is stressed, but may be judged similar to the red square when attribute color is stressed).", "context": ["A second major problem with a similarity metric based on distance in the generalization hierarchy is that it is context invariant; contextual information has no way of affecting the assessments.", "As shown by Tversky (1977) and others, human judgments of object similarity have been found to shift both when the set of objects under discussion are altered (e.g., a violin and an electric guitar may be judged quite similar when in a group with a clarinet and an oboe, and may be judged quite different when the other members of the group are Computational Linguistics, Volume 14, Number 3, September 1988 57 Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions a cello and an electric bass), and when the salience of attributes are altered (e.g., in a group containing a red triangle, a blue triangle, and a red square, the red triangle might be judged similar to the blue triangle when attribute shape is stressed, but may be judged similar to the red square when attribute color is stressed).", "One metric that avoids these problems was introduced by Tversky (1977)."], "vector_1": {"respond": 1, "model": 1, "major": 1, "distanc": 1, "comput": 1, "eg": 2, "color": 1, "metric": 2, "number": 1, "one": 1, "assess": 1, "second": 1, "set": 1, "human": 1, "hierarchi": 1, "violin": 1, "clarinet": 1, "bass": 1, "blue": 2, "quit": 2, "shown": 1, "group": 3, "squar": 2, "kathleen": 1, "avoid": 1, "member": 1, "other": 1, "salienc": 1, "misconcept": 1, "way": 1, "electr": 2, "mccoy": 1, "might": 1, "alter": 2, "red": 4, "attribut": 3, "tverski": 2, "cello": 1, "may": 3, "gener": 1, "object": 2, "differ": 1, "guitar": 1, "reason": 1, "base": 1, "volum": 1, "user": 1, "highlight": 1, "affect": 1, "judgment": 1, "discuss": 1, "obo": 1, "stress": 2, "linguist": 1, "septemb": 1, "f": 1, "shift": 1, "judg": 4, "shape": 1, "contextu": 1, "inform": 1, "introduc": 1, "context": 1, "contain": 1, "triangl": 4, "found": 1, "problem": 2, "invari": 1, "similar": 5}, "marker": "(1977)", "article": "J88-3005", "vector_2": [11, 0.5573833910544543, 2, 2, 0, 0]}, {"label": "Pos", "current": "We employ the Gibbs sampling algorithm (Gilks et al., 1996).", "context": ["5 Inference", "We employ the Gibbs sampling algorithm (Gilks et al., 1996).", "Unlike in (Marecek and Zabokrtsky, 2012), where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming."], "vector_1": {"given": 1, "gibb": 1, "edg": 1, "algorithm": 1, "possibl": 1, "sentenc": 1, "individu": 1, "tree": 1, "use": 1, "employ": 1, "unlik": 1, "program": 1, "sampl": 3, "whole": 1, "infer": 1, "dynam": 1}, "marker": "(Gilks et al., 1996)", "article": "P13-1028", "vector_2": [17, 0.5232579240261311, 2, 1, 0, 0]}, {"label": "Pos", "current": "We use the standard generative Dependency Model with Valence (Klein and Manning, 2004).", "context": ["4 Model", "We use the standard generative Dependency Model with Valence (Klein and Manning, 2004).", "The generative story is the following: First, the head of the sentence is generated."], "vector_1": {"use": 1, "depend": 1, "sentenc": 1, "gener": 3, "head": 1, "standard": 1, "stori": 1, "valenc": 1, "follow": 1, "model": 2, "first": 1}, "marker": "(Klein and Manning, 2004)", "article": "P13-1028", "vector_2": [9, 0.39595935156060974, 1, 3, 0, 0]}, {"label": "Pos", "current": "After the last iteration, we use these collected counts as weights and compute maximum directed spanning trees using the Chu-Liu/Edmonds' algorithm (Chu and Liu, 1965).", "context": ["After each iteration is finished (all the trees in the corpus are re-sampled), we increment the counter of all directed pairs of nodes which are connected by a dependency edge in the current trees.", "After the last iteration, we use these collected counts as weights and compute maximum directed spanning trees using the Chu-Liu/Edmonds' algorithm (Chu and Liu, 1965).", "Therefore, the resulting trees consist of edges maximizing the sum of individual counts:"], "vector_1": {"corpu": 1, "comput": 1, "edg": 2, "weight": 1, "individu": 1, "resampl": 1, "direct": 2, "connect": 1, "increment": 1, "use": 2, "depend": 1, "sum": 1, "current": 1, "therefor": 1, "node": 1, "finish": 1, "maxim": 1, "pair": 1, "span": 1, "count": 2, "last": 1, "consist": 1, "algorithm": 1, "chuliuedmond": 1, "counter": 1, "tree": 4, "maximum": 1, "iter": 2, "collect": 1, "result": 1}, "marker": "(Chu and Liu, 1965)", "article": "P13-1028", "vector_2": [48, 0.6403036535204452, 1, 3, 0, 0]}, {"label": "Neut", "current": "The first type are CoNLL treebanks from the year 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007), which we use for inference and for evaluation.", "context": ["We use two types of resources in our experiments.", "The first type are CoNLL treebanks from the year 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007), which we use for inference and for evaluation.", "As is the standard practice in unsupervised parsing evaluation, we removed all punctuation marks from the trees."], "vector_1": {"conll": 1, "use": 2, "resourc": 1, "practic": 1, "evalu": 2, "treebank": 1, "tree": 1, "two": 1, "standard": 1, "mark": 1, "punctuat": 1, "pars": 1, "year": 1, "unsupervis": 1, "experi": 1, "type": 2, "infer": 1, "remov": 1, "first": 1}, "marker": "(Buchholz and Marsi, 2006)", "article": "P13-1028", "vector_2": [7, 0.6633801112992983, 2, 1, 0, 0]}, {"label": "Pos", "current": "We use the IBM Model 1 (Brown et al., 1993) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996)) to estimate the alignment model.", "context": ["As in statistical machine translation, we make modelling assumptions.", "We use the IBM Model 1 (Brown et al., 1993) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996)) to estimate the alignment model.", "The lexicon probability of a sentence pair is modelled as a product of single-word based probabilities of the aligned words."], "vector_1": {"lexicon": 1, "product": 1, "probabl": 2, "use": 1, "depend": 1, "singleword": 1, "make": 1, "uniform": 1, "estim": 1, "hidden": 1, "firstord": 1, "machin": 1, "assumpt": 1, "distribut": 1, "sentenc": 1, "hmm": 1, "base": 1, "translat": 1, "pair": 1, "model": 5, "word": 1, "ibm": 1, "align": 2, "statist": 1, "markov": 1}, "marker": "(Brown et al., 1993)", "article": "E06-1005", "vector_2": [13, 0.23307769256418806, 2, 1, 0, 0]}, {"label": "Pos", "current": "In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions.", "context": ["AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.", "In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions.", "This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP-attachment disambiguation, text categorization, and word sense disambiguation."], "vector_1": {"nlp": 1, "classif": 1, "featur": 1, "stump": 1, "predict": 1, "appli": 1, "obtain": 1, "high": 1, "rate": 1, "tag": 1, "design": 1, "domain": 1, "effici": 1, "highli": 1, "use": 1, "accur": 2, "space": 1, "abl": 1, "system": 1, "classifi": 1, "version": 1, "disambigu": 3, "decis": 1, "text": 1, "sens": 1, "simpl": 1, "method": 1, "confid": 1, "may": 1, "gener": 2, "weak": 2, "number": 1, "particular": 1, "word": 1, "categor": 1, "ppattach": 1, "task": 1, "moder": 1, "success": 1, "algorithm": 2, "boost": 1, "hypothes": 1, "work": 2, "adaboostmh": 1, "signific": 1, "rule": 1, "po": 1, "combin": 1, "adaboost": 2, "partit": 1, "mani": 1, "dimension": 1}, "marker": "(Schapire and Singer, 1999)", "article": "W01-0726", "vector_2": [2, 0.0866620865639415, 2, 2, 0, 0]}, {"label": "Neut", "current": "Particularly, Thompson and Mulac (1991) consider this epistemic phrase has achieved a hedging state through a process of grammaticalization.", "context": ["Early discussion about interpreting epistemic phrases as hedges originated in the analysis I think.", "Particularly, Thompson and Mulac (1991) consider this epistemic phrase has achieved a hedging state through a process of grammaticalization.", "Their view is that I think is roughly similar to maybe when used to express the degree of speaker commitment, thus comprising a grammatical sub-category of adverbs."], "vector_1": {"origin": 1, "epistem": 2, "process": 1, "particularli": 1, "subcategori": 1, "phrase": 2, "use": 1, "adverb": 1, "degre": 1, "state": 1, "speaker": 1, "thompson": 1, "hedg": 2, "earli": 1, "express": 1, "mayb": 1, "mulac": 1, "analysi": 1, "consid": 1, "discuss": 1, "interpret": 1, "grammat": 1, "thu": 1, "roughli": 1, "achiev": 1, "compris": 1, "grammatic": 1, "commit": 1, "similar": 1, "think": 2, "view": 1}, "marker": "(1991)", "article": "W15-0302", "vector_2": [24, 0.4846324242004707, 1, 2, 0, 0]}, {"label": "Neut", "current": "They also describe particular properties of these phrases such as: a) representing the speaker's attitude with respect to the subsequent piece of discourse in contrast to when third person is used, there the piece of discourse is seen as a description (Scheibman, 2001), b) it comprises explicitly subjective claims in contrast to impersonal expressions where the hedging source is obscure (Hyland, 1998) , c) used to express knowledge states, as a boundary marker for turn-taking in conversation, a speaker's perspective marker, and as a way to align the speaker's with the listener's stance (Karkkainen, 2010).", "context": ["Scheibman (2001), Karkkainen (2010) and Wierzbicka (2006) showed that first person epistemic phrases used to express personal stance are highly frequent in various registers in contemporary English.", "They also describe particular properties of these phrases such as: a) representing the speaker's attitude with respect to the subsequent piece of discourse in contrast to when third person is used, there the piece of discourse is seen as a description (Scheibman, 2001), b) it comprises explicitly subjective claims in contrast to impersonal expressions where the hedging source is obscure (Hyland, 1998) , c) used to express knowledge states, as a boundary marker for turn-taking in conversation, a speaker's perspective marker, and as a way to align the speaker's with the listener's stance (Karkkainen, 2010).", "Besides, Wierzbicka (2006) suggests that this category of phrases merits recognition as a major grammatical and semantic class in modern English."], "vector_1": {"claim": 1, "major": 1, "show": 1, "semant": 1, "knowledg": 1, "imperson": 1, "obscur": 1, "repres": 1, "seen": 1, "attitud": 1, "grammat": 1, "respect": 1, "marker": 2, "phrase": 3, "contemporari": 1, "highli": 1, "use": 3, "explicitli": 1, "describ": 1, "recognit": 1, "suggest": 1, "boundari": 1, "wierzbicka": 2, "regist": 1, "categori": 1, "also": 1, "state": 1, "speaker": 3, "way": 1, "besid": 1, "hedg": 1, "subsequ": 1, "perspect": 1, "contrast": 2, "listen": 1, "epistem": 1, "discours": 2, "sourc": 1, "variou": 1, "turntak": 1, "express": 3, "subject": 1, "piec": 2, "merit": 1, "particular": 1, "class": 1, "c": 1, "b": 1, "convers": 1, "third": 1, "align": 1, "properti": 1, "descript": 1, "modern": 1, "person": 3, "stanc": 2, "scheibman": 1, "compris": 1, "english": 2, "karkkainen": 1, "frequent": 1, "first": 1}, "marker": "(Scheibman, 2001)", "article": "W15-0302", "vector_2": [14, 0.5053301952097466, 7, 2, 0, 0]}, {"label": "Pos", "current": "In our particular setting, weak rules are extended to arbitrarily deep decision trees following the suggestion of (Schapire and Singer, 1999) and the definition presented in (Carreras and Marquez, 2001).", "context": ["This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP-attachment disambiguation, text categorization, and word sense disambiguation.", "In our particular setting, weak rules are extended to arbitrarily deep decision trees following the suggestion of (Schapire and Singer, 1999) and the definition presented in (Carreras and Marquez, 2001).", "These more complex weak rules allow the algorithm to work in a higher dimensional feature space that contains conjunctions of simple features, and this fact has turned out to be crucial for improving results in the present domain."], "vector_1": {"nlp": 1, "domain": 1, "featur": 3, "effici": 1, "appli": 1, "number": 1, "deep": 1, "high": 1, "signific": 1, "tag": 1, "result": 1, "follow": 1, "conjunct": 1, "definit": 1, "space": 2, "suggest": 1, "abl": 1, "complex": 1, "disambigu": 3, "decis": 1, "text": 1, "sens": 1, "set": 1, "boost": 1, "po": 1, "higher": 1, "crucial": 1, "extend": 1, "weak": 2, "particular": 2, "categor": 1, "present": 2, "ppattach": 1, "arbitrarili": 1, "task": 1, "word": 1, "algorithm": 2, "success": 1, "simpl": 1, "work": 2, "tree": 1, "rule": 2, "turn": 1, "allow": 1, "contain": 1, "improv": 1, "dimension": 2, "fact": 1}, "marker": "(Schapire and Singer, 1999)", "article": "W01-0726", "vector_2": [2, 0.12012955147708312, 2, 2, 0, 0]}, {"label": "Neut", "current": "Chekuri et al (2001) proposed an integer linear programming (ILP) formulation of the metric labeling problem, with both assignment cost and separation costs being modeled as binary variables of the linear cost function.", "context": ["the costs of selecting a pair of labels for two related objects3.", "Chekuri et al (2001) proposed an integer linear programming (ILP) formulation of the metric labeling problem, with both assignment cost and separation costs being modeled as binary variables of the linear cost function.", "Recently, Roth & Yih (2004) applied an ILP model to the task of the simultaneous assignment of semantic roles to the entities mentioned in a sentence and recognition of the relations holding between them."], "vector_1": {"semant": 1, "formul": 1, "metric": 1, "al": 1, "yih": 1, "cost": 4, "et": 1, "simultan": 1, "select": 1, "entiti": 1, "variabl": 1, "recognit": 1, "two": 1, "label": 2, "binari": 1, "program": 1, "role": 1, "appli": 1, "function": 1, "relat": 2, "linear": 2, "sentenc": 1, "object": 1, "mention": 1, "pair": 1, "hold": 1, "problem": 1, "recent": 1, "task": 1, "chekuri": 1, "integ": 1, "separ": 1, "roth": 1, "ilp": 2, "model": 2, "assign": 2, "propos": 1}, "marker": "(2001)", "article": "W05-0618", "vector_2": [4, 0.21889417027820496, 2, 2, 0, 0]}, {"label": "Neut", "current": "In the post-editing step, a prediction engine helps to decrease the amount of human interaction (Och et al., 2003).", "context": ["Then, one possible procedure for skilled human translators is to provide the oral translation of a given source text and then to post-edit the recognized text.", "In the post-editing step, a prediction engine helps to decrease the amount of human interaction (Och et al., 2003).", "In a CAT system with integrated speech, two sources of information are available to recognize the speech input: the target language speech and the given source language text."], "vector_1": {"help": 1, "predict": 1, "text": 3, "procedur": 1, "one": 1, "decreas": 1, "human": 2, "skill": 1, "postedit": 2, "languag": 2, "given": 2, "engin": 1, "interact": 1, "system": 1, "avail": 1, "speech": 3, "input": 1, "sourc": 3, "recogn": 2, "step": 1, "translat": 2, "target": 1, "possibl": 1, "provid": 1, "cat": 1, "inform": 1, "amount": 1, "integr": 1, "oral": 1, "two": 1}, "marker": "(Och et al., 2003)", "article": "P06-2061", "vector_2": [3, 0.05725563909774436, 1, 4, 14, 1]}, {"label": "Pos", "current": "Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons.", "context": ["However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning.", "Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons.", "In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015)."], "vector_1": {"classif": 1, "featur": 2, "special": 2, "paper": 1, "still": 1, "best": 1, "use": 1, "depend": 1, "perform": 1, "make": 1, "system": 3, "messag": 1, "larg": 1, "includ": 1, "multiword": 1, "semev": 2, "hundr": 1, "treatment": 1, "emoticon": 1, "polar": 1, "resourc": 2, "string": 1, "thousand": 1, "express": 1, "extens": 2, "engin": 1, "inescid": 1, "tune": 1, "like": 1, "task": 1, "look": 1, "howev": 1, "negat": 1, "inde": 1, "present": 1, "stateoftheart": 1, "linguist": 1}, "marker": "(Zhu et al., 2014)", "article": "S15-2109", "vector_2": [1, 0.11707249965607373, 3, 1, 1, 0]}, {"label": "Pos", "current": "Parameters We used the parameters described by Mitchell and Lapata (2008) for the noun and verb vectors.", "context": ["The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011).", "Parameters We used the parameters described by Mitchell and Lapata (2008) for the noun and verb vectors.", "All vectors were built from a lemmatised version of the BNC."], "vector_1": {"use": 1, "vector": 2, "develop": 1, "describ": 1, "lapata": 1, "lemmatis": 1, "dataset": 1, "mitchel": 1, "built": 1, "verb": 1, "version": 1, "bnc": 1, "noun": 1, "experi": 1, "paramet": 2}, "marker": "(2008)", "article": "W11-2507", "vector_2": [3, 0.4980239943542696, 2, 7, 0, 0]}, {"label": "CoCo", "current": "Similar to our approach, Electric Knowledge transforms a natural language question into a series of increasingly more general keyword queries (Bierner, 2001).", "context": ["User questions are compared against those in the database, and links to webpages for the closest matches are returned.", "Similar to our approach, Electric Knowledge transforms a natural language question into a series of increasingly more general keyword queries (Bierner, 2001).", "However, their query formulation process utilizes hard-crafted regular expressions, while we adopt a more general machine learning approach for transformation rule application."], "vector_1": {"process": 1, "queri": 2, "knowledg": 1, "increasingli": 1, "languag": 1, "compar": 1, "databas": 1, "question": 2, "transform": 2, "formul": 1, "electr": 1, "approach": 2, "match": 1, "machin": 1, "applic": 1, "return": 1, "closest": 1, "webpag": 1, "gener": 2, "express": 1, "util": 1, "regular": 1, "link": 1, "user": 1, "natur": 1, "keyword": 1, "howev": 1, "adopt": 1, "rule": 1, "learn": 1, "similar": 1, "hardcraft": 1, "seri": 1}, "marker": "(Bierner, 2001)", "article": "W02-1024", "vector_2": [1, 0.16571449365015828, 1, 1, 0, 0]}, {"label": "Neut", "current": "We can also add the Unique Stress Constraint (USC) (Yang, 2004) by excluding all variants of rule (18) that generate two or more stressed syllables.", "context": ["This yields the colloc3-phon-stress model.", "We can also add the Unique Stress Constraint (USC) (Yang, 2004) by excluding all variants of rule (18) that generate two or more stressed syllables.", "For example, while the lexical generator for the colloc3-nophon-stress model will include the rule Word  SSyll SSyll, the lexical generator embodying the USC lacks this rule."], "vector_1": {"lack": 1, "collocphonstress": 1, "rule": 3, "usc": 2, "two": 1, "also": 1, "add": 1, "includ": 1, "ssyll": 2, "gener": 3, "variant": 1, "lexic": 2, "collocnophonstress": 1, "stress": 2, "word": 1, "constraint": 1, "embodi": 1, "uniqu": 1, "yield": 1, "exclud": 1, "syllabl": 1, "exampl": 1, "model": 2}, "marker": "(Yang, 2004)", "article": "Q14-1008", "vector_2": [10, 0.42426718684099535, 1, 9, 3, 0]}, {"label": "Neut", "current": "Initial work in recognizing and indexing abstract configurations of planning relations is discussed in Dolan and Dyer (1985, 1986).", "context": ["We have left the automatic creation of this taxonomy from advisor experiences to future research.", "Initial work in recognizing and indexing abstract configurations of planning relations is discussed in Dolan and Dyer (1985, 1986).", "9.3 OTHER CLASSES OF MISCONCEPTIONS"], "vector_1": {"class": 1, "index": 1, "work": 1, "recogn": 1, "relat": 1, "dolan": 1, "misconcept": 1, "abstract": 1, "creation": 1, "automat": 1, "research": 1, "configur": 1, "futur": 1, "advisor": 1, "dyer": 1, "taxonomi": 1, "plan": 1, "experi": 1, "initi": 1, "discuss": 1, "left": 1}, "marker": "(1985, 1986)", "article": "J88-3004", "vector_2": [3, 0.9282319094377385, 1, 1, 4, 0]}, {"label": "Pos", "current": "The automatic alignments have then been manually corrected through a graphical editing tool within the GATE framework (Cunningham et al., 2002).", "context": ["aligning algorithm based on Hidden Markov Models (Bott and Saggion, 2011) has been applied to obtain sentence-level alignments.", "The automatic alignments have then been manually corrected through a graphical editing tool within the GATE framework (Cunningham et al., 2002).", "A total of 570 sentences have been aligned (246 in original and 324 in simple texts), with the following correlations between them: one to one, one to many or many to one, as well as cases where there is no correlation (cases of content reduction through summarisation or information expansion through the introduction of definitions)."], "vector_1": {"origin": 1, "expans": 1, "appli": 1, "within": 1, "obtain": 1, "automat": 1, "one": 4, "correl": 2, "follow": 1, "sentencelevel": 1, "total": 1, "well": 1, "definit": 1, "reduct": 1, "content": 1, "text": 1, "gate": 1, "hidden": 1, "simpl": 1, "correct": 1, "sentenc": 1, "tool": 1, "framework": 1, "base": 1, "model": 1, "introduct": 1, "case": 2, "graphic": 1, "algorithm": 1, "edit": 1, "align": 4, "manual": 1, "inform": 1, "summaris": 1, "mani": 2, "markov": 1}, "marker": "(Cunningham et al., 2002)", "article": "W12-2202", "vector_2": [10, 0.2495570487747802, 2, 1, 0, 0]}, {"label": "Neut", "current": "Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation (Moens and Steedman, 1988; Passonneau, 1988; Dorr, 1992; Klavans, 1994).", "context": ["Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause.", "Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation (Moens and Steedman, 1988; Passonneau, 1988; Dorr, 1992; Klavans, 1994).", "Researchers have used empirical analysis of corpora to develop linguistically-based numerical indicators that aid in aspectual classification (Klavans and Chodorow, 1992; Siegel and McKeown, 1996)."], "vector_1": {"stativ": 1, "classif": 2, "indic": 1, "three": 1, "empir": 1, "select": 1, "use": 1, "develop": 1, "distinct": 1, "linguisticallybas": 1, "system": 1, "research": 1, "compon": 1, "compos": 1, "tens": 1, "analyz": 1, "machin": 1, "aspectu": 3, "numer": 1, "claus": 1, "lexic": 1, "analysi": 1, "translat": 1, "class": 1, "choic": 1, "furthermor": 1, "constraint": 1, "corpora": 1, "tempor": 2, "perform": 1, "necessari": 1, "fundament": 1, "aid": 1, "first": 1}, "marker": "(Moens and Steedman, 1988", "article": "W97-0318", "vector_2": [9, 0.03975170599358711, 6, 3, 0, 0]}, {"label": "CoCo", "current": "The approach described in (Auli et al., 2009) is very similar to ours: in this study, the authors propose to find and analyze the limits of machine translation systems by studying the reference reachability.", "context": ["To the best of our knowledge, there are only a few works that try to study the expressive power of phrase-based machine translation systems or to provide tools for analyzing potential causes of failure.", "The approach described in (Auli et al., 2009) is very similar to ours: in this study, the authors propose to find and analyze the limits of machine translation systems by studying the reference reachability.", "A reference is reachable for a given system if it can be exactly generated by this system."], "vector_1": {"reachabl": 2, "knowledg": 1, "find": 1, "best": 1, "given": 1, "describ": 1, "author": 1, "system": 4, "analyz": 2, "approach": 1, "gener": 1, "refer": 2, "machin": 2, "power": 1, "tool": 1, "express": 1, "translat": 2, "failur": 1, "exactli": 1, "tri": 1, "provid": 1, "work": 1, "phrasebas": 1, "caus": 1, "limit": 1, "potenti": 1, "studi": 3, "similar": 1, "propos": 1}, "marker": "(Auli et al., 2009)", "article": "D10-1091", "vector_2": [1, 0.8712335883630868, 1, 10, 6, 0]}, {"label": "Pos", "current": "We used the StackingC implementation of WEKA (Seewald, 2002) to combine the models, with a linear regression model as our meta classifier.", "context": ["We trained three classification models on the entire feature set, using the same train-test sets as explained before and trained an ensemble model with three classifiers.", "We used the StackingC implementation of WEKA (Seewald, 2002) to combine the models, with a linear regression model as our meta classifier.", "Table 4 shows the classification accuracies for the individual classifiers as well as the ensemble on a 10-fold CV of the training set and on the held out test set."], "vector_1": {"classif": 2, "featur": 1, "show": 1, "train": 3, "traintest": 1, "fold": 1, "set": 4, "tabl": 1, "cv": 1, "use": 2, "weka": 1, "explain": 1, "three": 2, "classifi": 3, "accuraci": 1, "test": 1, "individu": 1, "linear": 1, "entir": 1, "model": 4, "held": 1, "stackingc": 1, "well": 1, "meta": 1, "combin": 1, "ensembl": 2, "regress": 1, "implement": 1}, "marker": "(Seewald, 2002)", "article": "W13-1708", "vector_2": [11, 0.6185211816166337, 1, 1, 0, 0]}, {"label": "Neut", "current": "It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al (2008).", "context": ["Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years.", "It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al (2008).", "Headden III et al (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing."], "vector_1": {"cohen": 1, "al": 2, "year": 1, "et": 2, "depend": 2, "klein": 1, "smith": 1, "add": 1, "recent": 1, "approach": 1, "introduc": 2, "extend": 1, "lexic": 1, "pars": 1, "iii": 1, "man": 1, "grammar": 1, "unsupervis": 1, "smooth": 1, "valenc": 2, "dmv": 1, "popular": 1, "improv": 1, "model": 1, "headden": 1}, "marker": "(2004)", "article": "P13-1028", "vector_2": [9, 0.2191204935881926, 4, 3, 0, 0]}, {"label": "Neut", "current": "1 shows a sample Speech Graffiti dialog User interactions with Speech Graffiti (independent of other speech interfaces) have previously been assessed in Rosenfeld et al (2000).", "context": ["Fig.", "1 shows a sample Speech Graffiti dialog User interactions with Speech Graffiti (independent of other speech interfaces) have previously been assessed in Rosenfeld et al (2000).", "Here we consider a head-to-head comparison: given the chance to interact with both types of interfaces, which would people choose?"], "vector_1": {"rosenfeld": 1, "show": 1, "independ": 1, "al": 1, "assess": 1, "sampl": 1, "headtohead": 1, "et": 1, "previous": 1, "given": 1, "would": 1, "interact": 2, "speech": 3, "fig": 1, "interfac": 2, "type": 1, "peopl": 1, "user": 1, "consid": 1, "comparison": 1, "dialog": 1, "choos": 1, "graffiti": 2, "chanc": 1}, "marker": "(2000)", "article": "N04-4019", "vector_2": [4, 0.1427758816837315, 1, 1, 2, 1]}, {"label": "CoCo", "current": "This result conforms to the results reported in (Kudo and Matsumoto, 2003).", "context": ["Without fstries, little speed-up of SVM-HKE classifiers versus the SVM-KE classifiers (in Table 3) was obtained due to the mild reduction in the average number of active features |xd |in the classification.", "This result conforms to the results reported in (Kudo and Matsumoto, 2003).", "The parsing speed reached 14,937 sentences per second with accuracy of 90.91% (SVM-HKE, d = 3, Q = 0.002)."], "vector_1": {"conform": 1, "classif": 1, "featur": 1, "number": 1, "xd": 1, "mild": 1, "classifi": 2, "tabl": 1, "speed": 1, "reduct": 1, "per": 1, "versu": 1, "fstri": 1, "activ": 1, "due": 1, "littl": 1, "accuraci": 1, "svmhke": 2, "svmke": 1, "sentenc": 1, "reach": 1, "speedup": 1, "pars": 1, "1497": 1, "report": 1, "averag": 1, "second": 1, "q": 1, "without": 1, "obtain": 1, "result": 2}, "marker": "(Kudo and Matsumoto, 2003)", "article": "D09-1160", "vector_2": [6, 0.905850315158191, 1, 7, 4, 0]}, {"label": "Pos", "current": "We use the IBM Model 1 (Brown et al., 1993) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996)) to estimate the alignment model.", "context": ["As in statistical machine translation, we make modelling assumptions.", "We use the IBM Model 1 (Brown et al., 1993) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996)) to estimate the alignment model.", "The lexicon probability of a sentence pair is modelled as a product of single-word based probabilities of the aligned words."], "vector_1": {"lexicon": 1, "product": 1, "probabl": 2, "use": 1, "depend": 1, "singleword": 1, "make": 1, "uniform": 1, "estim": 1, "hidden": 1, "firstord": 1, "machin": 1, "assumpt": 1, "distribut": 1, "sentenc": 1, "hmm": 1, "base": 1, "translat": 1, "pair": 1, "model": 5, "word": 1, "ibm": 1, "align": 2, "statist": 1, "markov": 1}, "marker": "(Brown et al., 1993)", "article": "E06-1005", "vector_2": [13, 0.23307769256418806, 2, 1, 0, 0]}, {"label": "Pos", "current": "A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high.", "context": ["However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a).", "A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high.", "On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004)."], "vector_1": {"often": 1, "proven": 1, "high": 1, "particularli": 1, "higherord": 1, "use": 2, "accur": 1, "depend": 2, "label": 1, "complex": 1, "factor": 1, "import": 1, "method": 1, "unlabel": 1, "graphbas": 1, "hand": 1, "joint": 2, "twostag": 1, "edgelabel": 1, "algorithm": 1, "provid": 1, "howev": 1, "tree": 2, "structur": 1, "inform": 1, "benefit": 1, "unaccept": 1, "learn": 2, "pars": 1, "model": 2, "produc": 1}, "marker": "(McDonald, 2006)", "article": "D15-1154", "vector_2": [9, 0.07520598375504003, 6, 8, 2, 0]}, {"label": "Weak", "current": "Elming (2006) suggests using tranformation-based learning to automatically acquire error-correcting rules from such data; however, the proposed method only applies to lexical choice errors.", "context": ["Allen and Hogan (2000) sketch the outline of such an automated post-editing (APE) system, which would automatically learn post-editing rules from a tri-parallel corpus of source, raw MT and post-edited text.", "Elming (2006) suggests using tranformation-based learning to automatically acquire error-correcting rules from such data; however, the proposed method only applies to lexical choice errors.", "Knight and Chander (1994) also argue in favor of using a separate APE module, which is then portable across multiple MT systems and language pairs, and suggest that the post-editing task could be performed using statistical machine translation techniques."], "vector_1": {"corpu": 1, "machin": 1, "sketch": 1, "chander": 1, "text": 1, "modul": 1, "automat": 2, "raw": 1, "allen": 1, "postedit": 4, "tranformationbas": 1, "languag": 1, "ape": 2, "use": 3, "multipl": 1, "acquir": 1, "would": 1, "elm": 1, "perform": 1, "suggest": 2, "system": 2, "also": 1, "appli": 1, "method": 1, "hogan": 1, "errorcorrect": 1, "sourc": 1, "error": 1, "argu": 1, "lexic": 1, "autom": 1, "translat": 1, "pair": 1, "triparallel": 1, "data": 1, "techniqu": 1, "choic": 1, "task": 1, "knight": 1, "howev": 1, "outlin": 1, "favor": 1, "rule": 2, "across": 1, "separ": 1, "mt": 2, "statist": 1, "learn": 2, "portabl": 1, "could": 1, "propos": 1}, "marker": "(2006)", "article": "N07-1064", "vector_2": [1, 0.13140575316128078, 3, 1, 0, 0]}, {"label": "Neut", "current": "For example, MWEs have been found to exhibit greater phonological consistency than free expressions (Hickey, 1993).", "context": ["As such, MWEs exhibit unique phonological and prosodic characteristics.", "For example, MWEs have been found to exhibit greater phonological consistency than free expressions (Hickey, 1993).", "Specifically, pauses have been found to be less acceptable in lexicalized phrases (Pawley, 1985)."], "vector_1": {"exhibit": 2, "specif": 1, "greater": 1, "consist": 1, "prosod": 1, "phonolog": 2, "express": 1, "uniqu": 1, "free": 1, "less": 1, "lexic": 1, "exampl": 1, "accept": 1, "mwe": 2, "phrase": 1, "found": 2, "characterist": 1, "paus": 1}, "marker": "(Hickey, 1993)", "article": "W15-0914", "vector_2": [22, 0.21108620338292236, 2, 1, 0, 0]}, {"label": "Neut", "current": "In addition, and most relevant to our study, Dahlmann and Adolph study how pausality differs in and around MWEs (Dahlmann and Adolphs, 2007).", "context": ["Specifically, pauses have been found to be less acceptable in lexicalized phrases (Pawley, 1985).", "In addition, and most relevant to our study, Dahlmann and Adolph study how pausality differs in and around MWEs (Dahlmann and Adolphs, 2007).", "They conclude that ...where pauses occur they give valuable indications of possible [MWE] boundaries."], "vector_1": {"dahlmann": 1, "paus": 2, "give": 1, "pausal": 1, "indic": 1, "phrase": 1, "occur": 1, "differ": 1, "boundari": 1, "less": 1, "mwe": 2, "around": 1, "conclud": 1, "adolph": 1, "lexic": 1, "accept": 1, "relev": 1, "addit": 1, "specif": 1, "valuabl": 1, "possibl": 1, "found": 1, "studi": 2}, "marker": "(Dahlmann and Adolphs, 2007)", "article": "W15-0914", "vector_2": [8, 0.21817811289993885, 2, 6, 0, 0]}, {"label": "Neut", "current": "They conclude, Although motor execution is more demanding for slow typists, this higher demand neither prevented them from activating high-level processes concurrently with typing, nor changed the distribution of occurrences of the writing processes. (Alves et al., 2008, p. 10) The importance of pauses during the typing process is borne out in a number of studies.", "context": ["However, Alves et al (2008), in studying narrative construction in typing conclude that while differences do exist between the populations, this might not be as significant a differentiation as originally thought.", "They conclude, Although motor execution is more demanding for slow typists, this higher demand neither prevented them from activating high-level processes concurrently with typing, nor changed the distribution of occurrences of the writing processes. (Alves et al., 2008, p. 10) The importance of pauses during the typing process is borne out in a number of studies.", "Schilperoord (2002) concludes that writers pause for a number of reasons, such as cognitive overload, writing apprehension or fatigue."], "vector_1": {"origin": 1, "prevent": 1, "schilperoord": 1, "paus": 2, "execut": 1, "process": 3, "al": 1, "born": 1, "signific": 1, "concurr": 1, "exist": 1, "overload": 1, "motor": 1, "alv": 1, "et": 1, "cognit": 1, "apprehens": 1, "differ": 1, "slow": 1, "writer": 1, "activ": 1, "construct": 1, "narr": 1, "write": 2, "occurr": 1, "import": 1, "neither": 1, "highlevel": 1, "might": 1, "higher": 1, "distribut": 1, "conclud": 3, "differenti": 1, "number": 2, "reason": 1, "although": 1, "demand": 2, "chang": 1, "popul": 1, "type": 3, "howev": 1, "thought": 1, "p": 1, "fatigu": 1, "studi": 2, "typist": 1}, "marker": "(Alves et al., 2008, ", "article": "W15-0914", "vector_2": [7, 0.32484206235989405, 3, 4, 0, 0]}, {"label": "Neut", "current": "Gentner, et al (1988) investigated the   linguistic characteristics of skilled versus unskilled typists, finding marked differences in the behavior (and thus cognitive model) of each population.", "context": ["The typing task is especially daunting for novice typists.", "Gentner, et al (1988) investigated the   linguistic characteristics of skilled versus unskilled typists, finding marked differences in the behavior (and thus cognitive model) of each population.", "A novice typist is so burdened by the physical execution cycle of typing that the quality of his or her writing is noticeably diminished."], "vector_1": {"novic": 2, "execut": 1, "qualiti": 1, "al": 1, "et": 1, "skill": 1, "cognit": 1, "find": 1, "versu": 1, "gentner": 1, "mark": 1, "write": 1, "unskil": 1, "daunt": 1, "type": 2, "investig": 1, "especi": 1, "differ": 1, "notic": 1, "cycl": 1, "burden": 1, "diminish": 1, "characterist": 1, "physic": 1, "popul": 1, "task": 1, "thu": 1, "behavior": 1, "model": 1, "linguist": 1, "typist": 3}, "marker": "(1988)", "article": "W15-0914", "vector_2": [27, 0.3100061137151009, 1, 1, 0, 0]}, {"label": "Neut", "current": "jMWE has reported an F1 measure of 83.4 in detecting continuous, unbroken MWEs in the Semcor (Mihalcea, 1998) Brown Concordance (Finlayson and Kulkarni, 2011).", "context": ["For the present studies we only looked at contiguous MWEs.", "jMWE has reported an F1 measure of 83.4 in detecting continuous, unbroken MWEs in the Semcor (Mihalcea, 1998) Brown Concordance (Finlayson and Kulkarni, 2011).", "Contiguous MWEs should show more signs of being a cohesive lexical unit, although non-contiguous MWEs should still exhibit some degree of the same."], "vector_1": {"exhibit": 1, "concord": 1, "jmwe": 1, "continu": 1, "sign": 1, "cohes": 1, "still": 1, "unit": 1, "detect": 1, "contigu": 2, "show": 1, "semcor": 1, "mwe": 4, "unbroken": 1, "brown": 1, "noncontigu": 1, "lexic": 1, "although": 1, "report": 1, "present": 1, "measur": 1, "look": 1, "f": 1, "degre": 1, "studi": 1}, "marker": "(Mihalcea, 1998)", "article": "W15-0914", "vector_2": [17, 0.47731811697574894, 2, 1, 0, 0]}, {"label": "Neut", "current": "As proposed by Johanssen et al (2010), touch-typists and visual typists employ distinct cognitive models, as visual typists also need to dedicate cognitive effort to figuring out where the next key is.", "context": ["This is in comparison to visual typists who look at their fingers when typing.", "As proposed by Johanssen et al (2010), touch-typists and visual typists employ distinct cognitive models, as visual typists also need to dedicate cognitive effort to figuring out where the next key is.", "For touch typists, this is a less conscious process."], "vector_1": {"less": 1, "process": 1, "al": 1, "touch": 1, "consciou": 1, "need": 1, "et": 1, "dedic": 1, "cognit": 2, "employ": 1, "touchtypist": 1, "distinct": 1, "next": 1, "also": 1, "figur": 1, "finger": 1, "type": 1, "visual": 3, "key": 1, "effort": 1, "comparison": 1, "look": 1, "johanssen": 1, "model": 1, "typist": 4, "propos": 1}, "marker": "(2010)", "article": "W15-0914", "vector_2": [5, 0.3924597513755859, 1, 1, 0, 0]}, {"label": "Neut", "current": "The effects of predictability are well documented, in that more likely sequences are produced and comprehended at a faster rate (GoldmanEisler, 1958; Hale, 2006; Nottbusch et al., 2007; Levy, 2008; Smith and Levy, 2013, and references therein).", "context": ["A final confound to be investigated was sequence likelihood.", "The effects of predictability are well documented, in that more likely sequences are produced and comprehended at a faster rate (GoldmanEisler, 1958; Hale, 2006; Nottbusch et al., 2007; Levy, 2008; Smith and Levy, 2013, and references therein).", "Since MWEs are frequently made up of collocations, i.e."], "vector_1": {"therein": 1, "predict": 1, "confound": 1, "rate": 1, "ie": 1, "colloc": 1, "comprehend": 1, "mwe": 1, "document": 1, "final": 1, "refer": 1, "investig": 1, "sequenc": 2, "effect": 1, "likelihood": 1, "sinc": 1, "faster": 1, "made": 1, "like": 1, "well": 1, "produc": 1, "frequent": 1}, "marker": "Nottbusch et al., 2007", "article": "W15-0914", "vector_2": [8, 0.6084369268392092, 5, 2, 0, 0]}, {"label": "Weak", "current": "Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26].", "context": ["The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithms.", "Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26].", "The worst size complexity of such a chart is only a square function of the size of the input2."], "vector_1": {"process": 1, "repres": 1, "parsetre": 1, "still": 1, "extract": 1, "size": 2, "describ": 1, "associ": 1, "publish": 1, "categori": 1, "complex": 1, "analyz": 2, "nontrivi": 1, "input": 1, "function": 1, "sentenc": 2, "chart": 2, "pars": 1, "kay": 1, "characterist": 1, "segment": 1, "requir": 1, "squar": 1, "kind": 1, "essenti": 1, "algorithm": 2, "thu": 1, "structur": 1, "nontermin": 1, "worst": 1, "produc": 2}, "marker": "[11]", "article": "P89-1018", "vector_2": [24, 0.09869278860619839, 7, 1, 0, 0]}, {"label": "Pos", "current": "Hence we are reusing and developing our theoretical [18] and experimental [36] approach in this much more general setting which is more likely to be effectively usable for natural language parsing.", "context": ["We have formally extended the concept of PDA into that of Logical PDA which is an operational push-down stack device for parsing unification based grammars [17,18] or other non-CF grammars such as Tree Adjoining Grammars [19].", "Hence we are reusing and developing our theoretical [18] and experimental [36] approach in this much more general setting which is more likely to be effectively usable for natural language parsing.", "Furthermore, these extensions can also express, within the PDA model, non-left-to-right behavior such as is used in island parsing [38] or in Shell's approach [26]."], "vector_1": {"oper": 1, "concept": 1, "natur": 1, "within": 1, "unif": 1, "set": 1, "nonlefttoright": 1, "pushdown": 1, "languag": 1, "use": 1, "develop": 1, "much": 1, "devic": 1, "theoret": 1, "also": 1, "experiment": 1, "adjoin": 1, "shell": 1, "approach": 2, "noncf": 1, "extend": 1, "henc": 1, "gener": 1, "express": 1, "effect": 1, "usabl": 1, "extens": 1, "base": 1, "pars": 3, "stack": 1, "formal": 1, "grammar": 3, "like": 1, "furthermor": 1, "island": 1, "tree": 1, "reus": 1, "behavior": 1, "logic": 1, "model": 1, "pda": 3}, "marker": "[18]", "article": "P89-1018", "vector_2": [2, 0.9353080806392298, 7, 3, 0, 0]}, {"label": "Neut", "current": "Many experiments and extensions still remain to be made: improved dynamic programming interpretation of bottomup parsers, more extensive experimental measurements with a variety of languages and parsing schemata, or generalization of this approach to more complex situations, such as word lattice parsing [21,30], or even handling of \"secondary\" language features.", "context": ["The approach taken here supports both theoretical analysis and actual experimentation, both for the computational behavior of parsers and for the structure of the resulting shared forest.", "Many experiments and extensions still remain to be made: improved dynamic programming interpretation of bottomup parsers, more extensive experimental measurements with a variety of languages and parsing schemata, or generalization of this approach to more complex situations, such as word lattice parsing [21,30], or even handling of \"secondary\" language features.", "Early research in that latter direction is promising: our framework and the corresponding paradigm for parser construction have been extended to full first-order Horn clauses [17,18], and are hence applicable to unification based grammatical formalisms [27]."], "vector_1": {"program": 1, "featur": 1, "comput": 1, "parser": 3, "share": 1, "direct": 1, "horn": 1, "result": 1, "claus": 1, "still": 1, "dynam": 1, "even": 1, "unif": 1, "support": 1, "complex": 1, "theoret": 1, "research": 1, "experiment": 2, "forest": 1, "varieti": 1, "taken": 1, "languag": 2, "experi": 1, "approach": 2, "firstord": 1, "analysi": 1, "applic": 1, "full": 1, "construct": 1, "extend": 1, "handl": 1, "henc": 1, "gener": 1, "lattic": 1, "situat": 1, "secondari": 1, "extens": 2, "base": 1, "pars": 2, "improv": 1, "word": 1, "interpret": 1, "formal": 1, "measur": 1, "grammat": 1, "made": 1, "actual": 1, "framework": 1, "correspond": 1, "structur": 1, "promis": 1, "remain": 1, "schemata": 1, "behavior": 1, "earli": 1, "mani": 1, "bottomup": 1, "paradigm": 1, "latter": 1}, "marker": "[30]", "article": "P89-1018", "vector_2": [1, 0.9706128180268129, 5, 1, 3, 0]}, {"label": "Neut", "current": "PPM is an adaptive text compression algorithm that has been applied to the segmentation problem (Teahan et al., 1998; Teahan et al., 2000).", "context": ["Since this paper is concerned primarily with adaptability, we will focus on the best available methods that do not require a pre-existing dictionary.", "PPM is an adaptive text compression algorithm that has been applied to the segmentation problem (Teahan et al., 1998; Teahan et al., 2000).", "Many text compression algorithms, including PPM, work by estimating a probability distribution on the next symbol in a text given the previous context."], "vector_1": {"primarili": 1, "text": 3, "ppm": 2, "paper": 1, "best": 1, "concern": 1, "given": 1, "next": 1, "avail": 1, "estim": 1, "adapt": 2, "appli": 1, "probabl": 1, "method": 1, "includ": 1, "distribut": 1, "symbol": 1, "previou": 1, "compress": 2, "preexist": 1, "dictionari": 1, "segment": 1, "sinc": 1, "requir": 1, "algorithm": 2, "work": 1, "focu": 1, "context": 1, "mani": 1, "problem": 1}, "marker": "(Teahan et al., 1998", "article": "P01-1013", "vector_2": [3, 0.11322485635652552, 2, 1, 6, 0]}, {"label": "CoCo", "current": "Comparison between the classifications of Kay [14] and Griffith dc Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers'.", "context": ["The idea of this approach is to separate the dynamic programming constructs needed for efficient chart parsing from the chosen parsing schema.", "Comparison between the classifications of Kay [14] and Griffith dc Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers'.", "These PDTs are usually non-deterministic and cannot be used as produced for actual parsing."], "vector_1": {"classif": 1, "effici": 1, "show": 1, "parser": 1, "idea": 1, "pushdown": 1, "need": 1, "griffith": 1, "dynam": 1, "use": 1, "transduc": 1, "chosen": 1, "construct": 2, "program": 1, "approach": 1, "strategi": 1, "nondeterminist": 1, "may": 1, "express": 1, "dc": 1, "chart": 1, "cannot": 1, "pars": 5, "kay": 1, "formal": 1, "comparison": 1, "actual": 1, "pdt": 2, "well": 1, "lefttoright": 1, "separ": 1, "cf": 1, "petrick": 1, "studi": 1, "schema": 2, "produc": 1, "usual": 1}, "marker": "[14]", "article": "P89-1018", "vector_2": [1, 0.23466283307144528, 2, 2, 9, 1]}, {"label": "Weak", "current": "Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26].", "context": ["The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithms.", "Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26].", "The worst size complexity of such a chart is only a square function of the size of the input2."], "vector_1": {"process": 1, "repres": 1, "parsetre": 1, "still": 1, "extract": 1, "size": 2, "describ": 1, "associ": 1, "publish": 1, "categori": 1, "complex": 1, "analyz": 2, "nontrivi": 1, "input": 1, "function": 1, "sentenc": 2, "chart": 2, "pars": 1, "kay": 1, "characterist": 1, "segment": 1, "requir": 1, "squar": 1, "kind": 1, "essenti": 1, "algorithm": 2, "thu": 1, "structur": 1, "nontermin": 1, "worst": 1, "produc": 2}, "marker": "[26]", "article": "P89-1018", "vector_2": [4, 0.09869278860619839, 7, 4, 1, 0]}, {"label": "Neut", "current": "Schilperoord (2002) concludes that writers pause for a number of reasons, such as cognitive overload, writing apprehension or fatigue.", "context": ["They conclude, Although motor execution is more demanding for slow typists, this higher demand neither prevented them from activating high-level processes concurrently with typing, nor changed the distribution of occurrences of the writing processes. (Alves et al., 2008, p. 10) The importance of pauses during the typing process is borne out in a number of studies.", "Schilperoord (2002) concludes that writers pause for a number of reasons, such as cognitive overload, writing apprehension or fatigue.", "Alves et al (2008) similarly concluded that pauses are usually a sign of cognitive competition."], "vector_1": {"prevent": 1, "schilperoord": 1, "paus": 3, "execut": 1, "process": 3, "number": 2, "sign": 1, "born": 1, "concurr": 1, "fatigu": 1, "motor": 1, "alv": 1, "et": 1, "cognit": 2, "apprehens": 1, "slow": 1, "writer": 1, "activ": 1, "overload": 1, "write": 2, "similarli": 1, "import": 1, "neither": 1, "occurr": 1, "higher": 1, "competit": 1, "distribut": 1, "conclud": 3, "al": 1, "reason": 1, "although": 1, "demand": 2, "studi": 1, "highlevel": 1, "type": 2, "p": 1, "chang": 1, "usual": 1, "typist": 1}, "marker": "(2002)", "article": "W15-0914", "vector_2": [13, 0.338781332789892, 3, 1, 0, 0]}, {"label": "Neut", "current": "Specifically, Riggenbach (1991) found that in speech, placement and length of pausing around MWEs is seen as a sign of fluency.", "context": ["This was to avoid the additional confound of language familiarity, though this is certainly an important area for study.", "Specifically, Riggenbach (1991) found that in speech, placement and length of pausing around MWEs is seen as a sign of fluency.", "Further, we limited our study to only touch typists, or those subjects who only look at the screen when typing."], "vector_1": {"paus": 1, "fluenci": 1, "sign": 1, "confound": 1, "touch": 1, "seen": 1, "languag": 1, "subject": 1, "area": 1, "avoid": 1, "limit": 1, "certainli": 1, "speech": 1, "mwe": 1, "import": 1, "type": 1, "around": 1, "though": 1, "familiar": 1, "screen": 1, "addit": 1, "placement": 1, "look": 1, "specif": 1, "length": 1, "riggenbach": 1, "found": 1, "studi": 2, "typist": 1}, "marker": "(1991)", "article": "W15-0914", "vector_2": [24, 0.3769716731200326, 1, 1, 0, 0]}, {"label": "Neut", "current": "Specifically, pauses have been found to be less acceptable in lexicalized phrases (Pawley, 1985).", "context": ["For example, MWEs have been found to exhibit greater phonological consistency than free expressions (Hickey, 1993).", "Specifically, pauses have been found to be less acceptable in lexicalized phrases (Pawley, 1985).", "In addition, and most relevant to our study, Dahlmann and Adolph study how pausality differs in and around MWEs (Dahlmann and Adolphs, 2007)."], "vector_1": {"exhibit": 1, "paus": 1, "less": 1, "phonolog": 1, "pausal": 1, "accept": 1, "phrase": 1, "around": 1, "differ": 1, "dahlmann": 1, "mwe": 2, "greater": 1, "express": 1, "adolph": 1, "free": 1, "lexic": 1, "relev": 1, "addit": 1, "specif": 1, "consist": 1, "exampl": 1, "found": 2, "studi": 2}, "marker": "(Pawley, 1985)", "article": "W15-0914", "vector_2": [30, 0.21353168942327289, 3, 2, 0, 0]}, {"label": "Neut", "current": "For theoretical linguists, MWEs occupy a liminal space between the lexicon and syntax (Langacker, 2008).", "context": ["  Multi-word expressions (MWEs) are vexing for both theoretical linguists and those working in Natural Language Processing.", "For theoretical linguists, MWEs occupy a liminal space between the lexicon and syntax (Langacker, 2008).", "For NLP practitioners, MWEs are notoriously difficult to detect and parse (Sag et al., 2002)."], "vector_1": {"occupi": 1, "work": 1, "nlp": 1, "multiword": 1, "limin": 1, "process": 1, "natur": 1, "express": 1, "practition": 1, "space": 1, "vex": 1, "syntax": 1, "lexicon": 1, "pars": 1, "theoret": 2, "mwe": 3, "notori": 1, "detect": 1, "linguist": 2, "languag": 1, "difficult": 1}, "marker": "(Langacker, 2008)", "article": "W15-0914", "vector_2": [7, 0.04890972080701039, 2, 1, 0, 0]}, {"label": "Neut", "current": "The effects of predictability are well documented, in that more likely sequences are produced and comprehended at a faster rate (GoldmanEisler, 1958; Hale, 2006; Nottbusch et al., 2007; Levy, 2008; Smith and Levy, 2013, and references therein).", "context": ["A final confound to be investigated was sequence likelihood.", "The effects of predictability are well documented, in that more likely sequences are produced and comprehended at a faster rate (GoldmanEisler, 1958; Hale, 2006; Nottbusch et al., 2007; Levy, 2008; Smith and Levy, 2013, and references therein).", "Since MWEs are frequently made up of collocations, i.e."], "vector_1": {"therein": 1, "predict": 1, "confound": 1, "rate": 1, "ie": 1, "colloc": 1, "comprehend": 1, "mwe": 1, "document": 1, "final": 1, "refer": 1, "investig": 1, "sequenc": 2, "effect": 1, "likelihood": 1, "sinc": 1, "faster": 1, "made": 1, "like": 1, "well": 1, "produc": 1, "frequent": 1}, "marker": "Smith and Levy, 2013, ", "article": "W15-0914", "vector_2": [2, 0.6084369268392092, 5, 1, 1, 0]}, {"label": "Neut", "current": "Pauses that occur on the edges of MWEs may represent distinct barrier pauses (Dahlmann and Adolphs, 2007), and therefore merit a further, but distinct study.", "context": ["In this figure, the underscores represent measured pauses, while a whitespace gap represents a pause   that was not taken into consideration for the present study.", "Pauses that occur on the edges of MWEs may represent distinct barrier pauses (Dahlmann and Adolphs, 2007), and therefore merit a further, but distinct study.", "In each task, words within MWEs were consistently produced with a shorter preceding pause than were words in free expressions."], "vector_1": {"paus": 5, "within": 1, "merit": 1, "repres": 3, "consider": 1, "occur": 1, "edg": 1, "distinct": 2, "shorter": 1, "figur": 1, "mwe": 2, "taken": 1, "therefor": 1, "barrier": 1, "may": 1, "express": 1, "free": 1, "gap": 1, "underscor": 1, "present": 1, "measur": 1, "task": 1, "word": 2, "consist": 1, "whitespac": 1, "preced": 1, "studi": 2, "produc": 1}, "marker": "(Dahlmann and Adolphs, 2007)", "article": "W15-0914", "vector_2": [8, 0.7166089260240472, 1, 6, 0, 0]}]