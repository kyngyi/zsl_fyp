{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FYP_SpaceCnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWvybEnjh0sf",
        "colab_type": "code",
        "outputId": "6381c5d0-a7df-4360-e017-73d430e0ddbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1H_Y6_-dhNr",
        "colab_type": "code",
        "outputId": "5f4f6100-7f2b-413b-9ebf-b05767d5f73d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o70RMAnxF3O",
        "colab_type": "code",
        "outputId": "3ff0f42f-0775-4e34-e5a5-2dcb52853cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8IbBDQwoGu8",
        "colab_type": "code",
        "outputId": "bf71bb7e-64f9-466b-c295-1bdcf5ad88a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "pip install json-lines"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting json-lines\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/0f/79c96c0d26b276c583484fe8209e5ebbb416a920309568650325f6e1de73/json_lines-0.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from json-lines) (1.12.0)\n",
            "Installing collected packages: json-lines\n",
            "Successfully installed json-lines-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7pIGP5L3nFt3",
        "colab": {}
      },
      "source": [
        "pip install sentence-transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnxfWXxjL4LR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Config\n",
        "\"\"\"\n",
        "Configuration file for the project.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Base directory.\n",
        "\"\"\"\n",
        "PWD = '/content/drive/My Drive/KY, FYP/Code/'\n",
        "\n",
        "\"\"\"\n",
        "File directories.\n",
        "\"\"\"\n",
        "# Directory for the word embeddings\n",
        "GLOVE_DIR = PWD + '/glove.6B'\n",
        "\n",
        "# Directory for storing citation function data\n",
        "DATA_DIR = PWD + '/data/data'\n",
        "\n",
        "\"\"\"\n",
        "Data files: the citation and provenance dataset.\n",
        "MTL refers to the aligned dataset.\n",
        "\"\"\"\n",
        "DATA_FILES = {\n",
        "    'func': {\n",
        "        'golden_train': 'processed/golden_train.func.json',\n",
        "        'golden_test': 'processed/golden_test.func.json',\n",
        "    },\n",
        "    'scicite': {\n",
        "        'train': 'scicite/train.jsonl',\n",
        "        'test': 'scicite/test.jsonl',\n",
        "        'dev': 'scicite/dev.jsonl',\n",
        "        'test2': 'scicite/test2.jsonl'\n",
        "    },\n",
        "    'acl-arc': {\n",
        "        'train': 'acl-arc/train.jsonl',\n",
        "        'test': 'acl-arc/test.jsonl',\n",
        "        'dev': 'acl-arc/dev.jsonl'\n",
        "    },\n",
        "    'prov': {\n",
        "        'golden_train': 'processed/golden_train.prov.json',\n",
        "        'golden_test': 'processed/golden_test.prov.json',\n",
        "    },\n",
        "    'mtl': {\n",
        "        'golden_train': 'processed/golden_train.mtl.json',\n",
        "        'golden_test': 'processed/golden_test.mtl.json'\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yy6scFMoNxK",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Data\n",
        "\"\"\"\n",
        "Common data operations.\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "import json_lines\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def read_json_data(filename):\n",
        "    \"\"\"\n",
        "    Read the given JSON file.\n",
        "    \"\"\"\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    with open(path, 'rb') as fp:\n",
        "        content = json.load(fp)\n",
        "        return content\n",
        "\n",
        "def read_jsonl_data(filename):\n",
        "    \"\"\"\n",
        "    Read the given JSONL file.\n",
        "    \"\"\"\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    content = []\n",
        "    print (type(content))\n",
        "    with open(path, 'rb') as fp:\n",
        "        for item in json_lines.reader(fp):\n",
        "            content.append(item)\n",
        "        return content\n",
        "\n",
        "\"\"\"\n",
        "Custom cross validation.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def compress_y(ys):\n",
        "    \"\"\"\n",
        "    For each y in ys, if y is of the form [0 0 ... 1 ... 0], compress it to a\n",
        "    single integer.\n",
        "    \"\"\"\n",
        "    if len(ys) < 1:\n",
        "        return ys\n",
        "\n",
        "    if isinstance(ys[0], np.ndarray):\n",
        "        # A hack >.<\n",
        "        return map(lambda x: x.tolist().index(1), ys)\n",
        "    else:\n",
        "        return ys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZDbEnBl0GFy",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title acl-arc (6 Classes)\n",
        "\n",
        "# Integrated Cosine Sim into the model\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "\n",
        "seed = 663\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils,optimizers\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras.losses import cosine_proximity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def embed_sentence(sentence):\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    message_embeddings = session.run(embed(sentence))\n",
        "    return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "    # Flatten feature vector\n",
        "    flat_dim_size = np.prod(model.output_shape[1:])\n",
        "    x = Reshape(target_shape=(flat_dim_size,),\n",
        "                name='features_flat')(model.output)\n",
        "\n",
        "    # Dot product between feature vector and reference vectors\n",
        "    x = Dense(units=output_size,\n",
        "              activation='linear',\n",
        "              use_bias=False)(x)\n",
        "\n",
        "    classifier = Model(inputs=[model.input], outputs=x)\n",
        "    return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "               'Extends': 5}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['acl-arc']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "# random.shuffle(dataset_func)\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "  \n",
        "s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "words = {}\n",
        "\n",
        "# ## Raw Description\n",
        "# words[0] = [\"P provides relevant information for this domain.\"]\n",
        "# words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "# words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "# words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "# words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "# words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "# Summarized\n",
        "words[0] = [\"provides relevant information\"]\n",
        "words[1] = [\"Illustrates need\"]\n",
        "words[2] = [\"Uses\"]\n",
        "words[3] = [\"Extends\"]\n",
        "words[4] = [\"similarity differences\"]\n",
        "words[5] = [\"Potential Future\"]\n",
        "\n",
        "# Class Title\n",
        "# words[0] = [\"Background\"]\n",
        "# words[1] = [\"Motivation\"]\n",
        "# words[2] = [\"Uses\"]\n",
        "# words[3] = [\"Extension\"]\n",
        "# words[4] = [\"Compare Or Contrast\"]\n",
        "# words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # words[i] = embed_sentence(words[i])\n",
        "    words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# -------------------------\n",
        "texts = map(lambda d: d['text'], dataset_func)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "ys = np.asarray(ys)\n",
        "\n",
        "encoded_classes = words[0]\n",
        "for i in range(len(words)-1):\n",
        "    encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "# Proportional Reduction\n",
        "# ------------------------------------\n",
        "\n",
        "new_x_train = []\n",
        "new_y_train = []\n",
        "arr = {}\n",
        "for index in range(len(funcs_index)):\n",
        "    arr[index] = []\n",
        "    for i, value in enumerate(y_train):\n",
        "        if (value == index):\n",
        "            arr[index].append(i)\n",
        "    # print(index, \":\", len(arr[index]))\n",
        "    # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "    # sample_length = int(sample_length)\n",
        "    sample_length = 5\n",
        "    for j in range(sample_length):\n",
        "        new_x_train.append(x_train[arr[index][j]])\n",
        "        new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "new_x_train = np.asarray(new_x_train)\n",
        "new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "indices = np.arange(new_x_train.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "new_x_train = new_x_train[indices]\n",
        "new_y_train = new_y_train[indices]\n",
        "x_train = new_x_train\n",
        "y_train = new_y_train\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "# One Shot\n",
        "# ------------------------------------------\n",
        "\n",
        "# x_train_unique = [x_train[i] for i in indices]\n",
        "# x_train_unique = np.asarray(x_train_unique)\n",
        "# y_train_unique = [y_train[i] for i in indices]\n",
        "# y_train_unique = np.asarray(y_train_unique)\n",
        "# x_train = x_train_unique\n",
        "# y_train = y_train_unique\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "y_train = y_train.tolist()\n",
        "y_test = y_test.tolist()\n",
        "\n",
        "for i,element in enumerate(y_train):\n",
        "  y_train[i] = words[y_train[i]]\n",
        "\n",
        "for i,element in enumerate(y_test):\n",
        "  y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "# ############# Calculate Sample Weights #########################\n",
        "# y_trainz = []\n",
        "# for i, sample in enumerate(y_train):\n",
        "#     for j in range(len(funcs_index)):\n",
        "#         if np.array_equal(y_train[i], words[j]):\n",
        "#             y_trainz.append(j)\n",
        "\n",
        "# countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "# for a in y_trainz:\n",
        "#   countDict[a] += 1\n",
        "\n",
        "# sampleWeight = {}\n",
        "\n",
        "# for i in countDict.keys():\n",
        "#   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "# x_sample_weights = []\n",
        "\n",
        "# for i in y_trainz:\n",
        "#   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "# x_sample_weights = np.array(x_sample_weights)\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 100 # 20\n",
        "indices = []\n",
        "indices_type = []\n",
        "\n",
        "# ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerA\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "# preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "preds = Dense(768, name=\"output_layer\")(x)\n",
        "output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, output_reshape)\n",
        "\n",
        "# rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "model.compile(loss=cosine_proximity,\n",
        "            # optimizer='adam',\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# import datetime\n",
        "# from keras.callbacks import TensorBoard\n",
        "\n",
        "# log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "model.load_weights('model-scicite.h5', by_name=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "# model.save_weights('model-acl.h5')\n",
        "\n",
        "# https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "new_model = build_knn(model, encoded_classes.shape[1])\n",
        "print(new_model.summary())\n",
        "encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "temp_weights = new_model.get_weights()\n",
        "temp_weights[-1] = encoded_classes_norm\n",
        "new_model.set_weights(temp_weights)\n",
        "\n",
        "y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test_list = []\n",
        "sim = {}\n",
        "\n",
        "for i, sample in enumerate(y_pred_probs):\n",
        "    for j in range(len(funcs_index)):\n",
        "        if np.array_equal(y_test[i], words[j]):\n",
        "            y_test_list.append(j)\n",
        "\n",
        "y_test = y_test_list\n",
        "\n",
        "        # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "print(metrics.confusion_matrix(y_test, y_pred_func))\n",
        "print(metrics.classification_report(y_test, y_pred_func, digits=4))\n",
        "print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmLegpxU7VOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_func "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3OLTClGSrTBf",
        "colab": {}
      },
      "source": [
        "o_correct = []\n",
        "for index,value in enumerate(original_func):\n",
        "   if (value==1) and (value==y_test[index]):\n",
        "     o_correct.append(index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ws-xgxAVrS1N",
        "colab": {}
      },
      "source": [
        "o_correct3 = []\n",
        "for index,value in enumerate(y_pred_func):\n",
        "   if (value==1) and (value==y_test[index]):\n",
        "     o_correct3.append(index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DgSpMSTorQGa",
        "colab": {}
      },
      "source": [
        "o_correct3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwd_BmhVw2WQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(6):\n",
        "  print(str(i) + \"INDEX:\")\n",
        "  for index,value in enumerate(y_test):\n",
        "    if (i == value) and y_pred_func[index] == 2:\n",
        "      print(index)\n",
        "      continue\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBGNT84ayX0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test[73]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0xHlZC2yx5Y",
        "colab_type": "code",
        "outputId": "323ec3bf-8ffa-4769-93fd-30791019cee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for index,value in enumerate(xs):\n",
        "  if np.array_equal(value,x_test[73]):\n",
        "    print (index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5ae66f8e-0396-45a8-a5ea-728baf74fcd5",
        "id": "iNUh-F87rP6u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "texts[1282]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and Korelsky , 1992 ) , and LFS ( Iordanskaja et al. , 1992 ) .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ddcece2f-0c95-49e3-9664-f5239de88e37",
        "id": "qA_Pod7qrPxt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "texts[1124]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1bf4381f-9027-4d9e-a11e-a9c469de709b",
        "id": "vaoeqRNErPM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ys[1067]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjvAuPzspsx8",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Commented out ( f(x,y) = 0,1)) unfinished\n",
        "\n",
        "\n",
        "# # Experiment f(x,y) = 0,1 -------- EXP2\n",
        "# # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "# \"\"\"\n",
        "# Perform the experiments on bootstrapped data and actual annotated data.\n",
        "# \"\"\"\n",
        "# # import lib.logger, os, sys, random, math\n",
        "# import numpy as np\n",
        "# import os\n",
        "\n",
        "# from functools import reduce\n",
        "\n",
        "# # import config.config as config\n",
        "# # import data.data as data\n",
        "# # import data.data_func as data_func\n",
        "# import sklearn.metrics as metrics\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from sklearn.model_selection import KFold\n",
        "# import pandas as pd\n",
        "\n",
        "# # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "# from sklearn.utils import class_weight\n",
        "\n",
        "# # import keras.backend as K\n",
        "# from tensorflow.keras import utils\n",
        "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "#     GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "# from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "# from tensorflow.keras.models import Model, Sequential, load_model\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.losses import cosine_proximity\n",
        "\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# # import matplotlib.pyplot as plt\n",
        "# # from sklearn.decomposition import PCA\n",
        "\n",
        "# import random\n",
        "# \"\"\"\n",
        "# Set random seed and fix bug on Dropout usage.\n",
        "# \"\"\"\n",
        "# import tensorflow as tf\n",
        "\n",
        "# import tensorflow_hub as hub\n",
        "\n",
        "# def embed_sentence(sentence):\n",
        "#   with tf.Session() as session:\n",
        "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "#     message_embeddings = session.run(embed(sentence))\n",
        "#     return message_embeddings\n",
        "\n",
        "# def ilen(iterable):\n",
        "#     return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "# def build_knn(model, output_size):\n",
        "#     # Flatten feature vector\n",
        "#     flat_dim_size = np.prod(model.output_shape[1:])\n",
        "#     x = Reshape(target_shape=(flat_dim_size,),\n",
        "#                 name='features_flat')(model.output)\n",
        "\n",
        "#     # Dot product between feature vector and reference vectors\n",
        "#     x = Dense(units=output_size,\n",
        "#               activation='linear',\n",
        "#               use_bias=False)(x)\n",
        "\n",
        "#     classifier = Model(inputs=[model.input], outputs=x)\n",
        "#     return classifier\n",
        "\n",
        "# def normalize_encodings(encodings):\n",
        "#     ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "#     return encodings / ref_norms\n",
        "\n",
        "# seed = 1020\n",
        "# np.random.seed(seed)\n",
        "# # tf.python.control_flow_ops = tf\n",
        "# tf.compat.v1.set_random_seed(seed)\n",
        "# random.seed(seed)\n",
        "\n",
        "# MAX_NB_WORDS = 20000\n",
        "# MAX_SEQUENCE_LENGTH = 50\n",
        "# EMBEDDING_DIM = 100\n",
        "\n",
        "# \"\"\"\n",
        "# Data reading and saving from disk (so that data processing is done only once).\n",
        "# \"\"\"\n",
        "# directory = DATA_DIR\n",
        "# funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "#                'Extends': 5}\n",
        "\n",
        "\n",
        "# # Function dataset start\n",
        "# datafiles = DATA_FILES['acl-arc']\n",
        "# test = read_jsonl_data(datafiles['test'])\n",
        "# train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "# dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "# dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "# train_data = list(filter(lambda d: d['intent'] != 'Error', train))\n",
        "# test_data = list(filter(lambda d: d['intent'] != 'Error', test))\n",
        "\n",
        "# #\n",
        "# # dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "# # embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# # print(\"loaded Hub Module\")\n",
        "\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "  \n",
        "# s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# words = {}\n",
        "\n",
        "# # ## Raw Description\n",
        "# # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "# # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "# # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "# # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "# # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "# # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "# # Summarized\n",
        "# # words[0] = [\"provides relevant information\"]\n",
        "# # words[1] = [\"Illustrates need\"]\n",
        "# # words[2] = [\"Uses\"]\n",
        "# # words[3] = [\"Extends\"]\n",
        "# # words[4] = [\"similarity differences\"]\n",
        "# # words[5] = [\"Potential Future\"]\n",
        "\n",
        "# # Class Title\n",
        "# words[0] = [\"Background\"]\n",
        "# words[1] = [\"Motivation\"]\n",
        "# words[2] = [\"Uses\"]\n",
        "# words[3] = [\"Extension\"]\n",
        "# words[4] = [\"Compare Or Contrast\"]\n",
        "# words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "# for i, word in enumerate(words):\n",
        "#     # words[i] = embed_sentence(words[i])\n",
        "#     words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# # Function dataset end\n",
        "# #############################################################################3\n",
        "\n",
        "# # texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "# train_texts = list(map(lambda d: d['text'], train_data))\n",
        "# test_texts = list(map(lambda d: d['text'], test_data))\n",
        "\n",
        "# train_y = list(map(lambda d: funcs_index[d['intent']], train_data))\n",
        "# test_y = list(map(lambda d: funcs_index[d['intent']], test_data))\n",
        "\n",
        "# # ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "# # for i,element in enumerate(ys):\n",
        "# #     ys[i] = words[ys[i]]\n",
        "\n",
        "# for i,element in enumerate(train_y):\n",
        "#     train_y[i] = words[train_y[i]]\n",
        "\n",
        "# for i,element in enumerate(test_y):\n",
        "#     test_y[i] = words[test_y[i]]\n",
        "\n",
        "# print('Found %s texts.' % (len(train_texts) + len(test_texts)))\n",
        "\n",
        "# texts = train_texts + test_texts\n",
        "\n",
        "# tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "# tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# word_index = tokenizer.word_index\n",
        "# print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# kf = KFold(n_splits=5)\n",
        "\n",
        "# y_pred_func_all = []\n",
        "# y_test_func_all = []\n",
        "# y_pred_prov_all = []\n",
        "# y_test_prov_all = []\n",
        "# y_pred_only_func_all = []\n",
        "# y_test_only_func_all = []\n",
        "# y_pred_only_prov_all = []\n",
        "# y_test_only_prov_all = []\n",
        "# y_pred_func = []\n",
        "# y_test_func = []\n",
        "\n",
        "# embeddings_index = {}\n",
        "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "# for line in f:\n",
        "#     values = line.split()\n",
        "#     word = values[0]\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "\n",
        "# print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# # for word, i in word_index.items():\n",
        "# #     embedding_vector = embeddings_index.get(word)\n",
        "# #     if embedding_vector is not None:\n",
        "# #         # words not found in embedding index will be all-zeros.\n",
        "# #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# # -------------------------\n",
        "# texts = map(lambda d: d['text'], dataset_func)\n",
        "# sequences = tokenizer.texts_to_sequences(texts)\n",
        "# xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "# ys = np.asarray(ys)\n",
        "\n",
        "# x_train = pad_sequences(sequences, )\n",
        "\n",
        "# batch_num = 0\n",
        "# average_list = {}\n",
        "# encoded_classes = words[0]\n",
        "# for i in range(len(words)-1):\n",
        "#     encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "# encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "# for train_index, test_index in kf.split(xs):\n",
        "#     x_train, x_test = xs[train_index], xs[test_index]\n",
        "#     y_train, y_test = ys[train_index], ys[test_index]\n",
        "\n",
        "#     x_train = np.array(x_train)\n",
        "#     x_test = np.array(x_test)\n",
        "#     y_train = np.array(y_train)\n",
        "#     y_test = np.array(y_test)\n",
        "\n",
        "\n",
        "#     # indexes = []\n",
        "#     # for i, sample in enumerate(y_test):\n",
        "#     #     if np.array_equal(y_test[i],word2):\n",
        "#     #         indexes.append(i)\n",
        "#     # y_test = np.delete(y_test,indexes,0)\n",
        "#     # x_test = np.delete(x_test,indexes,0)\n",
        "#     # x_test = np.asarray(x_test)\n",
        "#     # y_test = np.asarray(y_test)\n",
        "\n",
        "#     # y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "\n",
        "#     print (x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "#     NB_FILTER = 128\n",
        "#     BATCH_SIZE = 32\n",
        "#     count = 0\n",
        "#     EPOCH = 15 # 20\n",
        "#     indices = []\n",
        "#     indices_type = []\n",
        "\n",
        "#     # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "#     embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "#     for word, i in word_index.items():\n",
        "#         embedding_vector = embeddings_index.get(word)\n",
        "#         if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#             embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#     embedding_layer = Embedding(len(word_index) + 1,\n",
        "#                                 EMBEDDING_DIM,\n",
        "#                                 weights=[embedding_matrix],\n",
        "#                                 input_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "#     sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "#     embedded_sequences = embedding_layer(sequence_input)\n",
        "#     x = Convolution1D(filters=NB_FILTER,\n",
        "#                     kernel_size=5,\n",
        "#                     padding='valid',\n",
        "#                     activation='relu')(embedded_sequences)\n",
        "\n",
        "#     x = GlobalMaxPooling1D()(x)\n",
        "#     x = Dense(NB_FILTER, activation='relu')(x)\n",
        "#     # x = Dropout(0.3)(x)\n",
        "#     # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "#     preds = Dense(768)(x)\n",
        "#     output_reshape = Reshape((1,768))(preds)\n",
        "\n",
        "#     model = Model(sequence_input, output_reshape)\n",
        "\n",
        "#     model.compile(loss=cosine_proximity,\n",
        "#                 # optimizer='adam',\n",
        "#                 optimizer='rmsprop',\n",
        "#                 metrics=['acc'])\n",
        "\n",
        "#     print(model.summary())\n",
        "\n",
        "#     # import datetime\n",
        "#     # from keras.callbacks import TensorBoard\n",
        "\n",
        "#     # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "#     # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "#     model.fit(x_train, y_train,\n",
        "#             nb_epoch=EPOCH, batch_size=BATCH_SIZE)\n",
        "\n",
        "#     new_model = build_knn(model, encoded_classes.shape[1])\n",
        "#     print(new_model.summary())\n",
        "#     encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "#     temp_weights = new_model.get_weights()\n",
        "#     temp_weights[-1] = encoded_classes_norm\n",
        "#     new_model.set_weights(temp_weights)\n",
        "\n",
        "#     y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "#     y_pred_func = []\n",
        "\n",
        "#     y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "#     y_test_list = []\n",
        "#     sim = {}\n",
        "\n",
        "#     for i, sample in enumerate(y_pred_probs):\n",
        "#         for j in range(len(funcs_index)):\n",
        "#             # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "#             if np.array_equal(y_test[i], words[j]):\n",
        "#                 y_test_list.append(j)\n",
        "#         # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "#         # y_pred_func.append(greatest_sim)\n",
        "\n",
        "#     y_test = y_test_list\n",
        "\n",
        "\n",
        "#     # y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "#     # new_y_pred = [1] * len(y_pred_func)\n",
        "#         # Generate classificat\n",
        "#     # y_pred_func = new_y_pred\n",
        "#     # y_test = data.compress_y(y_test)\n",
        "\n",
        "#             #print('y_pred_func_A')\n",
        "#             #print(y_pred_func)\n",
        "\n",
        "#     y_pred_only_func_all += y_pred_func\n",
        "#     y_test_only_func_all += y_test\n",
        "\n",
        "#         # ---------- End of citation function ----------\n",
        "\n",
        "# print('Plain_Func')\n",
        "# # print(average_list)\n",
        "# print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "# print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOoiAKkk9t_p",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Transferred model with Training Acl anthology (4 classes)\n",
        "\n",
        "# New file authored 28 Jan 2018\n",
        "# For citation provenance\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "\"\"\"\n",
        "This file tests the model that is trained (on acl-arc dataset, 6 classes) that outputs a 512 dimensional vector based on USE\n",
        "Using the 4 Citation taxonomy dataset, (golden_test) as the target test set\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Perform the experiments on bootstrapped data and actual annotated data.\n",
        "\"\"\"\n",
        "# import lib.logger, os, sys, random, math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# import tensorflow_hub as hub\n",
        "\n",
        "# def embed_sentence(sentence):\n",
        "#   with tf.Session() as session:\n",
        "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "#     message_embeddings = session.run(embed(sentence))\n",
        "#     return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "  # Flatten feature vector\n",
        "  flat_dim_size = np.prod(model.output_shape[1:])\n",
        "  x = Reshape(target_shape=(flat_dim_size,),\n",
        "              name='features_flat')(model.output)\n",
        "\n",
        "  # Dot product between feature vector and reference vectors\n",
        "  x = Dense(units=output_size,\n",
        "            activation='linear',\n",
        "            use_bias=False)(x)\n",
        "\n",
        "  classifier = Model(inputs=[model.input], outputs=x)\n",
        "  return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "\n",
        "seed = 1020\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "# GLOVE_DIR = GLOVE_DIR\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['func']\n",
        "test = read_json_data(datafiles['golden_test'])\n",
        "# train = data.read_jsonl_data((datafiles['train']))\n",
        "train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_train + dataset_test\n",
        "# dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "# random.shuffle(dataset_func)\n",
        "\n",
        "\n",
        "# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# print(\"loaded Hub Module\")\n",
        "\n",
        "s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "words = {}\n",
        "\n",
        "## Title ##\n",
        "# words[0] = [\"Weak\"]\n",
        "# words[1] = [\"Compare and Contrast\"]\n",
        "# words[2] = [\"Positive\"]\n",
        "# words[3] = [\"Neutral\"]\n",
        "\n",
        "## Description ##\n",
        "# words[0] = [\"The citation points to weaknesses or problems of the cited paper\"]\n",
        "# words[1] = [\"The citation compares or contrasts the results or methodology from the cited paper with another work. \"]\n",
        "# words[2] = [\"The citation expresses approval of the cited paper. For example, the citing paper adopts an idea,\"\n",
        "#             \" method or dataset from the cited paper, or it shows compliment of the cited paper. \"]\n",
        "# words[3] = [\"The citation serves a neutral purpose: background, mere mentioning, etc; or its function is not decidable.\"]\n",
        "\n",
        "## Mod Description ##\n",
        "words[0] = [\"points Weaknesses problems\"]\n",
        "words[1] = [\"Compares Contrasts\"]\n",
        "# words[1] = [\"similarity differences\"]\n",
        "words[2] = [\"Expresses Approval\"]\n",
        "words[3] = [\"serves Neutral Purpose Function not decidable\"]\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # words[i] = embed_sentence(words[i])\n",
        "    words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts_train = list(map(lambda d: d['context'][0], dataset_train))\n",
        "\n",
        "texts_test = list(map(lambda d: d['context'][0], dataset_test))\n",
        "\n",
        "texts = texts_train + texts_test\n",
        "\n",
        "y_train = list(map(lambda d: funcs_index[d['label']], dataset_train))\n",
        "y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n",
        "\n",
        "ys = y_train + y_test\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=42)\n",
        "\n",
        "y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# -------------------------\n",
        "texts = map(lambda d: d['context'][0], dataset)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "ys = np.asarray(ys)\n",
        "xs = np.asarray(xs)\n",
        "\n",
        "print(xs.shape, ys.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Proportional Reduction\n",
        "# ------------------------------------\n",
        "\n",
        "new_x_train = []\n",
        "new_y_train = []\n",
        "arr = {}\n",
        "for index in range(len(funcs_index)):\n",
        "    arr[index] = []\n",
        "    for i, value in enumerate(y_train):\n",
        "        if (value == index):\n",
        "            arr[index].append(i)\n",
        "    print(index, \":\", len(arr[index]))\n",
        "    # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "    # sample_length = int(sample_length)\n",
        "    sample_length = 5\n",
        "    for j in range(sample_length):\n",
        "        new_x_train.append(x_train[arr[index][j]])\n",
        "        new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "new_x_train = np.asarray(new_x_train)\n",
        "new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "indices = np.arange(new_x_train.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "new_x_train = new_x_train[indices]\n",
        "new_y_train = new_y_train[indices]\n",
        "x_train = new_x_train\n",
        "y_train = new_y_train\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "# One Shot\n",
        "# ------------------------------------------\n",
        "\n",
        "# x_train_unique = [x_train[i] for i in indices]\n",
        "# x_train_unique = np.asarray(x_train_unique)\n",
        "# y_train_unique = [y_train[i] for i in indices]\n",
        "# y_train_unique = np.asarray(y_train_unique)\n",
        "# x_train = x_train_unique\n",
        "# y_train = y_train_unique\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "y_train = y_train.tolist()\n",
        "\n",
        "for i,element in enumerate(y_train):\n",
        "  y_train[i] = words[y_train[i]]\n",
        "\n",
        "for i,element in enumerate(y_test):\n",
        "  y_test[i] = words[y_test[i]]\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_train = np.asarray(y_train)\n",
        "x_train = np.asarray(x_train)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_test)\n",
        "x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_test = np.asarray(y_test)\n",
        "x_test = np.asarray(x_test)\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "\n",
        "encoded_classes = words[0]\n",
        "for i in range(len(words)-1):\n",
        "  encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerC\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\") \n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "# preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "preds = Dense(768, name=\"output_layer\")(x)\n",
        "output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, output_reshape)\n",
        "\n",
        "rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "model.compile(loss=cosine_proximity,\n",
        "            # optimizer='adam',\n",
        "\n",
        "            optimizer=rmsprop,\n",
        "            metrics=['acc'])\n",
        "\n",
        "model.load_weights('model-acl.h5', by_name=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2)\n",
        "\n",
        "new_model = build_knn(model, encoded_classes.shape[1])\n",
        "print(new_model.summary())\n",
        "encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "temp_weights = new_model.get_weights()\n",
        "temp_weights[-1] = encoded_classes_norm\n",
        "new_model.set_weights(temp_weights)\n",
        "\n",
        "y_pred_probs = new_model.predict(x_test)\n",
        "# y_test = ys\n",
        "\n",
        "# total_diff = 0\n",
        "# sample_count = 0\n",
        "# for i, sample in enumerate(y_pred_probs):\n",
        "#     total_diff += 1-cosine_similarity(y_pred_probs[i],ys[i])\n",
        "#     sample_count += 1\n",
        "# average = total_diff/sample_count\n",
        "# print(\"The average cosine difference between the predictions and test are :\")\n",
        "# print(average)\n",
        "# average_list[batch_num] = average\n",
        "# batch_num += 1\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test_list = []\n",
        "sim = {}\n",
        "\n",
        "# for i, sample in enumerate(y_pred_probs):\n",
        "#     for j in range(len(funcs_index)):\n",
        "#         sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "#         if np.array_equal(ys[i], words[j]):\n",
        "#             y_test_list.append(j)\n",
        "#     greatest_sim = max(sim, key=sim.get)\n",
        "#     y_pred_func.append(greatest_sim)\n",
        "\n",
        "# y_test = y_test_list  \n",
        "\n",
        "for i, sample in enumerate(y_pred_probs):\n",
        "    for j in range(len(funcs_index)):\n",
        "        if np.array_equal(y_test[i], words[j]):\n",
        "            y_test_list.append(j)\n",
        "    # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "    # y_pred_func.append(greatest_sim)\n",
        "\n",
        "y_test = y_test_list\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "    # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "# print(average_list)\n",
        "print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all))\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "# print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCKKTxPT-zWL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Transferred model with Training SciCite (3 Classes)\n",
        "\n",
        "seed = 663\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# import tensorflow_hub as hub\n",
        "\n",
        "# def embed_sentence(sentence):\n",
        "#   with tf.Session() as session:\n",
        "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "#     message_embeddings = session.run(embed(sentence))\n",
        "#     return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "  # Flatten feature vector\n",
        "  flat_dim_size = np.prod(model.output_shape[1:])\n",
        "  x = Reshape(target_shape=(flat_dim_size,),\n",
        "              name='features_flat')(model.output)\n",
        "\n",
        "  # Dot product between feature vector and reference vectors\n",
        "  x = Dense(units=output_size,\n",
        "            activation='linear',\n",
        "            use_bias=False)(x)\n",
        "\n",
        "  classifier = Model(inputs=[model.input], outputs=x)\n",
        "  return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "# GLOVE_DIR = GLOVE_DIR\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'background': 0, 'method': 1, 'result': 2}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['scicite']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data(datafiles['train'])\n",
        "\n",
        "dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_train + dataset_test\n",
        "\n",
        "# dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "# random.shuffle(dataset_func)\n",
        "\n",
        "\n",
        "# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# print(\"loaded Hub Module\")\n",
        "\n",
        "s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "words = {}\n",
        "\n",
        "## Title ##\n",
        "# words[0] = [\"Background Information\"]\n",
        "# words[1] = [\"Method\"]\n",
        "# words[2] = [\"Result Comparison\"]\n",
        "\n",
        "## Description ##\n",
        "# words[0] = [\"The citation states, mentions, or points to the background\"\n",
        "#               \" information giving more context about a problem,concept, approach, topic,\"\n",
        "#               \"or importance of the problem in the field\"]\n",
        "# words[1] = [\"Making use of a method, tool, approach or dataset\"]\n",
        "# words[2] = [\"Comparison of the paper’s results/findings\"\n",
        "#               \"with the results/findings of other work\"]\n",
        "\n",
        "## Mod Description ##\n",
        "words[0] = [\"states background\"]\n",
        "words[1] = [\"making use method approach\"]\n",
        "words[2] = [\"Comparison results findings\"]\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # words[i] = embed_sentence(words[i])\n",
        "    words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts_train = list(map(lambda d: d['string'], dataset_train))\n",
        "\n",
        "texts_test = list(map(lambda d: d['string'], dataset_test))\n",
        "\n",
        "texts = texts_train + texts_test\n",
        "\n",
        "y_train = list(map(lambda d: funcs_index[d['label']], dataset_train))\n",
        "y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n",
        "\n",
        "ys = y_train + y_test\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=42)\n",
        "\n",
        "y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "for i,element in enumerate(y_train):\n",
        "  y_train[i] = words[y_train[i]]\n",
        "\n",
        "\n",
        "for i,element in enumerate(y_test):\n",
        "  y_test[i] = words[y_test[i]]\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_train = np.asarray(y_train)\n",
        "x_train = np.asarray(x_train)\n",
        "\n",
        "\n",
        "# Proportional Reduction\n",
        "# ------------------------------------\n",
        "\n",
        "# new_x_train = []\n",
        "# new_y_train = []\n",
        "# arr = {}\n",
        "# for index in range(len(funcs_index)):\n",
        "#     print(index)\n",
        "#     print(\"-----------------------\")\n",
        "#     arr[index] = []\n",
        "#     for i, value in enumerate(y_train):\n",
        "#         if (value == index):\n",
        "#             arr[index].append(i)\n",
        "#     print(len(arr[index]))\n",
        "#     sample_length = len(arr[index]) / 20  # 5% of data\n",
        "#     sample_length = int(sample_length)\n",
        "#     for j in range(sample_length):\n",
        "#         new_x_train.append(x_train[arr[index][j]])\n",
        "#         new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "\n",
        "# for i,element in enumerate(new_x_train):\n",
        "#   new_y_train[i] = words[new_y_train[i]]\n",
        "\n",
        "# new_x_train = np.asarray(new_x_train)\n",
        "# new_y_train = np.asarray(new_y_train)\n",
        "# x_train = new_x_train\n",
        "# y_train = new_y_train\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "# One Shot\n",
        "# ------------------------------------------\n",
        "\n",
        "# x_train_unique = [x_train[i] for i in indices]\n",
        "# x_train_unique = np.asarray(x_train_unique)\n",
        "# y_train_unique = [y_train[i] for i in indices]\n",
        "# y_train_unique = np.asarray(y_train_unique)\n",
        "# x_train = x_train_unique\n",
        "# y_train = y_train_unique\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_test)\n",
        "x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_test = np.asarray(y_test)\n",
        "x_test = np.asarray(x_test)\n",
        "\n",
        "\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "\n",
        "encoded_classes = words[0]\n",
        "for i in range(len(words)-1):\n",
        "  encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerB\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "# preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "preds = Dense(768, name=\"output_layer\")(x)\n",
        "output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, output_reshape)\n",
        "\n",
        "model.compile(loss=cosine_proximity,\n",
        "            # optimizer='adam',\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "model.load_weights('model-scicite.h5', by_name=True)\n",
        "\n",
        "# model.fit(x_train, y_train,\n",
        "#         nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2)\n",
        "\n",
        "# filepath = 'model-scicite.h5'\n",
        "# model.save_weights(filepath)\n",
        "\n",
        "new_model = build_knn(model, encoded_classes.shape[1])\n",
        "print(new_model.summary())\n",
        "encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "temp_weights = new_model.get_weights()\n",
        "temp_weights[-1] = encoded_classes_norm\n",
        "new_model.set_weights(temp_weights)\n",
        "\n",
        "y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test_list = []\n",
        "sim = {}\n",
        "\n",
        "for i, sample in enumerate(y_pred_probs):\n",
        "    for j in range(len(funcs_index)):\n",
        "        # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "        if np.array_equal(y_test[i], words[j]):\n",
        "            y_test_list.append(j)\n",
        "    # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "    # y_pred_func.append(greatest_sim)\n",
        "\n",
        "y_test = y_test_list\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "    # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "# print(average_list)\n",
        "print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all))\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "# print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiYCnrtj0uKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_sample = [\n",
        "            '( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .',\n",
        "            \"One approach to this more general problem , taken by the ` Nitrogen ' generator ( Langkilde and Knight , 1998a ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .\",\n",
        "            'We would like to use features that look at wide context on the input side , which is inexpensive ( Jiampojamarn et al. , 2007 ) .',\n",
        "'Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and Korelsky , 1992 ) , and LFS ( Iordanskaja et al. , 1992 ) .'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF4NR44q06T-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(x_sample)\n",
        "x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "# y_test = np.asarray(y_test)\n",
        "x_test = np.asarray(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLXV9a6C1PP3",
        "colab_type": "code",
        "outputId": "f0d26066-4dc7-4fa6-ff32-0aa88b3d9121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ8lEhR01FqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = new_model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD4HcpGu1RoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(),y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPFq1Utd1VQk",
        "colab_type": "code",
        "outputId": "4e215b86-8cf0-4010-aa6b-6268ffef6272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_pred_func"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRgKotkqoR-t",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title SciCite (4 Classes with resultsS and resultsNS)\n",
        "\n",
        "# New file authored 28 Jan 2018\n",
        "# For citation provenance\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "\"\"\"\n",
        "This file tests the model that is trained (on acl-arc dataset, 6 classes) that outputs a 512 dimensional vector based on USE\n",
        "Using the 4 Citation taxonomy dataset, (golden_test) as the target test set\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Perform the experiments on bootstrapped data and actual annotated data.\n",
        "\"\"\"\n",
        "# import lib.logger, os, sys, random, math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# import tensorflow_hub as hub\n",
        "\n",
        "# def embed_sentence(sentence):\n",
        "#   with tf.Session() as session:\n",
        "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "#     message_embeddings = session.run(embed(sentence))\n",
        "#     return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "  # Flatten feature vector\n",
        "  flat_dim_size = np.prod(model.output_shape[1:])\n",
        "  x = Reshape(target_shape=(flat_dim_size,),\n",
        "              name='features_flat')(model.output)\n",
        "\n",
        "  # Dot product between feature vector and reference vectors\n",
        "  x = Dense(units=output_size,\n",
        "            activation='linear',\n",
        "            use_bias=False)(x)\n",
        "\n",
        "  classifier = Model(inputs=[model.input], outputs=x)\n",
        "  return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "seed = 1020\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "# GLOVE_DIR = GLOVE_DIR\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'background': 0, 'method': 1, 'resultS': 2, 'resultNS': 3}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['scicite']\n",
        "test = read_jsonl_data(datafiles['test2'])\n",
        "\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_test\n",
        "\n",
        "# dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "# random.shuffle(dataset_func)\n",
        "\n",
        "\n",
        "# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# print(\"loaded Hub Module\")\n",
        "\n",
        "s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "words = {}\n",
        "\n",
        "## Title ##\n",
        "# words[0] = [\"Background Information\"]\n",
        "# words[1] = [\"Method\"]\n",
        "# words[2] = [\"Result Comparison\"]\n",
        "\n",
        "## Description ##\n",
        "# words[0] = [\"The citation states, mentions, or points to the background\"\n",
        "#               \" information giving more context about a problem,concept, approach, topic,\"\n",
        "#               \"or importance of the problem in the field\"]\n",
        "# words[1] = [\"Making use of a method, tool, approach or dataset\"]\n",
        "# words[2] = [\"Paper's results/findings are supportive of other work\"]\n",
        "# words[3] = [\"Paper's results/findings are unsupportive of other work\"]\n",
        "\n",
        "\n",
        "\n",
        "## Mod Description ##\n",
        "words[0] = [\"states background\"]\n",
        "words[1] = [\"making use method approach\"]\n",
        "words[2] = [\"comparison supportive results findings\"]\n",
        "words[3] = [\"comparison unsupportive results findings\"]\n",
        "\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # words[i] = embed_sentence(words[i])\n",
        "    words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "\n",
        "texts_test = list(map(lambda d: d['string'], dataset_test))\n",
        "\n",
        "texts = texts_test\n",
        "\n",
        "\n",
        "y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n",
        "\n",
        "ys = y_test\n",
        "\n",
        "x_test = texts\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=42)\n",
        "\n",
        "y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# for i,element in enumerate(y_train):\n",
        "#   y_train[i] = words[y_train[i]]\n",
        "\n",
        "\n",
        "for i,element in enumerate(y_test):\n",
        "  y_test[i] = words[y_test[i]]\n",
        "\n",
        "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# -------------------------\n",
        "# texts = map(lambda d: d['string'], dataset)\n",
        "# sequences = tokenizer.texts_to_sequences(texts)\n",
        "# xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "# ys = np.asarray(ys)\n",
        "# xs = np.asarray(xs)\n",
        "\n",
        "# print(xs.shape, ys.shape)\n",
        "\n",
        "# sequences = tokenizer.texts_to_sequences(x_train)\n",
        "# x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "# y_train = np.asarray(y_train)\n",
        "# x_train = np.asarray(x_train)\n",
        "\n",
        "\n",
        "# Proportional Reduction\n",
        "# ------------------------------------\n",
        "\n",
        "# new_x_train = []\n",
        "# new_y_train = []\n",
        "# arr = {}\n",
        "# for index in range(len(funcs_index)):\n",
        "#     print(index)\n",
        "#     print(\"-----------------------\")\n",
        "#     arr[index] = []\n",
        "#     for i, value in enumerate(y_train):\n",
        "#         if (value == index):\n",
        "#             arr[index].append(i)\n",
        "#     print(len(arr[index]))\n",
        "#     sample_length = len(arr[index]) / 20  # 5% of data\n",
        "#     sample_length = int(sample_length)\n",
        "#     for j in range(sample_length):\n",
        "#         new_x_train.append(x_train[arr[index][j]])\n",
        "#         new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "\n",
        "# for i,element in enumerate(new_x_train):\n",
        "#   new_y_train[i] = words[new_y_train[i]]\n",
        "\n",
        "# new_x_train = np.asarray(new_x_train)\n",
        "# new_y_train = np.asarray(new_y_train)\n",
        "# x_train = new_x_train\n",
        "# y_train = new_y_train\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "# One Shot\n",
        "# ------------------------------------------\n",
        "\n",
        "# x_train_unique = [x_train[i] for i in indices]\n",
        "# x_train_unique = np.asarray(x_train_unique)\n",
        "# y_train_unique = [y_train[i] for i in indices]\n",
        "# y_train_unique = np.asarray(y_train_unique)\n",
        "# x_train = x_train_unique\n",
        "# y_train = y_train_unique\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_test)\n",
        "x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_test = np.asarray(y_test)\n",
        "x_test = np.asarray(x_test)\n",
        "\n",
        "\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "\n",
        "encoded_classes = words[0]\n",
        "for i in range(len(words)-1):\n",
        "  encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerZ\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "# preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "preds = Dense(768, name=\"output_layer\")(x)\n",
        "output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, output_reshape)\n",
        "\n",
        "model.compile(loss=cosine_proximity,\n",
        "            # optimizer='adam',\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "# model.load_weights(path, by_name=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2)\n",
        "\n",
        "# model.save_weights('model-scicite.h5')\n",
        "\n",
        "new_model = build_knn(model, encoded_classes.shape[1])\n",
        "print(new_model.summary())\n",
        "encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "temp_weights = new_model.get_weights()\n",
        "temp_weights[-1] = encoded_classes_norm\n",
        "new_model.set_weights(temp_weights)\n",
        "\n",
        "y_pred_probs = new_model.predict(x_test)\n",
        "# y_test = ys\n",
        "\n",
        "# total_diff = 0\n",
        "# sample_count = 0\n",
        "# for i, sample in enumerate(y_pred_probs):\n",
        "#     total_diff += 1-cosine_similarity(y_pred_probs[i],ys[i])\n",
        "#     sample_count += 1\n",
        "# average = total_diff/sample_count\n",
        "# print(\"The average cosine difference between the predictions and test are :\")\n",
        "# print(average)\n",
        "# average_list[batch_num] = average\n",
        "# batch_num += 1\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test_list = []\n",
        "sim = {}\n",
        "\n",
        "# for i, sample in enumerate(y_pred_probs):\n",
        "#     for j in range(len(funcs_index)):\n",
        "#         sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "#         if np.array_equal(ys[i], words[j]):\n",
        "#             y_test_list.append(j)\n",
        "#     greatest_sim = max(sim, key=sim.get)\n",
        "#     y_pred_func.append(greatest_sim)\n",
        "\n",
        "# y_test = y_test_list  \n",
        "\n",
        "for i, sample in enumerate(y_pred_probs):\n",
        "    for j in range(len(funcs_index)):\n",
        "        # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "        if np.array_equal(y_test[i], words[j]):\n",
        "            y_test_list.append(j)\n",
        "    # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "    # y_pred_func.append(greatest_sim)\n",
        "\n",
        "y_test = y_test_list\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "    # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "# print(average_list)\n",
        "print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all))\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "# print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz5c5ZJziaoa",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title SciCite Function Training: sciCite(seed,filepath)\n",
        "def sciCite(seed,filepath,text_path):\n",
        "\n",
        "  \"\"\"\n",
        "  Perform the experiments on bootstrapped data and actual annotated data.\n",
        "  \"\"\"\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "\n",
        "  # import tensorflow_hub as hub\n",
        "\n",
        "  # def embed_sentence(sentence):\n",
        "  #   with tf.Session() as session:\n",
        "  #     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  #     message_embeddings = session.run(embed(sentence))\n",
        "  #     return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "    # Flatten feature vector\n",
        "    flat_dim_size = np.prod(model.output_shape[1:])\n",
        "    x = Reshape(target_shape=(flat_dim_size,),\n",
        "                name='features_flat')(model.output)\n",
        "\n",
        "    # Dot product between feature vector and reference vectors\n",
        "    x = Dense(units=output_size,\n",
        "              activation='linear',\n",
        "              use_bias=False)(x)\n",
        "\n",
        "    classifier = Model(inputs=[model.input], outputs=x)\n",
        "    return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  # GLOVE_DIR = GLOVE_DIR\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'background': 0, 'method': 1, 'result': 2}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['scicite']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data(datafiles['train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n",
        "\n",
        "  # dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "\n",
        "  # embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "  # print(\"loaded Hub Module\")\n",
        "\n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  ## Title ##\n",
        "  # words[0] = [\"Background Information\"]\n",
        "  # words[1] = [\"Method\"]\n",
        "  # words[2] = [\"Result Comparison\"]\n",
        "\n",
        "  ## Description ##\n",
        "  # words[0] = [\"The citation states, mentions, or points to the background\"\n",
        "  #               \" information giving more context about a problem,concept, approach, topic,\"\n",
        "  #               \"or importance of the problem in the field\"]\n",
        "  # words[1] = [\"Making use of a method, tool, approach or dataset\"]\n",
        "  # words[2] = [\"Comparison of the paper’s results/findings\"\n",
        "  #               \"with the results/findings of other work\"]\n",
        "\n",
        "  ## Mod Description ##\n",
        "  words[0] = [\"states background\"]\n",
        "  words[1] = [\"making use method approach\"]\n",
        "  words[2] = [\"Comparison results findings\"]\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts_train = list(map(lambda d: d['string'], dataset_train))\n",
        "\n",
        "  texts_test = list(map(lambda d: d['string'], dataset_test))\n",
        "\n",
        "  texts = texts_train + texts_test\n",
        "\n",
        "  y_train = list(map(lambda d: funcs_index[d['label']], dataset_train))\n",
        "  y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n",
        "\n",
        "  ys = y_train + y_test\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  # texts = map(lambda d: d['string'], dataset)\n",
        "  # sequences = tokenizer.texts_to_sequences(texts)\n",
        "  # xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  # ys = np.asarray(ys)\n",
        "  # xs = np.asarray(xs)\n",
        "\n",
        "  # print(xs.shape, ys.shape)\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(x_train)\n",
        "  x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  y_train = np.asarray(y_train)\n",
        "  x_train = np.asarray(x_train)\n",
        "\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  # new_x_train = []\n",
        "  # new_y_train = []\n",
        "  # arr = {}\n",
        "  # for index in range(len(funcs_index)):\n",
        "  #     print(index)\n",
        "  #     print(\"-----------------------\")\n",
        "  #     arr[index] = []\n",
        "  #     for i, value in enumerate(y_train):\n",
        "  #         if (value == index):\n",
        "  #             arr[index].append(i)\n",
        "  #     print(len(arr[index]))\n",
        "  #     sample_length = len(arr[index]) / 20  # 5% of data\n",
        "  #     sample_length = int(sample_length)\n",
        "  #     for j in range(sample_length):\n",
        "  #         new_x_train.append(x_train[arr[index][j]])\n",
        "  #         new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "\n",
        "  # for i,element in enumerate(new_x_train):\n",
        "  #   new_y_train[i] = words[new_y_train[i]]\n",
        "\n",
        "  # new_x_train = np.asarray(new_x_train)\n",
        "  # new_y_train = np.asarray(new_y_train)\n",
        "  # x_train = new_x_train\n",
        "  # y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(x_test)\n",
        "  x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  y_test = np.asarray(y_test)\n",
        "  x_test = np.asarray(x_test)\n",
        "\n",
        "\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 20 # 20\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "    encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerB\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  x = Dropout(0.7)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # model.load_weights('model-acl.h5', by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2)\n",
        "\n",
        "  model.save_weights(filepath)\n",
        "\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "  # y_test = ys\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  # for i, sample in enumerate(y_pred_probs):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "  #         if np.array_equal(ys[i], words[j]):\n",
        "  #             y_test_list.append(j)\n",
        "  #     greatest_sim = max(sim, key=sim.get)\n",
        "  #     y_pred_func.append(greatest_sim)\n",
        "\n",
        "  # y_test = y_test_list  \n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "      # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "      # y_pred_func.append(greatest_sim)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "      # ---------- End of citation function ----------\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Scicite Training Results Seed \",file=text_file)\n",
        "    print(seed,file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "    print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAqIMIntW4Yc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title SciCite Function Training: aclant(seed,filepath,text_path) TRAINING ON HOLD\n",
        "def aclant(seed,filepath,text_path):\n",
        "\n",
        "  \"\"\"\n",
        "  Perform the experiments on bootstrapped data and actual annotated data.\n",
        "  \"\"\"\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "\n",
        "  # import tensorflow_hub as hub\n",
        "\n",
        "  # def embed_sentence(sentence):\n",
        "  #   with tf.Session() as session:\n",
        "  #     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  #     message_embeddings = session.run(embed(sentence))\n",
        "  #     return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "    # Flatten feature vector\n",
        "    flat_dim_size = np.prod(model.output_shape[1:])\n",
        "    x = Reshape(target_shape=(flat_dim_size,),\n",
        "                name='features_flat')(model.output)\n",
        "\n",
        "    # Dot product between feature vector and reference vectors\n",
        "    x = Dense(units=output_size,\n",
        "              activation='linear',\n",
        "              use_bias=False)(x)\n",
        "\n",
        "    classifier = Model(inputs=[model.input], outputs=x)\n",
        "    return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  # GLOVE_DIR = GLOVE_DIR\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['func']\n",
        "  test = read_json_data(datafiles['golden_test'])\n",
        "  # train = data.read_jsonl_data((datafiles['train']))\n",
        "  train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n",
        "\n",
        "  # dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "\n",
        "  # embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "  # print(\"loaded Hub Module\")\n",
        "\n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  words[0] = [\"points Weaknesses problems\"]\n",
        "  words[1] = [\"Compares Contrasts\"]\n",
        "  # words[1] = [\"similarity differences\"]\n",
        "  words[2] = [\"Expresses Approval\"]\n",
        "  words[3] = [\"serves Neutral Purpose Function not decidable\"]\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts_train = list(map(lambda d: d['context'][0], dataset_train))\n",
        "\n",
        "  texts_test = list(map(lambda d: d['context'][0], dataset_test))\n",
        "\n",
        "  texts = texts_train + texts_test\n",
        "\n",
        "  y_train = list(map(lambda d: funcs_index[d['label']], dataset_train))\n",
        "  y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n",
        "\n",
        "  ys = y_train + y_test\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  # texts = map(lambda d: d['string'], dataset)\n",
        "  # sequences = tokenizer.texts_to_sequences(texts)\n",
        "  # xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  # ys = np.asarray(ys)\n",
        "  # xs = np.asarray(xs)\n",
        "\n",
        "  # print(xs.shape, ys.shape)\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(x_train)\n",
        "  x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  y_train = np.asarray(y_train)\n",
        "  x_train = np.asarray(x_train)\n",
        "\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  # new_x_train = []\n",
        "  # new_y_train = []\n",
        "  # arr = {}\n",
        "  # for index in range(len(funcs_index)):\n",
        "  #     print(index)\n",
        "  #     print(\"-----------------------\")\n",
        "  #     arr[index] = []\n",
        "  #     for i, value in enumerate(y_train):\n",
        "  #         if (value == index):\n",
        "  #             arr[index].append(i)\n",
        "  #     print(len(arr[index]))\n",
        "  #     sample_length = len(arr[index]) / 20  # 5% of data\n",
        "  #     sample_length = int(sample_length)\n",
        "  #     for j in range(sample_length):\n",
        "  #         new_x_train.append(x_train[arr[index][j]])\n",
        "  #         new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "\n",
        "  # for i,element in enumerate(new_x_train):\n",
        "  #   new_y_train[i] = words[new_y_train[i]]\n",
        "\n",
        "  # new_x_train = np.asarray(new_x_train)\n",
        "  # new_y_train = np.asarray(new_y_train)\n",
        "  # x_train = new_x_train\n",
        "  # y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(x_test)\n",
        "  x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  y_test = np.asarray(y_test)\n",
        "  x_test = np.asarray(x_test)\n",
        "\n",
        "\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 30 # 20\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "    encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerB\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # model.load_weights('model-acl.h5', by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2)\n",
        "\n",
        "  model.save_weights(filepath)\n",
        "\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "  # y_test = ys\n",
        "\n",
        "  # total_diff = 0\n",
        "  # sample_count = 0\n",
        "  # for i, sample in enumerate(y_pred_probs):\n",
        "  #     total_diff += 1-cosine_similarity(y_pred_probs[i],ys[i])\n",
        "  #     sample_count += 1\n",
        "  # average = total_diff/sample_count\n",
        "  # print(\"The average cosine difference between the predictions and test are :\")\n",
        "  # print(average)\n",
        "  # average_list[batch_num] = average\n",
        "  # batch_num += 1\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  # for i, sample in enumerate(y_pred_probs):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "  #         if np.array_equal(ys[i], words[j]):\n",
        "  #             y_test_list.append(j)\n",
        "  #     greatest_sim = max(sim, key=sim.get)\n",
        "  #     y_pred_func.append(greatest_sim)\n",
        "\n",
        "  # y_test = y_test_list  \n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "      # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "      # y_pred_func.append(greatest_sim)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "      # ---------- End of citation function ----------\n",
        "\n",
        "  print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all))\n",
        "  print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "  # path = text_path\n",
        "  # with open(path, \"a\") as text_file:\n",
        "  #   print(\"Scicite Training Results Seed \",file=text_file)\n",
        "  #   print(seed,file=text_file)\n",
        "  #   print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "  #   print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "  #   print(\"=======================================================================\",file=text_file)\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIKhrhY4IlgF",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title acl-arc function Testing aclarc(seed,filepath) -- 0shot\n",
        "\n",
        "def aclarc(seed,filepath,text_path):\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  # ## Raw Description\n",
        "  # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "  # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "  # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "  # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "  # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "  # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "  # Summarized ## Test *aclarc\n",
        "  words[0] = [\"provides relevant information\"]\n",
        "  words[1] = [\"Illustrates need\"]\n",
        "  words[2] = [\"Uses\"]\n",
        "  words[3] = [\"Extends\"]\n",
        "  words[4] = [\"similarity differences\"]\n",
        "  words[5] = [\"Potential Future\"]\n",
        "\n",
        "  # Class Title\n",
        "  # words[0] = [\"Background\"]\n",
        "  # words[1] = [\"Motivation\"]\n",
        "  # words[2] = [\"Uses\"]\n",
        "  # words[3] = [\"Extension\"]\n",
        "  # words[4] = [\"Compare Or Contrast\"]\n",
        "  # words[5] = [\"Future\"]\n",
        "  \n",
        "  #### SciCite (training) ######\n",
        "  # words[0] = [\"states background\"]\n",
        "  # words[1] = [\"making use method approach\"]\n",
        "  # words[2] = [\"Comparison results findings\"]\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "  for i,element in enumerate(ys):\n",
        "      ys[i] = words[ys[i]]\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  ############# Calculate Sample Weights #########################\n",
        "  y_trainz = []\n",
        "  for i, sample in enumerate(y_train):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_train[i], words[j]):\n",
        "              y_trainz.append(j)\n",
        "\n",
        "  countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  for a in y_trainz:\n",
        "    countDict[a] += 1\n",
        "\n",
        "  sampleWeight = {}\n",
        "\n",
        "  for i in countDict.keys():\n",
        "    sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  x_sample_weights = []\n",
        "\n",
        "  for i in y_trainz:\n",
        "    x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  # model.load_weights('model-scicite.h5', by_name=True)\n",
        "  # unique, count = np.unique(y_train, axis=0,return_counts=True)\n",
        "  # count = count[0] / count\n",
        "  # class_weights = {}\n",
        "  # for i in range(len(funcs_index)):\n",
        "  #   class_weights[i] = count[i]\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  # model.fit(x_train, y_train,\n",
        "  #         nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclArc Test (Zero Shot) Results Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_NpFP09TEnP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title acl-arc function Testing aclarcF(seed,filepath) -- 5 FewShot\n",
        "\n",
        "def aclarcF(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  # ## Raw Description\n",
        "  # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "  # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "  # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "  # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "  # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "  # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "  # Summarized\n",
        "  words[0] = [\"provides relevant information\"]\n",
        "  words[1] = [\"Illustrates need\"]\n",
        "  words[2] = [\"Uses\"]\n",
        "  words[3] = [\"Extends\"]\n",
        "  words[4] = [\"similarity differences\"]\n",
        "  words[5] = [\"Potential Future\"]\n",
        "\n",
        "  # Class Title\n",
        "  # words[0] = [\"Background\"]\n",
        "  # words[1] = [\"Motivation\"]\n",
        "  # words[2] = [\"Uses\"]\n",
        "  # words[3] = [\"Extension\"]\n",
        "  # words[4] = [\"Compare Or Contrast\"]\n",
        "  # words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  x = Dropout(0.7)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclArc Test (5-shot) Results Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsSW40LTSSi3",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title acl-arc function Testing aclarcF30(seed,filepath) -- 5 FewShot 30 Epochs\n",
        "\n",
        "def aclarcF30(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  # ## Raw Description\n",
        "  # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "  # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "  # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "  # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "  # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "  # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "  # Summarized\n",
        "  words[0] = [\"provides relevant information\"]\n",
        "  words[1] = [\"Illustrates need\"]\n",
        "  words[2] = [\"Uses\"]\n",
        "  words[3] = [\"Extends\"]\n",
        "  words[4] = [\"similarity differences\"]\n",
        "  words[5] = [\"Potential Future\"]\n",
        "\n",
        "  # Class Title\n",
        "  # words[0] = [\"Background\"]\n",
        "  # words[1] = [\"Motivation\"]\n",
        "  # words[2] = [\"Uses\"]\n",
        "  # words[3] = [\"Extension\"]\n",
        "  # words[4] = [\"Compare Or Contrast\"]\n",
        "  # words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 30 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclArc Test (5-shot) 30 EpochsResults Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIU4qz1wwGNK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title acl-arc function Testing aclarcF30(seed,filepath) -- 5 FewShot 50 Epochs\n",
        "\n",
        "def aclarcF50(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  # ## Raw Description\n",
        "  # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "  # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "  # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "  # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "  # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "  # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "  # Summarized\n",
        "  words[0] = [\"provides relevant information\"]\n",
        "  words[1] = [\"Illustrates need\"]\n",
        "  words[2] = [\"Uses\"]\n",
        "  words[3] = [\"Extends\"]\n",
        "  words[4] = [\"similarity differences\"]\n",
        "  words[5] = [\"Potential Future\"]\n",
        "\n",
        "  # Class Title\n",
        "  # words[0] = [\"Background\"]\n",
        "  # words[1] = [\"Motivation\"]\n",
        "  # words[2] = [\"Uses\"]\n",
        "  # words[3] = [\"Extension\"]\n",
        "  # words[4] = [\"Compare Or Contrast\"]\n",
        "  # words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 50 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclArc Test (5-shot) 50 EpochsResults Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HOoc0TgxQ_z",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title acl-arc function Testing aclarcF30(seed,filepath) -- 5 FewShot 100 Epochs\n",
        "\n",
        "def aclarcF100(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  # ## Raw Description\n",
        "  # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "  # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "  # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "  # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "  # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "  # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "  # Summarized\n",
        "  words[0] = [\"provides relevant information\"]\n",
        "  words[1] = [\"Illustrates need\"]\n",
        "  words[2] = [\"Uses\"]\n",
        "  words[3] = [\"Extends\"]\n",
        "  words[4] = [\"similarity differences\"]\n",
        "  words[5] = [\"Potential Future\"]\n",
        "\n",
        "  # Class Title\n",
        "  # words[0] = [\"Background\"]\n",
        "  # words[1] = [\"Motivation\"]\n",
        "  # words[2] = [\"Uses\"]\n",
        "  # words[3] = [\"Extension\"]\n",
        "  # words[4] = [\"Compare Or Contrast\"]\n",
        "  # words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 100 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclArc Test (5-shot) 100 EpochsResults Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEN5e7snW-JH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title acl-arc function Testing aclarcF(seed,filepath) -- Full\n",
        "\n",
        "def aclarcFull(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  # ## Raw Description\n",
        "  # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "  # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "  # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "  # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "  # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "  # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "  # Summarized\n",
        "  words[0] = [\"provides relevant information\"]\n",
        "  words[1] = [\"Illustrates need\"]\n",
        "  words[2] = [\"Uses\"]\n",
        "  words[3] = [\"Extends\"]\n",
        "  words[4] = [\"similarity differences\"]\n",
        "  words[5] = [\"Potential Future\"]\n",
        "\n",
        "  # Class Title\n",
        "  # words[0] = [\"Background\"]\n",
        "  # words[1] = [\"Motivation\"]\n",
        "  # words[2] = [\"Uses\"]\n",
        "  # words[3] = [\"Extension\"]\n",
        "  # words[4] = [\"Compare Or Contrast\"]\n",
        "  # words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  # new_x_train = []\n",
        "  # new_y_train = []\n",
        "  # arr = {}\n",
        "  # for index in range(len(funcs_index)):\n",
        "  #     arr[index] = []\n",
        "  #     for i, value in enumerate(y_train):\n",
        "  #         if (value == index):\n",
        "  #             arr[index].append(i)\n",
        "  #     # print(index, \":\", len(arr[index]))\n",
        "  #     # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "  #     # sample_length = int(sample_length)\n",
        "  #     sample_length = 5\n",
        "  #     for j in range(sample_length):\n",
        "  #         new_x_train.append(x_train[arr[index][j]])\n",
        "  #         new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  # new_x_train = np.asarray(new_x_train)\n",
        "  # new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  # indices = np.arange(new_x_train.shape[0])\n",
        "  # np.random.shuffle(indices)\n",
        "\n",
        "  # new_x_train = new_x_train[indices]\n",
        "  # new_y_train = new_y_train[indices]\n",
        "  # x_train = new_x_train\n",
        "  # y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclArc Test (Full Data) Results Seed \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSXbuGMrSVGZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title acl-arc function Testing aclarc10(seed,filepath) -- 10 FewShot\n",
        "\n",
        "def aclarc10(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  # ## Raw Description\n",
        "  # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "  # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "  # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "  # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "  # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "  # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "  # Summarized\n",
        "  words[0] = [\"provides relevant information\"]\n",
        "  words[1] = [\"Illustrates need\"]\n",
        "  words[2] = [\"Uses\"]\n",
        "  words[3] = [\"Extends\"]\n",
        "  words[4] = [\"similarity differences\"]\n",
        "  words[5] = [\"Potential Future\"]\n",
        "\n",
        "  # Class Title\n",
        "  # words[0] = [\"Background\"]\n",
        "  # words[1] = [\"Motivation\"]\n",
        "  # words[2] = [\"Uses\"]\n",
        "  # words[3] = [\"Extension\"]\n",
        "  # words[4] = [\"Compare Or Contrast\"]\n",
        "  # words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 10\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclArc Test (10-shot) Results Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb3gkB-hSdlr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title acl-arc function Testing aclarc20(seed,filepath) -- 20 FewShot\n",
        "\n",
        "def aclarc20(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  # ## Raw Description\n",
        "  # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "  # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "  # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "  # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "  # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "  # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "  # Summarized\n",
        "  words[0] = [\"provides relevant information\"]\n",
        "  words[1] = [\"Illustrates need\"]\n",
        "  words[2] = [\"Uses\"]\n",
        "  words[3] = [\"Extends\"]\n",
        "  words[4] = [\"similarity differences\"]\n",
        "  words[5] = [\"Potential Future\"]\n",
        "\n",
        "  # Class Title\n",
        "  # words[0] = [\"Background\"]\n",
        "  # words[1] = [\"Motivation\"]\n",
        "  # words[2] = [\"Uses\"]\n",
        "  # words[3] = [\"Extension\"]\n",
        "  # words[4] = [\"Compare Or Contrast\"]\n",
        "  # words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 20\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclArc Test (20-shot) Results Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdOk4h7SSkW3",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title acl-arc function Testing aclarc10(seed,filepath) -- 50 FewShot\n",
        "\n",
        "def aclarc50(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  # ## Raw Description\n",
        "  # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "  # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "  # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "  # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "  # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "  # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "  # Summarized\n",
        "  words[0] = [\"provides relevant information\"]\n",
        "  words[1] = [\"Illustrates need\"]\n",
        "  words[2] = [\"Uses\"]\n",
        "  words[3] = [\"Extends\"]\n",
        "  words[4] = [\"similarity differences\"]\n",
        "  words[5] = [\"Potential Future\"]\n",
        "\n",
        "  # Class Title\n",
        "  # words[0] = [\"Background\"]\n",
        "  # words[1] = [\"Motivation\"]\n",
        "  # words[2] = [\"Uses\"]\n",
        "  # words[3] = [\"Extension\"]\n",
        "  # words[4] = [\"Compare Or Contrast\"]\n",
        "  # words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 50\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclArc Test (50-shot) Results Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2q8MqCUSvCH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title acl-arc function Testing aclarc100(seed,filepath) -- 100 FewShot\n",
        "\n",
        "def aclarc100(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  # ## Raw Description\n",
        "  # words[0] = [\"P provides relevant information for this domain.\"]\n",
        "  # words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "  # words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "  # words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "  # words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "  # words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "  # Summarized\n",
        "  words[0] = [\"provides relevant information\"]\n",
        "  words[1] = [\"Illustrates need\"]\n",
        "  words[2] = [\"Uses\"]\n",
        "  words[3] = [\"Extends\"]\n",
        "  words[4] = [\"similarity differences\"]\n",
        "  words[5] = [\"Potential Future\"]\n",
        "\n",
        "  # Class Title\n",
        "  # words[0] = [\"Background\"]\n",
        "  # words[1] = [\"Motivation\"]\n",
        "  # words[2] = [\"Uses\"]\n",
        "  # words[3] = [\"Extension\"]\n",
        "  # words[4] = [\"Compare Or Contrast\"]\n",
        "  # words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 100\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclArc Test (100-shot) Results Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkkejPRjR5y2",
        "colab_type": "text"
      },
      "source": [
        "============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYC-Iwb23cNl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title aclant function Testing aclant(seed,filepath,text_path) -- 0-shot\n",
        "\n",
        "def aclant(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['func']\n",
        "  test = read_json_data(datafiles['golden_test'])\n",
        "  # train = data.read_jsonl_data((datafiles['train']))\n",
        "  train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "  dataset = list(filter(lambda x: x['label'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['label'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  ## Title ##\n",
        "  # words[0] = [\"Weak\"]\n",
        "  # words[1] = [\"Compare and Contrast\"]\n",
        "  # words[2] = [\"Positive\"]\n",
        "  # words[3] = [\"Neutral\"]\n",
        "\n",
        "  ## Description ##\n",
        "  # words[0] = [\"The citation points to weaknesses or problems of the cited paper\"]\n",
        "  # words[1] = [\"The citation compares or contrasts the results or methodology from the cited paper with another work. \"]\n",
        "  # words[2] = [\"The citation expresses approval of the cited paper. For example, the citing paper adopts an idea,\"\n",
        "  #             \" method or dataset from the cited paper, or it shows compliment of the cited paper. \"]\n",
        "  # words[3] = [\"The citation serves a neutral purpose: background, mere mentioning, etc; or its function is not decidable.\"]\n",
        "\n",
        "  ## Mod Description ##\n",
        "  words[0] = [\"points Weaknesses problems\"]\n",
        "  words[1] = [\"Compares Contrasts\"]\n",
        "  # words[1] = [\"similarity differences\"]\n",
        "  words[2] = [\"Expresses Approval\"]\n",
        "  words[3] = [\"serves Neutral Purpose Function not decidable\"]\n",
        "\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['context'][0], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['label']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['context'][0], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 20 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  # model.fit(x_train, y_train,\n",
        "  #         nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclAnt Test (Zero shot) Results Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZAr8ky3sFNK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title aclant function Testing aclantf(seed,filepath,text_path) -- 5-shot\n",
        "\n",
        "def aclantf(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['func']\n",
        "  test = read_json_data(datafiles['golden_test'])\n",
        "  # train = data.read_jsonl_data((datafiles['train']))\n",
        "  train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "  dataset = list(filter(lambda x: x['label'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['label'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  ## Title ##\n",
        "  # words[0] = [\"Weak\"]\n",
        "  # words[1] = [\"Compare and Contrast\"]\n",
        "  # words[2] = [\"Positive\"]\n",
        "  # words[3] = [\"Neutral\"]\n",
        "\n",
        "  ## Description ##\n",
        "  # words[0] = [\"The citation points to weaknesses or problems of the cited paper\"]\n",
        "  # words[1] = [\"The citation compares or contrasts the results or methodology from the cited paper with another work. \"]\n",
        "  # words[2] = [\"The citation expresses approval of the cited paper. For example, the citing paper adopts an idea,\"\n",
        "  #             \" method or dataset from the cited paper, or it shows compliment of the cited paper. \"]\n",
        "  # words[3] = [\"The citation serves a neutral purpose: background, mere mentioning, etc; or its function is not decidable.\"]\n",
        "\n",
        "  ## Mod Description ##\n",
        "  words[0] = [\"points Weaknesses problems\"]\n",
        "  words[1] = [\"Compares Contrasts\"]\n",
        "  # words[1] = [\"similarity differences\"]\n",
        "  words[2] = [\"Expresses Approval\"]\n",
        "  words[3] = [\"serves Neutral Purpose Function not decidable\"]\n",
        "\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['context'][0], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['label']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['context'][0], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclAnt Test (5-shot) Results Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHfCdRWm3xvo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title aclant function Testing aclantfull(seed,filepath,text_path) -- Full\n",
        "\n",
        "def aclantfull(seed,filepath,text_path):\n",
        "  #@title acl-arc (6 Classes)\n",
        "\n",
        "  # Integrated Cosine Sim into the model\n",
        "  # https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils,optimizers\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras import backend as K\n",
        "  from keras.losses import cosine_proximity\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['func']\n",
        "  test = read_json_data(datafiles['golden_test'])\n",
        "  # train = data.read_jsonl_data((datafiles['train']))\n",
        "  train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "  dataset = list(filter(lambda x: x['label'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['label'] != 'Error', test + train))\n",
        "\n",
        "  # random.shuffle(dataset_func)\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    \n",
        "  s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  words = {}\n",
        "\n",
        "  ## Title ##\n",
        "  # words[0] = [\"Weak\"]\n",
        "  # words[1] = [\"Compare and Contrast\"]\n",
        "  # words[2] = [\"Positive\"]\n",
        "  # words[3] = [\"Neutral\"]\n",
        "\n",
        "  ## Description ##\n",
        "  # words[0] = [\"The citation points to weaknesses or problems of the cited paper\"]\n",
        "  # words[1] = [\"The citation compares or contrasts the results or methodology from the cited paper with another work. \"]\n",
        "  # words[2] = [\"The citation expresses approval of the cited paper. For example, the citing paper adopts an idea,\"\n",
        "  #             \" method or dataset from the cited paper, or it shows compliment of the cited paper. \"]\n",
        "  # words[3] = [\"The citation serves a neutral purpose: background, mere mentioning, etc; or its function is not decidable.\"]\n",
        "\n",
        "  ## Mod Description ##\n",
        "  words[0] = [\"points Weaknesses problems\"]\n",
        "  words[1] = [\"Compares Contrasts\"]\n",
        "  # words[1] = [\"similarity differences\"]\n",
        "  words[2] = [\"Expresses Approval\"]\n",
        "  words[3] = [\"serves Neutral Purpose Function not decidable\"]\n",
        "\n",
        "\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "      # words[i] = embed_sentence(words[i])\n",
        "      words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['context'][0], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['label']], dataset_func))\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['context'][0], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  encoded_classes = words[0]\n",
        "  for i in range(len(words)-1):\n",
        "      encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "  encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  # new_x_train = []\n",
        "  # new_y_train = []\n",
        "  # arr = {}\n",
        "  # for index in range(len(funcs_index)):\n",
        "  #     arr[index] = []\n",
        "  #     for i, value in enumerate(y_train):\n",
        "  #         if (value == index):\n",
        "  #             arr[index].append(i)\n",
        "  #     # print(index, \":\", len(arr[index]))\n",
        "  #     # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "  #     # sample_length = int(sample_length)\n",
        "  #     sample_length = 5\n",
        "  #     for j in range(sample_length):\n",
        "  #         new_x_train.append(x_train[arr[index][j]])\n",
        "  #         new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "  # new_x_train = np.asarray(new_x_train)\n",
        "  # new_y_train = np.asarray(new_y_train)\n",
        "\n",
        "  # indices = np.arange(new_x_train.shape[0])\n",
        "  # np.random.shuffle(indices)\n",
        "\n",
        "  # new_x_train = new_x_train[indices]\n",
        "  # new_y_train = new_y_train[indices]\n",
        "  # x_train = new_x_train\n",
        "  # y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_train = y_train.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  for i,element in enumerate(y_train):\n",
        "    y_train[i] = words[y_train[i]]\n",
        "\n",
        "  for i,element in enumerate(y_test):\n",
        "    y_test[i] = words[y_test[i]]\n",
        "\n",
        "\n",
        "  # ############# Calculate Sample Weights #########################\n",
        "  # y_trainz = []\n",
        "  # for i, sample in enumerate(y_train):\n",
        "  #     for j in range(len(funcs_index)):\n",
        "  #         if np.array_equal(y_train[i], words[j]):\n",
        "  #             y_trainz.append(j)\n",
        "\n",
        "  # countDict = {0: 0,1: 0,2: 0,3: 0,4: 0,5: 0,}\n",
        "  # for a in y_trainz:\n",
        "  #   countDict[a] += 1\n",
        "\n",
        "  # sampleWeight = {}\n",
        "\n",
        "  # for i in countDict.keys():\n",
        "  #   sampleWeight[i] = max(countDict.values()) / countDict[i]\n",
        "\n",
        "  # x_sample_weights = []\n",
        "\n",
        "  # for i in y_trainz:\n",
        "  #   x_sample_weights.append(sampleWeight[i])\n",
        "\n",
        "  # x_sample_weights = np.array(x_sample_weights)\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  # preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  preds = Dense(768, name=\"output_layer\")(x)\n",
        "  output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, output_reshape)\n",
        "\n",
        "  # rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "  model.compile(loss=cosine_proximity,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # https://medium.com/@sorenlind/nearest-neighbors-with-keras-and-coreml-755e76fedf36\n",
        "  new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  temp_weights = new_model.get_weights()\n",
        "  temp_weights[-1] = encoded_classes_norm\n",
        "  new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test_list = []\n",
        "  sim = {}\n",
        "\n",
        "  for i, sample in enumerate(y_pred_probs):\n",
        "      for j in range(len(funcs_index)):\n",
        "          if np.array_equal(y_test[i], words[j]):\n",
        "              y_test_list.append(j)\n",
        "\n",
        "  y_test = y_test_list\n",
        "\n",
        "          # ---------- End of citation function ----------\n",
        "          \n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"AclAnt Test (full-shot) Results Seed \" + str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred_func),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred_func, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjHcPGY9vhy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aclant(1,\"a.pt\",\"a.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qTpfDkuwOP5",
        "colab_type": "code",
        "outputId": "84df3b95-defe-4ec7-daf3-be004ab08b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiW8pVdZwFPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aclarcF(1,\"a.pt\",\"a.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noZRIISETKKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seeds = [883,544,201]\n",
        "path = '/content/drive/My Drive/KY, FYP/Code/output.txt'\n",
        "filepath = 'model-scicite.h5'\n",
        "for seed in seeds:\n",
        "  text_path = '/content/drive/My Drive/KY, FYP/Code/results/output_'\n",
        "  text_path = text_path + str(seed) + \".txt\"\n",
        "  sciCite(seed, filepath,text_path)\n",
        "  aclarc(seed, filepath,text_path)\n",
        "  aclarcF(seed,filepath,text_path)\n",
        "  # aclarc10(seed,filepath,text_path)\n",
        "  # aclarc20(seed,filepath,text_path)\n",
        "  # aclarc50(seed,filepath,text_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDD_qnrMo6Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seeds = [663,883,544,201,356,648,898,88,997,788]\n",
        "path = '/content/drive/My Drive/KY, FYP/Code/output.txt'\n",
        "filepath = 'model-scicite.h5'\n",
        "for seed in seeds:\n",
        "  text_path = '/content/drive/My Drive/KY, FYP/Code/results/output_'\n",
        "  text_path = text_path + str(seed) + \".txt\"\n",
        "  sciCite(seed, filepath,text_path)\n",
        "  aclarc(seed, filepath,text_path)\n",
        "  aclarcF(seed,filepath,text_path)\n",
        "  aclarcFull(seed,filepath,text_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCIH03rSbMJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seeds = [663,883,544,201,356,648,898,88,997,788]\n",
        "path = '/content/drive/My Drive/KY, FYP/Code/output.txt'\n",
        "filepath = 'model-scicite.h5'\n",
        "for seed in seeds:\n",
        "  text_path = '/content/drive/My Drive/KY, FYP/Code/results/output_'\n",
        "  text_path = text_path + str(seed) + \".txt\"\n",
        "  sciCite(seed, filepath,text_path)\n",
        "  aclarcF(seed, filepath,text_path)\n",
        "  aclarcF30(seed,filepath,text_path)\n",
        "  aclarcF50(seed,filepath,text_path)\n",
        "  aclarcF100(seed,filepath,text_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvyXAFMPOM0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seeds = [883,544,201]\n",
        "path = '/content/drive/My Drive/KY, FYP/Code/output.txt'\n",
        "filepath = 'model-scicite.h5'\n",
        "for seed in seeds:\n",
        "  text_path = '/content/drive/My Drive/KY, FYP/Code/results/output_raw_'\n",
        "  text_path = text_path + str(seed) + \".txt\"\n",
        "  sciCite(seed, filepath,text_path)\n",
        "  aclarc(seed, filepath,text_path)\n",
        "  aclarcF(seed, filepath,text_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GifqDUybLx7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcSoS-izTpjV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Combination (Aclarc + Scicite)\n",
        "\n",
        "# Integrated Cosine Sim into the model\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "# import lib.logger, os, sys, random, math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.utils.random import sample_without_replacement\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def embed_sentence(sentence):\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    message_embeddings = session.run(embed(sentence))\n",
        "    return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "    # Flatten feature vector\n",
        "    flat_dim_size = np.prod(model.output_shape[1:])\n",
        "    x = Reshape(target_shape=(flat_dim_size,),\n",
        "                name='features_flat')(model.output)\n",
        "\n",
        "    # Dot product between feature vector and reference vectors\n",
        "    x = Dense(units=output_size,\n",
        "              activation='linear',\n",
        "              use_bias=False)(x)\n",
        "\n",
        "    classifier = Model(inputs=[model.input], outputs=x)\n",
        "    return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "seed = 1020\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "               'Extends': 5, 'background': 6, 'method': 7, 'result': 8}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['acl-arc']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "s_datafiles = DATA_FILES['scicite']\n",
        "s_test = read_jsonl_data(s_datafiles['test'])\n",
        "s_train = read_jsonl_data(s_datafiles['train'])\n",
        "\n",
        "s_dataset_train = list(filter(lambda x: x['label'] != 'Error',s_train))\n",
        "s_dataset_test = list(filter(lambda x: x['label'] != 'Error', s_test))\n",
        "\n",
        "texts_train = list(map(lambda d: d['string'], s_train))\n",
        "\n",
        "texts_test = list(map(lambda d: d['string'], s_test))\n",
        "\n",
        "s_texts = texts_train + texts_test\n",
        "\n",
        "y_train = list(map(lambda d: funcs_index[d['label']], s_dataset_train))\n",
        "y_test = list(map(lambda d: funcs_index[d['label']], s_dataset_test))\n",
        "\n",
        "s_ys = y_train + y_test\n",
        "\n",
        "temp_list0 = []\n",
        "temp_list1 = []\n",
        "r_indices = sample_without_replacement(len(s_test + s_train),len(test + train),random_state=42)\n",
        "for index in r_indices:\n",
        "  temp_list0.append(s_texts[index])\n",
        "  temp_list1.append(s_ys[index])\n",
        "s_texts = temp_list0\n",
        "s_ys = temp_list1\n",
        "\n",
        "dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "random.shuffle(dataset_func)\n",
        "\n",
        "\n",
        "# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# print(\"loaded Hub Module\")\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "  \n",
        "s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "words = {}\n",
        "\n",
        "# ## Raw Description\n",
        "# words[0] = [\"P provides relevant information for this domain.\"]\n",
        "# words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "# words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "# words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "# words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "# words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "# Summarized\n",
        "words[0] = [\"provides relevant information\"]\n",
        "words[1] = [\"Illustrates need\"]\n",
        "words[2] = [\"Uses\"]\n",
        "words[3] = [\"Extends\"]\n",
        "words[4] = [\"similarity differences\"]\n",
        "words[5] = [\"Potential Future\"]\n",
        "## Mod Description ##\n",
        "words[6] = [\"states background\"]\n",
        "words[7] = [\"making use method approach\"]\n",
        "words[8] = [\"Comparison results findings\"]\n",
        "\n",
        "\n",
        "# Class Title\n",
        "# words[0] = [\"Background\"]\n",
        "# words[1] = [\"Motivation\"]\n",
        "# words[2] = [\"Uses\"]\n",
        "# words[3] = [\"Extension\"]\n",
        "# words[4] = [\"Compare Or Contrast\"]\n",
        "# words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # words[i] = embed_sentence(words[i])\n",
        "    words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts = list(map(lambda d: d['text'], dataset_func))\n",
        "texts = texts + s_texts\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "ys = ys + s_ys\n",
        "\n",
        "for i,element in enumerate(ys):\n",
        "    ys[i] = words[ys[i]]\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_prov_all = []\n",
        "y_test_prov_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_only_prov_all = []\n",
        "y_test_only_prov_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# -------------------------\n",
        "# texts = map(lambda d: d['text'], dataset_func)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "ys = np.asarray(ys)\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "encoded_classes = words[0]\n",
        "for i in range(len(words)-1):\n",
        "    encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=42)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "indices = []\n",
        "indices_type = []\n",
        "\n",
        "# ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerA\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "# preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "preds = Dense(768, name=\"output_layer\")(x)\n",
        "output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, output_reshape)\n",
        "\n",
        "model.compile(loss=cosine_proximity,\n",
        "            # optimizer='adam',\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# import datetime\n",
        "# from keras.callbacks import TensorBoard\n",
        "\n",
        "# log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "# model.load_weights('model-scicite.h5', by_name=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "model.save_weights('model-aclarc_scicite.h5')\n",
        "\n",
        "new_model = build_knn(model, encoded_classes.shape[1])\n",
        "print(new_model.summary())\n",
        "encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "temp_weights = new_model.get_weights()\n",
        "temp_weights[-1] = encoded_classes_norm\n",
        "new_model.set_weights(temp_weights)\n",
        "\n",
        "y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test_list = []\n",
        "sim = {}\n",
        "\n",
        "for i, sample in enumerate(y_pred_probs):\n",
        "    for j in range(len(funcs_index)):\n",
        "        # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "        if np.array_equal(y_test[i], words[j]):\n",
        "            y_test_list.append(j)\n",
        "    # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "    # y_pred_func.append(greatest_sim)\n",
        "\n",
        "y_test = y_test_list\n",
        "\n",
        "\n",
        "# y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "# new_y_pred = [1] * len(y_pred_func)\n",
        "    # Generate classificat\n",
        "# y_pred_func = new_y_pred\n",
        "# y_test = data.compress_y(y_test)\n",
        "\n",
        "        #print('y_pred_func_A')\n",
        "        #print(y_pred_func)\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "        # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "# print(average_list)\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycGnAPoJcT_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  datafiles = DATA_FILES['func']\n",
        "  test = read_json_data(datafiles['golden_test'])\n",
        "  # train = data.read_jsonl_data((datafiles['train']))\n",
        "  train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjWvoAv_cU1k",
        "colab_type": "code",
        "outputId": "17bd8360-0d81-4eff-fd9e-bcc227a41167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1432"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}