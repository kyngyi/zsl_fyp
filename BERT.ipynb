{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9opfHSipt4G",
        "colab_type": "text"
      },
      "source": [
        "https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=JhUZO9vc_l6T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERWmLrIFhPbA",
        "colab_type": "code",
        "outputId": "650d2bd6-942b-4bfb-f9be-c6ca2e685a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3bjWImB_BS6",
        "colab_type": "code",
        "outputId": "c6f050b4-ca09-4eae-c6bc-f999ddd290cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73tee5iRiT5B",
        "colab_type": "code",
        "outputId": "ef795f2c-57a7-47d7-922b-84b58e7d5b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "!pip install json-lines\n",
        "!pip install transformers\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting json-lines\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/0f/79c96c0d26b276c583484fe8209e5ebbb416a920309568650325f6e1de73/json_lines-0.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from json-lines) (1.12.0)\n",
            "Installing collected packages: json-lines\n",
            "Successfully installed json-lines-0.5.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl (544kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.33)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 20.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 50.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=ea5b30519d2564215aae60223cdf1c48ed5ab3cefc7aea26fa2ea98e6de74c8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NButH-G-iU9A",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Config\n",
        "\"\"\"\n",
        "Configuration file for the project.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Base directory.\n",
        "\"\"\"\n",
        "PWD = '/content/drive/My Drive/KY, FYP/Code/'\n",
        "\n",
        "\"\"\"\n",
        "File directories.\n",
        "\"\"\"\n",
        "# Directory for the word embeddings\n",
        "GLOVE_DIR = PWD + '/glove.6B'\n",
        "\n",
        "# Directory for storing citation function data\n",
        "DATA_DIR = PWD + '/data/data'\n",
        "\n",
        "\"\"\"\n",
        "Data files: the citation and provenance dataset.\n",
        "MTL refers to the aligned dataset.\n",
        "\"\"\"\n",
        "DATA_FILES = {\n",
        "    'func': {\n",
        "        'golden_train': 'processed/golden_train.func.json',\n",
        "        'golden_test': 'processed/golden_test.func.json',\n",
        "    },\n",
        "    'scicite': {\n",
        "        'train': 'scicite/train.jsonl',\n",
        "        'test': 'scicite/test.jsonl',\n",
        "        'dev': 'scicite/dev.jsonl'\n",
        "    },\n",
        "    'acl-arc': {\n",
        "        'train': 'acl-arc/train.jsonl',\n",
        "        'test': 'acl-arc/test.jsonl',\n",
        "        'dev': 'acl-arc/dev.jsonl'\n",
        "    },\n",
        "    'prov': {\n",
        "        'golden_train': 'processed/golden_train.prov.json',\n",
        "        'golden_test': 'processed/golden_test.prov.json',\n",
        "    },\n",
        "    'mtl': {\n",
        "        'golden_train': 'processed/golden_train.mtl.json',\n",
        "        'golden_test': 'processed/golden_test.mtl.json'\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWV9AXG_ign5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Data\n",
        "\"\"\"\n",
        "Common data operations.\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "import json_lines\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def read_json_data(filename):\n",
        "    \"\"\"\n",
        "    Read the given JSON file.\n",
        "    \"\"\"\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    with open(path, 'rb') as fp:\n",
        "        content = json.load(fp)\n",
        "        return content\n",
        "\n",
        "def read_jsonl_data(filename):\n",
        "    \"\"\"\n",
        "    Read the given JSONL file.\n",
        "    \"\"\"\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    content = []\n",
        "    print (type(content))\n",
        "    with open(path, 'rb') as fp:\n",
        "        for item in json_lines.reader(fp):\n",
        "            content.append(item)\n",
        "        return content\n",
        "\n",
        "\"\"\"\n",
        "Custom cross validation.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def compress_y(ys):\n",
        "    \"\"\"\n",
        "    For each y in ys, if y is of the form [0 0 ... 1 ... 0], compress it to a\n",
        "    single integer.\n",
        "    \"\"\"\n",
        "    if len(ys) < 1:\n",
        "        return ys\n",
        "\n",
        "    if isinstance(ys[0], np.ndarray):\n",
        "        # A hack >.<\n",
        "        return map(lambda x: x.tolist().index(1), ys)\n",
        "    else:\n",
        "        return ys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1x6nerZikWj",
        "colab_type": "code",
        "outputId": "fdd6ae03-89a5-488a-eb29-2cf659a3644f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALoTAHpei60V",
        "colab_type": "code",
        "outputId": "21b6ab17-ad6f-4576-9a6e-b386679547b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rbf5l-8jCqJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACL dataset (4 Classes) Initialization\n",
        "\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['func']\n",
        "test = read_json_data(datafiles['golden_test'])\n",
        "# train = data.read_jsonl_data((datafiles['train']))\n",
        "train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_train + dataset_test\n",
        "\n",
        "texts = list(map(lambda d: d['context'][0], dataset))\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['label']], dataset))\n",
        "\n",
        "seed = 2\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sample in texts:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sample,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 200\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train(80%) and test(20%) sets\n",
        "x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                random_state=seed, test_size=0.2)\n",
        "# Do the same for the masks.\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                             random_state=seed, test_size=0.2)\n",
        "\n",
        "\n",
        "\n",
        "# Futher split train data into train(80%) and validation(20%) sets\n",
        "train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "x_train = torch.tensor(x_train)\n",
        "x_test = torch.tensor(x_test)\n",
        "x_val = torch.tensor(x_val)\n",
        "\n",
        "y_train = torch.tensor(y_train)\n",
        "y_test = torch.tensor(y_test)\n",
        "y_val = torch.tensor(y_val)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgDyUbjYxP6d",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Model Structure\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "# model = BertForSequenceClassification.from_pretrained(\n",
        "#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "#     num_labels = 4, # The number of output labels--2 for binary classification.\n",
        "#                     # You can increase this for multi-class tasks.   \n",
        "#     output_attentions = False, # Whether the model returns attentions weights.\n",
        "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "# )\n",
        "\n",
        "#### Import from scicite(3 Classes) ####\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "transfered_model = PWD + 'Sciicite.pt'\n",
        "model.load_state_dict(torch.load(transfered_model))\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "# model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "R9QtI10GN7Yt",
        "colab": {}
      },
      "source": [
        "#@title Training + Validation\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs =  4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 40\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask)\n",
        "                    # labels=b_labels)\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        # loss = outputs[0]\n",
        "        loss = criterion(outputs[0], b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0QBDiEYxUhf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test\n",
        "# Create the DataLoader for our validation set.\n",
        "test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# ========================================\n",
        "#               Testing\n",
        "# ========================================\n",
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('DONE.')\n",
        "\n",
        "y_test = y_test.tolist()\n",
        "\n",
        "y_pred = []\n",
        "for i in predictions:\n",
        "  y_pred += i.tolist()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(metrics.classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRYHIgZHoRc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'Model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YvOSOO26EeX",
        "colab_type": "text"
      },
      "source": [
        "###################################### END ##################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0nvEpt886KdV",
        "colab": {}
      },
      "source": [
        "model2 = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "model2.load_state_dict(torch.load(path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wdmG0xos6KJT",
        "colab": {}
      },
      "source": [
        "model2.classifier = torch.nn.Linear(768,6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaiQX3khffQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AGT3vK5W6Jnt",
        "colab": {}
      },
      "source": [
        "model2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FloXs9jKrwZ4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title SciCite\n",
        "\n",
        "# Function dataset start\n",
        "funcs_index = {'background': 0, 'method': 1, 'result': 2}\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['scicite']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data(datafiles['train'])\n",
        "\n",
        "dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_train + dataset_test\n",
        "\n",
        "texts = list(map(lambda d: d['string'], dataset))\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['label']], dataset))\n",
        "\n",
        "seed = 3\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sample in texts:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sample,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "# Filter out the instances with >512 tokens\n",
        "for i, sample in enumerate(input_ids):\n",
        "  if len(sample) > 511:\n",
        "    texts.pop(i)\n",
        "    input_ids.pop(i)\n",
        "    ys.pop(i)\n",
        "    print(\"removed Index: \" +  str(i))\n",
        "\n",
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 500\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                random_state=seed, test_size=0.2)\n",
        "# Do the same for the masks.\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                             random_state=seed, test_size=0.2)\n",
        "\n",
        "train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "x_train = torch.tensor(x_train)\n",
        "x_test = torch.tensor(x_test)\n",
        "x_val = torch.tensor(x_val)\n",
        "\n",
        "y_train = torch.tensor(y_train)\n",
        "y_test = torch.tensor(y_test)\n",
        "y_val = torch.tensor(y_val)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs =  0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 40\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('DONE.')\n",
        "\n",
        "y_test = y_test.tolist()\n",
        "\n",
        "y_pred = []\n",
        "for i in predictions:\n",
        "  y_pred += i.tolist()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(metrics.classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk3BaG2SkpkD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for Training scicite scicite(seed,filepath,text_path)\n",
        "\n",
        "def scicite(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "  import random\n",
        "  \n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "  funcs_index = {'background': 0, 'method': 1, 'result': 2}\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['scicite']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data(datafiles['train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n",
        "\n",
        "  texts = list(map(lambda d: d['string'], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['label']], dataset))\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  # Filter out the instances with >512 tokens\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 500\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Use 90% for training and 10% for validation.\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  # Tell pytorch to run this model on the GPU.\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  2\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "  # Set the seed value all over the place to make this reproducible.\n",
        "  seed_val = seed\n",
        "\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Scicite \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n",
        "\n",
        "  torch.save(model.state_dict(), filepath)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii3T6b2WU0fk",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclarc dataset (6 Classes) aclarc(seed,filepath,text_path) -- 0 Shot\n",
        "\n",
        "\n",
        "\n",
        "def aclarc(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      fl\n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  new_train_mask = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "          new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  new_train_mask = new_train_mask[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "  train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  0\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclarc test zero shot \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcyjSWjnZRvA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclarc dataset (6 Classes) aclarcf(seed,filepath,text_path) -- 5 Shot\n",
        "\n",
        "\n",
        "def aclarcf(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  new_train_mask = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "          new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  new_train_mask = new_train_mask[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "  train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  15\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclarc test Few shot \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnXCoswqL8pZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclarc dataset (6 Classes) aclarc10(seed,filepath,text_path) -- 10 Shot\n",
        "\n",
        "\n",
        "def aclarc10(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  new_train_mask = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 10\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "          new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  new_train_mask = new_train_mask[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "  train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  15\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclarc test 10 shot \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS1zId6zMHqA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclarc dataset (6 Classes) aclarcf(seed,filepath,text_path) -- 20 Shot\n",
        "\n",
        "\n",
        "def aclarc20(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  new_train_mask = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 20\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "          new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  new_train_mask = new_train_mask[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "  train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  15\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclarc test 20 shot \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuD491mlMPr-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclarc dataset (6 Classes) aclarc50(seed,filepath,text_path) -- 50 Shot\n",
        "\n",
        "\n",
        "def aclarc50(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  new_train_mask = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 50\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "          new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  new_train_mask = new_train_mask[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "  train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  15\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclarc test 50 shot \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHXJUHAba7qJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclarc dataset (6 Classes) aclarcfull(seed,filepath,text_path) -- full Shot\n",
        "\n",
        "\n",
        "def aclarcfull(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  # new_x_train = []\n",
        "  # new_y_train = []\n",
        "  # new_train_mask = []\n",
        "  # arr = {}\n",
        "  # for index in range(len(funcs_index)):\n",
        "  #     arr[index] = []\n",
        "  #     for i, value in enumerate(y_train):\n",
        "  #         if (value == index):\n",
        "  #             arr[index].append(i)\n",
        "  #     # print(index, \":\", len(arr[index]))\n",
        "  #     # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "  #     # sample_length = int(sample_length)\n",
        "  #     sample_length = 5\n",
        "  #     for j in range(sample_length):\n",
        "  #         new_x_train.append(x_train[arr[index][j]])\n",
        "  #         new_y_train.append(y_train[arr[index][j]])\n",
        "  #         new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  # new_x_train = np.asarray(new_x_train)\n",
        "  # new_y_train = np.asarray(new_y_train)\n",
        "  # new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  # indices = np.arange(new_x_train.shape[0])\n",
        "  # np.random.shuffle(indices)\n",
        "\n",
        "  # new_x_train = new_x_train[indices]\n",
        "  # new_y_train = new_y_train[indices]\n",
        "  # new_train_mask = new_train_mask[indices]\n",
        "  # x_train = new_x_train\n",
        "  # y_train = new_y_train\n",
        "  # train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  2\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclarc test full \",file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUcP5HHbMWpF",
        "colab_type": "text"
      },
      "source": [
        "========================================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPGVYfuXAdDP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclant dataset (4 Classes) aclarc(seed,filepath,text_path) -- 0 Shot\n",
        "\n",
        "\n",
        "def aclant(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['func']\n",
        "  test = read_json_data(datafiles['golden_test'])\n",
        "  # train = data.read_jsonl_data((datafiles['train']))\n",
        "  train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n",
        "\n",
        "  texts = list(map(lambda d: d['context'][0], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['label']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  new_train_mask = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "          new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  new_train_mask = new_train_mask[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "  train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 4, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "\n",
        "  # model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,4)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  0\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclant test Zero shot \" + str(seed),file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p05i5kxU260X",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclant dataset ( Classes) aclarcf(seed,filepath,text_path) -- 5 Shot\n",
        "\n",
        "\n",
        "def aclantF(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['func']\n",
        "  test = read_json_data(datafiles['golden_test'])\n",
        "  # train = data.read_jsonl_data((datafiles['train']))\n",
        "  train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n",
        "\n",
        "  texts = list(map(lambda d: d['context'][0], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['label']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  new_train_mask = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # print(index, \":\", len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "          new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  indices = np.arange(new_x_train.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  new_x_train = new_x_train[indices]\n",
        "  new_y_train = new_y_train[indices]\n",
        "  new_train_mask = new_train_mask[indices]\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "  train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 4, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "\n",
        "  # model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,4)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  15\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclant test Few shot \" + str(seed),file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fcxS0E-Am0s",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Method for aclant dataset ( 4Classes) aclarcFull(seed,filepath,text_path) -- 5 Shot\n",
        "\n",
        "\n",
        "def aclantFull(seed,filepath,text_path):\n",
        "\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import datetime\n",
        "\n",
        "  def flat_accuracy(preds, labels):\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "      labels_flat = labels.flatten()\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "      \n",
        "  def format_time(elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      \n",
        "      # Format as hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "  import random\n",
        "\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['func']\n",
        "  test = read_json_data(datafiles['golden_test'])\n",
        "  # train = data.read_jsonl_data((datafiles['train']))\n",
        "  train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n",
        "\n",
        "  texts = list(map(lambda d: d['context'][0], dataset))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['label']], dataset))\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sample in texts:\n",
        "      # `encode` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      encoded_sent = tokenizer.encode(\n",
        "                          sample,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                          # This function also supports truncation and conversion\n",
        "                          # to pytorch tensors, but we need to do padding, so we\n",
        "                          # can't use these features :( .\n",
        "                          #max_length = 128,          # Truncate all sentences.\n",
        "                          #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_sent)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "\n",
        "  for i, sample in enumerate(input_ids):\n",
        "    if len(sample) > 511:\n",
        "      texts.pop(i)\n",
        "      input_ids.pop(i)\n",
        "      ys.pop(i)\n",
        "      print(\"removed Index: \" +  str(i))\n",
        "\n",
        "  print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "  # We'll borrow the `pad_sequences` utility function to do this.\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "  # Set the maximum sequence length.\n",
        "  # I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "  # maximum training sentence length of 47...\n",
        "  MAX_LEN = 200\n",
        "\n",
        "  print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "  print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "  # Pad our input tokens with value 0.\n",
        "  # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "  # as opposed to the beginning.\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  print('\\nDone.')\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # For each sentence...\n",
        "  for sent in input_ids:\n",
        "      \n",
        "      # Create the attention mask.\n",
        "      #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "      att_mask = [int(token_id > 0) for token_id in sent]\n",
        "      \n",
        "      # Store the attention mask for this sentence.\n",
        "      attention_masks.append(att_mask)\n",
        "\n",
        "  # Use train_test_split to split our data into train and validation sets for\n",
        "  # training\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Split into train(80%) and test(20%) sets\n",
        "  x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                  random_state=seed, test_size=0.2)\n",
        "  # Do the same for the masks.\n",
        "  train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                              random_state=seed, test_size=0.2)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  # new_x_train = []\n",
        "  # new_y_train = []\n",
        "  # new_train_mask = []\n",
        "  # arr = {}\n",
        "  # for index in range(len(funcs_index)):\n",
        "  #     arr[index] = []\n",
        "  #     for i, value in enumerate(y_train):\n",
        "  #         if (value == index):\n",
        "  #             arr[index].append(i)\n",
        "  #     # print(index, \":\", len(arr[index]))\n",
        "  #     # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "  #     # sample_length = int(sample_length)\n",
        "  #     sample_length = 5\n",
        "  #     for j in range(sample_length):\n",
        "  #         new_x_train.append(x_train[arr[index][j]])\n",
        "  #         new_y_train.append(y_train[arr[index][j]])\n",
        "  #         new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "  # new_x_train = np.asarray(new_x_train)\n",
        "  # new_y_train = np.asarray(new_y_train)\n",
        "  # new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "  # indices = np.arange(new_x_train.shape[0])\n",
        "  # np.random.shuffle(indices)\n",
        "\n",
        "  # new_x_train = new_x_train[indices]\n",
        "  # new_y_train = new_y_train[indices]\n",
        "  # new_train_mask = new_train_mask[indices]\n",
        "  # x_train = new_x_train\n",
        "  # y_train = new_y_train\n",
        "  # train_masks = new_train_mask\n",
        "  #----------------------------------------------\n",
        "\n",
        "  # Futher split train data into train(80%) and validation(20%) sets\n",
        "  train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    random_state=seed, test_size=0.2)\n",
        "\n",
        "  # Convert all inputs and labels into torch tensors, the required datatype \n",
        "  # for our model.\n",
        "  x_train = torch.tensor(x_train)\n",
        "  x_test = torch.tensor(x_test)\n",
        "  x_val = torch.tensor(x_val)\n",
        "\n",
        "  y_train = torch.tensor(y_train)\n",
        "  y_test = torch.tensor(y_test)\n",
        "  y_val = torch.tensor(y_val)\n",
        "\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "  val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here.\n",
        "  # For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "  # 16 or 32.\n",
        "\n",
        "  batch_size = 16\n",
        "\n",
        "  # Create the DataLoader for our training set.\n",
        "  train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = 4, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,3)\n",
        "\n",
        "  # model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  model.classifier = torch.nn.Linear(768,4)\n",
        "\n",
        "  # # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  #     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "  #                     # You can increase this for multi-class tasks.   \n",
        "  #     output_attentions = False, # Whether the model returns attentions weights.\n",
        "  #     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  # )\n",
        "  # path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "  # # model.load_state_dict(torch.load(path))\n",
        "\n",
        "  # model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "  model.cuda()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs =  2\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "  # This training code is based on the `run_glue.py` script here:\n",
        "  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "      tf.reset_default_graph()\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "          \n",
        "          # The call to `model` always returns a tuple, so we need to pull the \n",
        "          # loss value out of the tuple.\n",
        "          loss = outputs[0]\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over the training data.\n",
        "      avg_train_loss = total_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Store the loss value for plotting the learning curve.\n",
        "      loss_values.append(avg_train_loss)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  # Create the DataLoader for our validation set.\n",
        "  test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  y_pred = []\n",
        "  for i in predictions:\n",
        "    y_pred += i.tolist()\n",
        "\n",
        "  import pandas as pd\n",
        "\n",
        "  y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "  import sklearn.metrics as metrics\n",
        "\n",
        "  path = text_path\n",
        "\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclant test full shot \" + str(seed),file=text_file)\n",
        "    print(str(seed),file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test, y_pred),file=text_file)\n",
        "    print(metrics.classification_report(y_test, y_pred, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo4_p81NR7qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "seeds = [663,883,544,201,356,648,898,88,997,788]\n",
        "# seeds = [1,2]\n",
        "\n",
        "for seed in seeds:\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_'\n",
        "  text_path = '/content/drive/My Drive/KY, FYP/Code/results/output_'\n",
        "  path = path + str(seed) + \".pt\"\n",
        "  text_path = text_path + str(seed) + \".txt\"\n",
        "  aclarc(seed,path,text_path)\n",
        "  aclarcf(seed,path,text_path)\n",
        "  aclarcfull(seed,path,text_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsocZ58BA0dA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "seeds = [663,883,544,201,356,648,898,88,997,788]\n",
        "# seeds = [663]\n",
        "\n",
        "for seed in seeds:\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_'\n",
        "  text_path = '/content/drive/My Drive/KY, FYP/Code/results2/output_'\n",
        "  path = path + str(seed) + \".pt\"\n",
        "  text_path = text_path + str(seed) + \".txt\"\n",
        "  aclant(seed,path,text_path)\n",
        "  aclantF(seed,path,text_path)\n",
        "  aclantFull(seed,path,text_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzF-e9v2M0f-",
        "colab_type": "code",
        "outputId": "7cb76e48-bf5a-4f38-d8b0-b0b111fd8a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "seeds = [663,883,544,201,356]\n",
        "\n",
        "for seed in seeds:\n",
        "  path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_'\n",
        "  text_path = '/content/drive/My Drive/KY, FYP/Code/results2/output_'\n",
        "  path = path + str(seed) + \".pt\"\n",
        "  text_path = text_path + str(seed) + \".txt\"\n",
        "  aclarc(seed,path,text_path)\n",
        "  aclarcf(seed,path,text_path)\n",
        "  aclarc10(seed,path,text_path)\n",
        "  aclarc20(seed,path,text_path)\n",
        "  aclarc50(seed,path,text_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.88\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.72\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.62\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.62\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.47\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.41\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.47\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.38\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.25\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.29\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.34\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.23\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.25\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.19\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.85\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.72\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.64\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.57\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.47\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.43\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.35\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.33\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.22\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.26\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.20\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.17\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.16\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.16\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.14\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.83\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.71\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.16\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.60\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.47\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.16\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.40\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.26\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.19\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.12\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.28\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.93\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.28\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.94\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.28\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.90\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.28\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.87\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.28\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.85\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.28\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.81\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.14\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.70\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.31\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.55\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.38\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.38\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.40\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.20\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.43\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.51\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.93\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.48\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.82\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.48\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.72\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.48\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.63\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.51\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.49\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.49\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.51\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.49\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.84\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.70\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.64\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.59\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.48\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.43\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.36\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.34\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.30\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.28\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.22\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.23\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.18\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.79\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.75\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.08\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.68\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.08\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.63\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.52\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.45\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.42\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.35\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.31\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.29\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.18\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.21\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.16\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.17\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.80\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.12\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.69\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.12\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.63\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.03\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.53\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.06\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.44\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.39\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.29\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.21\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.15\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.10\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.03\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.01\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.96\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.95\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.83\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.30\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.71\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.59\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.37\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.42\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.36\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.25\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.40\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.12\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.49\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.93\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.46\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.49\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.55\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.53\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.54\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.80\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.70\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.65\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.58\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.55\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.56\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.50\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.44\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.37\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.39\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.31\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.36\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.34\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.27\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.29\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.83\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.69\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.42\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.62\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.55\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.55\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.45\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.40\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.39\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.37\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.08\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.30\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.30\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.26\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.25\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.80\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.16\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.73\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.65\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.62\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.09\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.56\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.09\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.48\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.09\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.42\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.06\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.37\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.12\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.29\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.12\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.25\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.12\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.12\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.20\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.12\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.16\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.16\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.15\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.13\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.81\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.20\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.70\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.57\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.29\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.40\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.40\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.25\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.48\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.48\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.91\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.53\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.55\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.55\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.84\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.72\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.64\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.55\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.50\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.45\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.41\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.34\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.29\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.33\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.26\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.28\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.25\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.80\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.71\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.42\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.61\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.56\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.56\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.51\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.44\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.08\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.43\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.08\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.41\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.08\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.34\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.31\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.30\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.27\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.29\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.89\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.16\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.82\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.80\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.77\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.28\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.75\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.72\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.31\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.66\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.38\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.65\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.57\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.16\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.53\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.16\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.45\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.41\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.34\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.33\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.29\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.83\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.18\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.69\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.21\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.62\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.35\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.47\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.39\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.34\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.42\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.23\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.45\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.11\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.43\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.01\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.45\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.88\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.43\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.81\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.45\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.73\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.47\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.62\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.59\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.59\n",
            "  Training epcoh took: 0:00:10\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.79\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.66\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.57\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.45\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.44\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.37\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.31\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.29\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.23\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.25\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.21\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.15\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.15\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.14\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.83\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.73\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.59\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.55\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.53\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.40\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.40\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.34\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.30\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.27\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.22\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.19\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.15\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.16\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.33\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.80\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.03\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.65\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.16\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.56\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.44\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.16\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.41\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.30\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.21\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.31\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.19\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.12\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.38\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.06\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.02\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.34\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.31\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.96\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.31\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.98\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.31\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Predicting labels for 366 test sentences...\n",
            "DONE.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Loading BERT tokenizer...\n",
            "Original:  Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
            "Token IDs: [101, 24501, 8238, 1006, 2786, 1007, 2988, 1037, 16902, 1997, 1054, 1027, 1012, 3938, 23833, 1012, 2184, 1996, 3463, 2024, 2025, 3495, 12435, 1010, 2138, 2002, 2069, 2109, 15156, 1011, 15156, 7689, 1010, 2616, 2612, 1997, 8474, 1010, 1037, 2172, 3760, 2951, 13462, 1010, 1998, 7594, 21641, 14402, 2612, 1997, 21641, 3141, 2791, 1012, 102]\n",
            "Max sentence length:  290\n",
            "\n",
            "Padding/truncating all sentences to 200 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6683cb7d9018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0maclarc10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0maclarc20\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0maclarc50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-7d1aa1a88c73>\u001b[0m in \u001b[0;36maclarc50\u001b[0;34m(seed, filepath, text_path)\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0msample_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m           \u001b[0mnew_x_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m           \u001b[0mnew_y_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m           \u001b[0mnew_train_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYW8beik_1so",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aclantf(0,'abc','a.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyK_y7qDWahW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "text_path = '/content/drive/My Drive/KY, FYP/Code/results/output_663.txt'\n",
        "\n",
        "aclarcf(0,path,text_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQr4sgBQcDgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aclarcfull(0,path,text_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NmUQXOScbR8",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title aclarc dataset (6 Classes) Initialization Transfer test\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "               'Extends': 5}\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['acl-arc']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "\n",
        "dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "\n",
        "texts = list(map(lambda d: d['text'], dataset))\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['intent']], dataset))\n",
        "\n",
        "seed = 2\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sample in texts:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sample,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "for i, sample in enumerate(input_ids):\n",
        "  if len(sample) > 511:\n",
        "    texts.pop(i)\n",
        "    input_ids.pop(i)\n",
        "    ys.pop(i)\n",
        "    print(\"removed Index: \" +  str(i))\n",
        "\n",
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 200\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train(80%) and test(20%) sets\n",
        "x_train, x_test , y_train, y_test = train_test_split(input_ids, ys, \n",
        "                                                random_state=seed, test_size=0.2)\n",
        "# Do the same for the masks.\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_masks, ys,\n",
        "                                             random_state=seed, test_size=0.2)\n",
        "\n",
        "y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "# Proportional Reduction\n",
        "# ------------------------------------\n",
        "\n",
        "new_x_train = []\n",
        "new_y_train = []\n",
        "new_train_mask = []\n",
        "arr = {}\n",
        "for index in range(len(funcs_index)):\n",
        "    arr[index] = []\n",
        "    for i, value in enumerate(y_train):\n",
        "        if (value == index):\n",
        "            arr[index].append(i)\n",
        "    # print(index, \":\", len(arr[index]))\n",
        "    # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "    # sample_length = int(sample_length)\n",
        "    sample_length = 5\n",
        "    for j in range(sample_length):\n",
        "        new_x_train.append(x_train[arr[index][j]])\n",
        "        new_y_train.append(y_train[arr[index][j]])\n",
        "        new_train_mask.append(train_masks[arr[index][j]])\n",
        "\n",
        "new_x_train = np.asarray(new_x_train)\n",
        "new_y_train = np.asarray(new_y_train)\n",
        "new_train_mask = np.asarray(new_train_mask)\n",
        "\n",
        "indices = np.arange(new_x_train.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "new_x_train = new_x_train[indices]\n",
        "new_y_train = new_y_train[indices]\n",
        "new_train_mask = new_train_mask[indices]\n",
        "x_train = new_x_train\n",
        "y_train = new_y_train\n",
        "train_masks = new_train_mask\n",
        "#----------------------------------------------\n",
        "\n",
        "# Futher split train data into train(80%) and validation(20%) sets\n",
        "train_masks, val_masks , _, _ = train_test_split(train_masks, x_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "x_train, x_val , y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                   random_state=seed, test_size=0.2)\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "x_train = torch.tensor(x_train)\n",
        "x_test = torch.tensor(x_test)\n",
        "x_val = torch.tensor(x_val)\n",
        "\n",
        "y_train = torch.tensor(y_train)\n",
        "y_test = torch.tensor(y_test)\n",
        "y_val = torch.tensor(y_val)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(x_train, train_masks, y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(x_val, val_masks, y_val)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "model.classifier = torch.nn.Linear(768,3)\n",
        "path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "model.load_state_dict(torch.load(path))\n",
        "\n",
        "model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "# # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(\n",
        "#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "#     num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "#                     # You can increase this for multi-class tasks.   \n",
        "#     output_attentions = False, # Whether the model returns attentions weights.\n",
        "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "# )\n",
        "# path = '/content/drive/My Drive/KY, FYP/Code/bert_models/bert_663.pt'\n",
        "\n",
        "# # model.load_state_dict(torch.load(path))\n",
        "\n",
        "# model.classifier = torch.nn.Linear(768,6)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs =  2\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 16\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "test_data = TensorDataset(x_test, test_masks, y_test)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(x_test)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('DONE.')\n",
        "\n",
        "y_test = y_test.tolist()\n",
        "\n",
        "y_pred = []\n",
        "for i in predictions:\n",
        "  y_pred += i.tolist()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "y_pred = list(map(lambda x: pd.Series(x).idxmax(), y_pred))\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(metrics.classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RACEB7HWJCQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La_ClDkSJHKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(new_x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}