{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FYP - CNN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWvybEnjh0sf",
        "colab_type": "code",
        "outputId": "fd351ead-f051-477b-db72-1531fbb6fcfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1H_Y6_-dhNr",
        "colab_type": "code",
        "outputId": "90650986-d76f-40b0-a140-2cc2e7f8e644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBjp-y6n4cxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8IbBDQwoGu8",
        "colab_type": "code",
        "outputId": "e71cf3fa-0c6d-4072-8420-b5d0a67a899f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "pip install json-lines"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting json-lines\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/0f/79c96c0d26b276c583484fe8209e5ebbb416a920309568650325f6e1de73/json_lines-0.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from json-lines) (1.12.0)\n",
            "Installing collected packages: json-lines\n",
            "Successfully installed json-lines-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8ekuuz7nMNA",
        "colab_type": "code",
        "outputId": "43745193-22c8-412c-d7dd-73e82eb2a68f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "pip install sentence-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/32/e3d405806ea525fd74c2c79164c3f7bc0b0b9811f27990484c6d6874c76f/sentence-transformers-0.2.5.1.tar.gz (52kB)\n",
            "\r\u001b[K     |██████▎                         | 10kB 26.2MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hCollecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\r\u001b[K     |▊                               | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 25.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 29.4MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 31.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.38.0)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 45.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (1.12.33)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.1.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.15.33)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers==2.3.0->sentence-transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5.1-cp36-none-any.whl size=67076 sha256=05dc2a7d9519a87c23c362c4d53a970bc6b4809f61d6dc1eb945cfe3bef7d1e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/ca/b4/7ca542b411730a8840f8e090df2ddacffa1c4dd9f209684c19\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=dde871bf72cb5f8fe1d8d258c328e3f1b891a143a9550da7d012e88df73e0271\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.38 sentence-transformers-0.2.5.1 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnxfWXxjL4LR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Config\n",
        "\"\"\"\n",
        "Configuration file for the project.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Base directory.\n",
        "\"\"\"\n",
        "PWD = '/content/drive/My Drive/KY, FYP/Code/'\n",
        "\n",
        "\"\"\"\n",
        "File directories.\n",
        "\"\"\"\n",
        "# Directory for the word embeddings\n",
        "GLOVE_DIR = PWD + '/glove.6B'\n",
        "\n",
        "# Directory for storing citation function data\n",
        "DATA_DIR = PWD + '/data/data'\n",
        "\n",
        "\"\"\"\n",
        "Data files: the citation and provenance dataset.\n",
        "MTL refers to the aligned dataset.\n",
        "\"\"\"\n",
        "DATA_FILES = {\n",
        "    'func': {\n",
        "        'golden_train': 'processed/golden_train.func.json',\n",
        "        'golden_test': 'processed/golden_test.func.json',\n",
        "    },\n",
        "    'scicite': {\n",
        "        'train': 'scicite/train.jsonl',\n",
        "        'test': 'scicite/test.jsonl',\n",
        "        'dev': 'scicite/dev.jsonl'\n",
        "    },\n",
        "    'acl-arc': {\n",
        "        'train': 'acl-arc/train.jsonl',\n",
        "        'test': 'acl-arc/test.jsonl',\n",
        "        'dev': 'acl-arc/dev.jsonl'\n",
        "    },\n",
        "    'prov': {\n",
        "        'golden_train': 'processed/golden_train.prov.json',\n",
        "        'golden_test': 'processed/golden_test.prov.json',\n",
        "    },\n",
        "    'mtl': {\n",
        "        'golden_train': 'processed/golden_train.mtl.json',\n",
        "        'golden_test': 'processed/golden_test.mtl.json'\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yy6scFMoNxK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Data\n",
        "\"\"\"\n",
        "Common data operations.\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "import json_lines\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def read_json_data(filename):\n",
        "    \"\"\"\n",
        "    Read the given JSON file.\n",
        "    \"\"\"\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    with open(path, 'rb') as fp:\n",
        "        content = json.load(fp)\n",
        "        return content\n",
        "\n",
        "def read_jsonl_data(filename):\n",
        "    \"\"\"\n",
        "    Read the given JSONL file.\n",
        "    \"\"\"\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    content = []\n",
        "    print (type(content))\n",
        "    with open(path, 'rb') as fp:\n",
        "        for item in json_lines.reader(fp):\n",
        "            content.append(item)\n",
        "        return content\n",
        "\n",
        "\"\"\"\n",
        "Custom cross validation.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def compress_y(ys):\n",
        "    \"\"\"\n",
        "    For each y in ys, if y is of the form [0 0 ... 1 ... 0], compress it to a\n",
        "    single integer.\n",
        "    \"\"\"\n",
        "    if len(ys) < 1:\n",
        "        return ys\n",
        "\n",
        "    if isinstance(ys[0], np.ndarray):\n",
        "        # A hack >.<\n",
        "        return map(lambda x: x.tolist().index(1), ys)\n",
        "    else:\n",
        "        return ys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF27POmr0J8c",
        "colab_type": "code",
        "outputId": "9714db46-5d73-4894-dc65-ebed74400a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "datafiles = DATA_FILES['scicite']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data(datafiles['train'])\n",
        "\n",
        "dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_train + dataset_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLiDsTpP0QoI",
        "colab_type": "code",
        "outputId": "b1bd45fe-b1dc-4eb3-cd35-252b958f6d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFZ9MQpfn9Jp",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACL-arc (6 classes)\n",
        "\n",
        "# import lib.logger, os, sys, random, math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "seed = 1020\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def embed_sentence(sentence):\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    message_embeddings = session.run(embed(sentence))\n",
        "    return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "    # Flatten feature vector\n",
        "    flat_dim_size = np.prod(model.output_shape[1:])\n",
        "    x = Reshape(target_shape=(flat_dim_size,),\n",
        "                name='features_flat')(model.output)\n",
        "\n",
        "    # Dot product between feature vector and reference vectors\n",
        "    x = Dense(units=output_size,\n",
        "              activation='linear',\n",
        "              use_bias=False)(x)\n",
        "\n",
        "    classifier = Model(inputs=[model.input], outputs=x)\n",
        "    return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "               'Extends': 5}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['acl-arc']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "# dataset_func = list(filter(lambda d: d['intent'] == 'CompareOrContrast', test + train))\n",
        "# dataset_func2 = list(filter(lambda d: d['intent'] == 'Background', test + train))\n",
        "# dataset_func3 = list(filter(lambda d: d['intent'] == 'Motivation', test + train))\n",
        "#\n",
        "# dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "\n",
        "\n",
        "# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# print(\"loaded Hub Module\")\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_prov_all = []\n",
        "y_test_prov_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_only_prov_all = []\n",
        "y_test_only_prov_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# -------------------------\n",
        "texts = map(lambda d: d['text'], dataset_func)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "ys = np.asarray(ys)\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "# Proportional Reduction\n",
        "# ------------------------------------\n",
        "\n",
        "# new_x_train = []\n",
        "# new_y_train = []\n",
        "# arr = {}\n",
        "# for index in range(len(funcs_index)):\n",
        "#     print(index)\n",
        "#     print(\"-----------------------\")\n",
        "#     arr[index] = []\n",
        "#     for i, value in enumerate(y_train):\n",
        "#         if (value == index):\n",
        "#             arr[index].append(i)\n",
        "#     print(len(arr[index]))\n",
        "#     sample_length = len(arr[index]) / 20  # 5% of data\n",
        "#     sample_length = int(sample_length)\n",
        "#     for j in range(sample_length):\n",
        "#         new_x_train.append(x_train[arr[index][j]])\n",
        "#         new_y_train.append(y_train[arr[index][j]])\n",
        "#\n",
        "# new_x_train = np.asarray(new_x_train)\n",
        "# new_y_train = np.asarray(new_y_train)\n",
        "# x_train = new_x_train\n",
        "# y_train = new_y_train\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "# Few Shot\n",
        "# ------------------------------------------\n",
        "\n",
        "# x_train_unique = [x_train[i] for i in indices]\n",
        "# x_train_unique = np.asarray(x_train_unique)\n",
        "# print(x_train_unique)\n",
        "# y_train_unique = [y_train[i] for i in indices]\n",
        "# y_train_unique = np.asarray(y_train_unique)\n",
        "# x_train = x_train_unique\n",
        "# y_train = y_train_unique\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "y_test = utils.to_categorical(np.asarray(y_test))\n",
        "y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "print (x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "indices = []\n",
        "indices_type = []\n",
        "\n",
        "# ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerA\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "# preds = Dense(768, name=\"output_layer\")(x)\n",
        "# output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy,\n",
        "            # optimizer='adam',\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# import datetime\n",
        "# from keras.callbacks import TensorBoard\n",
        "\n",
        "# log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "model.load_weights('model-scicite.h5', by_name=True)\n",
        "\n",
        "# model.fit(x_train, y_train,\n",
        "#         nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "# model.save_weights('model-acl.h5')\n",
        "\n",
        "# new_model = build_knn(model, encoded_classes.shape[1])\n",
        "# print(new_model.summary())\n",
        "# encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "# temp_weights = new_model.get_weights()\n",
        "# temp_weights[-1] = encoded_classes_norm\n",
        "# new_model.set_weights(temp_weights)\n",
        "\n",
        "y_pred_probs = model.predict(x_test)\n",
        "\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test = compress_y(y_test)\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "        # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "# print(average_list)\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvNMIDUcmvBi",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Transferred model with Training Acl anthology (4 classes)\n",
        "\n",
        "# New file authored 28 Jan 2018\n",
        "# For citation provenance\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "\"\"\"\n",
        "This file tests the model that is trained (on acl-arc dataset, 6 classes) that outputs a 512 dimensional vector based on USE\n",
        "Using the 4 Citation taxonomy dataset, (golden_test) as the target test set\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Perform the experiments on bootstrapped data and actual annotated data.\n",
        "\"\"\"\n",
        "# import lib.logger, os, sys, random, math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# import tensorflow_hub as hub\n",
        "\n",
        "# def embed_sentence(sentence):\n",
        "#   with tf.Session() as session:\n",
        "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "#     message_embeddings = session.run(embed(sentence))\n",
        "#     return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "  # Flatten feature vector\n",
        "  flat_dim_size = np.prod(model.output_shape[1:])\n",
        "  x = Reshape(target_shape=(flat_dim_size,),\n",
        "              name='features_flat')(model.output)\n",
        "\n",
        "  # Dot product between feature vector and reference vectors\n",
        "  x = Dense(units=output_size,\n",
        "            activation='linear',\n",
        "            use_bias=False)(x)\n",
        "\n",
        "  classifier = Model(inputs=[model.input], outputs=x)\n",
        "  return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "seed = 1020\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "# GLOVE_DIR = GLOVE_DIR\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['func']\n",
        "test = read_json_data(datafiles['golden_test'])\n",
        "# train = data.read_jsonl_data((datafiles['train']))\n",
        "train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_train + dataset_test\n",
        "# dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "# random.shuffle(dataset_func)\n",
        "\n",
        "\n",
        "# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# print(\"loaded Hub Module\")\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts_train = list(map(lambda d: d['context'][0], dataset_train))\n",
        "\n",
        "texts_test = list(map(lambda d: d['context'][0], dataset_test))\n",
        "\n",
        "texts = texts_train + texts_test\n",
        "\n",
        "y_train = list(map(lambda d: funcs_index[d['label']], dataset_train))\n",
        "y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n",
        "\n",
        "ys = y_train + y_test\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# -------------------------\n",
        "texts = map(lambda d: d['context'][0], dataset)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "ys = np.asarray(ys)\n",
        "xs = np.asarray(xs)\n",
        "\n",
        "print(xs.shape, ys.shape)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_train = np.asarray(y_train)\n",
        "x_train = np.asarray(x_train)\n",
        "\n",
        "\n",
        "# Proportional Reduction\n",
        "# ------------------------------------\n",
        "\n",
        "# new_x_train = []\n",
        "# new_y_train = []\n",
        "# arr = {}\n",
        "# for index in range(len(funcs_index)):\n",
        "#     print(index)\n",
        "#     print(\"-----------------------\")\n",
        "#     arr[index] = []\n",
        "#     for i, value in enumerate(y_train):\n",
        "#         if (value == index):\n",
        "#             arr[index].append(i)\n",
        "#     print(len(arr[index]))\n",
        "#     sample_length = len(arr[index]) / 20  # 5% of data\n",
        "#     sample_length = int(sample_length)\n",
        "#     for j in range(sample_length):\n",
        "#         new_x_train.append(x_train[arr[index][j]])\n",
        "#         new_y_train.append(y_train[arr[index][j]])\n",
        "#\n",
        "# new_x_train = np.asarray(new_x_train)\n",
        "# new_y_train = np.asarray(new_y_train)\n",
        "# x_train = new_x_train\n",
        "# y_train = new_y_train\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "# One Shot\n",
        "# ------------------------------------------\n",
        "\n",
        "# x_train_unique = [x_train[i] for i in indices]\n",
        "# x_train_unique = np.asarray(x_train_unique)\n",
        "# y_train_unique = [y_train[i] for i in indices]\n",
        "# y_train_unique = np.asarray(y_train_unique)\n",
        "# x_train = x_train_unique\n",
        "# y_train = y_train_unique\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_test)\n",
        "x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_test = np.asarray(y_test)\n",
        "x_test = np.asarray(x_test)\n",
        "\n",
        "y_test = utils.to_categorical(np.asarray(y_test))\n",
        "y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerC\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "# preds = Dense(768, name=\"output_layer\")(x)\n",
        "# output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy,\n",
        "            # optimizer='adam',\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "model.load_weights('model-scicite.h5', by_name=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2)\n",
        "\n",
        "y_pred_probs = model.predict(x_test)\n",
        "# y_test = ys\n",
        "\n",
        "# total_diff = 0\n",
        "# sample_count = 0\n",
        "# for i, sample in enumerate(y_pred_probs):\n",
        "#     total_diff += 1-cosine_similarity(y_pred_probs[i],ys[i])\n",
        "#     sample_count += 1\n",
        "# average = total_diff/sample_count\n",
        "# print(\"The average cosine difference between the predictions and test are :\")\n",
        "# print(average)\n",
        "# average_list[batch_num] = average\n",
        "# batch_num += 1\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "\n",
        "y_test = compress_y(y_test)\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "    # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "# print(average_list)\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "# print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCKKTxPT-zWL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Transferred model with Training SciCite (3 Classes)\n",
        "\n",
        "\"\"\"\n",
        "This file tests the model that is trained (on acl-arc dataset, 6 classes) that outputs a 512 dimensional vector based on USE\n",
        "Using the 4 Citation taxonomy dataset, (golden_test) as the target test set\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Perform the experiments on bootstrapped data and actual annotated data.\n",
        "\"\"\"\n",
        "# import lib.logger, os, sys, random, math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# import tensorflow_hub as hub\n",
        "\n",
        "# def embed_sentence(sentence):\n",
        "#   with tf.Session() as session:\n",
        "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "#     message_embeddings = session.run(embed(sentence))\n",
        "#     return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "  # Flatten feature vector\n",
        "  flat_dim_size = np.prod(model.output_shape[1:])\n",
        "  x = Reshape(target_shape=(flat_dim_size,),\n",
        "              name='features_flat')(model.output)\n",
        "\n",
        "  # Dot product between feature vector and reference vectors\n",
        "  x = Dense(units=output_size,\n",
        "            activation='linear',\n",
        "            use_bias=False)(x)\n",
        "\n",
        "  classifier = Model(inputs=[model.input], outputs=x)\n",
        "  return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "seed = 1020\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "# GLOVE_DIR = GLOVE_DIR\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'background': 0, 'method': 1, 'result': 2}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['scicite']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data(datafiles['train'])\n",
        "\n",
        "dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "dataset = dataset_train + dataset_test\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts_train = list(map(lambda d: d['string'], dataset_train))\n",
        "\n",
        "texts_test = list(map(lambda d: d['string'], dataset_test))\n",
        "\n",
        "texts = texts_train + texts_test\n",
        "\n",
        "y_train = list(map(lambda d: funcs_index[d['label']], dataset_train))\n",
        "y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n",
        "\n",
        "ys = y_train + y_test\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=42)\n",
        "\n",
        "y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_train = np.asarray(y_train)\n",
        "x_train = np.asarray(x_train)\n",
        "\n",
        "\n",
        "# Proportional Reduction\n",
        "# ------------------------------------\n",
        "\n",
        "# new_x_train = []\n",
        "# new_y_train = []\n",
        "# arr = {}\n",
        "# for index in range(len(funcs_index)):\n",
        "#     print(index)\n",
        "#     print(\"-----------------------\")\n",
        "#     arr[index] = []\n",
        "#     for i, value in enumerate(y_train):\n",
        "#         if (value == index):\n",
        "#             arr[index].append(i)\n",
        "#     print(len(arr[index]))\n",
        "#     sample_length = len(arr[index]) / 20  # 5% of data\n",
        "#     sample_length = int(sample_length)\n",
        "#     for j in range(sample_length):\n",
        "#         new_x_train.append(x_train[arr[index][j]])\n",
        "#         new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "\n",
        "# for i,element in enumerate(new_x_train):\n",
        "#   new_y_train[i] = words[new_y_train[i]]\n",
        "\n",
        "# new_x_train = np.asarray(new_x_train)\n",
        "# new_y_train = np.asarray(new_y_train)\n",
        "# x_train = new_x_train\n",
        "# y_train = new_y_train\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "# One Shot\n",
        "# ------------------------------------------\n",
        "\n",
        "# x_train_unique = [x_train[i] for i in indices]\n",
        "# x_train_unique = np.asarray(x_train_unique)\n",
        "# y_train_unique = [y_train[i] for i in indices]\n",
        "# y_train_unique = np.asarray(y_train_unique)\n",
        "# x_train = x_train_unique\n",
        "# y_train = y_train_unique\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(x_test)\n",
        "x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y_test = np.asarray(y_test)\n",
        "x_test = np.asarray(x_test)\n",
        "\n",
        "y_test = utils.to_categorical(np.asarray(y_test))\n",
        "y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerB\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy,\n",
        "            # optimizer='adam',\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "# model.load_weights('model-acl.h5', by_name=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2)\n",
        "\n",
        "model.save_weights('model-scicite.h5')\n",
        "\n",
        "y_pred_probs = model.predict(x_test)\n",
        "# y_test = ys\n",
        "\n",
        "# total_diff = 0\n",
        "# sample_count = 0\n",
        "# for i, sample in enumerate(y_pred_probs):\n",
        "#     total_diff += 1-cosine_similarity(y_pred_probs[i],ys[i])\n",
        "#     sample_count += 1\n",
        "# average = total_diff/sample_count\n",
        "# print(\"The average cosine difference between the predictions and test are :\")\n",
        "# print(average)\n",
        "# average_list[batch_num] = average\n",
        "# batch_num += 1\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test = compress_y(y_test)\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "    # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "# print(average_list)\n",
        "print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all))\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "# print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97xNNG-5E0h5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title  SciCite Function Training: sciCite(seed,filepath,text_path)\n",
        "\n",
        "def sciCite(seed,filepath,text_path):\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "\n",
        "  # import tensorflow_hub as hub\n",
        "\n",
        "  # def embed_sentence(sentence):\n",
        "  #   with tf.Session() as session:\n",
        "  #     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  #     message_embeddings = session.run(embed(sentence))\n",
        "  #     return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "    # Flatten feature vector\n",
        "    flat_dim_size = np.prod(model.output_shape[1:])\n",
        "    x = Reshape(target_shape=(flat_dim_size,),\n",
        "                name='features_flat')(model.output)\n",
        "\n",
        "    # Dot product between feature vector and reference vectors\n",
        "    x = Dense(units=output_size,\n",
        "              activation='linear',\n",
        "              use_bias=False)(x)\n",
        "\n",
        "    classifier = Model(inputs=[model.input], outputs=x)\n",
        "    return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  # GLOVE_DIR = GLOVE_DIR\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'background': 0, 'method': 1, 'result': 2}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['scicite']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data(datafiles['train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts_train = list(map(lambda d: d['string'], dataset_train))\n",
        "\n",
        "  texts_test = list(map(lambda d: d['string'], dataset_test))\n",
        "\n",
        "  texts = texts_train + texts_test\n",
        "\n",
        "  y_train = list(map(lambda d: funcs_index[d['label']], dataset_train))\n",
        "  y_test = list(map(lambda d: funcs_index[d['label']], dataset_test))\n",
        "\n",
        "  ys = y_train + y_test\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(texts, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(x_train)\n",
        "  x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  y_train = np.asarray(y_train)\n",
        "  x_train = np.asarray(x_train)\n",
        "\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  # new_x_train = []\n",
        "  # new_y_train = []\n",
        "  # arr = {}\n",
        "  # for index in range(len(funcs_index)):\n",
        "  #     print(index)\n",
        "  #     print(\"-----------------------\")\n",
        "  #     arr[index] = []\n",
        "  #     for i, value in enumerate(y_train):\n",
        "  #         if (value == index):\n",
        "  #             arr[index].append(i)\n",
        "  #     print(len(arr[index]))\n",
        "  #     sample_length = len(arr[index]) / 20  # 5% of data\n",
        "  #     sample_length = int(sample_length)\n",
        "  #     for j in range(sample_length):\n",
        "  #         new_x_train.append(x_train[arr[index][j]])\n",
        "  #         new_y_train.append(y_train[arr[index][j]])\n",
        "\n",
        "\n",
        "  # for i,element in enumerate(new_x_train):\n",
        "  #   new_y_train[i] = words[new_y_train[i]]\n",
        "\n",
        "  # new_x_train = np.asarray(new_x_train)\n",
        "  # new_y_train = np.asarray(new_y_train)\n",
        "  # x_train = new_x_train\n",
        "  # y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # One Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(x_test)\n",
        "  x_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  y_test = np.asarray(y_test)\n",
        "  x_test = np.asarray(x_test)\n",
        "\n",
        "  y_test = utils.to_categorical(np.asarray(y_test))\n",
        "  y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerB\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "\n",
        "  model.compile(loss=categorical_crossentropy,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  # model.load_weights('model-acl.h5', by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE, validation_split=0.2)\n",
        "\n",
        "  model.save_weights(filepath)\n",
        "\n",
        "  y_pred_probs = model.predict(x_test)\n",
        "  # y_test = ys\n",
        "\n",
        "  # total_diff = 0\n",
        "  # sample_count = 0\n",
        "  # for i, sample in enumerate(y_pred_probs):\n",
        "  #     total_diff += 1-cosine_similarity(y_pred_probs[i],ys[i])\n",
        "  #     sample_count += 1\n",
        "  # average = total_diff/sample_count\n",
        "  # print(\"The average cosine difference between the predictions and test are :\")\n",
        "  # print(average)\n",
        "  # average_list[batch_num] = average\n",
        "  # batch_num += 1\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test = compress_y(y_test)\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "      # ---------- End of citation function ----------\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Scicite Training Results Seed \",file=text_file)\n",
        "    print(seed,file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "    print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjnl47S-Fn0l",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACL-arc Testing: aclarc(seed,filepath,text_path) -- 0 shot\n",
        "\n",
        "def aclarc(seed,filepath,text_path):\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # dataset_func = list(filter(lambda d: d['intent'] == 'CompareOrContrast', test + train))\n",
        "  # dataset_func2 = list(filter(lambda d: d['intent'] == 'Background', test + train))\n",
        "  # dataset_func3 = list(filter(lambda d: d['intent'] == 'Motivation', test + train))\n",
        "  #\n",
        "  # dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "\n",
        "\n",
        "  # embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "  # print(\"loaded Hub Module\")\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  kf = KFold(n_splits=5)\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_prov_all = []\n",
        "  y_test_prov_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_only_prov_all = []\n",
        "  y_test_only_prov_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  # new_x_train = []\n",
        "  # new_y_train = []\n",
        "  # arr = {}\n",
        "  # for index in range(len(funcs_index)):\n",
        "  #     print(index)\n",
        "  #     print(\"-----------------------\")\n",
        "  #     arr[index] = []\n",
        "  #     for i, value in enumerate(y_train):\n",
        "  #         if (value == index):\n",
        "  #             arr[index].append(i)\n",
        "  #     print(len(arr[index]))\n",
        "  #     sample_length = len(arr[index]) / 20  # 5% of data\n",
        "  #     sample_length = int(sample_length)\n",
        "  #     for j in range(sample_length):\n",
        "  #         new_x_train.append(x_train[arr[index][j]])\n",
        "  #         new_y_train.append(y_train[arr[index][j]])\n",
        "  #\n",
        "  # new_x_train = np.asarray(new_x_train)\n",
        "  # new_y_train = np.asarray(new_y_train)\n",
        "  # x_train = new_x_train\n",
        "  # y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # Few Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # print(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_test = utils.to_categorical(np.asarray(y_test))\n",
        "  y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "  print (x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  # preds = Dense(768, name=\"output_layer\")(x)\n",
        "  # output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "\n",
        "  model.compile(loss=categorical_crossentropy,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  # model.fit(x_train, y_train,\n",
        "  #         nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  # encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  # temp_weights = new_model.get_weights()\n",
        "  # temp_weights[-1] = encoded_classes_norm\n",
        "  # new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test = compress_y(y_test)\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"Aclarc testing results 0 shot \",file=text_file)\n",
        "    print(seed,file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "    print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBjehDDNFn2g",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACL-arc Testing: aclarcF(seed,filepath,text_path) -- 5 shot\n",
        "\n",
        "def aclarcF(seed,filepath,text_path):\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # dataset_func = list(filter(lambda d: d['intent'] == 'CompareOrContrast', test + train))\n",
        "  # dataset_func2 = list(filter(lambda d: d['intent'] == 'Background', test + train))\n",
        "  # dataset_func3 = list(filter(lambda d: d['intent'] == 'Motivation', test + train))\n",
        "  #\n",
        "  # dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "\n",
        "\n",
        "  # embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "  # print(\"loaded Hub Module\")\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  kf = KFold(n_splits=5)\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_prov_all = []\n",
        "  y_test_prov_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_only_prov_all = []\n",
        "  y_test_only_prov_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      print(index)\n",
        "      print(\"-----------------------\")\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      print(len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "  \n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # Few Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # print(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_test = utils.to_categorical(np.asarray(y_test))\n",
        "  y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  # preds = Dense(768, name=\"output_layer\")(x)\n",
        "  # output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "\n",
        "  model.compile(loss=categorical_crossentropy,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  # encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  # temp_weights = new_model.get_weights()\n",
        "  # temp_weights[-1] = encoded_classes_norm\n",
        "  # new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test = compress_y(y_test)\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"aclarc testing result (5 Shot) \",file=text_file)\n",
        "    print(seed,file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "    print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MofvmGVHFn4P",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACL-arc Testing: aclarcFull(seed,filepath,text_path) -- Full\n",
        "\n",
        "def aclarcFull(seed,filepath,text_path):\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # dataset_func = list(filter(lambda d: d['intent'] == 'CompareOrContrast', test + train))\n",
        "  # dataset_func2 = list(filter(lambda d: d['intent'] == 'Background', test + train))\n",
        "  # dataset_func3 = list(filter(lambda d: d['intent'] == 'Motivation', test + train))\n",
        "  #\n",
        "  # dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "\n",
        "\n",
        "  # embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "  # print(\"loaded Hub Module\")\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  kf = KFold(n_splits=5)\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_prov_all = []\n",
        "  y_test_prov_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_only_prov_all = []\n",
        "  y_test_only_prov_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  # new_x_train = []\n",
        "  # new_y_train = []\n",
        "  # arr = {}\n",
        "  # for index in range(len(funcs_index)):\n",
        "  #     print(index)\n",
        "  #     print(\"-----------------------\")\n",
        "  #     arr[index] = []\n",
        "  #     for i, value in enumerate(y_train):\n",
        "  #         if (value == index):\n",
        "  #             arr[index].append(i)\n",
        "  #     print(len(arr[index]))\n",
        "  #     # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "  #     # sample_length = int(sample_length)\n",
        "  #     sample_length = 5\n",
        "  #     for j in range(sample_length):\n",
        "  #         new_x_train.append(x_train[arr[index][j]])\n",
        "  #         new_y_train.append(y_train[arr[index][j]])\n",
        "  \n",
        "  # new_x_train = np.asarray(new_x_train)\n",
        "  # new_y_train = np.asarray(new_y_train)\n",
        "  # x_train = new_x_train\n",
        "  # y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # Few Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # print(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_test = utils.to_categorical(np.asarray(y_test))\n",
        "  y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  # preds = Dense(768, name=\"output_layer\")(x)\n",
        "  # output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "\n",
        "  model.compile(loss=categorical_crossentropy,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  # encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  # temp_weights = new_model.get_weights()\n",
        "  # temp_weights[-1] = encoded_classes_norm\n",
        "  # new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test = compress_y(y_test)\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"aclarc testing result (Full data) \",file=text_file)\n",
        "    print(seed,file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "    print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qqGyYIiIrFz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACL-arc Testing: aclarcF(seed,filepath,text_path) -- 10 shot\n",
        "\n",
        "def aclarc10(seed,filepath,text_path):\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # dataset_func = list(filter(lambda d: d['intent'] == 'CompareOrContrast', test + train))\n",
        "  # dataset_func2 = list(filter(lambda d: d['intent'] == 'Background', test + train))\n",
        "  # dataset_func3 = list(filter(lambda d: d['intent'] == 'Motivation', test + train))\n",
        "  #\n",
        "  # dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "\n",
        "\n",
        "  # embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "  # print(\"loaded Hub Module\")\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  kf = KFold(n_splits=5)\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_prov_all = []\n",
        "  y_test_prov_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_only_prov_all = []\n",
        "  y_test_only_prov_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      print(index)\n",
        "      print(\"-----------------------\")\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      print(len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 10\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "  \n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # Few Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # print(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_test = utils.to_categorical(np.asarray(y_test))\n",
        "  y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  # preds = Dense(768, name=\"output_layer\")(x)\n",
        "  # output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "\n",
        "  model.compile(loss=categorical_crossentropy,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  # encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  # temp_weights = new_model.get_weights()\n",
        "  # temp_weights[-1] = encoded_classes_norm\n",
        "  # new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test = compress_y(y_test)\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"aclarc testing result (10 Shot) \",file=text_file)\n",
        "    print(seed,file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "    print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rSXMSVkIz_l",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACL-arc Testing: aclarcF(seed,filepath,text_path) -- 20 shot\n",
        "\n",
        "def aclarc20(seed,filepath,text_path):\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # dataset_func = list(filter(lambda d: d['intent'] == 'CompareOrContrast', test + train))\n",
        "  # dataset_func2 = list(filter(lambda d: d['intent'] == 'Background', test + train))\n",
        "  # dataset_func3 = list(filter(lambda d: d['intent'] == 'Motivation', test + train))\n",
        "  #\n",
        "  # dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "\n",
        "\n",
        "  # embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "  # print(\"loaded Hub Module\")\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  kf = KFold(n_splits=5)\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_prov_all = []\n",
        "  y_test_prov_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_only_prov_all = []\n",
        "  y_test_only_prov_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      print(index)\n",
        "      print(\"-----------------------\")\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      print(len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 20\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "  \n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # Few Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # print(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_test = utils.to_categorical(np.asarray(y_test))\n",
        "  y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  # preds = Dense(768, name=\"output_layer\")(x)\n",
        "  # output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "\n",
        "  model.compile(loss=categorical_crossentropy,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  # encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  # temp_weights = new_model.get_weights()\n",
        "  # temp_weights[-1] = encoded_classes_norm\n",
        "  # new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test = compress_y(y_test)\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"aclarc testing result (20 Shot) \",file=text_file)\n",
        "    print(seed,file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "    print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4-vVWvYI75n",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACL-arc Testing: aclarcF(seed,filepath,text_path) -- 50 shot\n",
        "\n",
        "def aclarc50(seed,filepath,text_path):\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "                'Extends': 5}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['acl-arc']\n",
        "  test = read_jsonl_data(datafiles['test'])\n",
        "  train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "  dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "  dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "  # dataset_func = list(filter(lambda d: d['intent'] == 'CompareOrContrast', test + train))\n",
        "  # dataset_func2 = list(filter(lambda d: d['intent'] == 'Background', test + train))\n",
        "  # dataset_func3 = list(filter(lambda d: d['intent'] == 'Motivation', test + train))\n",
        "  #\n",
        "  # dataset_func = dataset_func + dataset_func2 + dataset_func3\n",
        "\n",
        "\n",
        "\n",
        "  # embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "  # print(\"loaded Hub Module\")\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['text'], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  kf = KFold(n_splits=5)\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_prov_all = []\n",
        "  y_test_prov_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_only_prov_all = []\n",
        "  y_test_only_prov_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['text'], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      print(index)\n",
        "      print(\"-----------------------\")\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      print(len(arr[index]))\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 50\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "  \n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # Few Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # print(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_test = utils.to_categorical(np.asarray(y_test))\n",
        "  y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  # preds = Dense(768, name=\"output_layer\")(x)\n",
        "  # output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "\n",
        "  model.compile(loss=categorical_crossentropy,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  # encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  # temp_weights = new_model.get_weights()\n",
        "  # temp_weights[-1] = encoded_classes_norm\n",
        "  # new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test = compress_y(y_test)\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"aclarc testing result (50 Shot) \",file=text_file)\n",
        "    print(seed,file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "    print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juY-63j2IpUU",
        "colab_type": "text"
      },
      "source": [
        "========================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0MS9Epk50PN",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACLant Testing: aclarcF(seed,filepath,text_path) -- 0 shot\n",
        "\n",
        "def aclant(seed,filepath,text_path):\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['func']\n",
        "  test = read_json_data(datafiles['golden_test'])\n",
        "  # train = data.read_jsonl_data((datafiles['train']))\n",
        "  train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n",
        "  dataset_func = dataset\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['context'][0], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['label']], dataset_func))\n",
        "\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  kf = KFold(n_splits=5)\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_prov_all = []\n",
        "  y_test_prov_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_only_prov_all = []\n",
        "  y_test_only_prov_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['context'][0], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "  \n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # Few Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # print(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_test = utils.to_categorical(np.asarray(y_test))\n",
        "  y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 15 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  # preds = Dense(768, name=\"output_layer\")(x)\n",
        "  # output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "\n",
        "  model.compile(loss=categorical_crossentropy,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  # model.fit(x_train, y_train,\n",
        "  #         nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  # encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  # temp_weights = new_model.get_weights()\n",
        "  # temp_weights[-1] = encoded_classes_norm\n",
        "  # new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test = compress_y(y_test)\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"aclant testing result (0 Shot) \",file=text_file)\n",
        "    print(seed,file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "    print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jx7NyT63ipo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACLant Testing: aclarcF(seed,filepath,text_path) -- 5 shot\n",
        "\n",
        "def aclantF(seed,filepath,text_path):\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['func']\n",
        "  test = read_json_data(datafiles['golden_test'])\n",
        "  # train = data.read_jsonl_data((datafiles['train']))\n",
        "  train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n",
        "  dataset_func = dataset\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['context'][0], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['label']], dataset_func))\n",
        "\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  kf = KFold(n_splits=5)\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_prov_all = []\n",
        "  y_test_prov_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_only_prov_all = []\n",
        "  y_test_only_prov_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['context'][0], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  new_x_train = []\n",
        "  new_y_train = []\n",
        "  arr = {}\n",
        "  for index in range(len(funcs_index)):\n",
        "      arr[index] = []\n",
        "      for i, value in enumerate(y_train):\n",
        "          if (value == index):\n",
        "              arr[index].append(i)\n",
        "      # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "      # sample_length = int(sample_length)\n",
        "      sample_length = 5\n",
        "      for j in range(sample_length):\n",
        "          new_x_train.append(x_train[arr[index][j]])\n",
        "          new_y_train.append(y_train[arr[index][j]])\n",
        "  \n",
        "  new_x_train = np.asarray(new_x_train)\n",
        "  new_y_train = np.asarray(new_y_train)\n",
        "  x_train = new_x_train\n",
        "  y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # Few Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # print(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_test = utils.to_categorical(np.asarray(y_test))\n",
        "  y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 20 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  # preds = Dense(768, name=\"output_layer\")(x)\n",
        "  # output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "\n",
        "  model.compile(loss=categorical_crossentropy,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  # encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  # temp_weights = new_model.get_weights()\n",
        "  # temp_weights[-1] = encoded_classes_norm\n",
        "  # new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test = compress_y(y_test)\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"aclant testing result (5 Shot) \",file=text_file)\n",
        "    print(seed,file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "    print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AErsy-ZO5jbO",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ACLant Testing: aclarcF(seed,filepath,text_path) -- Full shot\n",
        "\n",
        "def aclantFull(seed,filepath,text_path):\n",
        "  # import lib.logger, os, sys, random, math\n",
        "  import numpy as np\n",
        "  import os\n",
        "\n",
        "  from functools import reduce\n",
        "\n",
        "  # import config.config as config\n",
        "  # import data.data as data\n",
        "  # import data.data_func as data_func\n",
        "  import sklearn.metrics as metrics\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from sklearn.model_selection import KFold, train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  # from tensorflow.python import debug as tf_debug\n",
        "\n",
        "  from sklearn.utils import class_weight\n",
        "\n",
        "  # import keras.backend as K\n",
        "  from tensorflow.keras import utils\n",
        "  from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "      GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "  from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "  from tensorflow.keras.models import Model, Sequential, load_model\n",
        "  from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  # import matplotlib.pyplot as plt\n",
        "  # from sklearn.decomposition import PCA\n",
        "\n",
        "  import random\n",
        "  \"\"\"\n",
        "  Set random seed and fix bug on Dropout usage.\n",
        "  \"\"\"\n",
        "  import tensorflow as tf\n",
        "\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  # tf.python.control_flow_ops = tf\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  def embed_sentence(sentence):\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(sentence))\n",
        "      return message_embeddings\n",
        "\n",
        "  def ilen(iterable):\n",
        "      return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "  def build_knn(model, output_size):\n",
        "      # Flatten feature vector\n",
        "      flat_dim_size = np.prod(model.output_shape[1:])\n",
        "      x = Reshape(target_shape=(flat_dim_size,),\n",
        "                  name='features_flat')(model.output)\n",
        "\n",
        "      # Dot product between feature vector and reference vectors\n",
        "      x = Dense(units=output_size,\n",
        "                activation='linear',\n",
        "                use_bias=False)(x)\n",
        "\n",
        "      classifier = Model(inputs=[model.input], outputs=x)\n",
        "      return classifier\n",
        "\n",
        "  def normalize_encodings(encodings):\n",
        "      ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "      return encodings / ref_norms\n",
        "\n",
        "\n",
        "\n",
        "  MAX_NB_WORDS = 20000\n",
        "  MAX_SEQUENCE_LENGTH = 50\n",
        "  EMBEDDING_DIM = 100\n",
        "\n",
        "  \"\"\"\n",
        "  Data reading and saving from disk (so that data processing is done only once).\n",
        "  \"\"\"\n",
        "  directory = DATA_DIR\n",
        "  funcs_index = {'Neut': 3, 'Pos': 2, 'CoCo': 1, 'Weak': 0}\n",
        "\n",
        "\n",
        "  # Function dataset start\n",
        "  datafiles = DATA_FILES['func']\n",
        "  test = read_json_data(datafiles['golden_test'])\n",
        "  # train = data.read_jsonl_data((datafiles['train']))\n",
        "  train = read_json_data(datafiles['golden_train'])\n",
        "\n",
        "  dataset_train = list(filter(lambda x: x['label'] != 'Error',train))\n",
        "  dataset_test = list(filter(lambda x: x['label'] != 'Error', test))\n",
        "\n",
        "  dataset = dataset_train + dataset_test\n",
        "  dataset_func = dataset\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  # Function dataset end\n",
        "  #############################################################################3\n",
        "\n",
        "  texts = list(map(lambda d: d['context'][0], dataset_func))\n",
        "\n",
        "  ys = list(map(lambda d: funcs_index[d['label']], dataset_func))\n",
        "\n",
        "\n",
        "  print('Found %s texts.' % len(texts))\n",
        "\n",
        "  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  kf = KFold(n_splits=5)\n",
        "\n",
        "  y_pred_func_all = []\n",
        "  y_test_func_all = []\n",
        "  y_pred_prov_all = []\n",
        "  y_test_prov_all = []\n",
        "  y_pred_only_func_all = []\n",
        "  y_test_only_func_all = []\n",
        "  y_pred_only_prov_all = []\n",
        "  y_test_only_prov_all = []\n",
        "  y_pred_func = []\n",
        "  y_test_func = []\n",
        "\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  # for word, i in word_index.items():\n",
        "  #     embedding_vector = embeddings_index.get(word)\n",
        "  #     if embedding_vector is not None:\n",
        "  #         # words not found in embedding index will be all-zeros.\n",
        "  #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # -------------------------\n",
        "  texts = map(lambda d: d['context'][0], dataset_func)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  ys = np.asarray(ys)\n",
        "\n",
        "  batch_num = 0\n",
        "  average_list = {}\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=seed)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  y_train_unique, indices = np.unique(y_train, return_index=True)\n",
        "\n",
        "  # Proportional Reduction\n",
        "  # ------------------------------------\n",
        "\n",
        "  # new_x_train = []\n",
        "  # new_y_train = []\n",
        "  # arr = {}\n",
        "  # for index in range(len(funcs_index)):\n",
        "  #     arr[index] = []\n",
        "  #     for i, value in enumerate(y_train):\n",
        "  #         if (value == index):\n",
        "  #             arr[index].append(i)\n",
        "  #     # sample_length = len(arr[index]) / 20  # 5% of data\n",
        "  #     # sample_length = int(sample_length)\n",
        "  #     sample_length = 5\n",
        "  #     for j in range(sample_length):\n",
        "  #         new_x_train.append(x_train[arr[index][j]])\n",
        "  #         new_y_train.append(y_train[arr[index][j]])\n",
        "  \n",
        "  # new_x_train = np.asarray(new_x_train)\n",
        "  # new_y_train = np.asarray(new_y_train)\n",
        "  # x_train = new_x_train\n",
        "  # y_train = new_y_train\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # Few Shot\n",
        "  # ------------------------------------------\n",
        "\n",
        "  # x_train_unique = [x_train[i] for i in indices]\n",
        "  # x_train_unique = np.asarray(x_train_unique)\n",
        "  # print(x_train_unique)\n",
        "  # y_train_unique = [y_train[i] for i in indices]\n",
        "  # y_train_unique = np.asarray(y_train_unique)\n",
        "  # x_train = x_train_unique\n",
        "  # y_train = y_train_unique\n",
        "\n",
        "  # ------------------------------------------\n",
        "\n",
        "  y_test = utils.to_categorical(np.asarray(y_test))\n",
        "  y_train = utils.to_categorical(np.asarray(y_train))\n",
        "\n",
        "  NB_FILTER = 128\n",
        "  BATCH_SIZE = 32\n",
        "  count = 0\n",
        "  EPOCH = 20 # 20\n",
        "  indices = []\n",
        "  indices_type = []\n",
        "\n",
        "  # ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              name=\"embedding_layerA\")\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                          name=\"sequence_input\")\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Convolution1D(filters=NB_FILTER,\n",
        "                  kernel_size=5,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "  x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "  x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "  # x = Dropout(0.3)(x)\n",
        "  preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "  # preds = Dense(768, name=\"output_layer\")(x)\n",
        "  # output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "\n",
        "  model.compile(loss=categorical_crossentropy,\n",
        "              # optimizer='adam',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # import datetime\n",
        "  # from keras.callbacks import TensorBoard\n",
        "\n",
        "  # log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  # tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "  model.load_weights(filepath, by_name=True)\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "          nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "  # model.save_weights('model-acl.h5')\n",
        "\n",
        "  # new_model = build_knn(model, encoded_classes.shape[1])\n",
        "  # print(new_model.summary())\n",
        "  # encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "  # temp_weights = new_model.get_weights()\n",
        "  # temp_weights[-1] = encoded_classes_norm\n",
        "  # new_model.set_weights(temp_weights)\n",
        "\n",
        "  y_pred_probs = model.predict(x_test)\n",
        "\n",
        "  y_pred_func = []\n",
        "\n",
        "  y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "  y_test = compress_y(y_test)\n",
        "\n",
        "  y_pred_only_func_all += y_pred_func\n",
        "  y_test_only_func_all += y_test\n",
        "\n",
        "  path = text_path\n",
        "  with open(path, \"a\") as text_file:\n",
        "    print(\"aclant testing result (Full Data) \",file=text_file)\n",
        "    print(seed,file=text_file)\n",
        "    print(metrics.confusion_matrix(y_test_only_func_all, y_pred_only_func_all),file=text_file)\n",
        "    print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4),file=text_file)\n",
        "    print(\"=======================================================================\",file=text_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZstkYm2vEwMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seeds = [663,883,544,201,356,648,898,88,997,788]\n",
        "# seeds = [1,2]\n",
        "path = '/content/drive/My Drive/KY, FYP/Code/output.txt'\n",
        "filepath = 'model-scicite.h5'\n",
        "for seed in seeds:\n",
        "  text_path = '/content/drive/My Drive/KY, FYP/Code/results/output_'\n",
        "  text_path = text_path + str(seed) + \".txt\"\n",
        "  sciCite(seed, filepath,text_path)\n",
        "  aclarc(seed, filepath,text_path)\n",
        "  aclarcF(seed,filepath,text_path)\n",
        "  aclarcFull(seed,filepath,text_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKa760be4M3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seeds = [663,883,544,201,356,648,898,88,997,788]\n",
        "# seeds = [1,2]\n",
        "path = '/content/drive/My Drive/KY, FYP/Code/output.txt'\n",
        "filepath = 'model-scicite.h5'\n",
        "for seed in seeds:\n",
        "  text_path = '/content/drive/My Drive/KY, FYP/Code/results/output_'\n",
        "  text_path = text_path + str(seed) + \".txt\"\n",
        "  sciCite(seed, filepath,text_path)\n",
        "  aclant(seed, filepath,text_path)\n",
        "  aclantF(seed,filepath,text_path)\n",
        "  aclantFull(seed,filepath,text_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTGWAG7QJJdQ",
        "colab_type": "code",
        "outputId": "dbfee3c5-ab3f-499e-a528-ae45ccf6c857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "seeds = [663,883]\n",
        "# seeds = [1,2]\n",
        "path = '/content/drive/My Drive/KY, FYP/Code/output.txt'\n",
        "filepath = 'model-scicite.h5'\n",
        "for seed in seeds:\n",
        "  text_path = '/content/drive/My Drive/KY, FYP/Code/results/output_'\n",
        "  text_path = text_path + str(seed) + \".txt\"\n",
        "  sciCite(seed, filepath,text_path)\n",
        "  aclarc(seed, filepath,text_path)\n",
        "  aclarcF(seed,filepath,text_path)\n",
        "  aclarc10(seed,filepath,text_path)\n",
        "  aclarc20(seed,filepath,text_path)\n",
        "  aclarc50(seed,filepath,text_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Found 10104 texts.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 35643 unique tokens.\n",
            "Found 400000 word vectors.\n",
            "Train on 6466 samples, validate on 1617 samples\n",
            "Epoch 1/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.6993 - acc: 0.7072 - val_loss: 0.7244 - val_acc: 0.7211\n",
            "Epoch 2/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.4130 - acc: 0.8388 - val_loss: 0.6561 - val_acc: 0.7916\n",
            "Epoch 3/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.2843 - acc: 0.8907 - val_loss: 0.6950 - val_acc: 0.8002\n",
            "Epoch 4/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.1789 - acc: 0.9357 - val_loss: 1.1858 - val_acc: 0.5986\n",
            "Epoch 5/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.1049 - acc: 0.9641 - val_loss: 1.0192 - val_acc: 0.7842\n",
            "Epoch 6/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0631 - acc: 0.9816 - val_loss: 1.6010 - val_acc: 0.7434\n",
            "Epoch 7/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0479 - acc: 0.9852 - val_loss: 0.8742 - val_acc: 0.8169\n",
            "Epoch 8/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0379 - acc: 0.9899 - val_loss: 0.9051 - val_acc: 0.8077\n",
            "Epoch 9/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0323 - acc: 0.9918 - val_loss: 0.9833 - val_acc: 0.8077\n",
            "Epoch 10/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0395 - acc: 0.9904 - val_loss: 0.9945 - val_acc: 0.7860\n",
            "Epoch 11/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0238 - acc: 0.9949 - val_loss: 0.9875 - val_acc: 0.8169\n",
            "Epoch 12/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0257 - acc: 0.9943 - val_loss: 0.9468 - val_acc: 0.8132\n",
            "Epoch 13/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0190 - acc: 0.9958 - val_loss: 1.1124 - val_acc: 0.8139\n",
            "Epoch 14/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0123 - acc: 0.9972 - val_loss: 1.1650 - val_acc: 0.8089\n",
            "Epoch 15/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0182 - acc: 0.9954 - val_loss: 1.1440 - val_acc: 0.8151\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Found 1827 texts.\n",
            "Found 6073 unique tokens.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "(1461, 50) (366, 50) (1461, 6) (366, 6)\n",
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerA (Embedding) (None, 50, 100)           607400    \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 688,814\n",
            "Trainable params: 688,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Found 1827 texts.\n",
            "Found 6073 unique tokens.\n",
            "Found 400000 word vectors.\n",
            "0\n",
            "-----------------------\n",
            "267\n",
            "1\n",
            "-----------------------\n",
            "746\n",
            "2\n",
            "-----------------------\n",
            "62\n",
            "3\n",
            "-----------------------\n",
            "279\n",
            "4\n",
            "-----------------------\n",
            "52\n",
            "5\n",
            "-----------------------\n",
            "55\n",
            "Model: \"model_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerA (Embedding) (None, 50, 100)           607400    \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 688,814\n",
            "Trainable params: 688,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 24 samples, validate on 6 samples\n",
            "Epoch 1/15\n",
            "24/24 [==============================] - 0s 19ms/sample - loss: 2.5945 - acc: 0.0000e+00 - val_loss: 2.3408 - val_acc: 0.1667\n",
            "Epoch 2/15\n",
            "24/24 [==============================] - 0s 1ms/sample - loss: 1.4109 - acc: 0.3333 - val_loss: 2.3138 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "24/24 [==============================] - 0s 799us/sample - loss: 0.7919 - acc: 1.0000 - val_loss: 2.4651 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "24/24 [==============================] - 0s 839us/sample - loss: 0.4942 - acc: 1.0000 - val_loss: 2.6417 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "24/24 [==============================] - 0s 829us/sample - loss: 0.3369 - acc: 1.0000 - val_loss: 2.7657 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "24/24 [==============================] - 0s 915us/sample - loss: 0.2356 - acc: 1.0000 - val_loss: 2.8674 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "24/24 [==============================] - 0s 833us/sample - loss: 0.1723 - acc: 1.0000 - val_loss: 2.9434 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "24/24 [==============================] - 0s 860us/sample - loss: 0.1303 - acc: 1.0000 - val_loss: 3.0216 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "24/24 [==============================] - 0s 799us/sample - loss: 0.1020 - acc: 1.0000 - val_loss: 3.0745 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "24/24 [==============================] - 0s 892us/sample - loss: 0.0824 - acc: 1.0000 - val_loss: 3.1323 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "24/24 [==============================] - 0s 846us/sample - loss: 0.0677 - acc: 1.0000 - val_loss: 3.1718 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "24/24 [==============================] - 0s 842us/sample - loss: 0.0569 - acc: 1.0000 - val_loss: 3.2234 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "24/24 [==============================] - 0s 877us/sample - loss: 0.0484 - acc: 1.0000 - val_loss: 3.2589 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "24/24 [==============================] - 0s 961us/sample - loss: 0.0417 - acc: 1.0000 - val_loss: 3.3072 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "24/24 [==============================] - 0s 978us/sample - loss: 0.0362 - acc: 1.0000 - val_loss: 3.3451 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Found 1827 texts.\n",
            "Found 6073 unique tokens.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "0\n",
            "-----------------------\n",
            "267\n",
            "1\n",
            "-----------------------\n",
            "746\n",
            "2\n",
            "-----------------------\n",
            "62\n",
            "3\n",
            "-----------------------\n",
            "279\n",
            "4\n",
            "-----------------------\n",
            "52\n",
            "5\n",
            "-----------------------\n",
            "55\n",
            "Model: \"model_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerA (Embedding) (None, 50, 100)           607400    \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 688,814\n",
            "Trainable params: 688,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 48 samples, validate on 12 samples\n",
            "Epoch 1/15\n",
            "48/48 [==============================] - 1s 11ms/sample - loss: 1.9969 - acc: 0.1875 - val_loss: 3.1314 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "48/48 [==============================] - 0s 1ms/sample - loss: 1.0930 - acc: 0.6667 - val_loss: 3.2330 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "48/48 [==============================] - 0s 846us/sample - loss: 0.6411 - acc: 0.9167 - val_loss: 3.0509 - val_acc: 0.1667\n",
            "Epoch 4/15\n",
            "48/48 [==============================] - 0s 830us/sample - loss: 0.4069 - acc: 1.0000 - val_loss: 3.3038 - val_acc: 0.1667\n",
            "Epoch 5/15\n",
            "48/48 [==============================] - 0s 840us/sample - loss: 0.2611 - acc: 1.0000 - val_loss: 3.6240 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "48/48 [==============================] - 0s 882us/sample - loss: 0.1901 - acc: 1.0000 - val_loss: 3.7299 - val_acc: 0.0833\n",
            "Epoch 7/15\n",
            "48/48 [==============================] - 0s 816us/sample - loss: 0.1413 - acc: 1.0000 - val_loss: 3.7856 - val_acc: 0.0833\n",
            "Epoch 8/15\n",
            "48/48 [==============================] - 0s 957us/sample - loss: 0.1030 - acc: 1.0000 - val_loss: 3.8688 - val_acc: 0.0833\n",
            "Epoch 9/15\n",
            "48/48 [==============================] - 0s 844us/sample - loss: 0.0764 - acc: 1.0000 - val_loss: 4.0314 - val_acc: 0.0833\n",
            "Epoch 10/15\n",
            "48/48 [==============================] - 0s 919us/sample - loss: 0.0585 - acc: 1.0000 - val_loss: 4.1178 - val_acc: 0.0833\n",
            "Epoch 11/15\n",
            "48/48 [==============================] - 0s 840us/sample - loss: 0.0475 - acc: 1.0000 - val_loss: 4.2091 - val_acc: 0.0833\n",
            "Epoch 12/15\n",
            "48/48 [==============================] - 0s 862us/sample - loss: 0.0379 - acc: 1.0000 - val_loss: 4.2647 - val_acc: 0.0833\n",
            "Epoch 13/15\n",
            "48/48 [==============================] - 0s 822us/sample - loss: 0.0308 - acc: 1.0000 - val_loss: 4.3497 - val_acc: 0.0833\n",
            "Epoch 14/15\n",
            "48/48 [==============================] - 0s 854us/sample - loss: 0.0254 - acc: 1.0000 - val_loss: 4.4208 - val_acc: 0.0833\n",
            "Epoch 15/15\n",
            "48/48 [==============================] - 0s 984us/sample - loss: 0.0210 - acc: 1.0000 - val_loss: 4.5061 - val_acc: 0.0833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Found 1827 texts.\n",
            "Found 6073 unique tokens.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "0\n",
            "-----------------------\n",
            "267\n",
            "1\n",
            "-----------------------\n",
            "746\n",
            "2\n",
            "-----------------------\n",
            "62\n",
            "3\n",
            "-----------------------\n",
            "279\n",
            "4\n",
            "-----------------------\n",
            "52\n",
            "5\n",
            "-----------------------\n",
            "55\n",
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerA (Embedding) (None, 50, 100)           607400    \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 688,814\n",
            "Trainable params: 688,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 96 samples, validate on 24 samples\n",
            "Epoch 1/15\n",
            "96/96 [==============================] - 1s 10ms/sample - loss: 1.9809 - acc: 0.1875 - val_loss: 3.1134 - val_acc: 0.1667\n",
            "Epoch 2/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 1.3017 - acc: 0.4271 - val_loss: 3.6285 - val_acc: 0.0417\n",
            "Epoch 3/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.8818 - acc: 0.8438 - val_loss: 3.7976 - val_acc: 0.0833\n",
            "Epoch 4/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.6594 - acc: 0.9688 - val_loss: 4.1404 - val_acc: 0.0417\n",
            "Epoch 5/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.4792 - acc: 0.9792 - val_loss: 4.1408 - val_acc: 0.0417\n",
            "Epoch 6/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.3296 - acc: 1.0000 - val_loss: 4.4060 - val_acc: 0.0417\n",
            "Epoch 7/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.2361 - acc: 1.0000 - val_loss: 4.6144 - val_acc: 0.0417\n",
            "Epoch 8/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.1946 - acc: 1.0000 - val_loss: 4.8954 - val_acc: 0.0417\n",
            "Epoch 9/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.1342 - acc: 1.0000 - val_loss: 4.7786 - val_acc: 0.0417\n",
            "Epoch 10/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.0951 - acc: 1.0000 - val_loss: 5.0323 - val_acc: 0.0417\n",
            "Epoch 11/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.0694 - acc: 1.0000 - val_loss: 5.0558 - val_acc: 0.0417\n",
            "Epoch 12/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.0536 - acc: 1.0000 - val_loss: 5.2666 - val_acc: 0.0417\n",
            "Epoch 13/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.0417 - acc: 1.0000 - val_loss: 5.2932 - val_acc: 0.0417\n",
            "Epoch 14/15\n",
            "96/96 [==============================] - 0s 1ms/sample - loss: 0.0322 - acc: 1.0000 - val_loss: 5.3195 - val_acc: 0.0417\n",
            "Epoch 15/15\n",
            "96/96 [==============================] - 0s 826us/sample - loss: 0.0243 - acc: 1.0000 - val_loss: 5.4450 - val_acc: 0.0417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Found 1827 texts.\n",
            "Found 6073 unique tokens.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "0\n",
            "-----------------------\n",
            "267\n",
            "1\n",
            "-----------------------\n",
            "746\n",
            "2\n",
            "-----------------------\n",
            "62\n",
            "3\n",
            "-----------------------\n",
            "279\n",
            "4\n",
            "-----------------------\n",
            "52\n",
            "5\n",
            "-----------------------\n",
            "55\n",
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerA (Embedding) (None, 50, 100)           607400    \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 688,814\n",
            "Trainable params: 688,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 240 samples, validate on 60 samples\n",
            "Epoch 1/15\n",
            "240/240 [==============================] - 1s 3ms/sample - loss: 1.7596 - acc: 0.2250 - val_loss: 3.5160 - val_acc: 0.0833\n",
            "Epoch 2/15\n",
            "240/240 [==============================] - 0s 775us/sample - loss: 1.1227 - acc: 0.7000 - val_loss: 4.3423 - val_acc: 0.0833\n",
            "Epoch 3/15\n",
            "240/240 [==============================] - 0s 703us/sample - loss: 0.8199 - acc: 0.8583 - val_loss: 4.9823 - val_acc: 0.0833\n",
            "Epoch 4/15\n",
            "240/240 [==============================] - 0s 705us/sample - loss: 0.5827 - acc: 0.9375 - val_loss: 5.3096 - val_acc: 0.0833\n",
            "Epoch 5/15\n",
            "240/240 [==============================] - 0s 679us/sample - loss: 0.3949 - acc: 0.9625 - val_loss: 5.7556 - val_acc: 0.1000\n",
            "Epoch 6/15\n",
            "240/240 [==============================] - 0s 693us/sample - loss: 0.2358 - acc: 0.9917 - val_loss: 6.0712 - val_acc: 0.0833\n",
            "Epoch 7/15\n",
            "240/240 [==============================] - 0s 684us/sample - loss: 0.1302 - acc: 1.0000 - val_loss: 6.6068 - val_acc: 0.1000\n",
            "Epoch 8/15\n",
            "240/240 [==============================] - 0s 757us/sample - loss: 0.0903 - acc: 1.0000 - val_loss: 6.7174 - val_acc: 0.0833\n",
            "Epoch 9/15\n",
            "240/240 [==============================] - 0s 695us/sample - loss: 0.0468 - acc: 1.0000 - val_loss: 7.0561 - val_acc: 0.1000\n",
            "Epoch 10/15\n",
            "240/240 [==============================] - 0s 710us/sample - loss: 0.0277 - acc: 1.0000 - val_loss: 7.7029 - val_acc: 0.0833\n",
            "Epoch 11/15\n",
            "240/240 [==============================] - 0s 673us/sample - loss: 0.0184 - acc: 1.0000 - val_loss: 7.6900 - val_acc: 0.0833\n",
            "Epoch 12/15\n",
            "240/240 [==============================] - 0s 706us/sample - loss: 0.0094 - acc: 1.0000 - val_loss: 8.0526 - val_acc: 0.0833\n",
            "Epoch 13/15\n",
            "240/240 [==============================] - 0s 694us/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 8.2158 - val_acc: 0.0833\n",
            "Epoch 14/15\n",
            "240/240 [==============================] - 0s 702us/sample - loss: 0.0037 - acc: 1.0000 - val_loss: 8.4022 - val_acc: 0.0833\n",
            "Epoch 15/15\n",
            "240/240 [==============================] - 0s 686us/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 8.7476 - val_acc: 0.0833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Found 10104 texts.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 35643 unique tokens.\n",
            "Found 400000 word vectors.\n",
            "Train on 6466 samples, validate on 1617 samples\n",
            "Epoch 1/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.6945 - acc: 0.7113 - val_loss: 0.5092 - val_acc: 0.7916\n",
            "Epoch 2/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.4302 - acc: 0.8317 - val_loss: 0.6066 - val_acc: 0.7619\n",
            "Epoch 3/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.2976 - acc: 0.8893 - val_loss: 0.7193 - val_acc: 0.7440\n",
            "Epoch 4/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.1839 - acc: 0.9338 - val_loss: 0.5767 - val_acc: 0.8058\n",
            "Epoch 5/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.1063 - acc: 0.9635 - val_loss: 0.6770 - val_acc: 0.8163\n",
            "Epoch 6/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0653 - acc: 0.9783 - val_loss: 0.7294 - val_acc: 0.8108\n",
            "Epoch 7/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0432 - acc: 0.9862 - val_loss: 1.0607 - val_acc: 0.7458\n",
            "Epoch 8/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0316 - acc: 0.9899 - val_loss: 0.9292 - val_acc: 0.8114\n",
            "Epoch 9/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0392 - acc: 0.9896 - val_loss: 0.9712 - val_acc: 0.8009\n",
            "Epoch 10/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0286 - acc: 0.9929 - val_loss: 0.9107 - val_acc: 0.8064\n",
            "Epoch 11/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0237 - acc: 0.9941 - val_loss: 1.3182 - val_acc: 0.7551\n",
            "Epoch 12/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0276 - acc: 0.9940 - val_loss: 1.1374 - val_acc: 0.7866\n",
            "Epoch 13/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0185 - acc: 0.9938 - val_loss: 1.1813 - val_acc: 0.8015\n",
            "Epoch 14/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0088 - acc: 0.9977 - val_loss: 1.2390 - val_acc: 0.8095\n",
            "Epoch 15/15\n",
            "6466/6466 [==============================] - 7s 1ms/sample - loss: 0.0092 - acc: 0.9978 - val_loss: 1.2682 - val_acc: 0.8040\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Found 1827 texts.\n",
            "Found 6073 unique tokens.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "(1461, 50) (366, 50) (1461, 6) (366, 6)\n",
            "Model: \"model_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerA (Embedding) (None, 50, 100)           607400    \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 688,814\n",
            "Trainable params: 688,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Found 1827 texts.\n",
            "Found 6073 unique tokens.\n",
            "Found 400000 word vectors.\n",
            "0\n",
            "-----------------------\n",
            "262\n",
            "1\n",
            "-----------------------\n",
            "749\n",
            "2\n",
            "-----------------------\n",
            "70\n",
            "3\n",
            "-----------------------\n",
            "269\n",
            "4\n",
            "-----------------------\n",
            "56\n",
            "5\n",
            "-----------------------\n",
            "55\n",
            "Model: \"model_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence_input (InputLayer)  [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_layerA (Embedding) (None, 50, 100)           607400    \n",
            "_________________________________________________________________\n",
            "convolution_layer (Conv1D)   (None, 46, 128)           64128     \n",
            "_________________________________________________________________\n",
            "pooling_layer (GlobalMaxPool (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "filter_layer (Dense)         (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 688,814\n",
            "Trainable params: 688,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 24 samples, validate on 6 samples\n",
            "Epoch 1/15\n",
            "24/24 [==============================] - 1s 29ms/sample - loss: 2.0653 - acc: 0.1250 - val_loss: 4.5495 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "24/24 [==============================] - 0s 988us/sample - loss: 1.0272 - acc: 0.7917 - val_loss: 4.2101 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "24/24 [==============================] - 0s 978us/sample - loss: 0.5944 - acc: 0.9583 - val_loss: 4.7656 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "24/24 [==============================] - 0s 880us/sample - loss: 0.3676 - acc: 1.0000 - val_loss: 4.4222 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "24/24 [==============================] - 0s 856us/sample - loss: 0.2228 - acc: 1.0000 - val_loss: 4.7345 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "24/24 [==============================] - 0s 834us/sample - loss: 0.1517 - acc: 1.0000 - val_loss: 4.8062 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "24/24 [==============================] - 0s 892us/sample - loss: 0.1165 - acc: 1.0000 - val_loss: 4.8989 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "24/24 [==============================] - 0s 886us/sample - loss: 0.0938 - acc: 1.0000 - val_loss: 4.9773 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "24/24 [==============================] - 0s 993us/sample - loss: 0.0770 - acc: 1.0000 - val_loss: 5.0442 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "24/24 [==============================] - 0s 879us/sample - loss: 0.0644 - acc: 1.0000 - val_loss: 5.1054 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "24/24 [==============================] - 0s 976us/sample - loss: 0.0547 - acc: 1.0000 - val_loss: 5.1645 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "24/24 [==============================] - 0s 980us/sample - loss: 0.0470 - acc: 1.0000 - val_loss: 5.2008 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "24/24 [==============================] - 0s 1ms/sample - loss: 0.0408 - acc: 1.0000 - val_loss: 5.2553 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "24/24 [==============================] - 0s 940us/sample - loss: 0.0358 - acc: 1.0000 - val_loss: 5.2934 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "24/24 [==============================] - 0s 982us/sample - loss: 0.0315 - acc: 1.0000 - val_loss: 5.3317 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Found 1827 texts.\n",
            "Found 6073 unique tokens.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM7jMgw44QFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aclantF(1,'abc','a.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcSoS-izTpjV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Combination (Aclarc + Scicite)\n",
        "\n",
        "# Integrated Cosine Sim into the model\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "# import lib.logger, os, sys, random, math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "# import config.config as config\n",
        "# import data.data as data\n",
        "# import data.data_func as data_func\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.utils.random import sample_without_replacement\n",
        "import pandas as pd\n",
        "\n",
        "# from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# import keras.backend as K\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, \\\n",
        "    GlobalMaxPooling1D, Embedding, Dropout, Masking, Input, Reshape\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.losses import cosine_proximity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "import random\n",
        "\"\"\"\n",
        "Set random seed and fix bug on Dropout usage.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def embed_sentence(sentence):\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    message_embeddings = session.run(embed(sentence))\n",
        "    return message_embeddings\n",
        "\n",
        "def ilen(iterable):\n",
        "    return reduce(lambda sum, element: sum + 1, iterable, 0)\n",
        "\n",
        "\n",
        "def build_knn(model, output_size):\n",
        "    # Flatten feature vector\n",
        "    flat_dim_size = np.prod(model.output_shape[1:])\n",
        "    x = Reshape(target_shape=(flat_dim_size,),\n",
        "                name='features_flat')(model.output)\n",
        "\n",
        "    # Dot product between feature vector and reference vectors\n",
        "    x = Dense(units=output_size,\n",
        "              activation='linear',\n",
        "              use_bias=False)(x)\n",
        "\n",
        "    classifier = Model(inputs=[model.input], outputs=x)\n",
        "    return classifier\n",
        "\n",
        "def normalize_encodings(encodings):\n",
        "    ref_norms = np.linalg.norm(encodings, axis=0)\n",
        "    return encodings / ref_norms\n",
        "\n",
        "seed = 1020\n",
        "np.random.seed(seed)\n",
        "# tf.python.control_flow_ops = tf\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\"\"\"\n",
        "Data reading and saving from disk (so that data processing is done only once).\n",
        "\"\"\"\n",
        "directory = DATA_DIR\n",
        "funcs_index = {'CompareOrContrast': 0, 'Background': 1, 'Motivation': 2, 'Uses': 3, 'Future': 4,\n",
        "               'Extends': 5, 'background': 6, 'method': 7, 'result': 8}\n",
        "\n",
        "\n",
        "# Function dataset start\n",
        "datafiles = DATA_FILES['acl-arc']\n",
        "test = read_jsonl_data(datafiles['test'])\n",
        "train = read_jsonl_data((datafiles['train']))\n",
        "\n",
        "s_datafiles = DATA_FILES['scicite']\n",
        "s_test = read_jsonl_data(s_datafiles['test'])\n",
        "s_train = read_jsonl_data(s_datafiles['train'])\n",
        "\n",
        "s_dataset_train = list(filter(lambda x: x['label'] != 'Error',s_train))\n",
        "s_dataset_test = list(filter(lambda x: x['label'] != 'Error', s_test))\n",
        "\n",
        "texts_train = list(map(lambda d: d['string'], s_train))\n",
        "\n",
        "texts_test = list(map(lambda d: d['string'], s_test))\n",
        "\n",
        "s_texts = texts_train + texts_test\n",
        "\n",
        "y_train = list(map(lambda d: funcs_index[d['label']], s_dataset_train))\n",
        "y_test = list(map(lambda d: funcs_index[d['label']], s_dataset_test))\n",
        "\n",
        "s_ys = y_train + y_test\n",
        "\n",
        "temp_list0 = []\n",
        "temp_list1 = []\n",
        "r_indices = sample_without_replacement(len(s_test + s_train),len(test + train),random_state=42)\n",
        "for index in r_indices:\n",
        "  temp_list0.append(s_texts[index])\n",
        "  temp_list1.append(s_ys[index])\n",
        "s_texts = temp_list0\n",
        "s_ys = temp_list1\n",
        "\n",
        "dataset = list(filter(lambda x: x['intent'] != 'Error', test + train))\n",
        "dataset_func = list(filter(lambda d: d['intent'] != 'Error', test + train))\n",
        "\n",
        "random.shuffle(dataset_func)\n",
        "\n",
        "\n",
        "# embed = hub.Module(\"/content/drive/My Drive/KY, FYP/Code/USE\")\n",
        "# print(\"loaded Hub Module\")\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "  \n",
        "s_transformer = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "words = {}\n",
        "\n",
        "# ## Raw Description\n",
        "# words[0] = [\"P provides relevant information for this domain.\"]\n",
        "# words[1] = [\"P illustrates need for data, goals,methods, etc.\"]\n",
        "# words[2] = [\"Uses data, methods, etc., from P\"]\n",
        "# words[3] = [\"Extends P’s data, methods, etc. \"]\n",
        "# words[4] = [\"Expresses similarity/differences to P.\"]\n",
        "# words[5] = [\"P is a potential avenue for future work.\"]\n",
        "\n",
        "# Summarized\n",
        "words[0] = [\"provides relevant information\"]\n",
        "words[1] = [\"Illustrates need\"]\n",
        "words[2] = [\"Uses\"]\n",
        "words[3] = [\"Extends\"]\n",
        "words[4] = [\"similarity differences\"]\n",
        "words[5] = [\"Potential Future\"]\n",
        "## Mod Description ##\n",
        "words[6] = [\"states background\"]\n",
        "words[7] = [\"making use method approach\"]\n",
        "words[8] = [\"Comparison results findings\"]\n",
        "\n",
        "\n",
        "# Class Title\n",
        "# words[0] = [\"Background\"]\n",
        "# words[1] = [\"Motivation\"]\n",
        "# words[2] = [\"Uses\"]\n",
        "# words[3] = [\"Extension\"]\n",
        "# words[4] = [\"Compare Or Contrast\"]\n",
        "# words[5] = [\"Future\"]\n",
        "\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # words[i] = embed_sentence(words[i])\n",
        "    words[i] = np.array(s_transformer.encode(words[i]))\n",
        "\n",
        "# Function dataset end\n",
        "#############################################################################3\n",
        "\n",
        "texts = list(map(lambda d: d['text'], dataset_func))\n",
        "texts = texts + s_texts\n",
        "\n",
        "ys = list(map(lambda d: funcs_index[d['intent']], dataset_func))\n",
        "ys = ys + s_ys\n",
        "\n",
        "for i,element in enumerate(ys):\n",
        "    ys[i] = words[ys[i]]\n",
        "\n",
        "print('Found %s texts.' % len(texts))\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "y_pred_func_all = []\n",
        "y_test_func_all = []\n",
        "y_pred_prov_all = []\n",
        "y_test_prov_all = []\n",
        "y_pred_only_func_all = []\n",
        "y_test_only_func_all = []\n",
        "y_pred_only_prov_all = []\n",
        "y_test_only_prov_all = []\n",
        "y_pred_func = []\n",
        "y_test_func = []\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"UTF-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# -------------------------\n",
        "# texts = map(lambda d: d['text'], dataset_func)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "xs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "ys = np.asarray(ys)\n",
        "\n",
        "batch_num = 0\n",
        "average_list = {}\n",
        "encoded_classes = words[0]\n",
        "for i in range(len(words)-1):\n",
        "    encoded_classes = np.concatenate((encoded_classes,words[i+1]))\n",
        "encoded_classes = encoded_classes.transpose()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=42)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "NB_FILTER = 128\n",
        "BATCH_SIZE = 32\n",
        "count = 0\n",
        "EPOCH = 15 # 20\n",
        "indices = []\n",
        "indices_type = []\n",
        "\n",
        "# ---------- Only citation function ----------\n",
        "\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            name=\"embedding_layerA\")\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',\n",
        "                        name=\"sequence_input\")\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Convolution1D(filters=NB_FILTER,\n",
        "                kernel_size=5,\n",
        "                padding='valid',\n",
        "                activation='relu',\n",
        "                name='convolution_layer')(embedded_sequences)\n",
        "\n",
        "x = GlobalMaxPooling1D(name=\"pooling_layer\")(x)\n",
        "x = Dense(NB_FILTER, activation='relu',name=\"filter_layer\")(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "# preds = Dense(len(funcs_index), activation='softmax')(x)\n",
        "preds = Dense(768, name=\"output_layer\")(x)\n",
        "output_reshape = Reshape((1,768),name=\"reshape_layer\")(preds)\n",
        "\n",
        "model = Model(sequence_input, output_reshape)\n",
        "\n",
        "model.compile(loss=cosine_proximity,\n",
        "            # optimizer='adam',\n",
        "            optimizer='rmsprop',\n",
        "            metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# import datetime\n",
        "# from keras.callbacks import TensorBoard\n",
        "\n",
        "# log_dir = \"logs/few_shot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "# model.load_weights('model-scicite.h5', by_name=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        nb_epoch=EPOCH, batch_size=BATCH_SIZE,validation_split=0.2)\n",
        "\n",
        "model.save_weights('model-aclarc_scicite.h5')\n",
        "\n",
        "new_model = build_knn(model, encoded_classes.shape[1])\n",
        "print(new_model.summary())\n",
        "encoded_classes_norm = normalize_encodings(encoded_classes)\n",
        "temp_weights = new_model.get_weights()\n",
        "temp_weights[-1] = encoded_classes_norm\n",
        "new_model.set_weights(temp_weights)\n",
        "\n",
        "y_pred_probs = new_model.predict(x_test)\n",
        "\n",
        "y_pred_func = []\n",
        "\n",
        "y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "y_test_list = []\n",
        "sim = {}\n",
        "\n",
        "for i, sample in enumerate(y_pred_probs):\n",
        "    for j in range(len(funcs_index)):\n",
        "        # sim[j] = cosine_similarity(y_pred_probs[i], words[j])\n",
        "        if np.array_equal(y_test[i], words[j]):\n",
        "            y_test_list.append(j)\n",
        "    # greatest_sim = max(sim, key=sim.get)\n",
        "\n",
        "    # y_pred_func.append(greatest_sim)\n",
        "\n",
        "y_test = y_test_list\n",
        "\n",
        "\n",
        "# y_pred_func = list(map(lambda x: pd.Series(x).idxmax(), y_pred_probs))\n",
        "\n",
        "# new_y_pred = [1] * len(y_pred_func)\n",
        "    # Generate classificat\n",
        "# y_pred_func = new_y_pred\n",
        "# y_test = data.compress_y(y_test)\n",
        "\n",
        "        #print('y_pred_func_A')\n",
        "        #print(y_pred_func)\n",
        "\n",
        "y_pred_only_func_all += y_pred_func\n",
        "y_test_only_func_all += y_test\n",
        "\n",
        "        # ---------- End of citation function ----------\n",
        "\n",
        "print('Plain_Func')\n",
        "# print(average_list)\n",
        "print(metrics.classification_report(y_test_only_func_all, y_pred_only_func_all, digits=4))\n",
        "print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll8Od_QDJr9c",
        "colab_type": "code",
        "outputId": "da435dc3-2a84-4814-bee4-32672dde2dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(s_texts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1827\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}